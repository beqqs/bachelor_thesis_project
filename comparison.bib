@article{Abdmeziem_2015,
 abstract = {null},
 author = {Mohammed Riyadh Abdmeziem and Mohammed Riyadh Abdmeziem and Djamel Tandjaoui and Djamel Tandjaoui},
 doi = {10.1016/j.compeleceng.2015.03.030},
 journal = {Computers & Electrical Engineering},
 mag_id = {2092160570},
 pmcid = {null},
 pmid = {null},
 title = {An end-to-end secure key management protocol for e-health applications},
 year = {2015}
}

@article{Abrahão_2011,
 abstract = {Numerous methods and techniques have been proposed for requirements modeling, although very few have had widespread use in practice. One drawback of requirements modeling methods is that they lack proper empirical evaluations. This means that there is a need for evaluation methods that consider both the theoretical and practical aspects of this type of methods and techniques. In this paper, we present a method for evaluating the quality of requirements modeling methods based on user perceptions. The evaluation method consists of a theoretical model that explains the relevant dimensions of quality for requirements modeling methods, along with a practical instrument with which to measure these quality dimensions. Basically, it allows us to predict the acceptance of a particular requirements modeling method in practice, based on the effort of applying the method, the quality of the requirements artifacts produced, and the user perceptions with regard to the quality of the method. The paper also presents an empirical test of the proposed method for evaluating a Rational Unified Process (RUP) extension for requirements modeling. That test was carried out through a family of experiments conducted with students and practitioners and provides evidence of the usefulness of the evaluation method proposed.},
 author = {Silvia Abrahão and Silvia Abrahão and Emilio Insfrán and Emilio Insfran and Emilio Insfran and Emilio Insfran and José Á. Carsí and José A. Carsí and Marcela Genero and Marcela Genero},
 doi = {10.1016/j.ins.2011.04.005},
 journal = {Information Sciences},
 mag_id = {2087898864},
 pmcid = {null},
 pmid = {null},
 title = {Evaluating requirements modeling methods based on user perceptions: A family of experiments},
 year = {2011}
}

@article{Abrahão_2013,
 abstract = {Modeling is a fundamental activity within the requirements engineering process and concerns the construction of abstract descriptions of requirements that are amenable to interpretation and validation. The choice of a modeling technique is critical whenever it is necessary to discuss the interpretation and validation of requirements. This is particularly true in the case of functional requirements and stakeholders with divergent goals and different backgrounds and experience. This paper presents the results of a family of experiments conducted with students and professionals to investigate whether the comprehension of functional requirements is influenced by the use of dynamic models that are represented by means of the UML sequence diagrams. The family contains five experiments performed in different locations and with 112 participants of different abilities and levels of experience with UML. The results show that sequence diagrams improve the comprehension of the modeled functional requirements in the case of high ability and more experienced participants.},
 author = {Silvia Abrahão and Silvia Abrahão and Carmine Gravino and Carmine Gravino and Emilio Insfrán and Emilio Insfran and Emilio Insfran and Emilio Insfran and Giuseppe Scanniello and Giuseppe Scanniello and Genoveffa Tortora and Genoveffa Tortora},
 doi = {10.1109/tse.2012.27},
 journal = {IEEE Transactions on Software Engineering},
 mag_id = {2007969951},
 pmcid = {null},
 pmid = {null},
 title = {Assessing the Effectiveness of Sequence Diagrams in the Comprehension of Functional Requirements: Results from a Family of Five Experiments},
 year = {2013}
}

@article{Abrial_1988,
 abstract = {null},
 author = {Jean-Raymond Abrial and Jean-Raymond Abrial},
 doi = {10.1007/3-540-50214-9_8},
 journal = {null},
 mag_id = {1543490761},
 pmcid = {null},
 pmid = {null},
 title = {The B Tool (Abstract)},
 year = {1988}
}

@article{Abrial_2010,
 abstract = {A practical text suitable for an introductory or advanced course in formal methods, this book presents a mathematical approach to modelling and designing systems using an extension of the B formal method: Event-B. Based on the idea of refinement, the author's systematic approach allows the user to construct models gradually and to facilitate a systematic reasoning method by means of proofs. Readers will learn how to build models of programs and, more generally, discrete systems, but this is all done with practice in mind. The numerous examples provided arise from various sources of computer system developments, including sequential programs, concurrent programs and electronic circuits. The book also contains a large number of exercises and projects ranging in difficulty. Each of the examples included in the book has been proved using the Rodin Platform tool set, which is available free for download at www.event-b.org.},
 author = {Jean-Raymond Abrial and Jean-Raymond Abrial},
 doi = {null},
 journal = {null},
 mag_id = {1556900989},
 pmcid = {null},
 pmid = {null},
 title = {Modeling in Event-B: System and Software Engineering},
 year = {2010}
}

@article{Abrial_2010,
 abstract = {Event-B is a formal method for system-level modelling and analysis. Key features of Event-B are the use of set theory as a modelling notation, the use of refinement to represent systems at different abstraction levels and the use of mathematical proof to verify consistency between refinement levels. In this article we present the Rodin modelling tool that seamlessly integrates modelling and proving. We outline how the Event-B language was designed to facilitate proof and how the tool has been designed to support changes to models while minimising the impact of changes on existing proofs. We outline the important features of the prover architecture and explain how well-definedness is treated. The tool is extensible and configurable so that it can be adapted more easily to different application domains and development methods.},
 author = {Jean-Raymond Abrial and Jean-Raymond Abrial and Michael Butler and Michael Butler and Stefan Hallerstede and Stefan Hallerstede and Thai Son Hoang and Thai Son Hoang and Farhad Mehta and Farhad Mehta and Laurent Voisin and Laurent Voisin},
 doi = {10.1007/s10009-010-0145-y},
 journal = {null},
 mag_id = {1971486022},
 pmcid = {null},
 pmid = {null},
 title = {Rodin: an open toolset for modelling and reasoning in Event-B},
 year = {2010}
}

@article{Abrial_null,
 abstract = {A practical text suitable for an introductory or advanced course in formal methods, this book presents a mathematical approach to modelling and designing systems using an extension of the B formal method: Event-B. Based on the idea of refinement, the author's systematic approach allows the user to construct models gradually and to facilitate a systematic reasoning method by means of proofs. Readers will learn how to build models of programs and, more generally, discrete systems, but this is all done with practice in mind. The numerous examples provided arise from various sources of computer system developments, including sequential programs, concurrent programs and electronic circuits. The book also contains a large number of exercises and projects ranging in difficulty. Each of the examples included in the book has been proved using the Rodin Platform tool set, which is available free for download at www.event-b.org.},
 author = {Jean-Raymond Abrial and Jean-Raymond Abrial},
 doi = {10.1017/cbo9781139195881},
 journal = {null},
 mag_id = {4244819751},
 pmcid = {null},
 pmid = {null},
 title = {Modeling in Event-B},
 year = {null}
}

@article{Addas_2014,
 abstract = {The introduction of e-Health applications has not only brought benefits, but also raised serious concerns regarding security and privacy of health data. The increasing demands of accessing health data, highlighted critical questions and challenges concerning the confidentiality of electronic patient records and the efficiency of accessing these records. Therefore, the aim of this paper is to provide secure and efficient access to electronic patient records. In this paper, we propose a novel protocol called the Linkable Anonymous Access protocol (LAA). We formally verify and analyse the protocol against security properties such as secrecy and authentication using the Casper/FDR2 verification tool. In addition, we have implemented the protocol using the Java technology to evaluate its performance. Our formal security analysis and performance evaluation proved that the LAA protocol supports secure access to electronic patient records without compromising performance.},
 author = {Rima Addas and Rima Addas and Ning Zhang and Ning Zhang},
 doi = {10.1007/978-3-642-55032-4_51},
 journal = {null},
 mag_id = {1855377449},
 pmcid = {null},
 pmid = {null},
 title = {Formal Security Analysis and Performance Evaluation of the Linkable Anonymous Access Protocol},
 year = {2014}
}

@article{Agha_2018,
 abstract = {Interactive, distributed, and embedded systems often behave stochastically, for example, when inputs, message delays, or failures conform to a probability distribution. However, reasoning analytically about the behavior of complex stochastic systems is generally infeasible. While simulations of systems are commonly used in engineering practice, they have not traditionally been used to reason about formal specifications. Statistical model checking (SMC) addresses this weakness by using a simulation-based approach to reason about precise properties specified in a stochastic temporal logic. A specification for a communication system may state that within some time bound, the probability that the number of messages in a queue will be greater than 5 must be less than 0.01. Using SMC, executions of a stochastic system are first sampled, after which statistical techniques are applied to determine whether such a property holds. While the output of sample-based methods are not always correct, statistical inference can quantify the confidence in the result produced. In effect, SMC provides a more widely applicable and scalable alternative to analysis of properties of stochastic systems using numerical and symbolic methods. SMC techniques have been successfully applied to analyze systems with large state spaces in areas such as computer networking, security, and systems biology. In this article, we survey SMC algorithms, techniques, and tools, while emphasizing current limitations and tradeoffs between precision and scalability.},
 author = {Gul Agha and Gul Agha and Karl Palmskog and Karl Palmskog},
 doi = {10.1145/3158668},
 journal = {ACM Transactions on Modeling and Computer Simulation},
 mag_id = {2787596386},
 pmcid = {null},
 pmid = {null},
 title = {A Survey of Statistical Model Checking},
 year = {2018}
}

@article{Ahouandjinou_2015,
 abstract = {In the context of human action recognition from video sequences in the medical environment, a Temporal Belief-based Hidden Markov Model (HMM) is presented. It allows to cope with human action temporality and enables to manage the data uncertainty and the knowledge incompleteness. The system of activity recognition is based on an HMM with explicit state duration. The global interpretation process uses the framework of the Transferable Belief Model (TBM). It enable us to model and manage the uncertainty over the video interpretation process. An application is proposed for human action analysis in medical video sequences provided by a patient monitoring system in the cardiology section in hospital. The proposed recognition method has been assessed on a database of 3000 video images of medical scenes and compared to the performance of the probabilistic Hidden Markov Models.},
 author = {Arnaud Ahouandjinou and Arnaud Ahouandjinou and Cina Motamed and Cina Motamed and Eugène C. Ezin and Eugène C. Ezin},
 doi = {10.1134/s1054661815030025},
 journal = {Pattern Recognition and Image Analysis},
 mag_id = {2177293271},
 pmcid = {null},
 pmid = {null},
 title = {A temporal belief-based hidden markov model for human action recognition in medical videos},
 year = {2015}
}

@article{Alkhammash_2015,
 abstract = {Bridging the gap between informal requirements and formal specifications is a key challenge in systems engineering. Constructing appropriate abstractions in formal models requires skill and managing the complexity of the relationships between requirements and formal models can be difficult. In this paper we present an approach that aims to address the twin challenges of finding appropriate abstractions and managing traceability between requirements and models. Our approach is based on the use of semi-formal structures to bridge the gap between requirements and Event-B models and retain traceability to requirements in Event-B models. In the stepwise refinement approach, design details are gradually introduced into formal models. Stepwise refinement allows each requirement to be introduced at the most appropriate stage in the development. Our approach makes use of the UML-B and Event Refinement Structures (ERS) approaches. UML-B provides UML graphical notation that enables the development of data structures for Event-B models, while the ERS approach provides a graphical notation to illustrate event refinement structures and assists in the organisation of refinement levels. The ERS approach also combines several constructor patterns to manage control flows in Event-B. The intent of this paper is to harness the benefits of the UML-B and ERS approaches to facilitate constructing Event-B models from requirements and provide traceability between requirements and Event-B models. We present an approach for incrementally constructing a formal model from informal requirements.The approach aims to retaining traceability to requirements in models.We have chosen Event-B to develop the case studies because Event-B is a stepwise formal method which has a platform with various plugins called Rodin.UML-B is used to enable the development of an Event-B formal model, ERS is used to structure refinements.To sum up, our approach provides traceability between requirements and the Event-B model and help to construct the Event-B models from requirements.},
 author = {Eman H. Alkhammash and Eman Alkhammash and Michael Butler and Michael Butler and Asieh Salehi Fathabadi and Asieh Salehi Fathabadi and Corina Ĉırstea and Corina Cîrstea},
 doi = {10.1016/j.scico.2015.06.002},
 journal = {Science of Computer Programming},
 mag_id = {2095991773},
 pmcid = {null},
 pmid = {null},
 title = {Building traceable Event-B models from requirements},
 year = {2015}
}

@article{Almeida_2011,
 abstract = {The goal of this chapter is to give an overview of the different approaches and tools pertaining to formal methods. We do not attempt to be exhaustive, but focus instead on the main approaches (formal specification, formal verification and proofs, transformation, and formal development). A consise introduction to basic logic concepts and methods is also provided. After reading the chapter the reader will be familiar with the terminology of the area, as well as with the most important concepts and techniques.},
 author = {José Bacelar Almeida and José B. Almeida and Maria João Frade and Maria João Frade and Jorge Sousa Pinto and Jorge Sousa Pinto and Simão Melo de Sousa and Simão Melo de Sousa},
 doi = {10.1007/978-0-85729-018-2_2},
 journal = {null},
 mag_id = {1950768068},
 pmcid = {null},
 pmid = {null},
 title = {An Overview of Formal Methods Tools and Techniques},
 year = {2011}
}

@article{Alonso_2011,
 abstract = {In the last years crowdsourcing has emerged as a viable platform for conducting relevance assessments. The main reason behind this trend is that makes possible to conduct experiments extremely fast, with good results and at low cost. However, like in any experiment, there are several details that would make an experiment work or fail. To gather useful results, user interface guidelines, inter-agreement metrics, and justification analysis are important aspects of a successful crowdsourcing experiment. In this work we explore the design and execution of relevance judgments using Amazon Mechanical Turk as crowdsourcing platform, introducing a methodology for crowdsourcing relevance assessments and the results of a series of experiments using TREC 8 with a fixed budget. Our findings indicate that workers are as good as TREC experts, even providing detailed feedback for certain query-document pairs. We also explore the importance of document design and presentation when performing relevance assessment tasks. Finally, we show our methodology at work with several examples that are interesting in their own.},
 author = {Omar Alonso and Omar Alonso and Ricardo Baeza–Yates and Ricardo Baeza-Yates},
 doi = {10.1007/978-3-642-20161-5_16},
 journal = {null},
 mag_id = {1497983610},
 pmcid = {null},
 pmid = {null},
 title = {Design and Implementation of Relevance Assessments Using Crowdsourcing},
 year = {2011}
}

@article{Alur_1994,
 abstract = {Alur, R. and D.L. Dill, A theory of timed automata, Theoretical Computer Science 126 (1994) 183-235. We propose timed (j&e) automata to model the behavior of real-time systems over time. Our definition provides a simple, and yet powerful, way to annotate state-transition graphs with timing constraints using finitely many real-valued clocks. A timed automaton accepts timed words-infinite sequences in which a real-valued time of occurrence is associated with each symbol. We study timed automata from the perspective of formal language theory: we consider closure properties, decision problems, and subclasses. We consider both nondeterministic and deterministic transition structures, and both Biichi and Muller acceptance conditions. We show that nondeterministic timed automata are closed under union and intersection, but not under complementation, whereas deterministic timed Muller automata are closed under all Boolean operations. The main construction of the paper is an (PSPACE) algorithm for checking the emptiness of the language of a (nondeterministic) timed automaton. We also prove that the universality problem and the language inclusion problem are solvable only for the deterministic automata: both problems are undecidable (II i-hard) in the nondeterministic case and PSPACE-complete in the deterministic case. Finally, we discuss the application of this theory to automatic verification of real-time requirements of finite-state systems.},
 author = {Rajeev Alur and Rajeev Alur and David L. Dill and David L. Dill},
 doi = {10.1016/0304-3975(94)90010-8},
 journal = {Theoretical Computer Science},
 mag_id = {2101508170},
 pmcid = {null},
 pmid = {null},
 title = {A theory of timed automata},
 year = {1994}
}

@article{Alwan_2011,
 abstract = {null},
 author = {Alan Alwan and Ala Alwan},
 doi = {null},
 journal = {null},
 mag_id = {1579760457},
 pmcid = {null},
 pmid = {null},
 title = {Global status report on noncommunicable diseases 2010.},
 year = {2011}
}

@article{Ammann_2008,
 abstract = {Extensively class tested, this text takes an innovative approach to explaining the process of software testing: it defines testing as the process of applying a few well-defined, general-purpose test criteria to a structure or model of the software. The structure of the text directly reflects the pedagogical approach and incorporates the latest innovations in testing, including techniques to test modern types of software such as OO, web applications, and embedded software.},
 author = {Paul Ammann and Paul Ammann and Jeff Offutt and Jeff Offutt},
 doi = {null},
 journal = {null},
 mag_id = {1486172410},
 pmcid = {null},
 pmid = {null},
 title = {Introduction To Software Testing},
 year = {2008}
}

@article{Amoore_2014,
 abstract = {Aim. Medical device-related adverse events are often ascribed to “device” or “operator” failure although there are more complex causes. A structured approach, viewing the device in its clinical context, is developed to assist in-depth investigations of the causes. Method. Medical device applications involve devices, clinical teams, patients, and supporting infrastructure. The literature was explored for investigations and approaches to investigations, particularly structured approaches. From this a conceptual framework of causes was developed based primarily on device and clinical team caring for the patient within a supporting infrastructure, each aspect having detailed subdivisions. The approach was applied to incidents from the literature and an anonymous incident database. Results. The approach identified and classified the underlying causes of incidents described in the literature, exploring the details of “device,” “operator,” or “infrastructure” failures. Applied to incident databases it suggested that causes differ between device types and identified the causes of device unavailability. Discussion. The structured approach enables digging deeper to uncover the wider causes rather than ascribing to device or user fault. It can assess global patterns of causes. It can help develop consistent terminology for describing and sharing information on the causes of medical device adverse events.},
 author = {John Amoore and John N. Amoore},
 doi = {10.1155/2014/314138},
 journal = {null},
 mag_id = {2101290163},
 pmcid = {4782710},
 pmid = {27006931},
 title = {A Structured Approach for Investigating the Causes of Medical Device Adverse Events},
 year = {2014}
}

@article{Andrés_2011,
 abstract = {We consider the formal representation and analysis of systems with fuzzy-time information. First, we present a formalism to represent specifications. This model exploits the concepts of fuzzy set theory and uses a mathematical framework to get a more flexible approach. As it is usually assumed in industrial case studies, we consider that the original requirements of the specification may change. The implementation is built with respect to these changes but the specification is not upgraded. Thus, it may be outdated. In order to continue using the formal framework, the specification must be adapted with respect to these new requirements. We consider that this update process should be as non-intrusive as possible, that is, without using the source-code of the implementation. We present a novel methodology for self-evolving fuzzy-time systems, without interacting with the source code.},
 author = {César Andrés and César Andrés and Luis Liana and Luis Liana and Manuel Núñez and Manuel Núñez},
 doi = {10.1109/cec.2011.5949607},
 journal = {null},
 mag_id = {1984756589},
 pmcid = {null},
 pmid = {null},
 title = {Self-adaptive fuzzy-timed systems},
 year = {2011}
}

@article{Arcaini_2010,
 abstract = {This paper presents AsmetaSMV, a model checker for Abstract State Machines (ASMs). It has been developed with the aim of enriching the ASMETA (ASM mETAmodeling) toolset – a set of tools for ASMs – with the capabilities of the model checker NuSMV to verify properties of ASM models written in the AsmetaL language. We describe the general architecture of AsmetaSMV and the process of automatically mapping ASM models into NuSMV programs. As a proof of concepts, we report the results of using AsmetaSMV to verify temporal properties of various case studies of different characteristics and complexity.},
 author = {Paolo Arcaini and Paolo Arcaini and Angelo Gargantini and Angelo Gargantini and Elvinia Riccobene and Elvinia Riccobene},
 doi = {10.1007/978-3-642-11811-1_6},
 journal = {null},
 mag_id = {1531296731},
 pmcid = {null},
 pmid = {null},
 title = {AsmetaSMV: a way to link high-level ASM models to low-level NuSMV specifications},
 year = {2010}
}

@article{Arcaini_2011,
 abstract = {This paper presents a model-driven software process suitable to develop a set of integrated tools around a formal method. This process exploits concepts and technologies of the Model-driven Engineering (MDE) approach, such as metamodelling and automatic generation of software artifacts from models. We describe the requirements to fulfill and the development steps of this model-driven process. As a proof-of-concept, we apply it to the Finite State Machines and we report our experience in engineering a metamodel-based language and a toolset for the Abstract State Machine formal method. Copyright © 2011 John Wiley & Sons, Ltd.

This work was partially supported by the Italian Government under the project PRIN 2007 D-ASAP (2007XKEHFA).},
 author = {Paolo Arcaini and Paolo Arcaini and Angelo Gargantini and Angelo Gargantini and Elvinia Riccobene and Elvinia Riccobene and Patrizia Scandurra and Patrizia Scandurra},
 doi = {10.1002/spe.1019},
 journal = {Software - Practice and Experience},
 mag_id = {2095929625},
 pmcid = {null},
 pmid = {null},
 title = {A model-driven process for engineering a toolset for a formal method},
 year = {2011}
}

@article{Arcaini_2011,
 abstract = {We present CoMA (Conformance Monitoring by Abstract State Machines), a specification-based approach and its supporting tool for runtime monitoring of Java software. Based on the information obtained from code execution and model simulation, the conformance of the concrete implementation is checked with respect to its formal specification given in terms of Abstract State Machines. At runtime, undesirable behaviors of the implementation, as well as incorrect specifications of the system behavior are recognized.

The technique we propose makes use of Java annotations, which link the concrete implementation to its formal model, without enriching the code with behavioral information contained only in the abstract specification. The approach fosters the separation between implementation and specification, and allows the reuse of specifications for other purposes (formal verification, simulation, model-based testing, etc.).},
 author = {Paolo Arcaini and Paolo Arcaini and Angelo Gargantini and Angelo Gargantini and Elvinia Riccobene and Elvinia Riccobene},
 doi = {10.1007/978-3-642-29860-8_17},
 journal = {null},
 mag_id = {1455759230},
 pmcid = {null},
 pmid = {null},
 title = {CoMA: conformance monitoring of java programs by abstract state machines},
 year = {2011}
}

@article{Arcaini_2013,
 abstract = {In case of underspecified or not fully predictable systems, models specifying system behaviors are nondeterministic. Nondeterminism poses several challenges for the validation and verification activities, including the problem of inconclusive tests in model-based testing with model checker. It is a validation technique that uses model checker counterexamples as test cases. In this paper, we tackle the problem of testing nondeterministic systems by combining model-based testing and runtime conformance monitoring: the input sequences of the tests are automatically generated from nondeterministic models; then their execution is runtime monitored to check conformance of the code w.r.t. its specification. This technique provides an oracle for the test data, it never bears inconclusive responses, and it allows measuring the requirement coverage. The approach uses the Abstract State Machines as formal method for specification purposes and Java as implementation language. As a proof of concepts, the Tic-Tac-Toe game is taken as example of a system with nondeterministic behavior (both at specification and code levels).},
 author = {Paolo Arcaini and Paolo Arcaini and Angelo Gargantini and Angelo Gargantini and Elvinia Riccobene and Elvinia Riccobene},
 doi = {10.1109/icstw.2013.29},
 journal = {null},
 mag_id = {1974416096},
 pmcid = {null},
 pmid = {null},
 title = {Combining Model-Based Testing and Runtime Monitoring for Program Testing in the Presence of Nondeterminism},
 year = {2013}
}

@article{Arcaini_2016,
 abstract = {Medical devices are nowadays more and more software dependent, and software malfunctioning can lead to injuries or death for patients. Several standards have been proposed for the development and the validation of medical devices, but they establish general guidelines on the use of common software engineering activities without any indication regarding methods and techniques to assure safety and reliability.

This paper takes advantage of the Hemodialysis machine case study to present a formal development process supporting most of the engineering activities required by the standards, and provides rigorous approaches for system validation and verification. The process is based on the Abstract State Machine formal method and its model refinement principle.},
 author = {Paolo Arcaini and Paolo Arcaini and Silvia Bonfanti and Silvia Bonfanti and Angelo Gargantini and Angelo Gargantini and Elvinia Riccobene and Elvinia Riccobene},
 doi = {10.1007/978-3-319-33600-8_30},
 journal = {Lecture Notes in Computer Science},
 mag_id = {2504780843},
 pmcid = {null},
 pmid = {null},
 title = {How to Assure Correctness and Safety of Medical Software: The Hemodialysis Machine Case Study},
 year = {2016}
}

@article{Arcaini_2016,
 abstract = {Model refinement is a technique indispensable for modeling large and complex systems. Many formal specification methods share this concept which usually comes together with the definition of refinement correctness, i.e., the mathematical proof of a logical relation between an abstract model and its refined models.},
 author = {Paolo Arcaini and Paolo Arcaini and Angelo Gargantini and Angelo Gargantini and Elvinia Riccobene and Elvinia Riccobene},
 doi = {10.1007/978-3-319-41591-8_17},
 journal = {null},
 mag_id = {2506749915},
 pmcid = {null},
 pmid = {null},
 title = {SMT-based automatic Proof of ASM model refinement},
 year = {2016}
}

@article{Arcaini_2017,
 abstract = {Abstract   Medical devices are safety-critical systems since their malfunctions can seriously compromise human safety. Correct operation of a medical device depends upon the controlling software, whose development should adhere to certification standards. However, these standards provide general descriptions of common software engineering activities without any indication regarding particular methods and techniques to assure safety and reliability.  This paper discusses how to integrate the use of a formal approach into the current normative for the medical software development. The rigorous process is based on the Abstract State Machine (ASM) formal method, its refinement principle, and model analysis approaches the method supports. The hemodialysis machine case study is used to show how the ASM-based design process covers most of the engineering activities required by the related standards, and provides rigorous approaches for medical software validation and verification.},
 author = {Paolo Arcaini and Paolo Arcaini and Silvia Bonfanti and Silvia Bonfanti and Angelo Gargantini and Angelo Gargantini and Atif Mashkoor and Atif Mashkoor and Elvinia Riccobene and Elvinia Riccobene},
 doi = {10.1016/j.scico.2017.07.003},
 journal = {Science of Computer Programming},
 mag_id = {2741412822},
 pmcid = {null},
 pmid = {null},
 title = {Integrating formal methods into medical software development: The ASM approach},
 year = {2017}
}

@article{Arcaini_2017,
 abstract = {The paper presents an approach for rigorous development of safety-critical systems based on the Abstract State Machine formal method. The development process starts from a high level formal view of the system and, through refinement, derives more detailed models till the desired level of specification. Along the process, different validation and verification activities are available, as simulation, model review, and model checking. Moreover, each refinement step can be proved correct using an SMT-based approach. As last step of the refinement process, a Java implementation can be developed and linked to the formal specification. The correctness of the implementation w.r.t. its formal specification can be proved by means of model-based testing and runtime verification. The process is exemplified by using a Landing Gear System as case study.},
 author = {Paolo Arcaini and Paolo Arcaini and Angelo Gargantini and Angelo Gargantini and Elvinia Riccobene and Elvinia Riccobene},
 doi = {10.1007/s10009-015-0394-x},
 journal = {International Journal on Software Tools for Technology Transfer},
 mag_id = {1046642269},
 pmcid = {null},
 pmid = {null},
 title = {Rigorous development process of a safety-critical system: from ASM models to Java code},
 year = {2017}
}

@article{Arney_2007,
 abstract = {As software becomes ever more ubiquitous and complex in medical devices, it becomes increasingly important to assure that it performs safely and effectively. The critical nature of medical devices necessitates that the software used therein be reliable and free of errors. It becomes imperative, therefore, to have a conformance review process in place to ascertain the correctness of the software and to ensure that it meets all requirements and standards. Formal methods have long been suggested as a means to design and develop medical device software. However, most manufacturers shy from using these techniques, citing them as too complex and time consuming. As a result, (potentially life-threatening) errors are often not discovered until a device is already on the market. In this paper we present a reference model based approach to software conformance checking. Reference models enable the application of formal methods to software conformance checking, and provide a framework for rigorous testing. To illustrate the approach, we develop the reference model for a generic patient controlled analgesic infusion pump, and explain how it can be used to aid software conformance checking in a regulatory environment.},
 author = {David Arney and David Arney and David Arney and Raoul Jetley and Raoul Jetley and Paul Jones and Paul Jones and Insup Lee and Insup Lee and Oleg Sokolsky and Oleg Sokolsky},
 doi = {10.1109/hcmdss-mdpnp.2007.36},
 journal = {null},
 mag_id = {2149519129},
 pmcid = {null},
 pmid = {null},
 title = {Formal Methods Based Development of a PCA Infusion Pump Reference Model: Generic Infusion Pump (GIP) Project},
 year = {2007}
}

@article{Atanassov_1986,
 abstract = {null},
 author = {Krassimir Atanassov and Krassimir T. Atanassov},
 doi = {10.1016/s0165-0114(86)80034-3},
 journal = {Fuzzy Sets and Systems},
 mag_id = {1980564456},
 pmcid = {null},
 pmid = {null},
 title = {Intuitionistic fuzzy sets},
 year = {1986}
}

@article{Atkinson_2010,
 abstract = {Gaming is used in a large number of medical situations including the diagnostic and treatment of a number of diseases. With rapid progression in hardware and software technologies, the possibilities for medical gaming are growing exponentially. One of the major innovations in human-computer interaction is referred to as Haptics, i.e., using the sense of touch. Hardware devices can be used to interact with computers in such a way that a patient can receive positive force-feedback, monitor and guide their movements. The collected data, when patients use such devices, can then be analyzed and used to diagnose a variety of diseases and manage them over their life-cycle. This paper describes the design of a medical diagnostic gaming environment that is used to gather patient information in a casual, non-obtrusive manner that is relaxing for the patient. The system employs the Novint Falcon Human Interface Device, a very sensitive device which records the user's movements and provides force feedback in almost all directions to guide a patient who may have problems reaching a specified goal within the game. Specifically, Parkinson's disease patients are measured both for the steadiness of their movements as they traverse through the game environment and their steady-state tremors. The information gathered can be invaluable in determining the progression or regression of the disease, detecting and arresting the disease at a given stage and for other related observations and inferences.},
 author = {Stephen D. Atkinson and Stephen D. Atkinson and V. Lakshmi Narasimhan and V. Lakshmi Narasimhan},
 doi = {10.1109/tisc.2010.5714615},
 journal = {null},
 mag_id = {2544528391},
 pmcid = {null},
 pmid = {null},
 title = {Design of an introductory medical gaming environment for diagnosis and management of Parkinson's disease},
 year = {2010}
}

@article{Azeem_2014,
 abstract = {Formal Method (FM) is an emergent feature that uses mathematical notations to mark accurate and explicit specifications that error and discrepancies are identified during early phases of the software development process. By using the Z specification, system specification, design and verification can improve the quality effectively. In this paper, we present the formal specification for the e-Health system by using Z schema. This paper will be motivation to Formal Methods that FM not only beneficial for mission critical systems but also in the commercial and business oriented applications, because the development time, verification and maintenance cost will drastically reduce.},
 author = {Muhammad Azeem and Muhammad Waqar Azeem and Muhammad Azeem and Muhammad Ahsan and Muhammad Ahsan and Nasir Mehmood Minhas and Nasir Mehmood Minhas and Khadija Noreen and Khadija Noreen},
 doi = {10.1109/i2ct.2014.7092123},
 journal = {null},
 mag_id = {2030452150},
 pmcid = {null},
 pmid = {null},
 title = {Specification of e-Health system using Z: A motivation to formal methods},
 year = {2014}
}

@article{Banerjee_2014,
 abstract = {In this paper, we use cross wavelet transform (XWT) for the analysis and classification of electrocardiogram (ECG) signals. The cross-correlation between two time-domain signals gives a measure of similarity between two waveforms. The application of the continuous wavelet transform to two time series and the cross examination of the two decompositions reveal localized similarities in time and frequency. Application of the XWT to a pair of data yields wavelet cross spectrum (WCS) and wavelet coherence (WCOH). The proposed algorithm analyzes ECG data utilizing XWT and explores the resulting spectral differences. A pathologically varying pattern from the normal pattern in the QT zone of the inferior leads shows the presence of inferior myocardial infarction. A normal beat ensemble is selected as the absolute normal ECG pattern template, and the coherence between various other normal and abnormal subjects is computed. The WCS and WCOH of various ECG patterns show distinguishing characteristics over two specific regions R1 and R2, where R1 is the QRS complex area and R2 is the T-wave region. The Physikalisch-Technische Bundesanstalt diagnostic ECG database is used for evaluation of the methods. A heuristically determined mathematical formula extracts the parameter(s) from the WCS and WCOH. Empirical tests establish that the parameter(s) are relevant for classification of normal and abnormal cardiac patterns. The overall accuracy, sensitivity, and specificity after combining the three leads are obtained as 97.6%, 97.3%, and 98.8%, respectively.},
 author = {Swati Banerjee and Swati Banerjee and Madhuchhanda Mitra and Madhuchhanda Mitra},
 doi = {10.1109/tim.2013.2279001},
 journal = {IEEE Transactions on Instrumentation and Measurement},
 mag_id = {2091076299},
 pmcid = {null},
 pmid = {null},
 title = {Application of Cross Wavelet Transform for ECG Pattern Analysis and Classification},
 year = {2014}
}

@article{Bano_2015,
 abstract = {Abstract   Context  For more than four decades it has been intuitively accepted that user involvement (UI) during system development lifecycle leads to system success. However when the researchers have evaluated the user involvement and system success (UI-SS) relationship empirically, the results were not always positive.    Objective  Our objective was to explore the UI-SS relationship by synthesizing the results of all the studies that have empirically investigated this complex phenomenon.    Method  We performed a Systematic Literature Review (SLR) following the steps provided in the guidelines of Evidence Based Software Engineering. From the resulting studies we extracted data to answer our 9 research questions related to the UI-SS relationship, identification of users, perspectives of UI, benefits, problems and challenges of UI, degree and level of UI, relevance of stages of software development lifecycle (SDLC) and the research method employed on the UI-SS relationship.    Results  Our systematic review resulted in selecting 87 empirical studies published during the period 1980–2012. Among 87 studies reviewed, 52 reported that UI positively contributes to system success, 12 suggested a negative contribution and 23 were uncertain. The UI-SS relationship is neither direct nor binary, and there are various confounding factors that play their role. The identification of users, their degree/level of involvement, stage of SDLC for UI, and choice of research method have been claimed to have impact on the UI-SS relationship. However, there is not sufficient empirical evidence available to support these claims.    Conclusion  Our results have revealed that UI does contribute positively to system success. But it is a double edged sword and if not managed carefully it may cause more problems than benefits. Based on the analysis of 87 studies, we were able to identify factors for effective management of UI alluding to the causes for inconsistency in the results of published literature.},
 author = {Muneera Bano and Muneera Bano and Didar Zowghi and Didar Zowghi},
 doi = {10.1016/j.infsof.2014.06.011},
 journal = {Information & Software Technology},
 mag_id = {2077980038},
 pmcid = {null},
 pmid = {null},
 title = {A systematic review on the relationship between user involvement and system success},
 year = {2015}
}

@article{Basile_2018,
 abstract = {The railway sector has seen a large number of successful applications of formal methods and tools. However, up-to-date, structured information about the industrial usage and needs related to formal tools in railways is limited. As a first step to address this, we present the results of a questionnaire submitted to 44 stakeholders with experience in the application of formal tools in railways. The questionnaire was oriented to gather information about industrial projects, and about the functional and quality features that a formal tool should have to be successfully applied in railways. The results show that the most used tools are, as expected, those of the B family, followed by an extensive list of about 40 tools, each one used by few respondents only, indicating a rich, yet scattered, landscape. The most desired features concern formal verification, maturity, learnability, quality of documentation, and ease of integration in a CENELEC process. This paper extends the body of knowledge on formal methods applications in the railway industry, and contributes with a ranked list of tool features considered relevant by railway stakeholders.},
 author = {Davide Basile and Davide Basile and Maurice H. ter Beek and Maurice H. ter Beek and Alessandro Fantechi and Alessandro Fantechi and Stefania Gnesi and Stefania Gnesi and Franco Mazzanti and Franco Mazzanti and Andrea Piattino and Andrea Piattino and Daniele Trentini and Daniele Trentini and Alessio Ferrari and Alessio Ferrari},
 doi = {10.1007/978-3-319-98938-9_2},
 journal = {null},
 mag_id = {2885628182},
 pmcid = {null},
 pmid = {null},
 title = {On the Industrial Uptake of Formal Methods in the Railway Domain},
 year = {2018}
}

@article{Behrmann_2004,
 abstract = {This is a tutorial paper on the tool Uppaal. Its goal is to
be a short introduction on the flavor of timed automata implemented in
the tool, to present its interface, and to explain how to use the tool. The
contribution of the paper is to provide reference examples and modeling
patterns.},
 author = {Gerd Behrmann and Gerd Behrmann and Albert David and Alexandre David and Kim Guldstrand Larsen and Kim Guldstrand Larsen},
 doi = {10.1007/978-3-540-30080-9_7},
 journal = {null},
 mag_id = {1962072139},
 pmcid = {null},
 pmid = {null},
 title = {A Tutorial on UPPAAL},
 year = {2004}
}

@article{Binder_2014,
 abstract = {MBT has positive effects on efficiency and effectiveness, even if it only partially fulfills high expectations.},
 author = {Robert V. Binder and Robert V. Binder and Bruno Legeard and Bruno Legeard and Anne Krämer and Anne Kramer},
 doi = {10.1145/2716276.2723708},
 journal = {ACM Queue},
 mag_id = {2078081802},
 pmcid = {null},
 pmid = {null},
 title = {Model-based testing: where does it stand?},
 year = {2014}
}

@article{Bloomfield_2000,
 abstract = {Based on a study by Adelard (UK) commissioned by the German Bundesamt fuer Sicherheit in der Informationstechnik this paper identifies crucial factors leading to the success or failure of the application of formal methods and provides ideas of improved technology adoption perspectives by analysing the formal methods market.},
 author = {Robin Bloomfield and Robin Bloomfield and Dan Craigen and Dan Craigen and Frank Koob and Frank Koob and Markus Ullmann and Markus Ullmann and Stefan Wittmann and Stefan Wittmann},
 doi = {10.1007/3-540-40891-6_19},
 journal = {null},
 mag_id = {1603233098},
 pmcid = {null},
 pmid = {null},
 title = {Formal Methods Diffusion: Past Lessons and Future Prospects},
 year = {2000}
}

@article{Bolton_2012,
 abstract = {Breakdowns in complex systems often occur as a result of system elements interacting in unanticipated ways. In systems with human operators, human-automation interaction associated with both normative and erroneous human behavior can contribute to such failures. Model-driven design and analysis techniques provide engineers with formal methods tools and techniques capable of evaluating how human behavior can contribute to system failures. This paper presents a novel method for automatically generating task analytic models encompassing both normative and erroneous human behavior from normative task models. The generated erroneous behavior is capable of replicating Hollnagel's zero-order phenotypes of erroneous action for omissions, jumps, repetitions, and intrusions. Multiple phenotypical acts can occur in sequence, thus allowing for the generation of higher order phenotypes. The task behavior model pattern capable of generating erroneous behavior can be integrated into a formal system model so that system safety properties can be formally verified with a model checker. This allows analysts to prove that a human-automation interactive system (as represented by the model) will or will not satisfy safety properties with both normative and generated erroneous human behavior. We present benchmarks related to the size of the statespace and verification time of models to show how the erroneous human behavior generation process scales. We demonstrate the method with a case study: the operation of a radiation therapy machine. A potential problem resulting from a generated erroneous human action is discovered. A design intervention is presented which prevents this problem from occurring. We discuss how our method could be used to evaluate larger applications and recommend future paths of development.},
 author = {Matthew L. Bolton and Matthew L. Bolton and Ellen J. Bass and Ellen J. Bass and Radu Siminiceanu and Radu I. Siminiceanu},
 doi = {10.1016/j.ijhcs.2012.05.010},
 journal = {International Journal of Human-computer Studies \/ International Journal of Man-machine Studies},
 mag_id = {2131860114},
 pmcid = {3480525},
 pmid = {23105914},
 title = {Generating phenotypical erroneous human behavior to evaluate human-automation interaction using model checking},
 year = {2012}
}

@article{Bonfanti_2017,
 abstract = {This paper presents Asm2C++, a tool that automatically generates executable C++ code for Arduino from a formal specification given as Abstract State Machines (ASMs). The code generation process follows the model-driven engineering approach, where the code is obtained from a formal abstract model by applying certain transformation rules. The translation process is highly configurable in order to correctly integrate the underlying hardware. The advantage of the Asm2C++ tool is that it is part of the Asmeta framework that allows to analyze, verify, and validate the correctness of a formal model.},
 author = {Silvia Bonfanti and Silvia Bonfanti and Marco Carissoni and Marco Carissoni and Angelo Gargantini and Angelo Gargantini and Atif Mashkoor and Atif Mashkoor},
 doi = {10.1007/978-3-319-57288-8_21},
 journal = {null},
 mag_id = {2604576922},
 pmcid = {null},
 pmid = {null},
 title = {Asm2C++: A tool for code generation from abstract state machines to Arduino},
 year = {2017}
}

@article{Bosschaart_2015,
 abstract = {null},
 author = {Mark Bosschaart and Mark Bosschaart and Egidio Quaglietta and Egidio Quaglietta and Bob Janssen and Bob Janssen and Oded Cats and Rob M.P. Goverde and Rob M.P. Goverde},
 doi = {10.1016/j.is.2014.11.007},
 journal = {Information Systems},
 mag_id = {2084642972},
 pmcid = {null},
 pmid = {null},
 title = {Efficient formalization of railway interlocking data in RailML},
 year = {2015}
}

@article{Bowen_1993,
 abstract = {Formal methods may be at the crossroads of acceptance by a wider industrial community. In order for the techniques to become widely used, the gap between theorists and practitioners must be bridged effectively. In particular, safety-critical systems offer an application area where formal methods may be engaged usefully to the benefit of all. This paper discusses some of the issues concerned with the general acceptance of formal methods and concludes with a summary of the current position and how the formal methods community could proceed to improve matters in the future.},
 author = {Jonathan P. Bowen and Jonathan P. Bowen and V. Stavridou and Victoria Stavridou},
 doi = {10.1007/bfb0024646},
 journal = {null},
 mag_id = {1644864267},
 pmcid = {null},
 pmid = {null},
 title = {The Industrial Take-up of Formal Methods in Safety-Critical and Other Areas: A Perspective},
 year = {1993}
}

@article{Bowen_2008,
 abstract = {There are many different ways of building software applications and of tackling the problems of understanding the system to be built, designing that system and finally implementing the design. One approach is to use formal methods, which we can generalise as meaning we follow a process which uses some formal language to specify the behaviour of the intended system, techniques such as theorem proving or model-checking to ensure the specification is valid (i.e., meets the requirements and has been shown, perhaps by proof or other means of inspection, to have the properties the client requires of it) and a refinement process to transform the specification into an implementation. Conversely, the approach we take may be less structured and rely on informal techniques. The design stage may involve jotting down ideas on paper, brainstorming with users etc. We may use prototyping to transform these ideas into working software and get users to test the implementation to find problems. Formal methods have been shown to be beneficial in describing the functionality of systems, what we may call application logic, and underlying system behaviour. Informal techniques, however, have also been shown to be useful in the design of the user interface to systems. Given that both styles of development are beneficial to different parts of the system we would like to be able to use both approaches in one integrated software development process. Their differences, however, make this a challenging objective. In this paper we describe models and techniques which allow us to incorporate informal design artefacts into a formal software development process.},
 author = {Judy Bowen and Judy Bowen and Steve Reeves and Steve Reeves},
 doi = {10.1007/s11334-008-0049-0},
 journal = {Innovations in Systems and Software Engineering},
 mag_id = {2019217423},
 pmcid = {null},
 pmid = {null},
 title = {Formal models for user interface design artefacts},
 year = {2008}
}

@article{Bowen_2013,
 abstract = {Formally modelling the software functionality and interactivity of safety-critical devices allows us to prove properties about their behaviours and be certain that they will respond to user interaction correctly. In domains such as medical environments, where many different devices may be used, it is equally important to ensure that all devices used adhere to a set of safety, and other, principles designed for that environment. In this paper we look at modelling important properties of interactive medical devices including safety considerations mandated by their users. We use ProZ for model checking to ensure that properties stated in temporal logic hold, and also to check invariants. In this way we gain confidence that important properties do hold of the device, and that models of particular devices adhere to the properties described.},
 author = {Judy Bowen and Judy Bowen and Steve Reeves and Steve Reeves},
 doi = {10.1145/2494603.2480314},
 journal = {null},
 mag_id = {2023753675},
 pmcid = {null},
 pmid = {null},
 title = {Modelling safety properties of interactive medical systems},
 year = {2013}
}

@article{Brat_2013,
 abstract = {We report on a study to determine the maturity of different verification and validation technologies (V&V) applied to a representative example of NASA flight software. The study consisted of a controlled experiment where three technologies (static analysis, runtime analysis and model checking) were compared to traditional testing with respect to their ability to find seeded errors in a prototype Mars Rover controller. What makes this study unique is that it is the first (to the best of our knowledge) controlled experiment to compare formal methods based tools to testing on a realistic industrial-size example, where the emphasis was on collecting as much data on the performance of the tools and the participants as possible. The paper includes a description of the Rover code that was analyzed, the tools used, as well as a detailed description of the experimental setup and the results. Due to the complexity of setting up the experiment, our results cannot be generalized, but we believe it can still serve as a valuable point of reference for future studies of this kind. It confirmed our belief that advanced tools can outperform testing when trying to locate concurrency errors. Furthermore, the results of the experiment inspired a novel framework for testing the next generation of the Rover.},
 author = {Guillaume Brat and Guillaume Brat and Doron Drusinsky and Doron Drusinsky and Dimitra Giannakopoulou and Dimitra Giannakopoulou and Allen Goldberg and Allen Goldberg and Klaus Havelund and Klaus Havelund and Mike Lowry and Michael Lowry and Corina S. Pǎsǎreanu and Corina S. Pasareanu and Corina S. Pasareanu and Arnaud Venet and Arnaud Venet and Willem Visser and Willem Visser and Richard Washington and Rich Washington},
 doi = {null},
 journal = {null},
 mag_id = {2016208194},
 pmcid = {null},
 pmid = {null},
 title = {Experimental Evaluation of Verification and Validation Tools on Martian Rover Software},
 year = {2013}
}

@article{Braun_2006,
 abstract = {Thematic analysis is a poorly demarcated, rarely acknowledged, yet widely used qualitative analytic method within psychology. In this paper, we argue that it offers an accessible and theoretically flexible approach to analysing qualitative data. We outline what thematic analysis is, locating it in relation to other qualitative analytic methods that search for themes or patterns, and in relation to different epistemological and ontological positions. We then provide clear guidelines to those wanting to start thematic analysis, or conduct it in a more deliberate and rigorous way, and consider potential pitfalls in conducting thematic analysis. Finally, we outline the disadvantages and advantages of thematic analysis. We conclude by advocating thematic analysis as a useful and flexible method for qualitative research in and beyond psychology.},
 author = {Virginia Braun and Virginia Braun and Victoria Clarke and Victoria Clarke},
 doi = {10.1191/1478088706qp063oa},
 journal = {Qualitative Research in Psychology},
 mag_id = {1979290264},
 pmcid = {null},
 pmid = {null},
 title = {Using thematic analysis in psychology},
 year = {2006}
}

@article{Breen_2006,
 abstract = {This article guides readers through the decisions and considerations involved in conducting focus-group research investigations into students' learning experiences. One previously published focus-group study is used as an illustrative example, along with other examples from the field of pedagogic research in geography higher education. An approach to deciding whether to use focus groups is suggested, which includes a consideration of when focus groups are preferred over one-to-one interviews. Guidelines for setting up and designing focus-group studies are outlined, ethical issues are highlighted, the purpose of a pilot study is reviewed, and common focus-group analysis and reporting styles are outlined.},
 author = {Rosanna Breen and Rosanna L. Breen},
 doi = {10.1080/03098260600927575},
 journal = {Journal of Geography in Higher Education},
 mag_id = {2006003012},
 pmcid = {null},
 pmid = {null},
 title = {A Practical Guide to Focus-Group Research},
 year = {2006}
}

@article{Broadfoot_2005,
 abstract = {Software is now an essential component that is embedded in an ever-increasing array of products. It has become an important means of realising product innovation and is a key determinant of both product quality and time-to-market. For many businesses, software has become business-critical and software development is a strategic business activity. At the same time, software development continues to suffer from poor predictability. Existing development methods appear to have reached a quality ceiling that incremental improvements in process and technology are unlikely to breach. To break through this ceiling, a different, more formal approach is needed, but one which can be introduced within existing development organisations.},
 author = {Guy H. Broadfoot and Guy H. Broadfoot},
 doi = {10.1007/11526841_39},
 journal = {null},
 mag_id = {1533209541},
 pmcid = {null},
 pmid = {null},
 title = {ASD case notes: costs and benefits of applying formal methods to industrial control software},
 year = {2005}
}

@article{Brunsch_2014,
 abstract = {null},
 author = {Thomas Brunsch and Laurent Hardouin and Jörg Raisch},
 doi = {10.1201/9781315216140},
 journal = {null},
 mag_id = {4205354444},
 pmcid = {null},
 pmid = {null},
 title = {Modelling Manufacturing Systems in a Dioid Framework},
 year = {2014}
}

@article{Bryans_2010,
 abstract = {The use of business process models has gone far beyond documentation purposes. In the development of business applications, they can play the role of an artifact on which high level properties can be verified and design errors can be revealed in an effort to reduce overhead at later software development and diagnosis stages. This paper demonstrates how formal verification may add value to the specification, design and development of business process models in an industrial setting. The analysis of these models is achieved via an algorithmic translation from the de-facto standard business process modeling language BPMN to Event-B, a widely used formal language supported by the Rodin platform which offers a range of simulation and verification technologies.},
 author = {Jeremy Bryans and Jeremy Bryans and Wei Wei and Wei Wei},
 doi = {10.1007/978-3-642-15898-8_3},
 journal = {null},
 mag_id = {2103381131},
 pmcid = {null},
 pmid = {null},
 title = {Formal analysis of BPMN models using event-B},
 year = {2010}
}

@article{Bulkowski_2008,
 abstract = {Encyclopedia of candlestick charts , Encyclopedia of candlestick charts , کتابخانه دیجیتال جندی شاپور اهواز},
 author = {Thomas N. Bulkowski and Thomas N. Bulkowski and Thomas N. Bulkowski},
 doi = {null},
 journal = {null},
 mag_id = {605142813},
 pmcid = {null},
 pmid = {null},
 title = {Encyclopedia of Candlestick Charts},
 year = {2008}
}

@article{Buttussi_2013,
 abstract = {null},
 author = {Fabio Buttussi and Fabio Buttussi and Tommaso Pellis and Tommaso Pellis and Tommaso Pellis and Alberto Cabas Vidani and Alberto Cabas Vidani and Daniele Pausler and Daniele Pausler and Elio Carchietti and Elio Carchietti and Luca Chittaro and Luca Chittaro},
 doi = {10.1016/j.ijmedinf.2013.05.007},
 journal = {International Journal of Medical Informatics},
 mag_id = {2058635456},
 pmcid = {null},
 pmid = {23763908},
 title = {Evaluation of a 3D serious game for advanced life support retraining},
 year = {2013}
}

@article{Börger_2003,
 abstract = {1 Introduction.- 1.1 Goals of the Book and Contours of its Method.- 1.1.1 Stepwise Refinable Abstract Operational Modeling.- 1.1.2 Abstract Virtual Machine Notation.- 1.1.3 Practical Benefits.- 1.1.4 Harness Pseudo-Code by Abstraction and Refinement.- 1.1.5 Adding Abstraction and Rigor to UML Models.- 1.2 Synopsis of the Book.- 2 ASM Design and Analysis Method.- 2.1 Principles of Hierarchical System Design.- 2.1.1 Ground Model Construction (Requirements Capture).- 2.1.2 Stepwise Refinement (Incremental Design).- 2.1.3 Integration into Software Practice.- 2.2 Working Definition.- 2.2.1 Basic ASMs.- 2.2.2 Definition.- 2.2.3 Classification of Locations and Updates.- 2.2.4 ASM Modules.- 2.2.5 Illustration by Small Examples.- 2.2.6 Control State ASMs.- 2.2.7 Exercises.- 2.3 Explanation by Example: Correct Lift Control.- 2.3.1 Exercises.- 2.4 Detailed Definition (Math. Foundation).- 2.4.1 Abstract States and Update Sets.- 2.4.2 Mathematical Logic.- 2.4.3 Transition Rules and Runs of ASMs.- 2.4.4 The Reserve of ASMs.- 2.4.5 Exercises.- 2.5 Notational Conventions.- 3 Basic ASMs.- 3.1 Requirements Capture by Ground Models.- 3.1.1 Fundamental Questions to be Asked.- 3.1.2 Illustration by Small Use Case Models.- 3.1.3 Exercises.- 3.2 Incremental Design by Refinements.- 3.2.1 Refinement Scheme and its Specializations.- 3.2.2 Two Refinement Verification Case Studies.- 3.2.3 Decomposing Refinement Verifications.- 3.2.4 Exercises.- 3.3 Microprocessor Design Case Study.- 3.3.1 Ground Model DLXseq.- 3.3.2 Parallel Model DLXpar Resolving Structural Hazards.- 3.3.3 Verifying Resolution of Structural Hazards (DLXpar).- 3.3.4 Resolving Data Hazards (Refinement DLXdata).- 3.3.5 Exercises.- 4 Structured ASMs (Composition Techniques).- 4.1 Turbo ASMs (seq, iterate, submachines, recursion).- 4.1.1 Seq and Iterate (Structured Programming).- 4.1.2 Submachines and Recursion (Encapsulation and Hiding).- 4.1.3 Analysis of Turbo ASM Steps.- 4.1.4 Exercises.- 4.2 Abstract State Processes (Interleaving).- 5 Synchronous Multi-Agent ASMs.- 5.1 Robot Controller Case Study.- 5.1.1 Production Cell Ground Model.- 5.1.2 Refinement of the Production Cell Component ASMs.- 5.1.3 Exercises.- 5.2 Real-Time Controller (Railroad Crossing Case Study).- 5.2.1 Real-TimeProcess Control Systems.- 5.2.2 Railroad Crossing Case Study.- 5.2.3 Exercises.- 6 Asynchronous Multi-Agent ASMs.- 6.1 Async ASMs: Definition and Network Examples.- 6.1.1 Mutual Exclusion.- 6.1.2 Master-Slave Agreement.- 6.1.3 Network Consensus.- 6.1.4 Load Balance.- 6.1.5 Leader Election and Shortest Path.- 6.1.6 Broadcast Acknowledgment (Echo).- 6.1.7 Phase Synchronization.- 6.1.8 Routing Layer Protocol for Mobile Ad Hoc Networks.- 6.1.9 Exercises.- 6.2 Embedded System Case Study.- 6.2.1 Light Control Ground Model.- 6.2.2 Signature (Agents and Their State).- 6.2.3 User Interaction (Manual Control).- 6.2.4 Automatic Control.- 6.2.5 Failure and Service.- 6.2.6 Component Structure.- 6.2.7 Exercises.- 6.3 Time-Constrained Async ASMs.- 6.3.1 Kermit Case Study (Alternating Bit/Sliding Window).- 6.3.2 Processor-Group-Membership Protocol Case Study.- 6.3.3 Exercises.- 6.4 Async ASMs with Durative Actions.- 6.4.1 Protocol Verification using Atomic Actions.- 6.4.2 Refining Atomic to Durative Actions.- 6.4.3 Exercises.- 6.5 Event-Driven ASMs.- 6.5.1 UML Diagrams for Dynamics.- 6.5.2 Exercises.- 7 Universal Design and Computation Model.- 7.1 Integrating Computation and Specification Models.- 7.1.1 Classical Computation Models.- 7.1.2 System Design Models.- 7.1.3 Exercises.- 7.2 Sequential ASM Thesis (A Proof from Postulates).- 7.2.1 Gurevich's Postulates for Sequential Algorithms.- 7.2.2 Bounded-Choice Non-Determinism.- 7.2.3 Critical Terms for ASMs.- 7.2.4 Exercises.- 8 Tool Support for ASMs.- 8.1 Verification of ASMs.- 8.1.1 Logic for ASMs.- 8.1.2 Formalizing the Consistency of ASMs.- 8.1.3 Basic Axioms and Proof Rules of the Logic.- 8.1.4 Why Deterministic Transition Rules?.- 8.1.5 Completeness for Hierarchical ASMs.- 8.1.6 The Henkin Model Construction.- 8.1.7 An Extension with Explicit Step Information.- 8.1.8 Exercises.- 8.2 Model Checking of ASMs.- 8.3 Execution of ASMs.- 9 History and Survey of ASM Research.- 9.1 The Idea of Sharpening Turing's Thesis.- 9.2 Recognizing the Practical Relevance of ASMs.- 9.3 Testing the Practicability of ASMs.- 9.3.1 Architecture Design and Virtual Machines.- 9.3.2 Protocols.- 9.3.3 Why use ASMs for Hw/Sw Engineering?.- 9.4 Making ASMs Fit for their Industrial Deployment.- 9.4.1 Practical Case Studies.- 9.4.2 Industrial Pilot Projects and Further Applications.- 9.4.3 Tool Integration.- 9.5 Conclusion and Outlook.- References.- List of Problems.- List of Figures.- List of Tables.},
 author = {Egon Börger and Egon Börger and Robert F. Stärk and Robert F. Stärk and Robert F. Stärk},
 doi = {null},
 journal = {null},
 mag_id = {1853368733},
 pmcid = {null},
 pmid = {null},
 title = {Abstract State Machines: A Method for High-Level System Design and Analysis},
 year = {2003}
}

@article{Campbell_1959,
 abstract = {null},
 author = {Donald T. Campbell and Donald T. Campbell and Donald W. Fiske and Donald W. Fiske and Donald W. Fiske and Donald W. Fiske},
 doi = {10.1037/h0046016},
 journal = {Psychological Bulletin},
 mag_id = {2024509488},
 pmcid = {null},
 pmid = {13634291},
 title = {Convergent and discriminant validation by the multitrait-multimethod matrix.},
 year = {1959}
}

@article{Campos_2008,
 abstract = {The paper explores the role that formal modeling may play in aiding the visualization and implementation of usability requirements of a control panel. We propose that this form of analysis should become a systematic and routine aspect of the development of such interfaces. We use a notation for describing the interface that is convenient to use by software engineers, and describe a set of tools designed to make the process systematic and exhaustive.},
 author = {José Creissac Campos and J. Creissac Campos and Michael D. Harrison and Michael D. Harrison},
 doi = {10.1007/978-3-540-70569-7_6},
 journal = {null},
 mag_id = {2103839338},
 pmcid = {null},
 pmid = {null},
 title = {Systematic Analysis of Control Panel Interfaces Using Formal Tools},
 year = {2008}
}

@article{Campos_2014,
 abstract = {Analysis of the usability of an interactive system requires both an understanding of how the system is to be used and a means of assessing the system against that understanding. Such analytic assessments are particularly important in safety-critical systems as latent vulnerabilities may exist which have negative consequences only in certain circumstances. Many existing approaches to assessment use tasks or scenarios to provide explicit representation of their understanding of use. These normative user behaviours have the advantage that they clarify assumptions about how the system will be used but have the disadvantage that they may exclude many plausible deviations from these norms. Assessments of how a design fails to support these user behaviours can be a matter of judgement based on individual experience rather than evidence. We present a systematic formal method for analysing interactive systems that is based on constraints rather than prescribed behaviour. These constraints capture precise assumptions about what information resources are used to perform action. These resources may either reside in the system itself or be external to the system. The approach is applied to two different medical device designs, comparing two infusion pumps currently in common use in hospitals. Comparison of the two devices is based on these resource assumptions to assess consistency of interaction within the design of each device.},
 author = {José Creissac Campos and José Creissac Campos and Gavin Doherty and Gavin Doherty and Michael D. Harrison and Michael D. Harrison},
 doi = {10.1016/j.ijhcs.2013.10.005},
 journal = {International Journal of Human-computer Studies \/ International Journal of Man-machine Studies},
 mag_id = {2103431672},
 pmcid = {null},
 pmid = {null},
 title = {Analysing interactive devices based on information resource constraints},
 year = {2014}
}

@article{Carioni_2008,
 abstract = {This paper presents the  AValLa language, a domain-specific modelling language for scenario-based validation of ASM models, and its supporting tool, the  AsmetaV validator. They have been developed according to the model-driven development principles as part of the  asmeta (ASM mETAmodelling) toolset, a set of tools around ASMs. As a proof-of-concepts, the paper reports the results of the scenario-based validation for the well-known LIFT control case study.},
 author = {Alessandro Carioni and Alessandro Carioni and Angelo Gargantini and Angelo Gargantini and Elvinia Riccobene and Elvinia Riccobene and Patrizia Scandurra and Patrizia Scandurra},
 doi = {10.1007/978-3-540-87603-8_7},
 journal = {null},
 mag_id = {1581430985},
 pmcid = {null},
 pmid = {null},
 title = {A Scenario-Based Validation Language for ASMs},
 year = {2008}
}

@article{Carlton_2013,
 abstract = {This chapter contains sections titled: Introduction Perfect Developer – its inspiration and foundations Theoretical foundations The Perfect language A Perfect Developer example Escher C verifier The C subset supported by eCv The annotation language of eCv Escher C verifier examples The theorem prover used by Perfect Developer and Escher C verifier Real-world applications of Perfect Developer and Escher C verifier Future work Glossary Bibliography},
 author = {Judith Carlton and Judith Carlton and D. Crocker and David Crocker},
 doi = {10.1002/9781118561829.ch5},
 journal = {null},
 mag_id = {1591563799},
 pmcid = {null},
 pmid = {null},
 title = {Escher Verification Studio Perfect Developer and Escher C Verifier},
 year = {2013}
}

@article{Cavalli_2015,
 abstract = {Testing has become an integral part of innovation, production and operation of systems. The activity of testing is already a flourishing area with the active participation of a large community of researchers and experts who are highly aware of its importance and impact for the future deployment and use of software and systems. Formal methods have proved to be very promising for developing automated and generic testing methods. Actually,the combination of formal methods and testing is currently well understood and tools to automate testing activities are widely available. (Formal) testing is the assessment, by means of experiments, that a product conforms to its (formal) requirements. Test cases are designed to test particular aspects of the system, called test objectives. In order to formally obtain testing objectives, it is necessary to provide mathematical models for the semantic of the studied system, formal frameworks for testing, languages to describe the expected properties or requirements of the system in a precise and unambiguous way and methods to generate the expected test cases. In addition, active methods require deploying a test environment(test architecture) to execute test cases and to observe implementation reactions. They may also interrupt the system normal functioning arbitrarily, for example by resetting it after each test case execution. However,when a system is deployed in an integrated environment,it becomes quite difficult to access it. Moreover,active methods may disturb the natural operation of the implementation under test. So, these ones may not be suitable in regards to the tested system. Passive testing represents another interesting alternative, which offers several advantages, for example, to not disturb the system while testing. In this paper we present a survey covering the main approaches to formal testing. We have divided the survey in three parts. First, we distinguish between active and passive approaches to formal testing. Next, we review some of the work on testing the cloud and on testing in the cloud.},
 author = {Ana Cavalli and Ana Cavalli and Teruo Higashino and Teruo Higashino and Manuel Núñez and Manuel Núñez},
 doi = {10.1007/s12243-015-0457-8},
 journal = {Annales Des Télécommunications},
 mag_id = {1982774961},
 pmcid = {null},
 pmid = {null},
 title = {A survey on formal active and passive testing with applications to the cloud},
 year = {2015}
}

@article{Chittaro_2012,
 abstract = {Exergames (video games that combine exercise and play) could encourage physical activity by making it more enjoyable. Mobile devices are an interesting platform for exergames because they can support outdoors activities such as walking and running. Different mobile exergames have been proposed in the literature, and typically evaluated with informal interviews and ad---hoc questionnaires. The research we present in this paper had two main goals. First, we wanted to design a fun and easy-to-use mobile exergame to encourage walking. To this purpose, we propose a location---based version of the classic Snake mobile game, in which users can control the snake by walking. Second, we wanted to introduce important measures (such as users' attitude towards walking) in the evaluation of exergames, by adopting validated questionnaires employed in the medical literature. The results of the study presented in this paper shed light on how differences in users' lifestyle can be related to exergame enjoyment and to attitude change fostered by the exergame.},
 author = {Luca Chittaro and Luca Chittaro and Riccardo Sioni and Riccardo Sioni},
 doi = {10.1007/978-3-642-31037-9_4},
 journal = {null},
 mag_id = {24961950},
 pmcid = {null},
 pmid = {null},
 title = {Turning the classic snake mobile game into a location---based exergame that encourages walking},
 year = {2012}
}

@article{Chow_2008,
 abstract = {null},
 author = {Tsun S. Chow and Tsun Chow and Dac-Buu Cao and Dac-Buu Cao},
 doi = {10.1016/j.jss.2007.08.020},
 journal = {Journal of Systems and Software},
 mag_id = {2001677911},
 pmcid = {null},
 pmid = {null},
 title = {A survey study of critical success factors in agile software projects},
 year = {2008}
}

@article{Clarke_1996,
 abstract = {article Free Access Share on Formal methods: state of the art and future directions Authors: Edmund M. Clarke Carnegie Mellon Univ. Pittsburgh, PA Carnegie Mellon Univ. Pittsburgh, PAView Profile , Jeannette M. Wing Carnegie Mellon Univ. Pittsburgh, PA Carnegie Mellon Univ. Pittsburgh, PAView Profile Authors Info & Claims ACM Computing SurveysVolume 28Issue 4pp 626–643https://doi.org/10.1145/242223.242257Published:01 December 1996Publication History 743citation9,390DownloadsMetricsTotal Citations743Total Downloads9,390Last 12 Months762Last 6 weeks84 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF},
 author = {E.M. Clarke and Edmund M. Clarke and Jeannette M. Wing and Jeannette M. Wing},
 doi = {10.1145/242223.242257},
 journal = {ACM Computing Surveys},
 mag_id = {4296471858},
 pmcid = {null},
 pmid = {null},
 title = {Formal methods},
 year = {1996}
}

@article{Clarke_2018,
 abstract = {Model checking is a computer-assisted method for the analysis of dynamical systems that can be modeled by state-transition systems. Drawing from research traditions in mathematical logic, programming},
 author = {Edmund M. Clarke and Edmund M. Clarke and Thomas A. Henzinger and Thomas A. Henzinger and Helmut Veith and Helmut Veith and Roderick Bloem and Roderick Bloem},
 doi = {10.1007/978-3-319-10575-8},
 journal = {null},
 mag_id = {2912640545},
 pmcid = {null},
 pmid = {null},
 title = {Handbook of Model Checking},
 year = {2018}
}

@article{Clavel_2019,
 abstract = {A domain-specific modeling language called Healthcare System Specifications (HSS) was proposed for developing clinical pathways. This high-level language was defined as a Unified Modeling Language (UML) profile. It was also proposed a model to model transformation which obtains from pathways HSS specification, a Stochastic Well-formed Net (SWN). The SWN has the capacity to model the complex healthcare system, however, it is hard to perform qualitative and quantitative analysis in this kind of nets. This paper presents a set of relaxations, abstractions and modifications to be applied in the SWNs in order to obtain subclasses of Petri Nets in which formal analysis can be performed. In particular, we obtain the classes of S4 PR and DSSP nets. The first one considers the shared resources of the hospital while the second class allows managing the interchange of information between clinical pathways.},
 author = {Daniel Clavel and Daniel Clavel and Cristian Mahulea and Cristian Mahulea and Manuel Silva and Manuel Silva},
 doi = {10.1109/smc.2019.8914654},
 journal = {null},
 mag_id = {2990484184},
 pmcid = {null},
 pmid = {null},
 title = {From Healthcare System Specifications to Formal Models},
 year = {2019}
}

@article{Cockburn_2000,
 abstract = {How do we determine the need for various software processes or methodologies, and what helps us choose the appropriate one for our project? To answer these questions, we need to get to the bottom of the controversy over methodologies and discover the dimensions along which they vary. This article describes a framework for methodology differentiation, principles for methodology selection, and project experiences using these ideas.},
 author = {Alistair Cockburn and Alistair Cockburn},
 doi = {10.1109/52.854070},
 journal = {IEEE Software},
 mag_id = {1976434045},
 pmcid = {null},
 pmid = {null},
 title = {Selecting a project's methodology},
 year = {2000}
}

@article{Collins_2015,
 abstract = {Prediction models are developed to aid health care providers in estimating the probability or risk that a specific disease or condition is present (diagnostic models) or that a specific event will occur in the future (prognostic models), to inform their decision making. However, the overwhelming evidence shows that the quality of reporting of prediction model studies is poor. Only with full and clear reporting of information on all aspects of a prediction model can risk of bias and potential usefulness of prediction models be adequately assessed. The Transparent Reporting of a multivariable prediction model for Individual Prognosis Or Diagnosis (TRIPOD) Initiative developed a set of recommendations for the reporting of studies developing, validating, or updating a prediction model, whether for diagnostic or prognostic purposes. This article describes how the TRIPOD Statement was developed. An extensive list of items based on a review of the literature was created, which was reduced after a Web-based survey and revised during a 3-day meeting in June 2011 with methodologists, health care professionals, and journal editors. The list was refined during several meetings of the steering group and in e-mail discussions with the wider group of TRIPOD contributors. The resulting TRIPOD Statement is a checklist of 22 items, deemed essential for transparent reporting of a prediction model study. The TRIPOD Statement aims to improve the transparency of the reporting of a prediction model study regardless of the study methods used. The TRIPOD Statement is best used in conjunction with the TRIPOD explanation and elaboration document. To aid the editorial process and readers of prediction model studies, it is recommended that authors include a completed checklist in their submission (also available at www.tripod-statement.org).},
 author = {Gary S. Collins and Gary S. Collins and Johannes B. Reitsma and Johannes B. Reitsma and Johannes B. Reitsma and Johannes B. Reitsma and Douglas G. Altman and Douglas G. Altman and Karel G.M. Moons and Karel G.M. Moons},
 doi = {10.1186/s12916-014-0241-z},
 journal = {BMC Medicine},
 mag_id = {2019694480},
 pmcid = {4284921},
 pmid = {25563062},
 title = {Transparent reporting of a multivariable prediction model for individual prognosis or diagnosis (TRIPOD): the TRIPOD Statement.},
 year = {2015}
}

@article{Colombo_2014,
 abstract = {One of reasons preventing a wider uptake of model-based testing in the industry is the difficulty which is encountered by developers when trying to think in terms of properties rather than linear specifications. A disparity has traditionally been perceived between the language spoken by customers who specify the system and the language required to construct models of that system. The dynamic nature of the specifications for commercial systems further aggravates this problem in that models would need to be rechecked after every specification change. In this paper, we propose an approach for converting specifications written in the commonly-used quasi-natural language Gherkin into models for use with a model-based testing tool. We have instantiated this approach using QuickCheck and demonstrate its applicability via a case study on the eHealth system, the national health portal for Maltese residents. In typical commercial contexts, modern software engineering is characterised by a strong customer focus and, as a result, fluctuating requirements. The industry has developed ways to adapt to this, most notably through Agile development processes [5] which have gained substantial popularity over the past decade. These processes start off by gathering requirements from the customer in the form of case scenarios and subsequently development is built around these scenarios culminating in their implementation and demonstration to the customer. This process, better known as behaviour-driven development [3], reorientates the development process so as to ground all work around business level customer specifications. Once the customer scenarios are implemented, the process tends to take on an iterative form whereby the customer is again asked to review the specification and possibly introduce further scenarios. To keep the specification language as simple as possible for non-technical customers, scenarios are typically specified in terms of a quasi-natural language called Gherkin [8]. In order to automate test execution, clauses from the Gherkin specification are associated with executable code using a technology such as Cucumber [6]. This executable code effectively turns the specification into an automated acceptance test which is used to test that the system adheres to the customer’s specifications. Whilst the automation achieved through the running of Gherkin specifications has significantly shortened the test execution process and made it repeatable, a significant limitation is that the scenarios are never executed in combination with one another: one would typically expect some classes of problems to arise through the interactions between a sequence of scenarios rather than within the context of individual scenarios. In this context, we present an approach to combine related scenarios into models which can be consumed by model-based testing tools. In turn, such tools can be used to explore the model, generating plausible sequences of scenarios that provide more thorough testing of the system for minimal extra effort. We show how this technique has been instantiated on the QuickCheck testing tool and applied to Malta’s national health portal web application. This paper makes three main contributions:},
 author = {Christian Colombo and Christian Colombo and Mark Micallef and Mark Micallef and Mark Scerri and Mark M. Scerri},
 doi = {10.4204/eptcs.141.2},
 journal = {null},
 mag_id = {2046475212},
 pmcid = {null},
 pmid = {null},
 title = {Verifying Web Applications: From Business Level Specifications to Automated Model-Based Testing},
 year = {2014}
}

@article{conf/euromicro/MotognaBM24,
 author = {Motogna S.},
 doi = {10.1109/SEAA64295.2024.00055},
 journal = {SEAA},
 note = {0},
 title = {Artificial Intelligence Methods in Software Refactoring: A Systematic Literature Review.},
 type = {InProceedings},
 year = {2024}
}

@article{conf/ficta/NazimMS21,
 author = {Nazim M.},
 doi = {10.1007/978-981-16-6616-2_11},
 journal = {FICTA},
 note = {0},
 title = {Fuzzy-Based Methods for the Selection and Prioritization of Software Requirements: A Systematic Literature Review.},
 type = {InProceedings},
 year = {2021}
}

@article{conf/fmfun/Zhumagambetov19,
 author = {Zhumagambetov R.},
 doi = {10.1007/978-3-030-71374-4_12},
 journal = {FMFun},
 note = {0},
 title = {Teaching Formal Methods in Academia: A Systematic Literature Review.},
 type = {InProceedings},
 year = {2019}
}

@article{conf/iccsa/HajouBJ14,
 author = {Hajou A.},
 doi = {10.1109/ICCSA.2014.19},
 journal = {ICCSA},
 note = {0},
 title = {How the Pharmaceutical Industry and Agile Software Development Methods Conflict: A Systematic Literature Review.},
 type = {InProceedings},
 year = {2014}
}

@article{conf/se/AletiBGKM14,
 author = {Aleti A.},
 doi = {},
 journal = {Software Engineering},
 note = {0},
 title = {Software Architecture Optimization Methods: A Systematic Literature Review.},
 type = {InProceedings},
 year = {2014}
}

@article{Corbin_1990,
 abstract = {Using grounded theory as an example, this paper examines three methodological questions that are generally applicable to all qualitative methods. How should the usual scientific canons be reinterpreted for qualitative research? How should researchers report the procedures and canons used in their research? What evaluative criteria should be used in judging the research products? We propose that the criteria should be adapted to fit the procedures of the method. We demonstrate how this can be done for grounded theory and suggest criteria for evaluating studies following this approach. We argue that other qualitative researchers might be similarly specific about their procedures and evaluative criteria.},
 author = {Juliet Corbin and Juliet Corbin and Juliet M. Corbin and Anselm L. Strauss and Anselm L. Strauss},
 doi = {10.1007/bf00988593},
 journal = {Zeitschrift Fur Soziologie},
 mag_id = {1607675442},
 pmcid = {null},
 pmid = {null},
 title = {Grounded Theory Research: Procedures, Canons and Evaluative Criteria},
 year = {1990}
}

@article{Cousot_1977,
 abstract = {A program denotes computations in some universe of objects. Abstract interpretation of programs consists in using that denotation to describe computations in another universe of abstract objects, so that the results of abstract execution give some information on the actual computations. An intuitive example (which we borrow from Sintzoff [72]) is the rule of signs. The text -1515 * 17 may be understood to denote computations on the abstract universe {(+), (-), (±)} where the semantics of arithmetic operators is defined by the rule of signs. The abstract execution -1515 * 17 → -(+) * (+) → (-) * (+) → (-), proves that -1515 * 17 is a negative number. Abstract interpretation is concerned by a particular underlying structure of the usual universe of computations (the sign, in our example). It gives a summary of some facets of the actual executions of a program. In general this summary is simple to obtain but inaccurate (e.g. -1515 + 17 → -(+) + (+) → (-) + (+) → (±)). Despite its fundamentally incomplete results abstract interpretation allows the programmer or the compiler to answer questions which do not need full knowledge of program executions or which tolerate an imprecise answer, (e.g. partial correctness proofs of programs ignoring the termination problems, type checking, program optimizations which are not carried in the absence of certainty about their feasibility, …).},
 author = {Patrick Cousot and Patrick Cousot and Radhia Cousot and Radhia Cousot},
 doi = {10.1145/512950.512973},
 journal = {null},
 mag_id = {2043100293},
 pmcid = {null},
 pmid = {null},
 title = {Abstract interpretation: a unified lattice model for static analysis of programs by construction or approximation of fixpoints},
 year = {1977}
}

@article{Crocker_2014,
 abstract = {SPARK offers a way to develop formally-verified software in a language (Ada) that is designed with safety in mind and is further restricted by the SPARK language subset. However, much critical embedded software is developed in C or C++. We look at whether and how benefits similar to those offered by the SPARK language subset and associated tools can be brought to a C++ development environment.},
 author = {D. Crocker and David Crocker},
 doi = {10.1145/2692956.2663179},
 journal = {ACM Sigada Ada Letters},
 mag_id = {2020391228},
 pmcid = {null},
 pmid = {null},
 title = {Can C++ be made as safe as SPARK?},
 year = {2014}
}

@article{Cruzes_2011,
 abstract = {Thematic analysis is an approach that is often used for identifying, analyzing, and reporting patterns (themes) within data in primary qualitative research. 'Thematic synthesis' draws on the principles of thematic analysis and identifies the recurring themes or issues from multiple studies, interprets and explains these themes, and draws conclusions in systematic reviews. This paper conceptualizes the thematic synthesis approach in software engineering as a scientific inquiry involving five steps that parallel those of primary research. The process and outcome associated with each step are described and illustrated with examples from systematic reviews in software engineering.},
 author = {Daniela S. Cruzes and Daniela S. Cruzes and Tore Dybå and Tore Dybå},
 doi = {10.1109/esem.2011.36},
 journal = {null},
 mag_id = {1969939902},
 pmcid = {null},
 pmid = {null},
 title = {Recommended Steps for Thematic Synthesis in Software Engineering},
 year = {2011}
}

@article{Dalkey_1963,
 abstract = {This paper gives an account of an experiment in the use of the so-called DELPHI method, which was devised in order to obtain the most reliable opinion consensus of a group of experts by subjecting them to a series of questionnaires in depth interspersed with controlled opinion feedback.},
 author = {Norman C. Dalkey and Norman C. Dalkey and Olaf Helmer and Olaf Helmer},
 doi = {10.1287/mnsc.9.3.458},
 journal = {Management Science},
 mag_id = {2007367842},
 pmcid = {null},
 pmid = {null},
 title = {An Experimental Application of the Delphi Method to the Use of Experts},
 year = {1963}
}

@article{Davis_1989,
 abstract = {Valid measurement scales for predicting user acceptance of computers are in short supply. Most subjective measures used in practice are unvalidated, and their relationship to system usage is unknown. The present research develops and validates new scales for two specific variables, perceived usefulness and perceived ease of use, which are hypothesized to be fundamental determinants of user acceptance. Definitions of these two variables were used to develop scale items that were pretested for content validity and then tested for reliability and construct validity in two studies involving a total of 152 users and four application programs. The measures were refined and streamlined, resulting in two six-item scales with reliabilities of .98 for usefulness and .94 for ease of use. The scales exhibited hgih convergent, discriminant, and factorial validity. Perceived usefulness was significnatly correlated with both self-reported current usage r = .63, Study 1) and self-predicted future usage r = .85, Study 2). Perceived ease of use was also significantly correlated with current usage r = .45, Study 1) and future usage r = .59, Study 2). In both studies, usefulness had a signficnatly greater correaltion with usage behavior than did ease of use. Regression analyses suggest that perceived ease of use may actually be a causal antecdent to perceived usefulness, as opposed to a parallel, direct determinant of system usage. Implications are drawn for future research on user acceptance.},
 author = {Fred D. Davis and Fred D. Davis},
 doi = {10.2307/249008},
 journal = {Management Information Systems Quarterly},
 mag_id = {1791587663},
 pmcid = {null},
 pmid = {null},
 title = {Perceived usefulness, perceived ease of use, and user acceptance of information technology},
 year = {1989}
}

@article{Daw_2014,
 abstract = {Objective
Software-based devices have increasingly become an important part of several clinical scenarios. Due to their critical impact on human life, medical devices have very strict safety requirements. It is therefore necessary to apply verification methods to ensure that the safety requirements are met. Verification of software-based devices is commonly limited to the verification of their internal elements without considering the interaction that these elements have with other devices as well as the application environment in which they are used. Medical guidelines define clinical procedures, which contain the necessary information to completely verify medical devices. The objective of this work was to incorporate medical guidelines into the verification process in order to increase the reliability of the software-based medical devices.},
 author = {Zamira Daw and Zamira Daw and Zamira Daw and Rance Cleaveland and Rance Cleaveland and Marcus Vetter and Marcus Vetter},
 doi = {10.1007/s11548-013-0919-2},
 journal = {null},
 mag_id = {2006553512},
 pmcid = {null},
 pmid = {23824830},
 title = {Formal verification of software-based medical devices considering medical guidelines.},
 year = {2014}
}

@article{De_2000,
 abstract = {null},
 author = {S. K. De and Supriya Kumar De and Supriya Kumar De and Ranjit Biswas and Ranjit Biswas and Ranjit Biswas and Arijit Roy and Akhil Ranjan Roy},
 doi = {10.1016/s0165-0114(98)00191-2},
 journal = {Fuzzy Sets and Systems},
 mag_id = {2083141276},
 pmcid = {null},
 pmid = {null},
 title = {Some operations on intuitionistic fuzzy sets},
 year = {2000}
}

@article{Dehnert_2015,
 abstract = {We present PROPhESY, a tool for analyzing parametric Markov chains (MCs). It can compute a rational function (i.e., a fraction of two polynomials in the model parameters) for reachability and expected reward objectives. Our tool outperforms state-of-the-art tools and supports the novel feature of conditional probabilities. PROPhESY supports incremental automatic parameter synthesis (using SMT techniques) to determine “safe” and “unsafe” regions of the parameter space. All values in these regions give rise to instantiated MCs satisfying or violating the (conditional) probability or expected reward objective. PROPhESY features a web front-end supporting visualization and user-guided parameter synthesis. Experimental results show that PROPhESY scales to MCs with millions of states and several parameters.
Open image in new window},
 author = {Christian Dehnert and Christian Dehnert and Sebastian Junges and Sebastian Junges and Nils Jansen and Nils Jansen and Nils Jansen and Florian Corzilius and Florian Corzilius and Matthias Volk and Matthias Volk and Harold Bruintjes and Harold Bruintjes and Joost-Pieter Katoen and Joost-Pieter Katoen and Erika Ábrahám and Erika Ábrahám},
 doi = {10.1007/978-3-319-21690-4_13},
 journal = {null},
 mag_id = {754964377},
 pmcid = {null},
 pmid = {null},
 title = {PROPhESY : A PRObabilistic ParamEter SYnthesis Tool},
 year = {2015}
}

@article{Dehnert_2017,
 abstract = {We launch the new probabilistic model checker Storm. It features the analysis of discrete- and continuous-time variants of both Markov chains and MDPs. It supports the Prism and JANI modeling languages, probabilistic programs, dynamic fault trees and generalized stochastic Petri nets. It has a modular set-up in which solvers and symbolic engines can easily be exchanged. It offers a Python API for rapid prototyping by encapsulating Storm’s fast and scalable algorithms. Experiments on a variety of benchmarks show its competitive performance.},
 author = {Christian Dehnert and Christian Dehnert and Sebastian Junges and Sebastian Junges and Joost-Pieter Katoen and Joost-Pieter Katoen and Matthias Volk and Matthias Volk},
 doi = {10.1007/978-3-319-63390-9_31},
 journal = {null},
 mag_id = {2593267253},
 pmcid = {null},
 pmid = {null},
 title = {A Storm is Coming: A Modern Probabilistic Model Checker},
 year = {2017}
}

@article{Deng_2012,
 abstract = {In this issue, “Best of the Web” presents the modified National Institute of Standards and Technology (MNIST) resources, consisting of a collection of handwritten digit images used extensively in optical character recognition and machine learning research.},
 author = {Li Deng and Li Deng},
 doi = {10.1109/msp.2012.2211477},
 journal = {IEEE Signal Processing Magazine},
 mag_id = {2007339694},
 pmcid = {null},
 pmid = {null},
 title = {The MNIST Database of Handwritten Digit Images for Machine Learning Research [Best of the Web]},
 year = {2012}
}

@article{Doostfatemeh_2005,
 abstract = {Automata are the prime example of general computational systems over discrete spaces. The incorporation of fuzzy logic into automata theory resulted in fuzzy auotomata which can handle continuous spaces. Moreover, they are able to model uncertainty which is inherent in many applications. Deterministic Finite-state Automata (DFA) have been the architecture, most used in many applications, but, the increasing interest in using fuzzy logic for many new areas necessitates that the formalism of fuzzy automata be more developed and better established to fulfill implementational requirements in a well-defined manner. This need is due to the fact that despite the long history of fuzzy automata and lots of research being done on that, there are still some issues which have not been well-established and issues which need some kind of revision. In particular, we focus on membership assignment, output mapping, multi-membership resolution, and the concept of acceptance for fuzzy automata. We develop a new general definition for fuzzy automata, and based on that, develop well-defined and application-driven methodologies to establish a better ground for fuzzy automata and pave the way for forthcoming applications.},
 author = {M. Doostfatemeh and Mansoor Doostfatemeh and Stefan C. Kremer and Stefan C. Kremer},
 doi = {10.1016/j.ijar.2004.08.001},
 journal = {International Journal of Approximate Reasoning},
 mag_id = {2017548357},
 pmcid = {null},
 pmid = {null},
 title = {New directions in fuzzy automata},
 year = {2005}
}

@article{Dupuy_2000,
 abstract = {This paper presents an approach and a tool to increase specification quality by using a combination of UML and formal languages. Our approach is based on the expression of the UML class diagram and its annotations into a Z formal specification. Our tool called RoZ supports this approach by making the transition between the UML world and the Z world : from an annotated class diagram, it automatically generates a complete Z specification, the specifications of some elementary operations and some proof obligations to validate the model constraints.},
 author = {Sophie Dupuy and Sophie Dupuy and Yves Ledru and Yves Ledru and Monique Chabre-Peccoud and Monique Chabre-Peccoud},
 doi = {10.1007/3-540-45140-4_28},
 journal = {null},
 mag_id = {1489858443},
 pmcid = {null},
 pmid = {null},
 title = {An Overview of RoZ: A Tool for Integrating UML and Z Specifications},
 year = {2000}
}

@article{Eckner_2012,
 abstract = {We evaluated a clinical “go/no-go” reaction time test (recognition RTclin) that is portable and does not require a computer, and used it to quantify the effect of age on recognition RTclin test scores. Fifty-two healthy adults aged 19 – 83 years completed simple and recognition RTclin testing. Simple RTclin was measured as the elapsed time from initial release of a suspended vertical shaft by the examiner until its arrest by participant pinch grip. Recognition RTclin was similar except that a light on the apparatus randomly illuminated in 50% of the trials to signal the participant to arrest the device. To help interpret the RTclin results we partitioned them into pre-movement time (PMT) and movement time (MT) using an optoelectronic camera system that is not ordinarily part of the RTclin test. Recognition RTclin scores were significantly slower than simple RTclin scores, with 71% of the prolongation attributable to PMT. While simple RTclin test scores correlated with age, recognition RTclin scores did not. A strong negative association between recognition RTclin accuracy and age was found. Recognition RTclin is feasible to measure in healthy adults and appears to represent a portable, computer-independent measure of cognitive processing speed and inhibitory capacity. Potential applications include assessment of brain injury, dementing illness, medication side-effects, fall risk, and driving safety.},
 author = {James T. Eckner and James T. Eckner and James K. Richardson and James K. Richardson and Hogene Kim and Hogene Kim and David B. Lipps and David B. Lipps and James A. Ashton‐Miller and James A. Ashton-Miller},
 doi = {10.1037/a0025042},
 journal = {Psychological Assessment},
 mag_id = {1976158665},
 pmcid = {3643808},
 pmid = {21859222},
 title = {A novel clinical test of recognition reaction time in healthy adults.},
 year = {2012}
}

@article{Elgendi_2013,
 abstract = {The current state-of-the-art in automatic QRS detection methods show high robustness and almost negligible error rates. In return, the methods are usually based on machine-learning approaches that require sufficient computational resources. However, simple-fast methods can also achieve high detection rates. There is a need to develop numerically efficient algorithms to accommodate the new trend towards battery-driven ECG devices and to analyze long-term recorded signals in a time-efficient manner. A typical QRS detection method has been reduced to a basic approach consisting of two moving averages that are calibrated by a knowledge base using only two parameters. In contrast to high-accuracy methods, the proposed method can be easily implemented in a digital filter design.},
 author = {Mohamed Elgendi and Mohamed Elgendi},
 doi = {10.1371/journal.pone.0073557},
 journal = {PLOS ONE},
 mag_id = {2080883481},
 pmcid = {3774726},
 pmid = {24066054},
 title = {Fast QRS Detection with an Optimized Knowledge-Based Method: Evaluation on 11 Standard ECG Databases},
 year = {2013}
}

@article{Everingham_2015,
 abstract = {The Pascal Visual Object Classes (VOC) challenge consists of two components: (i) a publicly available dataset of images together with ground truth annotation and standardised evaluation software; and (ii) an annual competition and workshop. There are five challenges: classification, detection, segmentation, action classification, and person layout. In this paper we provide a review of the challenge from 2008---2012. The paper is intended for two audiences: algorithm designers, researchers who want to see what the state of the art is, as measured by performance on the VOC datasets, along with the limitations and weak points of the current generation of algorithms; and, challenge designers, who want to see what we as organisers have learnt from the process and our recommendations for the organisation of future challenges. To analyse the performance of submitted algorithms on the VOC datasets we introduce a number of novel evaluation methods: a bootstrapping method for determining whether differences in the performance of two algorithms are significant or not; a normalised average precision so that performance can be compared across classes with different proportions of positive instances; a clustering method for visualising the performance across multiple algorithms so that the hard and easy images can be identified; and the use of a joint classifier over the submitted algorithms in order to measure their complementarity and combined performance. We also analyse the community's progress through time using the methods of Hoiem et al. (Proceedings of European Conference on Computer Vision, 2012) to identify the types of occurring errors. We conclude the paper with an appraisal of the aspects of the challenge that worked well, and those that could be improved in future challenges.},
 author = {Mark Everingham and Mark Everingham and S. M. Ali Eslami and S. M. Eslami and Luc Van Gool and Luc Van Gool and Christopher K. I. Williams and Christopher K. I. Williams and Christopher K. I. Williams and Christopher Williams and John Winn and John Winn and Andrew Zisserman and Andrew Zisserman},
 doi = {10.1007/s11263-014-0733-5},
 journal = {International Journal of Computer Vision},
 mag_id = {2037227137},
 pmcid = {null},
 pmid = {null},
 title = {The Pascal Visual Object Classes Challenge: A Retrospective},
 year = {2015}
}

@article{Falessi_2013,
 abstract = {Though very important in software engineering, linking artifacts of the same type (clone detection) or different types (traceability recovery) is extremely tedious, error-prone, and effort-intensive. Past research focused on supporting analysts with techniques based on Natural Language Processing (NLP) to identify candidate links. Because many NLP techniques exist and their performance varies according to context, it is crucial to define and use reliable evaluation procedures. The aim of this paper is to propose a set of seven principles for evaluating the performance of NLP techniques in identifying equivalent requirements. In this paper, we conjecture, and verify, that NLP techniques perform on a given dataset according to both ability and the odds of identifying equivalent requirements correctly. For instance, when the odds of identifying equivalent requirements are very high, then it is reasonable to expect that NLP techniques will result in good performance. Our key idea is to measure this random factor of the specific dataset(s) in use and then adjust the observed performance accordingly. To support the application of the principles we report their practical application to a case study that evaluates the performance of a large number of NLP techniques for identifying equivalent requirements in the context of an Italian company in the defense and aerospace domain. The current application context is the evaluation of NLP techniques to identify equivalent requirements. However, most of the proposed principles seem applicable to evaluating any estimation technique aimed at supporting a binary decision (e.g., equivalent/nonequivalent), with the estimate in the range [0,1] (e.g., the similarity provided by the NLP), when the dataset(s) is used as a benchmark (i.e., testbed), independently of the type of estimator (i.e., requirements text) and of the estimation method (e.g., NLP).},
 author = {Davide Falessi and Davide Falessi and Giovanni Cantone and Giovanni Cantone and Gerardo Canfora and Gerardo Canfora},
 doi = {10.1109/tse.2011.122},
 journal = {IEEE Transactions on Software Engineering},
 mag_id = {1975988951},
 pmcid = {null},
 pmid = {null},
 title = {Empirical Principles and an Industrial Case Study in Retrieving Equivalent Requirements via Natural Language Processing Techniques},
 year = {2013}
}

@article{Fantechi_2016,
 abstract = {Editorial preface for the JLAMP Special Issue on Formal Methods for Software Product Line Engineering},
 author = {Alessandro Fantechi and Maurice H. ter Beek and Maurice H. ter Beek and Dave Clarke and Dave Clarke and Dave Clarke and Dave Clarke and Ina Schaefer and Ina Schaefer},
 doi = {10.1016/j.jlamp.2015.09.006},
 journal = {The Journal of Logic and Algebraic Programming},
 mag_id = {2207887566},
 pmcid = {null},
 pmid = {null},
 title = {Editorial preface for the JLAMP Special Issue on Formal Methods for Software Product Line Engineering},
 year = {2016}
}

@article{Fantechi_2018,
 abstract = {research-article Free Access Share on Guest Editorial for the Special Issue on FORmal methods for the quantitative Evaluation of Collective Adaptive SysTems (FORECAST) Authors: Maurice H. Ter Beek Consiglio Nazionale delle Ricerche (CNR), Istituto di Scienza e Tecnologie dell’Informazione (ISTI), Pisa (PI), Italy Consiglio Nazionale delle Ricerche (CNR), Istituto di Scienza e Tecnologie dell’Informazione (ISTI), Pisa (PI), Italy 0000-0002-2930-6367View Profile , Michele Loreti Università di Camerino, Scuola di Scienze e Tecnologie, Camerino (MC), Italy Università di Camerino, Scuola di Scienze e Tecnologie, Camerino (MC), Italy 0000-0003-3061-863XView Profile Authors Info & Claims ACM Transactions on Modeling and Computer SimulationVolume 28Issue 2April 2018 Article No.: 8pp 1–4https://doi.org/10.1145/3177772Published:22 February 2018Publication History 3citation307DownloadsMetricsTotal Citations3Total Downloads307Last 12 Months33Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF},
 author = {Alessandro Fantechi and Maurice H. ter Beek and Maurice H. ter Beek and Michele Loreti and Michele Loreti},
 doi = {10.1145/3177772},
 journal = {ACM Transactions on Modeling and Computer Simulation},
 mag_id = {2790170367},
 pmcid = {null},
 pmid = {null},
 title = {Guest Editorial for the Special Issue on FORmal methods for the quantitative Evaluation of Collective Adaptive SysTems (FORECAST)},
 year = {2018}
}

@article{Fantechi_2018,
 abstract = {Formal methods and verification tools have been in use in the engineering of safety-critical transport systems for well over 30 years. In both the railway and the avionics domain, for instance, formal methods are specifically recommended in current international certification standards for ultra-dependable systems and for products at the highest integrity level. In fact, traditionally, the applications of formal methods and tools to such transport systems concern demonstrating, with the highest levels of assurance, the correct functioning of the software systems involved, such as train signalling systems to avoid collisions. More recently, however, formal methods and verification tools have started to be applied also to the scheduling and management of transport systems or networks, for instance to optimise the exploitation of a railway line or to improve the operational efficiency of a bus network. In this introduction to the special issue on “Formal Methods for Transport Systems”, we outline some recent achievements for each of the above-mentioned types of application of formal methods and tools. These achievements are represented by three selected papers: one was selected from the “Formal Methods and Safety Certification: Challenges in the Railways Domain” track at the seventh International Symposium On Leveraging Applications of Formal Methods, Verification and Validation (ISoLA 2016); another one was selected from the 21st International Workshop on Formal Methods for Industrial Critical Systems and the 16th International Workshop on Automated Verification of Critical Systems (FMICS-AVoCS 2016); a final one was selected after an open call for contributions.},
 author = {Alessandro Fantechi and Maurice H. ter Beek and Maurice H. ter Beek and Stefania Gnesi and Stefania Gnesi and Alexander Knapp and Alexander Knapp},
 doi = {10.1007/s10009-018-0487-4},
 journal = {International Journal on Software Tools for Technology Transfer},
 mag_id = {2791070386},
 pmcid = {null},
 pmid = {null},
 title = {Formal methods for transport systems},
 year = {2018}
}

@article{Fathabadi_2009,
 abstract = {Atomicity Decomposition is a technique in the Event-B formal method, which augments Event-B refinement with additional structuring in a diagrammatic notation to support complex refinement in Event-B. This paper presents an evaluation of Event-B atomicity decomposition technique in modeling a multi media case study with the diagrammatic notation. Firstly the existing technique and the diagrammatic notation are shown. Secondly an evaluation is performed by developing a model of a Media Channel System. A Media Channel is established between two endpoints for transferring multi-media data. Finally some extensions to the existing diagrammatic notation are proposed and applied to the multi-media case study.},
 author = {Asieh Salehi Fathabadi and Asieh Salehi Fathabadi and Michael Butler and Michael Butler},
 doi = {10.1007/978-3-642-17071-3_5},
 journal = {null},
 mag_id = {2097897694},
 pmcid = {null},
 pmid = {null},
 title = {Applying Event-B atomicity decomposition to a multi media protocol},
 year = {2009}
}

@article{Fathabadi_2015,
 abstract = {Event-B is a formal method for modelling and verifying the consistency of chains of model refinements. The event refinement structure (ERS) approach augments Event-B with a graphical notation which is capable of explicit representation of control flows and refinement relationships. In previous work, the ERS approach has been evaluated manually in the development of two large case studies, a multimedia protocol and a spacecraft sub-system. The evaluation results helped us to extend the ERS constructors, to develop a systematic definition of ERS, and to develop a tool supporting ERS. We propose the ERS language which systematically defines the semantics of the ERS graphical notation including the constructors. The ERS tool supports automatic construction of the Event-B models in terms of control flows and refinement relationships. In this paper we outline the systematic definition of ERS including the presentation of constructors, the tool that supports it and evaluate the contribution that ERS and its tool make. Also we present how the systematic definition of ERS and the corresponding tool can ensure a consistent encoding of the ERS diagrams in the Event-B models.},
 author = {Asieh Salehi Fathabadi and Asieh Salehi Fathabadi and Michael Butler and Michael Butler and Abdolbaghi Rezazadeh and Abdolbaghi Rezazadeh},
 doi = {10.1007/s00165-014-0311-1},
 journal = {Formal Aspects of Computing},
 mag_id = {2044648829},
 pmcid = {null},
 pmid = {null},
 title = {Language and tool support for event refinement structures in Event-B},
 year = {2015}
}

@article{Fayollas_2013,
 abstract = {The deployment of higher interactivity in avionic digital cockpits for critical applications is a challenge today both in terms of software engineering and fault-tolerance. The dependability of the user interface and its related supporting software must be consistent with the criticality of the functions to be controlled. The approach proposed in this paper combines fault prevention and fault-tolerance techniques to address this challenge. Following the ARINC 661 standard, a model-based development of interactive objects namely widgets and layers aims at providing zero-defect software. Regarding remaining software faults in the underlying runtime support and also physical faults, the approach is based on fault tolerance design patterns, like self-checking components and replication techniques. The proposed solution relies on the space and time partitioning provided by the executive support following the ARINC 653 standard. Defining and designing resilient interactive cockpits is a necessity in the near future as these command and control systems provide a great opportunity to improve maintenance, evolvability and usability of avionic systems.},
 author = {Camille Fayollas and Camille Fayollas and Jean-Charles Fabre and Jean-Charles Fabre and Jean-Charles Fabre and Jean-Charles Fabre and Philippe Palanque and Philippe Palanque and Éric Barboni and Eric Barboni and Eric Barboni and David Navarre and David Navarre and Yannick Déléris and Yannick Deleris},
 doi = {10.1504/ijccbs.2013.058407},
 journal = {International Journal of Critical Computer-based Systems},
 mag_id = {1964093798},
 pmcid = {null},
 pmid = {null},
 title = {Interactive cockpits as critical applications: a model-based and a fault-tolerant approach},
 year = {2013}
}

@article{Fayolle_2016,
 abstract = {This paper presents the specification of the hemodialysis case study, proposed by ABZ'16 conference. The specification was carried out by a coupling of Algebraic State-Transition Diagrams astd and B-like methods. astd are a graphical notation, based on automata and process algebra operators. They provide an easy-to-read specification of the dynamic behaviour of the system. The data model is specified using the Event-B language. The system is incrementally designed using extended refinement of both methods.},
 author = {Thomas Fayolle and Thomas Fayolle and Marc Frappier and Marc Frappier and Frédéríc Gervais and Frédéric Gervais and Régine Laleau and Régine Laleau},
 doi = {10.1007/978-3-319-33600-8_33},
 journal = {null},
 mag_id = {2503187831},
 pmcid = {null},
 pmid = {null},
 title = {Modelling a Hemodialysis Machine Using Algebraic State-Transition Diagrams and B-like Methods},
 year = {2016}
}

@article{Ferrari_2013,
 abstract = {null},
 author = {Alessio Ferrari and Alessio Ferrari and Alessandro Fantechi and Alessandro Fantechi and Gianluca Magnani and Gianluca Magnani and Daniele Grasso and Daniele Grasso and Matteo Tempestini and Matteo Tempestini},
 doi = {10.1016/j.scico.2012.04.003},
 journal = {Science of Computer Programming},
 mag_id = {2092137831},
 pmcid = {null},
 pmid = {null},
 title = {The Metrô Rio case study},
 year = {2013}
}

@article{Ferrari_2022,
 abstract = {Formal methods are mathematically based techniques for the rigorous development of software-intensive systems. The railway signaling domain is a field in which formal methods have traditionally been applied, with several success stories. This article reports on a mapping study that surveys the landscape of research on applications of formal methods to the development of railway systems. Following the guidelines of systematic reviews, we identify 328 relevant primary studies, and extract information about their demographics, the characteristics of formal methods used and railway-specific aspects. Our main results are as follows: (i) we identify a total of 328 primary studies relevant to our scope published between 1989 and 2020, of which 44% published during the last 5 years and 24% involving industry; (ii) the majority of studies are evaluated through Examples (41%) and Experience Reports (38%), while full-fledged Case Studies are limited (1.5%); (iii) Model checking is the most commonly adopted technique (47%), followed by simulation (27%) and theorem proving (19.5%); (iv) the dominant languages are UML (18%) and B (15%), while frequently used tools are ProB (9%), NuSMV (8%) and UPPAAL (7%); however, a diverse landscape of languages and tools is employed; (v) the majority of systems are interlocking products (40%), followed by models of high-level control logic (27%); (vi) most of the studies focus on the Architecture (66%) and Detailed Design (45%) development phases. Based on these findings, we highlight current research gaps and expected actions. In particular, the need to focus on more empirically sound research methods, such as Case Studies and Controlled Experiments, and to lower the degree of abstraction, by applying formal methods and tools to development phases that are closer to software development. Our study contributes with an empirically based perspective on the future of research and practice in formal methods applications for railways. It can be used by formal methods researchers to better focus their scientific inquiries, and by railway practitioners for an improved understanding of the interplay between formal methods and their specific application domain.},
 author = {Alessio Ferrari and Alessio Ferrari and Maurice H. ter Beek and Maurice H. ter Beek},
 doi = {10.1145/3520480},
 journal = {ACM Computing Surveys},
 mag_id = {4214928512},
 pmcid = {null},
 pmid = {null},
 title = {Formal Methods in Railways: a Systematic Mapping Study},
 year = {2022}
}

@article{Finney_1998,
 abstract = {null},
 author = {Kate Finney and K.M. Finney and K. Rennolls and Keith Rennolls and Alex Fedorec and Alexander M. Fedorec},
 doi = {10.1016/s0164-1212(98)00003-x},
 journal = {Journal of Systems and Software},
 mag_id = {1983211981},
 pmcid = {null},
 pmid = {null},
 title = {Measuring the comprehensibility of Z specifications},
 year = {1998}
}

@article{Fitzgerald_2013,
 abstract = {The DEPLOY project has provided a rare opportunity to explore and document the potential benefits of and challenges to creating and exploiting usable formal methods. Using the results of an updated review of 98 industrial applications, we identify trends relating to analytic power, robustness, stability and usability of tools, as well as to the quality of evidence on costs and benefits of deployment. A consideration of the DEPLOY applications reinforces these trends, additionally emphasising the importance of selecting formalisms suited to the problem domain and of effectively managing traceable links between requirements and models.},
 author = {JS Fitzgerald and John Fitzgerald and Juan Bicarregui and Juan Bicarregui and Peter Gorm Larsen and Peter Gorm Larsen and Jim Woodcock and Jim Woodcock},
 doi = {10.1007/978-3-642-33170-1_10},
 journal = {null},
 mag_id = {112017717},
 pmcid = {null},
 pmid = {null},
 title = {Industrial Deployment of Formal Methods: Trends and Challenges},
 year = {2013}
}

@article{Fix_1989,
 abstract = {Abstract : The discrimination problem (two population case) may be defined as follows: e random variable Z, of observed value z, is distributed over some space (say, p-dimensional) either according to distribution F, or according to distribution G. The problem is to decide, on the basis of z, which of the two distributions Z has.},
 author = {Evelyn Fix and Evelyn Fix and J. L. Hodges and J. L. Hodges},
 doi = {10.2307/1403797},
 journal = {International Statistical Review},
 mag_id = {1967320885},
 pmcid = {null},
 pmid = {null},
 title = {Discriminatory Analysis - Nonparametric Discrimination: Consistency Properties},
 year = {1989}
}

@article{Fleming_2017,
 abstract = {Computer games are ubiquitous and can be utilised for serious purposes such as health and education. ‘Applied games’ including serious games (in brief, computerised games for serious purposes) and gamification (gaming elements used outside of games) have the potential to increase the impact of mental health internet interventions via three processes. Firstly, by extending the reach of online programs to those who might not otherwise use them. Secondly, by improving engagement through both game-based and ‘serious’ motivational dynamics. Thirdly, by utilising varied mechanisms for change, including therapeutic processes and gaming features. In this scoping review, we aim to advance the field by exploring the potential and opportunities available in this area. We review engagement factors which may be exploited and demonstrate that there is promising evidence of effectiveness for serious games for depression from contemporary systematic reviews. We illustrate six major categories of tested applied games for mental health (exergames, virtual reality, CBT based serious games and gamification, entertainment games, biofeedback and cognitive training games) and demonstrate that it is feasible to translate traditional evidence-based interventions into computer gaming formats and to exploit features of computer games for therapeutic change. Serious games and gamification have considerable potential for increasing the impact of online interventions for mental health. However, there are few independent trials and direct comparisons of game-based and non-game-based interventions are lacking. Further research, faster iterations, rapid testing, non-traditional collaborations, and user-centred approaches are needed to respond to diverse user needs and preferences in rapidly changing environments.},
 author = {Theresa Fleming and Theresa Fleming and Lynda Bavin and Lynda Bavin and Lynda M. Bavin and Karolina Stasiak and Karolina Stasiak and Eve Hermansson-Webb and Eve Hermansson-Webb and Sally Merry and Sally N. Merry and Colleen Cheek and Colleen Cheek and Mathijs Lucassen and Mathijs F. G. Lucassen and Ho Ming Lau and Ho Ming Lau and Pollmuller Britta and Britta Pollmuller and Sarah Hetrick and Sarah E Hetrick},
 doi = {10.3389/fpsyt.2016.00215},
 journal = {Frontiers in Psychiatry},
 mag_id = {2571493223},
 pmcid = {5222787},
 pmid = {28119636},
 title = {Serious Games and Gamification for Mental Health: Current Status and Promising Directions},
 year = {2017}
}

@article{Fukuda_2015,
 abstract = {Afferent and efferent cardiac neurotransmission via the cardiac nerves intricately modulates nearly all physiological functions of the heart (chronotropy, dromotropy, lusitropy, and inotropy). Afferent information from the heart is transmitted to higher levels of the nervous system for processing (intrinsic cardiac nervous system, extracardiac-intrathoracic ganglia, spinal cord, brain stem, and higher centers), which ultimately results in efferent cardiomotor neural impulses (via the sympathetic and parasympathetic nerves). This system forms interacting feedback loops that provide physiological stability for maintaining normal rhythm and life-sustaining circulation. This system also ensures that there is fine-tuned regulation of sympathetic–parasympathetic balance in the heart under normal and stressed states in the short (beat to beat), intermediate (minutes to hours), and long term (days to years). This important neurovisceral/autonomic nervous system also plays a major role in the pathophysiology and progression of heart disease, including heart failure and arrhythmias leading to sudden cardiac death. Transdifferentiation of neurons in heart failure, functional denervation, cardiac and extracardiac neural remodeling has also been identified and characterized during the progression of disease. Recent advances in understanding the cellular and molecular processes governing innervation and the functional control of the myocardium in health and disease provide a rational mechanistic basis for the development of neuraxial therapies for preventing sudden cardiac death and other arrhythmias. Advances in cellular, molecular, and bioengineering realms have underscored the emergence of this area as an important avenue of scientific inquiry and therapeutic intervention.},
 author = {Keiichi Fukuda and Keiichi Fukuda and Hideaki Kanazawa and Hideaki Kanazawa and Yoshifusa Aizawa and Yoshiyasu Aizawa and Jeffrey L. Ardell and Jeffrey L. Ardell and Kalyanam Shivkumar and Kalyanam Shivkumar},
 doi = {10.1161/circresaha.116.304679},
 journal = {Circulation Research},
 mag_id = {1909363255},
 pmcid = {4465108},
 pmid = {26044253},
 title = {Cardiac Innervation and Sudden Cardiac Death},
 year = {2015}
}

@article{Garavel_2020,
 abstract = {Organised to celebrate the 25th anniversary of the FMICS international conference, the present survey addresses 30 questions on the past, present, and future of formal methods in research, industry, and education. Not less than 130 high-profile experts in formal methods (among whom three Turing award winners and many recipients of other prizes and distinctions) accepted to participate in this survey. We analyse their answers and comments, and present a collection of 111 position statements provided by these experts. The survey is both an exercise in collective thinking and a family picture of key actors in formal methods.},
 author = {Hubert Garavel and Hubert Garavel and Hubert Garavel and Hubert Garavel and Alessandro Fantechi and Maurice H. ter Beek and Maurice H. ter Beek and Jaco van de Pol and Jaco van de Pol},
 doi = {null},
 journal = {null},
 mag_id = {3082634688},
 pmcid = {null},
 pmid = {null},
 title = {The 2020 Expert Survey on Formal Methods},
 year = {2020}
}

@article{Gargantini_2001,
 abstract = {This paper tackles some aspects concerning the exploitation of Abstract State Machines (ASMs) for testing purposes. We define for ASM specifications a set of adequacy criteria measuring the coverage achieved by a test suite, and determining whether sufficient testing has been performed. We introduce a method to automatically generate from ASM specifications test sequences which accomplish a desired coverage. This method exploits the counter example generation of the model checker SMV. We use ASMs as test oracles to predict the expected outputs of units under test.},
 author = {Angelo Gargantini and Angelo Gargantini and Elvinia Riccobene and Elvinia Riccobene},
 doi = {10.3217/jucs-007-11-1050},
 journal = {Journal of Universal Computer Science},
 mag_id = {1485458514},
 pmcid = {null},
 pmid = {null},
 title = {ASM-based Testing: Coverage Criteria and Automatic Test Sequence Generation},
 year = {2001}
}

@article{Gargantini_2003,
 abstract = {In this paper we introduce an algorithm to automatically encode an ASM specification in PROMELA, the language of the model checker Spin, and we present a method exploiting the counter example generation feature of Spin, to automatically generate from ASM specifications test sequences which accomplish a desired coverage. ASMs are used as test oracles to predict the expected outputs of units under test. A prototype tool that implements the proposed method is also presented. Experimental results in evaluating the method are reported. The experiments include test sequence generation, tests execution, code coverage measurement for a case study implemented in Java, and comparison with random tests generation. Benefits and limitations in using model checking are discussed.},
 author = {Angelo Gargantini and Angelo Gargantini and Elvinia Riccobene and Elvinia Riccobene and Salvatore Rinzivillo and Salvatore Rinzivillo},
 doi = {10.1007/3-540-36498-6_15},
 journal = {Lecture Notes in Computer Science},
 mag_id = {2057249668},
 pmcid = {null},
 pmid = {null},
 title = {Using spin to generate tests from ASM specifications},
 year = {2003}
}

@article{Gargantini_2008,
 abstract = {In this paper, we present a concrete textual notation, called AsmetaL, and a general-purpose simulation engine, called AsmetaS, for Abstract State Machine (ASM) specifications. They have been developed as part of the ASMETA (ASMs mETAmodelling) toolset, which is a set of tools for ASMs based on the metamod- elling approach of the Model-driven Engineering. We briefly present the ASMETA framework, and we discuss how the language and the simulator have been developed exploiting the advantages offered by the metamodelling approach. We introduce the language AsmetaL used to write ASM specifications, and we provide the AsmetaL encoding of ASM specifications of increasing complexity. We explain the AsmetaS ar- chitecture, its kernel engine, and how the simulator works within the ASMETA tool set. We discuss the features currently supported by the simulator and how it has been validated.},
 author = {Angelo Gargantini and Angelo Gargantini and Elvinia Riccobene and Elvinia Riccobene and Patrizia Scandurra and Patrizia Scandurra},
 doi = {10.3217/jucs-014-12-1949},
 journal = {Journal of Universal Computer Science},
 mag_id = {1537902984},
 pmcid = {null},
 pmid = {null},
 title = {A metamodel-based language and a simulation engine for abstract state machines},
 year = {2008}
}

@article{Gawanmeh_2013,
 abstract = {Formal models are necessary to capture the semantics and behavior of processes of various systems. They characterize and provide insight into the behavior of real systems and thus identify their deterministic and non-deterministic features. The design and deployment of healthcare systems utilize the current technology development in order to improve healthcare services and accommodate the increasing demand on these services while maintaining high quality and error free service. However, healthcare systems lack a formal model that can precisely define specification requirements about their design and operation. This paper introduces a formal axiomatic model for ubiquitous healthcare systems. This formal model precisely defines the formal specification requirements for healthcare systems including functional and security related requirements.},
 author = {Amjad Gawanmeh and Amjad Gawanmeh},
 doi = {10.1109/ccnc.2013.6488581},
 journal = {null},
 mag_id = {1966323821},
 pmcid = {null},
 pmid = {null},
 title = {An axiomatic model for formal specification requirements of ubiquitous healthcare systems},
 year = {2013}
}

@article{Ghorabaee_2015,
 abstract = {Project selection can be a real problem of the multi-criteria group decision making if a group of decision makers express their preferences depending on the nature of the alternatives and different criteria with respect to their knowledge about them. The purpose of the project selection process is to analyze project viability and to approve or reject project proposals based on established criteria. Such decisions are often complex, because they require the identification, consideration and analysis of many tangible and intangible factors. This paper presents a multi-criteria group decision-making approach for project selection problem in the type-2 fuzzy environment. The proposed method is an extended version of Vlsekriterijumska Optimizacija I Kompromisno Resenje (VIKOR) method with interval type-2 fuzzy numbers; it is called type-2 fuzzy VIKOR (T2F-VIKOR). A stepwise procedure is used for ranking and evaluating the alternatives in the developed method, and the best solution is selected considering both the beneficial and nonbeneficial criteria. An illustrative example is presented to show the applicability of the proposed approach in the project selection problems, and the results are analyzed. The results are compared with some existing methods to show the validity of the extended method. We also utilize six sets of criteria weights for analyzing the stability of the proposed method. These analyses show that the obtained results of the proposed method are relatively consistent with other methods and have good stability in different criteria weights.},
 author = {Mehdi Keshavarz Ghorabaee and Mehdi Keshavarz Ghorabaee and Maghsoud Amiri and Maghsoud Amiri and Jamshid Salehi Sadaghiani and Jamshid Salehi Sadaghiani and Edmundas Kazimieras Zavadskas and Edmundas Kazimieras Zavadskas},
 doi = {10.1142/s0219622015500212},
 journal = {International Journal of Information Technology and Decision Making},
 mag_id = {2100333602},
 pmcid = {null},
 pmid = {null},
 title = {Multi-Criteria Project Selection Using an Extended VIKOR Method with Interval Type-2 Fuzzy Sets},
 year = {2015}
}

@article{Gleirscher_2020,
 abstract = {Context: Formal methods (FMs) have been around for a while, still being unclear how to leverage their benefits, overcome their challenges, and set new directions for their improvement towards a more successful transfer into practice. Objective: We study the use of formal methods in mission-critical software domains, examining industrial and academic views. Method: We perform a cross-sectional on-line survey. Results: Our results indicate an increased intent to apply FMs in industry, suggesting a positively perceived usefulness. But the results also indicate a negatively perceived ease of use. Scalability, skills, and education seem to be among the key challenges to support this intent. Conclusions: We present the largest study of this kind so far (N = 216), and our observations provide valuable insights, highlighting directions for future theoretical and empirical research of formal methods. Our findings are strongly coherent with earlier observations by Austin and Parkin (1993).},
 author = {Mario Gleirscher and Mario Gleirscher and Mario Gleirscher and Diego Marmsoler and Diego Marmsoler},
 doi = {null},
 journal = {arXiv: Software Engineering},
 mag_id = {3088608579},
 pmcid = {null},
 pmid = {null},
 title = {Formal Methods in Dependable Systems Engineering: A Survey of Professionals from Europe and North America},
 year = {2020}
}

@article{Green_1996,
 abstract = {Abstract   The cognitive dimensions framework is a broad-brush evaluation technique for interactive devices and for non-interactive notations. It sets out a small vocabulary of terms designed to capture the cognitively-relevant aspects of structure, and shows how they can be traded off against each other. The purpose of this paper is to propose the framework as an evaluation technique for visual programming environments. We apply it to two commercially-available dataflow languages (with further examples from other systems) and conclude that it is effective and insightful; other HCI-based evaluation techniques focus on different aspects and would make good complements. Insofar as the examples we used are representative, current VPLs are successful in achieving a good ‘closeness of match’, but designers need to consider the ‘viscosity ’ (resistance to local change) and the ‘secondary notation’ (possibility of conveying extra meaning by choice of layout, colour, etc.).},
 author = {Thomas R. G. Green and Thomas R. G. Green and Marian Petre and Marian Petre},
 doi = {10.1006/jvlc.1996.0009},
 journal = {Journal of Visual Languages and Computing},
 mag_id = {2064189581},
 pmcid = {null},
 pmid = {null},
 title = {Usability Analysis of Visual Programming Environments: A 'Cognitive Dimensions' Framework},
 year = {1996}
}

@article{Grov_2012,
 abstract = {Refinement is a powerful technique for tackling the complexities that arise when formally modelling systems. Here we focus on a posit-and-prove style of refinement, and specifically where a user requires guidance in order to overcome a failed refinement step. We take an integrated approach --- combining the complementary strengths of top-down planning and bottom-up theory formation. In this paper we focus mainly on the planning perspective. Specifically, we propose a new technique called refinement plans which combines both modelling and reasoning perspectives. When a refinement step fails, refinement plans provide a basis for automatically generating modelling guidance by abstracting away from the details of low-level proof failures. The refinement plans described here are currently being implemented for the Event-B modelling formalism, and have been assessed on paper using case studies drawn from the literature. Longer-term, our aim is to identify refinement plans that are applicable to a range of modelling formalisms.},
 author = {Gudmund Grov and Gudmund Grov and Andrew Ireland and Andrew Ireland and Maria Teresa Llano and Maria Teresa Llano},
 doi = {10.1007/978-3-642-30885-7_15},
 journal = {null},
 mag_id = {152184050},
 pmcid = {null},
 pmid = {null},
 title = {Refinement plans for informed formal design},
 year = {2012}
}

@article{Güdemann_2017,
 abstract = {The use of formal methods in industrial critical systems has a lot of potential to increase the quality and reliability of these systems. Three of the main challenges of the application of formal methods in industrial systems are scalability, their often inherent complexity and the integration into existing development processes. Due to this, their application is limited mainly to where usage is highly recommended by domain-specific standards, e.g., in light rail and avionics. The contributions in this special issue address some of these core problems of using formal methods in industry. The articles are based on papers selected from the 2015 Fmics workshop, an annual forum organized by the Ercim working group dedicated to formal methods for industrial critical systems.},
 author = {Matthias Güdemann and Matthias Güdemann and Matthias Güdemann and Manuel Núñez and Manuel Núñez},
 doi = {10.1007/s10009-017-0455-4},
 journal = {International Journal on Software Tools for Technology Transfer},
 mag_id = {2604184402},
 pmcid = {null},
 pmid = {null},
 title = {Preface of the special issue on formal methods in industrial critical systems},
 year = {2017}
}

@article{Hadjem_2015,
 abstract = {Various wearable devices are foreseen to be the key components in the future for vital signs monitoring as they offer a non-invasive, remote and real-time medical monitoring means. Among those, Wireless Body Sensors (WBS) for cardiac monitoring are of prominent help to early detect CardioVascular Diseases (CVD) by analyzing 24/24 and 7/7 collected cardiac data. Today, most of these WBS systems for CVD detection, include only limited automatic anomalies detection, particularly regarding ECG anomalies. Severe CVD, such as Myocardial Infarction or Ischemia, needs to achieve an advanced analysis of ECG waves known as P, Q, R, S and T. In particular, the T-wave and its specific changes. In this paper, we focus on T-wave anomalies detection in a context of WBS. Our study suggests an accurate and lightweight T-wave changes detection model which suits well an ECG monitoring system based on WBS architecture. We performed a comparative study of 7 well-known supervised learning classification models, on real ECG data sets from 7 different leads. We compared the results from both perspectives of classification and processing times. Our results show that the C4.5 Decision Tree technique performs better results with 92.54% Accuracy, 96.06% Sensibility, 55.41% Specificity and 7.41% Error Rate.},
 author = {Medina Hadjem and Medina Hadjem and Farid Naït‐Abdesselam and Farid Nait-Abdesselam},
 doi = {10.1109/iccw.2015.7247191},
 journal = {null},
 mag_id = {1563058454},
 pmcid = {null},
 pmid = {null},
 title = {An ECG T-wave anomalies detection using a lightweight classification model for wireless body sensors},
 year = {2015}
}

@article{Hahn_2019,
 abstract = {Quantitative formal models capture probabilistic behaviour, real-time aspects, or general continuous dynamics. A number of tools support their automatic analysis with respect to dependability or performance properties. QComp 2019 is the first, friendly competition among such tools. It focuses on stochastic formalisms from Markov chains to probabilistic timed automata specified in the Jani model exchange format, and on probabilistic reachability, expected-reward, and steady-state properties. QComp draws its benchmarks from the new Quantitative Verification Benchmark Set. Participating tools, which include probabilistic model checkers and planners as well as simulation-based tools, are evaluated in terms of performance, versatility, and usability. In this paper, we report on the challenges in setting up a quantitative verification competition, present the results of QComp 2019, summarise the lessons learned, and provide an outlook on the features of the next edition of QComp.},
 author = {Ernst Moritz Hahn and Ernst Moritz Hahn and Arnd Hartmanns and Arnd Hartmanns and Christian Hensel and Christian Hensel and Michaela Klauck and Michaela Klauck and Joachim Klein and Joachim Klein and Jan Křetínský and Jan Kretínský and David Parker and David Parker and Tim Quatmann and Tim Quatmann and Enno Jozef Johannes Ruijters and Enno Jozef Johannes Ruijters and Marcel Steinmetz and Marcel Steinmetz},
 doi = {10.1007/978-3-030-17502-3_5},
 journal = {null},
 mag_id = {2925395957},
 pmcid = {null},
 pmid = {null},
 title = {The 2019 Comparison of Tools for the Analysis of Quantitative Formal Models: (QComp 2019 Competition Report)},
 year = {2019}
}

@article{Hansen_2006,
 abstract = {Derived from the concept of self-adaptation in evolution strategies, the CMA (Covariance Matrix Adaptation) adapts the covariance matrix of a multi-variate normal search distribution. The CMA was originally designed to perform well with small populations. In this review, the argument starts out with large population sizes, reflecting recent extensions of the CMA algorithm. Commonalities and differences to continuous Estimation of Distribution Algorithms are analyzed. The aspects of reliability of the estimation, overall step size control, and independence from the coordinate system (invariance) become particularly important in small populations sizes. Consequently, performing the adaptation task with small populations is more intricate.},
 author = {Nikolaus Hansen and Nikolaus Hansen},
 doi = {10.1007/3-540-32494-1_4},
 journal = {null},
 mag_id = {102487131},
 pmcid = {null},
 pmid = {null},
 title = {The CMA Evolution Strategy: A Comparing Review},
 year = {2006}
}

@article{Hansson_1990,
 abstract = {We present a logic for stating properties such as, "after a request for service there is at least a 98\045 probability that the service will be carried out within 2 seconds". The logic extends the temporal logic CTL by Emerson, Clarke and Sistla with time and probabil- ities. Formulas are interpreted over discrete time Markov chains. We give algorithms for checking that a given Markov chain satis- fies a formula in the logic. The algorithms require a polynomial number of arithmetic operations, in size of both the formula and\003This research report is a revised and extended version of a paper that has appeared under the title "A Framework for Reasoning about Time and Reliability" in the Proceeding of the 10thIEEE Real-time Systems Symposium, Santa Monica CA, December 1989. This work was partially supported by the Swedish Board for Technical Development (STU) as part of Esprit BRA Project SPEC, and by the Swedish Telecommunication Administration.1the Markov chain. A simple example is included to illustrate the algorithms.},
 author = {Hans Hansson and Hans Hansson and Bengt Jönsson and Bengt Jonsson},
 doi = {10.1007/bf01211866},
 journal = {Formal Aspects of Computing},
 mag_id = {2005998857},
 pmcid = {null},
 pmid = {null},
 title = {A logic for reasoning about time and reability},
 year = {1990}
}

@article{Harel_1987,
 abstract = {We present a broad extension of the conventional formalism of state machines and state diagrams, that is relevant to the specification and design of complex discrete-event systems, such as multi-computer real-time systems, communication protocols and digital control units. Our diagrams, which we call statecharts, extend conventional state-transition diagrams with essentially three elements, dealing, respectively, with the notions of hierarchy, concurrency and communica- tion. These transform the language of state diagrams into a highly structured and economical description language. Statecharts are thus compact and expressiv-small diagrams can express complex behavior-as well as compositional and modular. When coupled with the capabilities of computerized graphics, statecharts enable viewing the description at different levels of detail, and make even very large specifications manageable and comprehensible. In fact, we intend to demonstrate here that statecharts counter many of the objections raised against conventional state diagrams, and thus appear to render specification by diagrams an attractive and plausible approach. Statecharts can be used either as a stand-alone behavioral description or as part of a more general design methodology that deals also with the system's other aspects, such as functional decomposi- tion and data-flow specification. We also discuss some practical experience that was gained over the last three years in applying the statechart formalism to the specification of a particularly complex system.},
 author = {David Harel and David Harel},
 doi = {10.1016/0167-6423(87)90035-9},
 journal = {Science of Computer Programming},
 mag_id = {2099529102},
 pmcid = {null},
 pmid = {null},
 title = {Statecharts: A visual formalism for complex systems},
 year = {1987}
}

@article{Harrison_2019,
 abstract = {Abstract   The paper is concerned with the practical use of formal techniques to contribute to the risk analysis of a new neonatal dialysis machine. The described formal analysis focuses on the controller component of the software implementation. The controller drives the dialysis cycle and deals with error management. The logic was analysed using model checking techniques and the source code was analysed formally, checking type correctness conditions, use of pointers and shared memory. The analysis provided evidence of the verification of risk control measures relating to the software component. The productive dialogue between the developers of the device, who had no experience or knowledge of formal methods, and the analyst using the formal analysis tools, provided a basis for the development of rationale for the effectiveness of the evidence.},
 author = {Michael D. Harrison and Michael D. Harrison and Leo Freitas and Leo Freitas and Michael Drinnan and Michael Drinnan and José Creissac Campos and José Creissac Campos and Paolo Masci and Paolo Masci and Costanzo Di Maria and Costanzo Di Maria and Michael Whitaker and Michael Whitaker},
 doi = {10.1016/j.scico.2019.02.003},
 journal = {Science of Computer Programming},
 mag_id = {2913028917},
 pmcid = {null},
 pmid = {null},
 title = {Formal Techniques in the Safety Analysis of Software Components of a new Dialysis Machine},
 year = {2019}
}

@article{Harvey_2012,
 abstract = {Harvey N, Holmes CA. International Journal of Nursing Practice 2012; 18: 188–194

Nominal group technique: An effective method for obtaining group consensus

This paper aims to demonstrate the versatility and application of nominal group technique as a method for generating priority information. Nominal group technique was used in the context of four focus groups involving clinical experts from the emergency department (ED) and obstetric and midwifery areas of a busy regional hospital to assess the triage and management of pregnant women in the ED. The data generated were used to create a priority list of discussion triggers for the subsequent Participatory Action Research Group. This technique proved to be a productive and efficient data collection method which produced information in a hierarchy of perceived importance and identified real world problems. This information was vital in initiating the participatory action research project and is recommended as an effective and reliable data collection method, especially when undertaking research with clinical experts.},
 author = {Nichole Harvey and Nichole Harvey and Colin Holmes and Colin Holmes},
 doi = {10.1111/j.1440-172x.2012.02017.x},
 journal = {International Journal of Nursing Practice},
 mag_id = {1918280324},
 pmcid = {null},
 pmid = {22435983},
 title = {Nominal group technique: an effective method for obtaining group consensus.},
 year = {2012}
}

@article{Hassan_2012,
 abstract = {Advanced computing and sensing capabilities of smartphones provide new opportunities for personal indoor positioning. A particular trend is to employ human activity recognition for autonomous calibration of pedestrian dead reckoning systems thereby achieving accurate indoor positioning even in the absence of any positioning infrastructure. The basic idea is that the activity context, such as switching from a walking to a stair climbing activity gives clues about pedestrian's current position. In this paper, we have made a first attempt in developing a performance model for such systems. For an unbiased random walk, we have obtained two interesting results in closed-form expressions. First, we have demonstrated that the distance a pedestrian is expected to travel before the PDR is recalibrated is reciprocal of the density of activity switching points (ASPs) in the indoor environment. The implication of this finding is that the continuous unaided use of PDR can be curbed drastically by identifying more ASPs in a given environmental setting. Second, we have shown that false negatives of the activity detection algorithms do not have a major impact as long as they are within a reasonable range of 0–30%. The system performance however degrades rapidly if false negatives continue to grow beyond 30%.},
 author = {Mahbub Hassan and Mahbub Hassan},
 doi = {10.1109/icon.2012.6506535},
 journal = {null},
 mag_id = {2024720822},
 pmcid = {null},
 pmid = {null},
 title = {A performance model of pedestrian dead reckoning with activity-based location updates},
 year = {2012}
}

@article{Heerink_1997,
 abstract = {This paper presents a testing theory that is parameterised with assumptions about the way implementations communicate with their environment. In this way some existing testing theories, such as refusal testing for labelled transition systems and (repetitive) quiescence testing for I/O automata, can be unified in a single framework. Starting point is the theory of refusal testing. We apply this theory to classes of implementations which communicate with their environment via clearly distinguishable input and output actions. These classes are induced by making assumptions about the geographical distribution of the points of control and observation (PCO’s) and about the way input actions of implementations are enabled. For specific instances of these classes our theory collapses with some well-known ones. For all these classes a single test generation algorithm is presented that is able to derive sound and complete test suites from a specification.},
 author = {Lex Heerink and Lex Heerink and Jan Tretmans and Jan Tretmans},
 doi = {10.1007/978-0-387-35271-8_2},
 journal = {null},
 mag_id = {1498203526},
 pmcid = {null},
 pmid = {null},
 title = {Refusal Testing for Classes of Transition Systems with Inputs and Outputs},
 year = {1997}
}

@article{Hevner_1993,
 abstract = {Box structures provide a rigorous and systematic process for performing systems development with objects. Box structures represent data abstractions as objects in three system views and combine the advantages of structured development with the advantages of object orientation. As data abstractions become more complex, the box structure usage hierarchy allows stepwise refinement of the system design with referential transparency and verification at every step. An integrated development environment based on box structures supports flexible object-based systems development patterns. We present a classic example of object-based systems development using box structures.},
 author = {Alan R. Hevner and Alan R. Hevner and Harlan D. Mills and Harlan D. Mills},
 doi = {10.1147/sj.322.0232},
 journal = {Ibm Systems Journal},
 mag_id = {2028323525},
 pmcid = {null},
 pmid = {null},
 title = {Box-structured methods for systems development with objects},
 year = {1993}
}

@article{Hierons_2000,
 abstract = {Software Testing, Verification and ReliabilityVolume 10, Issue 4 p. 201-202 EditorialFree Access Editorial: special issue on specification-based testing Rob Hierons, Rob Hierons rob.hierons@brunel.ac.uk Brunel University, U.K.Search for more papers by this authorJohn Derrick, John Derrick J.Derrick@ukc.ac.uk University of Kent at Canterbury, U.K.Search for more papers by this author Rob Hierons, Rob Hierons rob.hierons@brunel.ac.uk Brunel University, U.K.Search for more papers by this authorJohn Derrick, John Derrick J.Derrick@ukc.ac.uk University of Kent at Canterbury, U.K.Search for more papers by this author First published: 12 January 2001 https://doi.org/10.1002/1099-1689(200012)10:4<201::AID-STVR214>3.0.CO;2-ZCitations: 12AboutPDF ToolsRequest permissionExport citationAdd to favoritesTrack citation ShareShare Give accessShare full text accessShare full-text accessPlease review our Terms and Conditions of Use and check box below to share full-text version of article.I have read and accept the Wiley Online Library Terms and Conditions of UseShareable LinkUse the link below to share a full-text version of this article with your friends and colleagues. Learn more.Copy URL Share a linkShare onFacebookTwitterLinkedInRedditWechat No abstract is available for this article. References 1Gaudel M-C. Testing Can Be Formal, Too. Lecture Notes in Computer Science, vol. 915, Springer-Verlag, 1995; 82– 96. Citing Literature Volume10, Issue4Special Issue: Specification-based TestingDecember 2000Pages 201-202 ReferencesRelatedInformation},
 author = {Robert M. Hierons and Robert M. Hierons and John Derrick and John Derrick},
 doi = {10.1002/1099-1689(200012)10:4<201::aid-stvr214>3.0.co;2-z},
 journal = {Software Testing, Verification & Reliability},
 mag_id = {2022870016},
 pmcid = {null},
 pmid = {null},
 title = {Editorial: special issue on specification‐based testing},
 year = {2000}
}

@article{Hierons_2008,
 abstract = {In the distributed test architecture, a system with multiple ports is tested using a tester at each port/interface, these testers cannot communicate with one another and there is no global clock. Recent work has defined an implementation relation for testing against an input-output transition system in the distributed test architecture. However, this framework placed no restrictions on the test cases and, in particular, allowed them to produce some kind of nondeterminism. In addition, it did not consider the test generation problem. This paper explores the class of controllable test cases for the distributed test architecture, defining a new implementation relation and a test generation algorithm.},
 author = {Robert M. Hierons and Robert M. Hierons and Mercedes G. Merayo and Mercedes G. Merayo and Manuel Núñez and Manuel Núñez},
 doi = {10.1007/978-3-540-88387-6_16},
 journal = {null},
 mag_id = {1589870140},
 pmcid = {null},
 pmid = {null},
 title = {Controllable Test Cases for the Distributed Test Architecture},
 year = {2008}
}

@article{Hierons_2017,
 abstract = {Abstract   In passive testing a monitor observes the trace (sequence of inputs and outputs) of the system under test (SUT) and checks that this trace satisfies a given property  P , potentially triggering a response if an incorrect behaviour is observed. Recent work has explored a variant of passive testing, in which we have a required property  P  of the traces of the SUT and there is a first-in-first-out (FIFO) network between the SUT and the monitor. The problem here is that the trace observed by the monitor need not be that produced by the SUT. Previous work has shown how such asynchronous passive testing can be performed if the property  P  is defined by a pair    (  ρ  ,    O    ρ    )    that represents the requirement that if trace  ρ  is produced by the SUT then the next output must be from the set      O    ρ     . This paper generalises the previous work to the case where the property  P  is defined by a finite automaton.},
 author = {Robert M. Hierons and Robert M. Hierons and Mercedes G. Merayo and Mercedes G. Merayo and Manuel Núñez and Manuel Núñez},
 doi = {10.1016/j.jlamp.2016.02.004},
 journal = {The Journal of Logic and Algebraic Programming},
 mag_id = {2325039465},
 pmcid = {null},
 pmid = {null},
 title = {An extended framework for passive asynchronous testing},
 year = {2017}
}

@article{Hierons_2017,
 abstract = {Research partially supported by the MINECO/FEDER project DArDOS (TIN2015- 65845-C3-1-R) and the Comunidad de Madrid project SICOMORo-CM (S2013/ICE-3006).},
 author = {Robert M. Hierons and Robert M. Hierons and Manuel Núñez and Manuel Núñez},
 doi = {10.1016/j.jss.2017.03.011},
 journal = {Journal of Systems and Software},
 mag_id = {2603261448},
 pmcid = {null},
 pmid = {null},
 title = {Implementation relations and probabilistic schedulers in the distributed test architecture},
 year = {2017}
}

@article{Hinchey_1999,
 abstract = {1 It's Greek to Me: Method in the Madness?.- 2 The French Population Census for 1990.- 3 The Formal Verification of a Payment System.- 4 Specification of a Chemical Process Controller in B.- 5 Formal Analysis of the Motorola CAP DSP.- 6 Bridging the E-Business Gap Through Formal Verification.- 7 A CAD Environment for Safety-Critical Software.- 8 Scheduling and Rescheduling of Trains.- 9 Lessons from the Formal Development of a Radiation Therapy Machine Control Program.- 10 Using Formal Methods to Develop an ATC Information System.- 11 Rigorous Review Technique.- 12 Analysing Z Specifications with Z/EVES.- 13 How to Construct Formal Arguments that Persuade Certifiers.- 14 Formal Methods Through Domain Engineering.- 15 Formal Verification in Railways.- 16 Cleanroom Software Engineering: Theory and Practice.- References.},
 author = {Michael G. Hinchey and Mike Hinchey and Jonathan P. Bowen and Jonathan P. Bowen},
 doi = {null},
 journal = {null},
 mag_id = {1566925011},
 pmcid = {null},
 pmid = {null},
 title = {Industrial-Strength Formal Methods in Practice},
 year = {1999}
}

@article{Holtkamp_2015,
 abstract = {null},
 author = {P Holtkamp and Philipp Holtkamp and Jussi Jokinen and Jussi P. P. Jokinen and Jan Μ. Pawlowski and Jan M. Pawlowski},
 doi = {10.1016/j.jss.2014.12.010},
 journal = {Journal of Systems and Software},
 mag_id = {2030220885},
 pmcid = {null},
 pmid = {null},
 title = {Soft competency requirements in requirements engineering, software design, implementation, and testing},
 year = {2015}
}

@article{Holzinger_2005,
 abstract = {The human-computer interaction community aims to increase the awareness and acceptance of established methods among software practitioners. Indeed, awareness of the basic usability methods will drive an Information Society for all.},
 author = {Andreas Holzinger and Andreas Holzinger},
 doi = {10.1145/1039539.1039541},
 journal = {Communications of The ACM},
 mag_id = {2075949054},
 pmcid = {null},
 pmid = {null},
 title = {Usability engineering methods for software developers},
 year = {2005}
}

@article{Hopcroft_1979,
 abstract = {null},
 author = {John E. Hopcroft and Rajeev Motwani and Rotwani and Jeffrey D. Ullman},
 doi = {null},
 journal = {null},
 mag_id = {2002089154},
 pmcid = {null},
 pmid = {null},
 title = {Introduction to Automata Theory, Languages, and Computation},
 year = {1979}
}

@article{Hopcroft_2005,
 abstract = {In this paper, we combine the Box Structure Development Method (BSDM) [H.D. Mills, R.C. Linger, and A.R. Hevner. Principles of Information Systems Analysis and Design. Academic Press, 1986, S.J. Prowell, C.J. Trammell, R.C. Linger, and J.H. Poore. Cleanroom Software Engineering - Technology and Process. Addison-Wesley, 1998] and CSP [C.A.R. Hoare. Communicating Sequential Processes. Prentice Hall, 1985, A.W. Roscoe. The Theory and Practice of Concurrency. Prentice Hall, 1998], integrating them into industrial software development processes. BSDM was developed with practical software projects in mind and provides a framework for developing formal design specifications that are fully traceable to the informal requirements. It integrates well into an industrial setting and forms an ideal bridge between the actual system being developed and the abstract models used for formal analysis. CSP complements BSDM by providing the mathematical framework for formal verification, together with its model checker FDR. In this paper, we present generic algorithms for translating specifications from BSDM into CSP, illustrate how they can be formally verified using FDR and summarise an industrial case-study. to. The Program Committee consisted of Farhad Arbab (CWI, Amsterdam); Jean-Jacques Levy (Inria Roquencourt); Ugo Montanari (University of Pisa, co-chair); Antonio Porto (Universidade Nova De Lisboa); Vladimiro Sassone (University of Sussex and University of Catania, co-chair) and Bjorn Victor (Uppsala University). The Organizing Committee of ConCoord consisted of Alfredo Ferro (University of Catania), Ugo Montanari and Vladimiro Sassone. The papers in this volume were reviewed by the program committee members and by Chiara Bodei, Roberto Bruni, Michele Bugliesi, Luis Caires, Alessandro Fantechi, Kohei Honda, Francesca Levi, Massimo Merro, Giuseppe Milicia, Ana Moreira, Uwe Nestmann, Rosario Pugliese, Davide Sangiorgi, Francesca Scozzari, Alan Schmitt and Laura Semini. This volume will appear in the series Electronic Notes in Theoretical Computer Science (ENTCS), a series published electronically through the facilities of Elsevier Science B.V. and its auspices. The volumes in the ENTCS series can be accessed at the URL http://www.elsevier.nl/locate/entcs July 2001 Ugo Montanari and Vladimiro Sassone},
 author = {Philippa J. Hopcroft and Philippa J. Hopcroft and Guy H. Broadfoot and Guy Hampson Broadfoot},
 doi = {10.1016/j.entcs.2005.04.008},
 journal = {Electronic Notes in Theoretical Computer Science},
 mag_id = {1967535316},
 pmcid = {null},
 pmid = {null},
 title = {Combining the Box Structure Development Method and CSP for Software Development},
 year = {2005}
}

@article{Hornbæk_2006,
 abstract = {null},
 author = {Kasper Hornbæk and Kasper Hornbæk},
 doi = {10.1016/j.ijhcs.2005.06.002},
 journal = {International Journal of Human-computer Studies \/ International Journal of Man-machine Studies},
 mag_id = {2101623580},
 pmcid = {null},
 pmid = {null},
 title = {Current practice in measuring usability: Challenges to usability studies and research},
 year = {2006}
}

@article{Huang_2014,
 abstract = {Taiwan is stepping into the aging society. Most of the elders have multiple chronic illnesses, and they use drugs to stabilize their health status. Pharmacists Association urged the family should be more concerned on medication safety of the elders. Thus, this paper designs an intelligent pill box and its back-end monitoring system. The implemented pill box can remind the elders to take medicine in time and can inform the families remotely when the elders take the medicine. The safety design of this pill box can prevent the drugs abusing. The caregivers can easily schedule the time for the elders to take medicine.},
 author = {Shih-Chang Huang and Shih-Chang Huang and Hong-Yi Chang and Hong-Yi Chang and Yu-Chen Jhu and Yu-Chen Jhu and Guan-You Chen and Guan-You Chen},
 doi = {10.1109/icce-tw.2014.6904076},
 journal = {null},
 mag_id = {2077616649},
 pmcid = {null},
 pmid = {null},
 title = {The intelligent pill box — Design and implementation},
 year = {2014}
}

@article{Hwang_2010,
 abstract = {Introduction   Usability evaluation is essential to make sure that software products newly released are easy to use, efficient, and effective to reach goals, and satisfactory to users. For example, when a software company wants to develop and sell a new product, the company needs to evaluate usability of the new product before launching it at a market to avoid the possibility that the new product may contain usability problems, which span from cosmetic problems to severe functional problems.   Three widely used methods for usability evaluation are Think Aloud (TA), Heuristic Evaluation (HE) and Cognitive Walkthrough (CW). TA method is commonly employed with a lab-based user testing, while there are variants of TA methods, including thinking out aloud at user's workplace instead of at labs. What we discuss here is the TA method that is combined with a lab-based user testing, in which test users use products while simultaneously and continuously thinking out aloud, and experimenters record users' behaviors and verbal protocols in the laboratory. HE is a usability inspection method, in which a small number of evaluators find usability problems in a user interface design by examining an interface and judging its compliance with well-known usability principles, called heuristics. CW is a theory-based method, in which evaluators evaluate every step necessary to perform a scenario-based task, and look for usability problems that would interfere with learning by exploration. These three methods have their own advantages and disadvantages. For instance, TA method provides good qualitative data from a small number of test users, but laboratory environment may influence test user's behaviors. HE is a cheap, fast and easy-to-use method, while it often finds too specific and low-priority usability problems, including even not real problems. CW helps find mismatches between users' and designers' conceptualization of a task, but it needs extensive knowledge of cognitive psychology and technical details to apply.   However, even though these advantages and disadvantages show overall characteristics of three major usability evaluation methods, we cannot compare them quantitatively and see their efficiency clearly. Because one of reasons why so-called discounted methods, such as HE and CW, were developed is to save costs of usability evaluation, cost-related criteria for comparing usability evaluation are meaningful to usability practitioners as well as usability researchers. One of the most disputable issues related to cost of usability evaluation is sample size. That is, how many users or evaluators are needed to achieve a targeted usability evaluation performance, for example, 80% of overall discovery rate? The sample size of usability evaluation is known to depend on an estimate of problem discovery rate across participants. The overall discovery rate is a common quantitative measure that is used to show the effectiveness of a specific usability evaluation method in most of usability evaluation studies. It is also called overall detection rate or thoroughness measure, which is the ratio of 'the sum of unique usability problems detected by all experiment participants' against 'the number of usability problems that exist in the evaluated systems', ranging between 0 and 1. The overall discovery rates were reported more than any other criterion measure in the usability evaluation experiments and also a key component for projecting required sample size for usability evaluation study. Thus, how many test users or evaluators participate in the usability evaluation is a critical issue, considering its cost-effectiveness.},
 author = {Wonil Hwang and Wonil Hwang and Gavriel Salvendy and Gavriel Salvendy},
 doi = {10.1145/1735223.1735255},
 journal = {Communications of The ACM},
 mag_id = {2016022053},
 pmcid = {null},
 pmid = {null},
 title = {Number of people required for usability evaluation: the 10±2 rule},
 year = {2010}
}

@article{Ikhu-Omoregbe_2008,
 abstract = {Adverse drug effects are a major cause of death in the world with tens of thousand deaths occurring each year because of medication or prescription errors. Many errors involve the prescription or administration of the wrong drug or dosage by care givers to patients due to illegible handwriting, dosage mistakes, confusing drug names. With the use of mobile devices such as personal digital assistants and smart phones some of these errors could be eliminated because they allow prescription information to be captured and viewed in type rather than handwriting. This paper presents a formal modelling, and design of a prescription application to improve health care services. This could lead to costs and life savings in healthcare centres across the world especially in developing countries where treatment processes are usually paper based.},
 author = {Nicholas Ikhu-Omoregbe and Nicholas A Ikhu-Omoregbe},
 doi = {null},
 journal = {Journal of Health Informatics in Developing Countries},
 mag_id = {1588397369},
 pmcid = {null},
 pmid = {null},
 title = {Formal modelling and design of mobile prescription applications},
 year = {2008}
}

@article{Jalal_2017,
 abstract = {Increase in number of elderly people who are living independently needs especial care in the form of healthcare monitoring systems. Recent advancements in depth video technologies have made human activity recognition (HAR) realizable for elderly healthcare applications. In this paper, a depth video-based novel method for HAR is presented using robust multi-features and embedded Hidden Markov Models (HMMs) to recognize daily life activities of elderly people living alone in indoor environment such as smart homes. In the proposed HAR framework, initially, depth maps are analyzed by temporal motion identification method to segment human silhouettes from noisy background and compute depth silhouette area for each activity to track human movements in a scene. Several representative features, including invariant, multi-view differentiation and spatiotemporal body joints features were fused together to explore gradient orientation change, intensity differentiation, temporal variation and local motion of specific body parts. Then, these features are processed by the dynamics of their respective class and learned, modeled, trained and recognized with specific embedded HMM having active feature values. Furthermore, we construct a new online human activity dataset by a depth sensor to evaluate the proposed features. Our experiments on three depth datasets demonstrated that the proposed multi-features are efficient and robust over the state of the art features for human action and activity recognition.},
 author = {Ahmad Jalal and Ahmad Jalal and Shaharyar Kamal and Shaharyar Kamal and Shaharyar Kamal and Daijin Kim and Daijin Kim},
 doi = {10.9781/ijimai.2017.447},
 journal = {International Journal of Interactive Multimedia and Artificial Intelligence},
 mag_id = {2568838907},
 pmcid = {null},
 pmid = {null},
 title = {A Depth Video-based Human Detection and Activity Recognition using Multi-features and Embedded Hidden Markov Models for Health Care Monitoring Systems},
 year = {2017}
}

@article{Jee_2010,
 abstract = {Guaranteeing timing properties is an important issue as we develop safety-critical real-time systems such as cardiac pacemakers. We present a safety assured development approach of real-time software using a pacemaker as our case study. Following the model-driven development techniques, measurement-based timing analysis is used to guarantee timing properties in implementation as well as in the formal model. Formal specification with timed automata is checked with respect to timing properties by model checking technique and is transformed into implementation systematically. When timing properties may be violated in the implementation due to timing delay, it is suggested to measure the time deviation and reflect it to the code explicitly by modifying guards. The model is altered according to the modifications in the code. These changes of the code and the model are considered safe if all the properties are still satisfied by the modified model in re-performed model checking. We demonstrate how the suggested approach can be applied to single-threaded and multi-threaded versions of implementation. This approach can provide developers with a useful time-guaranteeing technique applicable to several code generation schemes without imposing many restrictions.},
 author = {Eunkyoung Jee and Eunkyoung Jee and Shaohui Wang and Shaohui Wang and Jeong Ki Kim and Jeong Ki Kim and Jae Woo Lee and Jaewoo Lee and Jaewoo Lee and Oleg Sokolsky and Oleg Sokolsky and Insup Lee and Insup Lee},
 doi = {10.1109/rtcsa.2010.42},
 journal = {null},
 mag_id = {1985931450},
 pmcid = {null},
 pmid = {null},
 title = {A Safety-Assured Development Approach for Real-Time Software},
 year = {2010}
}

@article{Jetley_2006,
 abstract = {Formal methods have long been proposed as an effective technique for developing safety-critical software. However, few medical device manufacturers employ such methods. One way to encourage the use of formal methods is to leverage these techniques in such a way as to enhance the review process for regulatory bodies, such as the U.S. Food and Drug Administration. In this paper we explore a method for carrying out pre-market analysis of software designs and implementations, based on formal-methods techniques, to aid the process of reviewing software in medical devices. We discuss a methodology to validate medical device software with the help of formal methods based usage models, and test cases derived from such models. We present a case study involving the design and verification of a generic infusion pump usage model, detailing how the various stages of our approach can be carried out. Finally, we present experimental results and effort estimates to show that the proposed methodology is effective and feasible.},
 author = {Raoul Jetley and Raoul Jetley and S. Purushothaman Iyer and S.P. Iyer and Paul Jones and Paul Jones and William Spees and William Spees},
 doi = {10.1109/compsac.2006.9},
 journal = {null},
 mag_id = {2155520797},
 pmcid = {null},
 pmid = {null},
 title = {A Formal Approach to Pre-Market Review for Medical Device Software},
 year = {2006}
}

@article{Jiang_2010,
 abstract = {We present the design of an integrated modeling platform to investigate efficient methodologies for certifying medical device software. The outcome of this research has the potential to expedite medical device software certification for safer operation. Our specific focus in this study is on our ongoing research in artificial pacemaker software. Designing bug-free medical device software is difficult, especially in complex implantable devices that may be used in unanticipated contexts. In the 20-year period from 1985 to 2005, the US Food and Drug Administration's (FDA) Maude database records almost 30,000 deaths and almost 600,000 injuries from device failures [1]. There is currently no formal methodology or open experimental platform to validate and verify the correct operation of medical device software. To this effect, a real-time Virtual Heart Model (VHM) has been developed to model the electrophysiological operation of the functioning (i.e. during normal sinus rhythm) and malfunctioning (i.e. during arrhythmia) heart. We present a methodology to extract timing properties of the heart to construct a timed-automata model. The platform exposes functional and formal interfaces for validation and verification of implantable cardiac devices. We demonstrate the VHM is capable of generating clinically-relevant response to intrinsic (i.e. premature stimuli) and external (i.e. artificial pacemaker) signals for a variety of common arrhythmias. By connecting the VHM with a pacemaker model, we are able to pace and synchronize the heart during the onset of irregular heart rhythms. The VHM has been implemented on a hardware platform for closed-loop experimentation with existing and virtual medical devices.},
 author = {Zhihao Jiang and Zhihao Jiang and Miroslav Pajić and Miroslav Pajic and Allison T. Connolly and Allison Connolly and Allison T. Connolly and Sanjay Dixit and Sanjay Dixit and Rahul Mangharam and Rahul Mangharam},
 doi = {10.1145/1921081.1921115},
 journal = {null},
 mag_id = {2065372507},
 pmcid = {null},
 pmid = {null},
 title = {A platform for implantable medical device validation: demo abstract},
 year = {2010}
}

@article{Jiang_2010,
 abstract = {Designing bug-free medical device software is challenging, especially in complex implantable devices that may be used in unanticipated contexts. Safety recalls of pacemakers and implantable cardioverter defibrillators due to firmware problems between 1990 and 2000 affected over 200, 000 devices. This encompasses 41% of the devices recalled and continues to increase in frequency. There is currently no formal methodology or open experimental platform to validate and verify the correct operation of medical device software. To this effect, a real-time Virtual Heart Model (VHM) has been developed to model the electrophysiological operation of the functioning (i.e. during normal sinus rhythm) and malfunctioning (i.e. during arrhythmia) heart. We present a methodology to construct a timed-automata model by extracting timing properties of the heart. The platform employs functional and formal interfaces for validation and verification of implantable cardiac devices. We demonstrate the VHM is capable of generating clinically-relevant response to intrinsic (i.e. premature stimuli) and external (i.e. artificial pacemaker) signals for a variety of common arrhythmias. By connecting the VHM with a pacemaker model, we are able to pace and synchronize the heart during the onset of irregular heart rhythms. The VHM has also been implemented on a hardware platform for closed-loop experimentation with existing and virtual medical devices. This integrated functional and formal device design approach has potential to help expedite medical device certification for safe operation.},
 author = {Zhihao Jiang and Zhihao Jiang and Miroslav Pajić and Miroslav Pajic and Allison T. Connolly and Allison T. Connolly and Sanjay Dixit and Sanjay Dixit and Rahul Mangharam and Rahul Mangharam},
 doi = {10.1109/ecrts.2010.36},
 journal = {null},
 mag_id = {2101185304},
 pmcid = {null},
 pmid = {null},
 title = {Real-Time Heart Model for Implantable Cardiac Device Validation and Verification},
 year = {2010}
}

@article{Jiang_2012,
 abstract = {The design of bug-free and safe medical device software is challenging, especially in complex implantable devices that control and actuate organs in unanticipated contexts. Safety recalls of pacemakers and implantable cardioverter defibrillators between 1990 and 2000 affected over 600 000 devices. Of these, 200 000 or 41% were due to firmware issues and their effect continues to increase in frequency. There is currently no formal methodology or open experimental platform to test and verify the correct operation of medical device software within the closed-loop context of the patient. To this effect, a real-time virtual heart model (VHM) has been developed to model the electrophysiological operation of the functioning and malfunctioning (i.e., during arrhythmia) heart. By extracting the timing properties of the heart and pacemaker device, we present a methodology to construct a timed-automata model for functional and formal testing and verification of the closed-loop system. The VHM's capability of generating clinically relevant response has been validated for a variety of common arrhythmias. Based on a set of requirements, we describe a closed-loop testing environment that allows for interactive and physiologically relevant model-based test generation for basic pacemaker device operations such as maintaining the heart rate, atrial-ventricle synchrony, and complex conditions such as pacemaker-mediated tachycardia. This system is a step toward a testing and verification approach for medical cyber-physical systems with the patient in the loop.},
 author = {Zhihao Jiang and Zhihao Jiang and Miroslav Pajić and Miroslav Pajic and Rahul Mangharam and Rahul Mangharam},
 doi = {10.1109/jproc.2011.2161241},
 journal = {null},
 mag_id = {2081324807},
 pmcid = {null},
 pmid = {null},
 title = {Cyber–Physical Modeling of Implantable Cardiac Medical Devices},
 year = {2012}
}

@article{Jiang_2014,
 abstract = {The design and implementation of software for medical devices is challenging due to the closed-loop interaction with the patient, which is a stochastic physical environment. The safety-critical nature and the lack of existing industry standards for verification make this an ideal domain for exploring applications of formal modeling and closed-loop analysis. The biggest challenge is that the environment model(s) have to be both complex enough to express the physiological requirements and general enough to cover all possible inputs to the device. In this effort, we use a dual chamber implantable pacemaker as a case study to demonstrate verification of software specifications of medical devices as timed-automata models in UPPAAL. The pacemaker model is based on the specifications and algorithm descriptions from Boston Scientific. The heart is modeled using timed automata based on the physiology of heart. The model is gradually abstracted with timed simulation to preserve properties. A manual Counter-Example-Guided Abstraction and Refinement (CEGAR) framework has been adapted to refine the heart model when spurious counter-examples are found. To demonstrate the closed-loop nature of the problem and heart model refinement, we investigated two clinical cases of Pacemaker Mediated Tachycardia and verified their corresponding correction algorithms in the pacemaker. Along with our tools for code generation from UPPAAL models, this effort enables model-driven design and certification of software for medical devices.},
 author = {Zhihao Jiang and Zhihao Jiang and Miroslav Pajić and Miroslav Pajic and Rajeev Alur and Rajeev Alur and Rahul Mangharam and Rahul Mangharam},
 doi = {10.1007/s10009-013-0289-7},
 journal = {International Journal on Software Tools for Technology Transfer},
 mag_id = {2053398545},
 pmcid = {null},
 pmid = {null},
 title = {Closed-loop verification of medical devices with model abstraction and refinement},
 year = {2014}
}

@article{Jones_1986,
 abstract = {Logic of propositions reasoning about predicates functions and operations set notation composite objects and invariants map notation sequence notation data rectification more on data types operation decomposition a small case study.},
 author = {Cli B. Jones and Cliff B. Jones},
 doi = {null},
 journal = {null},
 mag_id = {1970495662},
 pmcid = {null},
 pmid = {null},
 title = {Systematic software development using VDM},
 year = {1986}
}

@article{Jordan_2006,
 abstract = {IEC 62304, if accepted, requires what reputable medical device manufacturers are already doing. The author therefore expect them to welcome it. It would remove the obligation to describe their processes in detail in regulatory submissions to the FDA. With all its limitations, the author commends IEC 62304. There are still medical device manufacturers whose software development processes are quite rudimentary, and this threatens the lives of patients. IEC 62304 will prevent this and provide a more level playing field for all medical device manufacturers. There appears to be no appetite among either medical device manufacturers or regulators for a more extensive standard prescribing methods and techniques. Holding out for more at this stage would ensure that we get nothing. IEC 62304 also takes a big step towards simpler and more uniform expectations on both sides of the Atlantic.},
 author = {Peter Jordan and Peter Jordan},
 doi = {10.1049/ic:20060141},
 journal = {null},
 mag_id = {2037889871},
 pmcid = {null},
 pmid = {null},
 title = {Standard IEC 62304 - Medical device software - Software lifecycle processes},
 year = {2006}
}

@article{journals/eaai/PachoulyAKSA22,
 author = {Pachouly J.},
 doi = {10.1016/J.ENGAPPAI.2022.104773},
 journal = {Eng. Appl. Artif. Intell.},
 note = {0},
 title = {A systematic literature review on software defect prediction using artificial intelligence: Datasets, Data Validation Methods, Approaches, and Tools.},
 type = {Journal},
 year = {2022}
}

@article{journals/jss/MatalongaRT17,
 author = {Matalonga S.},
 doi = {10.1016/J.JSS.2017.05.048},
 journal = {J. Syst. Softw.},
 note = {0},
 title = {Characterizing testing methods for context-aware software systems: Results from a quasi-systematic literature review.},
 type = {Journal},
 year = {2017}
}

@article{journals/jss/ZarourADA15,
 author = {Zarour M.},
 doi = {10.1016/J.JSS.2014.11.041},
 journal = {J. Syst. Softw.},
 note = {0},
 title = {An investigation into the best practices for the successful design and implementation of lightweight software process assessment methods: A systematic literature review.},
 type = {Journal},
 year = {2015}
}

@article{journals/smr/AliG19,
 author = {Ali A.},
 doi = {10.1002/SMR.2211},
 journal = {J. Softw. Evol. Process.},
 note = {0},
 title = {A systematic literature review of software effort prediction using machine learning methods.},
 type = {Journal},
 year = {2019}
}

@article{journals/smr/BonfantiGM18,
 author = {Bonfanti S.},
 doi = {10.1002/SMR.1943},
 journal = {J. Softw. Evol. Process.},
 note = {0},
 title = {A systematic literature review of the use of formal methods in medical software systems.},
 type = {Journal},
 year = {2018}
}

@article{journals/tse/AletiBGKM13,
 author = {Aleti A.},
 doi = {10.1109/TSE.2012.64},
 journal = {IEEE Trans. Software Eng.},
 note = {0},
 title = {Software Architecture Optimization Methods: A Systematic Literature Review.},
 type = {Journal},
 year = {2013}
}

@article{journals/tse/EdisonWC22,
 author = {Edison H.},
 doi = {10.1109/TSE.2021.3069039},
 journal = {IEEE Trans. Software Eng.},
 note = {0},
 title = {Comparing Methods for Large-Scale Agile Software Development: A Systematic Literature Review.},
 type = {Journal},
 year = {2022}
}

@article{Kairy_2009,
 abstract = {Purpose. To identify clinical outcomes, clinical process, healthcare utilization and costs associated with telerehabilitation for individuals with physical disabilities.Method. Relevant databases were searched for articles on telerehabilitation published until February 2007. Reference lists were examined and key journals were hand searched. Studies that included telerehabilitation for individuals with physical impairments and used experimental or observational study designs were included in the analysis, regardless of the specific clientele or location of services. Data was extracted using a form to record methodological aspects and results relating to clinical, process, healthcare utilization and cost outcomes. Study quality of randomized clinical trials was assessed using the PEDro rating scale.Results. Some 28 articles were analysed. These dealt with rehabilitation of individuals in the community, neurological rehabilitation, cardiac rehabilitation, follow-up of individuals with spinal cord injuries, r...},
 author = {Dahlia Kairy and Dahlia Kairy and Dahlia Kairy and Pascale Lehoux and Pascale Lehoux and Claude Vincent and Claude Vincent and Martha Visintin and Martha Visintin},
 doi = {10.1080/09638280802062553},
 journal = {Disability and Rehabilitation},
 mag_id = {1990262988},
 pmcid = {null},
 pmid = {18720118},
 title = {A systematic review of clinical outcomes, clinical process, healthcare utilization and costs associated with telerehabilitation},
 year = {2009}
}

@article{Keeney_2006,
 abstract = {The aim of this paper was to provide insight into the Delphi technique by outlining our personal experiences during its use over a 10-year period in a variety of applications. As a means of achieving consensus on an issue, the Delphi research method has become widely used in healthcare research generally and nursing research in particular. The literature on this technique is expanding, mainly addressing what it is and how it should be used. However, there is still much confusion and uncertainty surrounding it, particularly about issues such as modifications, consensus, anonymity, definition of experts, how `experts' are selected and how non-respondents are pursued. This issues that arise when planning and carrying out a Delphi study include the definition of consensus; the issue of anonymity vs. quasi-anonymity for participants; how to estimate the time needed to collect the data, analyse each `round', feed back results to participants, and gain their responses to this feedback; how to define and select the `experts' who will be asked to participate; how to enhance response rates; and how many `rounds' to conduct. Many challenges and questions are raised when using the Delphi technique, but there is no doubt that it is an important method for achieving consensus on issues where none previously existed. Researchers need to adapt the method to suit their particular study.},
 author = {Sinead Keeney and Sinead Keeney and Felicity Hasson and Felicity Hasson and Hugh McKenna and Hugh McKenna},
 doi = {10.1111/j.1365-2648.2006.03716.x},
 journal = {Journal of Advanced Nursing},
 mag_id = {1967766053},
 pmcid = {null},
 pmid = {16422719},
 title = {Consulting the oracle: ten lessons from using the Delphi technique in nursing research.},
 year = {2006}
}

@article{Keil_2013,
 abstract = {null},
 author = {Mark Keil and Mark Keil and Hyung Koo Lee and Hyung Koo Lee and Tianjie Deng and Tianjie Deng},
 doi = {10.1016/j.im.2013.05.005},
 journal = {Information & Management},
 mag_id = {1978374817},
 pmcid = {null},
 pmid = {null},
 title = {Understanding the most critical skills for managing IT projects: A Delphi study of IT project managers},
 year = {2013}
}

@article{Kemmerer_1985,
 abstract = {Formal specification and verification techniques are now apused to increase the reliability of software systems. However, these proaches sometimes result in specifying systems that cannot be realized or that are not usable. This paper demonstrates why it is necessary to test specifications early in the software life cycle to guarantee a system that meets its critical requirements and that also provides the desired functionality. Definitions to provide the framework for classifying the validity of a functional requirement with respect to a formal specification tion are also introduced. Finally, the design of two tools for testing formal specifications is discussed.},
 author = {Richard A. Kemmerer and R.A. Kemmerer and Richard A. Kemmerer},
 doi = {10.1109/tse.1985.231535},
 journal = {IEEE Transactions on Software Engineering},
 mag_id = {2096536569},
 pmcid = {null},
 pmid = {null},
 title = {Testing Formal Specifications to Detect Design Errors},
 year = {1985}
}

@article{Kim_2010,
 abstract = {In principle, activity recognition can be exploited to great societal benefits, especially in real-life, human centric applications such as elder care and healthcare. This article focused on recognizing simple human activities. Recognizing complex activities remains a challenging and active area of research and the nature of human activities poses different challenges. Human activity understanding encompasses activity recognition and activity pattern discovery. The first focuses on accurate detection of human activities based on a predefined activity model. An activity pattern discovery researcher builds a pervasive system first and then analyzes the sensor data to discover activity patterns.},
 author = {Eunju Kim and Eunju Kim and Sumi Helal and Sumi Helal and Diane J. Cook and Diane J. Cook},
 doi = {10.1109/mprv.2010.7},
 journal = {IEEE Pervasive Computing},
 mag_id = {2103388129},
 pmcid = {3023457},
 pmid = {21258659},
 title = {Human Activity Recognition and Pattern Discovery},
 year = {2010}
}

@article{Klein_2010,
 abstract = {We report on the formal, machine-checked verification of the seL4 microkernel from an abstract specification down to its C implementation. We assume correctness of compiler, assembly code, hardware, and boot code. seL4 is a third-generation microkernel of L4 provenance, comprising 8700 lines of C and 600 lines of assembler. Its performance is comparable to other high-performance L4 kernels. We prove that the implementation always strictly follows our high-level abstract specification of kernel behavior. This encompasses traditional design and implementation safety properties such as that the kernel will never crash, and it will never perform an unsafe operation. It also implies much more: we can predict precisely how the kernel will behave in every possible situation.},
 author = {Gerwin Klein and Gerwin Klein and June Andronick and June Andronick and Kevin Elphinstone and Kevin Elphinstone and Gernot Heiser and Gernot Heiser and David Cock and David Cock and Philip Derrin and Philip Derrin and Dhammika Elkaduwe and Dhammika Elkaduwe and Kai Engelhardt and Kai Engelhardt and Rafal Kolanski and Rafal Kolanski and Michael Norrish and Michael Norrish and Thomas Sewell and Thomas Sewell and Harvey Tuch and Harvey Tuch and Simon Winwood and Simon Winwood},
 doi = {10.1145/1743546.1743574},
 journal = {Communications of The ACM},
 mag_id = {2076409494},
 pmcid = {null},
 pmid = {null},
 title = {seL4: formal verification of an operating-system kernel},
 year = {2010}
}

@article{Ko_2015,
 abstract = {Empirical studies, often in the form of controlled experiments, have been widely adopted in software engineering research as a way to evaluate the merits of new software engineering tools. However, controlled experiments involving human participants actually using new tools are still rare, and when they are conducted, some have serious validity concerns. Recent research has also shown that many software engineering researchers view this form of tool evaluation as too risky and too difficult to conduct, as they might ultimately lead to inconclusive or negative results. In this paper, we aim both to help researchers minimize the risks of this form of tool evaluation, and to increase their quality, by offering practical methodological guidance on designing and running controlled experiments with developers. Our guidance fills gaps in the empirical literature by explaining, from a practical perspective, options in the recruitment and selection of human participants, informed consent, experimental procedures, demographic measurements, group assignment, training, the selecting and design of tasks, the measurement of common outcome variables such as success and time on task, and study debriefing. Throughout, we situate this guidance in the results of a new systematic review of the tool evaluations that were published in over 1,700 software engineering papers published from 2001 to 2011.},
 author = {Andrew J. Ko and Andrew J. Ko and Thomas D. LaToza and Thomas D. LaToza and Margaret Burnett and Margaret Burnett},
 doi = {10.1007/s10664-013-9279-3},
 journal = {Empirical Software Engineering},
 mag_id = {2084429940},
 pmcid = {null},
 pmid = {null},
 title = {A practical guide to controlled experiments of software engineering tools with human participants},
 year = {2015}
}

@article{Kojuri_2015,
 abstract = {Background:  Myocardial infarction remains one the leading causes of mortality and morbidity and involves a high cost of care. Early prediction can be helpful in preventing the development of myocardial infarction with appropriate diagnosis and treatment. Artificial neural networks have opened new horizons in learning about the natural history of diseases and predicting cardiac disease.  Methods:  A total of 935 cardiac patients with chest pain and nondiagnostic electrocardiogram (ECG) were enrolled and followed for 2 weeks in two groups based on the appearance of myocardial infarction. Two types of data were used for all patients: nominal (clinical data) and quantitative (ECG findings). Two different artificial neural networks – radial basis function (RBF) and multi-layer perceptron (MLP) – were used to classify the two groups.  Results:  The RBF neural network had an accuracy of 83% with ECG findings and an accuracy of 78% with clinical features. When and clinical data were used in an MLP neural network trained with a genetic algorithm, ECG results led to a classification accuracy of 96% and clinical data yielded an accuracy of 84.5%.  Conclusion:  Both neural network structures predicted MI within about 2 weeks of hospital referral with an acceptable degree of accuracy in patients with nondiagnostic ECG. The MLP neural network significantly outperformed the RBF network because of the use of the genetic algorithm, which provided a global strategy to accurately determine MLP weights (clinical trials registry: NCT01870258).   Key words:  Artificial neural networks, Electrocardiography, Myocardial infarction, Multi-layer perceptron (MLP), Radial basis function (RBF).},
 author = {Javad Kojuri and Javad Kojuri and Reza Boostani and Reza Boostani and Pooyan Dehghani and Pooyan Dehghani and Farzad Nowroozipour and Farzad Nowroozipour and Nasrin Saki and Nasrin Saki and Nasrin Saki},
 doi = {10.5530/jcdr.2015.2.2},
 journal = {Journal of cardiovascular disease research},
 mag_id = {1757870343},
 pmcid = {null},
 pmid = {null},
 title = {Prediction of acute myocardial infarction with artificial neural networks in patients with nondiagnostic electrocardiogram},
 year = {2015}
}

@article{Kordon_2014,
 abstract = {Benchmarking numerous programs in a reasonable time requires the use of several (potentially multicore) computers. We experimented such a situation in the context of the MCC (Model Checking Contest @ Petri net) where we had to operate more than 52000 runs for the 2013 edition. This paper presents BenchKit, a tool to operate programs on sets of potentially parallel machines and to gather monitoring information like CPU or memory usage. It also samples such data over the execution time. BenchKit has been elaborated in the context of the MCC and will be used for the 2014 edition.},
 author = {Fabrice Kordon and Fabrice Kordon and Francis Hulin-Hubard and Francis Hulin-Hubard},
 doi = {10.1109/acsd.2014.12},
 journal = {null},
 mag_id = {1997221584},
 pmcid = {null},
 pmid = {null},
 title = {BenchKit, a Tool for Massive Concurrent Benchmarking},
 year = {2014}
}

@article{Kosko_1986,
 abstract = {null},
 author = {Bart Kosko and Bart Kosko},
 doi = {10.1016/s0020-7373(86)80040-2},
 journal = {International Journal of Human-computer Studies \/ International Journal of Man-machine Studies},
 mag_id = {2095224843},
 pmcid = {null},
 pmid = {null},
 title = {Fuzzy cognitive maps},
 year = {1986}
}

@article{Krishnamurthi_2013,
 abstract = {Software and other digital artifacts are amongst the most valuable contributions of computer science. Yet our conferences treat these mostly as second-class artifacts--especially conferences in the software sciences, which ought to know better. This article argues for elevating these other artifacts by making them part of the evaluation process for papers, and reports on experience from an iteration of an Artifact Evaluation Committee for ESEC/FSE 2011.},
 author = {Shriram Krishnamurthi and Shriram Krishnamurthi},
 doi = {10.1145/2464526.2464530},
 journal = {ACM Sigsoft Software Engineering Notes},
 mag_id = {2020505539},
 pmcid = {null},
 pmid = {null},
 title = {Artifact evaluation for software conferences},
 year = {2013}
}

@article{Krishnamurthi_2015,
 abstract = {Sharing experiences running artifact evaluation committees for five major conferences.},
 author = {Shriram Krishnamurthi and Shriram Krishnamurthi and Jan Vítek and Jan Vitek and Jan Vitek},
 doi = {10.1145/2658987},
 journal = {Communications of The ACM},
 mag_id = {2032593675},
 pmcid = {null},
 pmid = {null},
 title = {The real software crisis: repeatability as a core value},
 year = {2015}
}

@article{Kwiatkowska_2007,
 abstract = {This tutorial presents an overview of model checking for both discrete and continuous-time Markov chains (DTMCs and CTMCs). Model checking algorithms are given for verifying DTMCs and CTMCs against specifications written in probabilistic extensions of temporal logic, including quantitative properties with rewards. Example properties include the probability that a fault occurs and the expected number of faults in a given time period. We also describe the practical application of stochastic model checking with the probabilistic model checker PRISM by outlining the main features supported by PRISM and three real-world case studies: a probabilistic security protocol, dynamic power management and a biological pathway.},
 author = {Marta Kwiatkowska and Marta Kwiatkowska and Gethin Norman and Gethin Norman and David Parker and David Parker},
 doi = {10.1007/978-3-540-72522-0_6},
 journal = {null},
 mag_id = {1516256348},
 pmcid = {null},
 pmid = {null},
 title = {Stochastic model checking},
 year = {2007}
}

@article{Kwiatkowska_2011,
 abstract = {This paper describes a major new release of the PRISMprobabilistic model checker, adding, in particular, quantitative verification of (priced) probabilistic timed automata. These model systems exhibiting probabilistic, nondeterministic and real-time characteristics. In many application domains, all three aspects are essential; this includes, for example, embedded controllers in automotive or avionic systems, wireless communication protocols such as Bluetooth or Zigbee, and randomised security protocols. PRISM, which is open-source, also contains several new components that are of independent use. These include: an extensible toolkit for building, verifying and refining abstractions of probabilistic models; an explicit-state probabilistic model checking library; a discrete-event simulation engine for statistical model checking; support for generation of optimal adversaries/strategies; and a benchmark suite.},
 author = {Marta Kwiatkowska and Marta Kwiatkowska and Gethin Norman and Gethin Norman and David Parker and David Parker},
 doi = {10.1007/978-3-642-22110-1_47},
 journal = {null},
 mag_id = {1862398452},
 pmcid = {null},
 pmid = {null},
 title = {PRISM 4.0: verification of probabilistic real-time systems},
 year = {2011}
}

@article{König_2015,
 abstract = {Over the last few years, the use of new technologies for the support of elderly people and in particular dementia patients received increasing interest. We investigated the use of a video monitoring system for automatic event recognition for the assessment of instrumental activities of daily living (IADL) in dementia patients. Participants (19 healthy subjects (HC) and 19 mild cognitive impairment (MCI) patients) had to carry out a standardized scenario consisting of several IADLs such as making a phone call while they were recorded by 2D video cameras. After the recording session, data was processed by a platform of video signal analysis in order to extract kinematic parameters detecting activities undertaken by the participant. We compared our automated activity quality prediction as well as cognitive health prediction with direct observation annotation and neuropsychological assessment scores. With a sensitivity of 85.31% and a precision of 75.90%, the overall activities were correctly automatically detected. Activity frequency differed significantly between MCI and HC participants (p < 0.05). In all activities, differences in the execution time could be identified in the manually and automatically extracted data. We obtained statistically significant correlations between manually as automatically extracted parameters and neuropsychological test scores (p < 0.05). However, no significant differences were found between the groups according to the IADL scale. The results suggest that it is possible to assess IADL functioning with the help of an automatic video monitoring system and that even based on the extracted data, significant group differences can be obtained.},
 author = {Alexandra König and Alexandra König and Carlos Fernando Crispim and Jr. Crispim Carlos Fernando and Carlos Fernando Crispim Junior and Francois Bremond and Alexandre Derreumaux and Alexandre Derreumaux and Grégory Ben-Sadoun and Grégory Ben-Sadoun and Pere Petit and Pierre-David Petit and Renaud David and Francois Bremond and Frans R.J. Verhey and Renaud David and Renaud David and Pauline Aalten and Frans R.J. Verhey and Philippe Robert and Pauline Aalten and Philippe Robert},
 doi = {10.3233/jad-141767},
 journal = {Journal of Alzheimer's Disease},
 mag_id = {1605398093},
 pmcid = {null},
 pmid = {25362036},
 title = {Validation of an automatic video monitoring system for the detection of instrumental activities of daily living in dementia patients},
 year = {2015}
}

@article{Labunets_2014,
 abstract = {Many security risk assessment methods have been proposed both from academia and industry. However, little em- pirical evaluation has been done to investigate how these methods are effective in practice. In this paper we report a controlled experiment that we conducted to compare the effectiveness and participants' perception of visual versus textual methods for security risk assessment used in industry. As instances of the methods we selected CORAS, a method by SINTEF used to provide security risk assessment consulting services, and Se- cRAM, a method by EUROCONTROL used to conduct security risk assessment within air traffic management. The experiment involved 29 MSc students who applied both methods to an application scenario from Smart Grid domain. The dependent variables were effectiveness of the methods measured as number of specific threats and security controls identified, and perception of the methods measured through post-task questionnaires based on the Technology Acceptance Model. The experiment shows that while there is no difference in the actual effectiveness of the two methods, the visual method is better perceived by the participants. Index Terms—controlled experiment, security risk assessment methods, technology acceptance model},
 author = {Katsiaryna Labunets and Katsiaryna Labunets and Federica Paci and Federica Paci and Fabio Massacci and Fabio Massacci and Raminder S. Ruprai and Raminder Ruprai},
 doi = {10.1109/empire.2014.6890113},
 journal = {null},
 mag_id = {1989287546},
 pmcid = {null},
 pmid = {null},
 title = {An experiment on comparing textual vs. visual industrial methods for security risk assessment},
 year = {2014}
}

@article{Lack_2007,
 abstract = {SUMMARY Given the rising number of information resources available, it is increasingly important for digital libraries and archives to create usable services that meet their users' needs. Seeking input from users at all stages of development can help achieve this goal. This article briefly defines four methodologies for gathering user input: focus groups, interviews, questionnaires, and usability testing. In addition, it presents the “top ten” themes that emerged from over four years of assessment and evaluation activities at the California Digital Library.},
 author = {Rosalie Lack and Rosalie Lack and Rosalie Lack},
 doi = {10.1300/j201v04n01_05},
 journal = {Journal of Archival Organization},
 mag_id = {2077473553},
 pmcid = {null},
 pmid = {null},
 title = {The Importance of User-Centered Design: Exploring Findings and Methods},
 year = {2007}
}

@article{Lang_2012,
 abstract = {null},
 author = {Frédéric Lang and Francesco Flammini},
 doi = {10.1002/9781118459898},
 journal = {null},
 mag_id = {4292890382},
 pmcid = {null},
 pmid = {null},
 title = {Formal Methods for Industrial Critical Systems},
 year = {2012}
}

@article{Lee_1996,
 abstract = {With advanced computer technology, systems are getting larger to fulfill more complicated tasks: however, they are also becoming less reliable. Consequently, testing is an indispensable part of system design and implementation; yet it has proved to be a formidable task for complex systems. This motivates the study of testing finite stare machines to ensure the correct functioning of systems and to discover aspects of their behavior. A finite state machine contains a finite number of states and produces outputs on state transitions after receiving inputs. Finite state machines are widely used to model systems in diverse areas, including sequential circuits, certain types of programs, and, more recently, communication protocols. In a testing problem we have a machine about which we lack some information; we would like to deduce this information by providing a sequence of inputs to the machine and observing the outputs produced. Because of its practical importance and theoretical interest, the problem of testing finite state machines has been studied in different areas and at various times. The earliest published literature on this topic dates back to the 1950's. Activities in the 1960's mid early 1970's were motivated mainly by automata theory and sequential circuit testing. The area seemed to have mostly died down until a few years ago when the testing problem was resurrected and is now being studied anew due to its applications to conformance testing of communication protocols. While some old problems which had been open for decades were resolved recently, new concepts and more intriguing problems from new applications emerge. We review the fundamental problems in testing finite state machines and techniques for solving these problems, tracing progress in the area from its inception to the present and the stare of the art. In addition, we discuss extensions of finite state machines and some other topics related to testing.},
 author = {D. Lee and David Lee and Mihalis Yannakakis and Mihalis Yannakakis},
 doi = {10.1109/5.533956},
 journal = {null},
 mag_id = {2004929506},
 pmcid = {null},
 pmid = {null},
 title = {Principles and methods of testing finite state machines-a survey},
 year = {1996}
}

@article{Lethbridge_2005,
 abstract = {Software engineering is an intensively people-oriented activity, yet too little is known about how designers, maintainers, requirements analysts and all other types of software engineers perform their work. In order to improve software engineering tools and practice, it is therefore essential to conduct field studies, i.e. to study real practitioners as they solve real problems. To do so effectively, however, requires an understanding of the techniques most suited to each type of field study task. In this paper, we provide a taxonomy of techniques, focusing on those for data collection. The taxonomy is organized according to the degree of human intervention each requires. For each technique, we provide examples from the literature, an analysis of some of its advantages and disadvantages, and a discussion of how to use it effectively. We also briefly talk about field study design in general, and data analysis.},
 author = {Timothy C. Lethbridge and Timothy C. Lethbridge and Susan Elliott Sim and Susan Elliott Sim and Janice Singer and Janice Singer},
 doi = {10.1007/s10664-005-1290-x},
 journal = {Empirical Software Engineering},
 mag_id = {2034628356},
 pmcid = {null},
 pmid = {null},
 title = {Studying Software Engineers: Data Collection Techniques for Software Field Studies},
 year = {2005}
}

@article{Leue_2008,
 abstract = {null},
 author = {Stefan Leue and Pedro Merino and Pedro Merino and Pedro Merino and Pedro Merino and Pedro Merino and Stefan Leue},
 doi = {null},
 journal = {null},
 mag_id = {2503619577},
 pmcid = {null},
 pmid = {null},
 title = {Formal Methods for Industrial Critical Systems},
 year = {2008}
}

@article{Leuschel_2008,
 abstract = {We present ProB, a validation toolset for the B method. ProB’s automated animation facilities allow users to gain confidence in their specifications. ProB also contains a model checker and a refinement checker, both of which can be used to detect various errors in B specifications. We describe the underlying methodology of ProB, and present the important aspects of the implementation. We also present empirical evaluations as well as several case studies, highlighting that ProB enables users to uncover errors that are not easily discovered by existing tools.},
 author = {Michaël Leuschel and Michael Leuschel and Michael Butler and Michael Butler},
 doi = {10.1007/s10009-007-0063-9},
 journal = {International Journal on Software Tools for Technology Transfer},
 mag_id = {2031111226},
 pmcid = {null},
 pmid = {null},
 title = {ProB : an automated analysis toolset for the B method},
 year = {2008}
}

@article{Leuschel_2011,
 abstract = {In this paper we describe the successful application of the ProB tool for data validation in several industrial applications. The initial case study centred on the San Juan metro system installed by Siemens. The control software was developed and formally proven with B. However, the development contains certain assumptions about the actual rail network topology which have to be validated separately in order to ensure safe operation. For this task, Siemens has developed custom proof rules for Atelier B. Atelier B, however, was unable to deal with about 80 properties of the deployment (running out of memory). These properties thus had to be validated by hand at great expense, and they need to be revalidated whenever the rail network infrastructure changes. In this paper we show how we were able to use ProB to validate all of the about 300 properties of the San Juan deployment, detecting exactly the same faults automatically in a few minutes that were manually uncovered in about one man-month. We have repeated this task for three ongoing projects at Siemens, notably the ongoing automatisation of the line 1 of the Paris Metro. Here again, about a man month of effort has been replaced by a few minutes of computation. This achievement required the extension of the ProB kernel for large sets as well as an improved constraint propagation algorithm. We also outline some of the effort and features that were required in moving from a tool capable of dealing with medium-sized examples towards a tool able to deal with actual industrial specifications. We also describe the issue of validating ProB, so that it can be integrated into the SIL4 development chain at Siemens.},
 author = {Michaël Leuschel and Michael Leuschel and Jérôme Falampin and Jérôme Falampin and Fabian Fritz and Fabian Fritz and Daniel Plagge and Daniel Plagge},
 doi = {10.1007/s00165-010-0172-1},
 journal = {Formal Aspects of Computing},
 mag_id = {2094442885},
 pmcid = {null},
 pmid = {null},
 title = {Automated property verification for large scale B models with ProB},
 year = {2011}
}

@article{Leveson_1993,
 abstract = {Between June 1985 and January 1987, the Therac-25 medical electron accelerator was involved in six massive radiation overdoses. As a result, several people died and others were seriously injured. A detailed investigation of the factors involved in the software-related overdoses and attempts by users, manufacturers, and government agencies to deal with the accidents is presented. The authors demonstrate the complex nature of accidents and the need to investigate all aspects of system development and operation in order to prevent future accidents. The authors also present some lessons learned in terms of system engineering, software engineering, and government regulation of safety-critical systems containing software components. >},
 author = {Nancy G. Leveson and Nancy G. Leveson and Clark Savage Turner and Clark Savage Turner},
 doi = {10.1109/mc.1993.274940},
 journal = {IEEE Computer},
 mag_id = {2100505193},
 pmcid = {null},
 pmid = {null},
 title = {An investigation of the Therac-25 accidents},
 year = {1993}
}

@article{Li_2008,
 abstract = {The aim of this article is further extending the linear programming techniques for multidimensional analysis of preference (LINMAP) to develop a new methodology for solving multiattribute decision making (MADM) problems under Atanassov's intuitionistic fuzzy (IF) environments. The LINMAP only can deal with MADM problems in crisp environments. However, fuzziness is inherent in decision data and decision making processes. In this methodology, Atanassov's IF sets are used to describe fuzziness in decision information and decision making processes by means of an Atanassov's IF decision matrix. A Euclidean distance is proposed to measure the difference between Atanassov's IF sets. Consistency and inconsistency indices are defined on the basis of preferences between alternatives given by the decision maker. Each alternative is assessed on the basis of its distance to an Atanassov's IF positive ideal solution (IFPIS) which is unknown a prior. The Atanassov's IFPIS and the weights of attributes are then estimated using a new linear programming model based upon the consistency and inconsistency indices defined. Finally, the distance of each alternative to the Atanassov's IFPIS can be calculated to determine the ranking order of all alternatives. A numerical example is examined to demonstrate the implementation process of this methodology. Also it has been proved that the methodology proposed in this article can deal with MADM problems under not only Atanassov's IF environments but also both fuzzy and crisp environments.},
 author = {Dengfeng Li and Deng-Feng Li},
 doi = {10.1007/s10700-007-9022-x},
 journal = {Fuzzy Optimization and Decision Making},
 mag_id = {1998162620},
 pmcid = {null},
 pmid = {null},
 title = {Extension of the LINMAP for multiattribute decision making under Atanassov's intuitionistic fuzzy environment},
 year = {2008}
}

@article{Lin_1998,
 abstract = {Objective. Medical instruments commonly have poorly designed user interfaces that promote human errors with life-threatening consequences. The primary hypothesis of this study was that a specific user interface could be made safer and more efficient if redesigned using human factors techniques and principles. Methods. The user interface of a commercially available patient-controlled analgesia (PCA) pump, the Abbott Lifecare 4100 PCA Plus II infuser, was evaluated using a cognitive task analysis of bench tests and field observations. Based on this analysis, the user interface was redesigned. Important elements of the new design include a dialog structure with fewer steps, a dialog overview showing the user's location in the programming sequence, better command feedback, easier error recovery, and clearer labels and messages. The changes were evaluated by comparing a computer prototype of the new interface with a computer simulation of the old one. Twelve student nurses performed six programming tasks with each interface. Task completion time, number of errors, and subjective mental workload were collected for each trial. Results. The results showed significantly faster programming times (F(1,11) = 6.85, P < 0.025), lower mental workload ratings (χ2(1) = 4.45, p < 0.025, one-tailed), and fewer errors (χ2(1) = 3.33, p < 0.05, one-tailed) with the new interface. Conclusion. Adopting a human factors approach to redesigning the PCA interface led to significantly faster, easier, and more reliable performance. These findings have important implications for improving the design of other computer-based medical equipment.},
 author = {Laura Lin and L Lin and Laura Lin and Racquel Isla and Racquel Isla and Karine Doniz and Karine Doniz and Heather Harkness and Heather Harkness and Kim J. Vicente and Kim J. Vicente and D. John Doyle and D J Doyle and D. John Doyle},
 doi = {10.1023/a:1009928203196},
 journal = {Journal of Clinical Monitoring and Computing},
 mag_id = {1571291045},
 pmcid = {null},
 pmid = {9754614},
 title = {Applying Human Factors to the Design of Medical Equipment: Patient-Controlled Analgesia},
 year = {1998}
}

@article{Loer_2006,
 abstract = {This paper discusses a method for the analysis of dependable interactive systems using model checking, and its support by a tool designed to make it accessible to a broader community. The method and the tool are designed to be of value to system engineers, usability engineers and software engineers. It has been designed to help usability engineers by making those aspects of the analysis relevant to them explicit while concealing those aspects of modelling and model checking that are not relevant. The paper presents the results of a user evaluation of the effectiveness of aspects of the tool and how it supports the proposed method.},
 author = {Karsten Loer and Karsten Loer and Michael D. Harrison and Michael D. Harrison},
 doi = {10.1007/s10515-006-7999-y},
 journal = {null},
 mag_id = {2035438947},
 pmcid = {null},
 pmid = {null},
 title = {An integrated framework for the analysis of dependable interactive systems (IFADIS): Its tool support and evaluation},
 year = {2006}
}

@article{Lova_2001,
 abstract = {Frequently, the availability of resources assigned to a project is limited and not sufficient to execute all the concurrent activities. In this situation, decision making about their schedule is necessary. Many times this schedule supposes an increase in the project completion time. Additionally, companies commonly manage various projects simultaneously, sharing a pool of renewable resources. Given these resource constraints, we often can only apply heuristic methods to solve the scheduling problem. In this work the effect of the schedule generation schemes – serial or parallel – and priority rules – MINLFT, MINSLK, MAXTWK, SASP or FCFS – with two approaches – multi-project and single-project – are analysed. The time criteria considered are the mean project delay and the multiproject duration increase. Through an extensive computational study, results show that with the parallel scheduling generation scheme and the multi-project approach the project manager can obtain a good multiproject schedule with the time criterion selected: minimising mean project delay or minimising multiproject duration increase. New heuristics – based on priority rules with a two-phase approach – that outperform classical ones are proposed to minimise mean project delay with a multi-project approach. Finally, the best heuristics analysed are evaluated together with a representative sample of commercial project management software.},
 author = {Antonio Lova and Antonio Lova and Pilar Tormos and Pilar Tormos},
 doi = {10.1023/a:1010966401888},
 journal = {Annals of Operations Research},
 mag_id = {1533019039},
 pmcid = {null},
 pmid = {null},
 title = {Analysis of Scheduling Schemes and Heuristic Rules Performance in Resource-Constrained Multiproject Scheduling},
 year = {2001}
}

@article{López_2002,
 abstract = {In this paper we present a formal framework for the definition of e-barter architectures. By e-barter we mean the possibility of (electronically) exchanging goods without reducing transactions to money. Actually, in our setting, money can be considered just as another good, so that e-barter generalizes seller/buyer architectures. An advantage of e-barter systems, in contrast with most current systems, is that multilateral exchanges can be performed. Customers are first grouped into local markets, according mainly to their localities. Next, a higher order construction allows to compose markets, so that a global market takes a tree-like shape.In order to methodically build our systems, we consider a process algebraic notation. This allows us to specify all the stages of a system (from customers to markets, markets of markets, etc). We introduce an operational semantics for our language so that exchanges of goods are formally defined. Besides, we use some concepts borrowed from microeconomic theory. Specifically, we consider utility functions (i.e. functions returning the valuation that customers/markets give to goods), exchange of goods, and equilibria.We will show that the integration of microeconomic theory and process algebras provides two important gains. Firstly, it allows to avoid ambiguity in the understanding of the behavior of systems. Secondly, it gives a scheme to appropriately structure, in a bottom-up way, e-barter systems.},
 author = {Natalia López and Natalia López and Manuel Núñez and Manuel Núñez and Ismael Rodrı́guez and Ismael Rodríguez and Fernando Rubio and Fernando Rubio},
 doi = {10.1007/3-540-48080-3_19},
 journal = {Lecture Notes in Computer Science},
 mag_id = {1829025913},
 pmcid = {null},
 pmid = {null},
 title = {A Formal Framework for E-Barter Based on Microeconomic Theory and Process Algebras},
 year = {2002}
}

@article{Macedo_2008,
 abstract = {The construction of formal models of real-time distributed systems is a considerable practical challenge. We propose and illustrate a pragmatic incremental approach in which detail is progressively added to abstract system-level specifications of functional and timing properties via intermediate models that express system architecture, concurrency and timing behaviour. The approach is illustrated by developing a new formal model of the cardiac pacemaker system proposed as a "grand challenge" problem in 2007. The models are expressed using the Vienna Development Method (VDM) and are validated primarily by scenario-based tests, including the analysis of timed traces. We argue that the insight gained using this staged modelling approach will be valuable in the subsequent development of implementations, and in detecting potential bottlenecks within suggested implementation architectures.},
 author = {Hugo Daniel Macedo and Hugo Daniel Macedo and Peter Gorm Larsen and Peter Gorm Larsen and JS Fitzgerald and John Fitzgerald},
 doi = {10.1007/978-3-540-68237-0_14},
 journal = {null},
 mag_id = {2125673566},
 pmcid = {null},
 pmid = {null},
 title = {Incremental Development of a Distributed Real-Time Model of a Cardiac Pacing System Using VDM},
 year = {2008}
}

@article{Magherini_2013,
 abstract = {Automated monitoring and the recognition of activities of daily living (ADLs) is a key challenge in ambient-assisted living (AAL) for the assistance of the elderly. Within this context, a formal approach may provide a means to fill the gap between the low-level observations acquired by sensing devices and the high-level concepts that are required for the recognition of human activities. We describe a system named ARA (Automated Recognizer of ADLs) that exploits propositional temporal logic and model checking to support automated real-time recognition of ADLs within a smart environment. The logic is shown to be expressive enough for the specification of realistic patterns of ADLs in terms of basic actions detected by a sensorized environment. The online model checking engine is shown to be capable of processing a stream of detected actions in real time. The effectiveness and viability of the approach are evaluated within the context of a smart kitchen, where different types of ADLs are repeatedly performed.},
 author = {Tommaso Magherini and Tommaso Magherini and Alessandro Fantechi and Alessandro Fantechi and Chris Nugent and Chris Nugent and Enrico Vicario and Enrico Vicario},
 doi = {10.1109/tsmc.2013.2283661},
 journal = {IEEE Transactions on Human-Machine Systems},
 mag_id = {2014084036},
 pmcid = {null},
 pmid = {null},
 title = {Using Temporal Logic and Model Checking in Automated Recognition of Human Activities for Ambient-Assisted Living},
 year = {2013}
}

@article{Mahatody_2010,
 abstract = {This article discusses interactive system evaluation from the perspective of inspection methods, specifically the Cognitive Walkthrough (CW) method. The basic principles of CW are reviewed as proposed in the original version and the first two revisions. Then 11 significant extensions of CW are examined: Heuristic Walkthrough, The Norman Cognitive Walkthrough Method, Streamlined Cognitive Walkthrough, Cognitive Walkthrough for the Web, Groupware Walkthrough, Activity Walkthrough, Interaction Walkthrough, Cognitive Walkthrough with Users, Extended Cognitive Walkthrough, Distributed Cognitive Walkthrough, and Enhanced Cognitive Walkthrough. Four summaries are proposed: The first one concerns the conceptual, methododological, and technological aspects; the next two summaries deal with existing studies, first comparative and then noncomparative; and the last summary provides help for choosing a version or variant.},
 author = {Thomas Mahatody and Thomas Mahatody and Mouldi Sagar and Mouldi Sagar and Christophe Kolski and Christophe Kolski and Christophe Kolski},
 doi = {10.1080/10447311003781409},
 journal = {International Journal of Human-computer Interaction},
 mag_id = {1990580455},
 pmcid = {null},
 pmid = {null},
 title = {State of the Art on the Cognitive Walkthrough Method, Its Variants and Evolutions},
 year = {2010}
}

@article{Margaria_2020,
 abstract = {The vast majority of basic research in formal methods takes place in halls of academia, yet the enormous amounts of applied use of formal methods quietly takes place in companies worldwide. So, while we will not avoid talking about research results and academic work, the main focus of this column is the evolution of the impact of formal methods in industry. That impact has grown from a rarely used niche (and once overhyped), requiring necessarily deep specialized knowledge, to a widely used, under-the-covers, set of tools and techniques usable by normal employees, which save companies millions and protect nationally critical infrastructure and life and limb.},
 author = {Tiziana Margaria and Tiziana Margaria and Joseph R. Kiniry and Joseph R. Kiniry and Joseph Kiniry},
 doi = {10.1109/mitp.2020.2968715},
 journal = {IT Professional},
 mag_id = {3005708925},
 pmcid = {null},
 pmid = {null},
 title = {Welcome to Formal Methods in Industry},
 year = {2020}
}

@article{Maria_2019,
 abstract = {Human activity recognition plays an important role especially in medical applications. This paper proposes a formal approach to model such activities, taking into account possible variations in human behavior. Starting from an activity description enriched with event occurrence probabilities, we translate it into a corresponding formal model based on discrete-time Markov chains (DTMCs). We use the PRISM framework and its model checking facilities to express and check interesting temporal logic properties (PCTL) concerning the dynamic evolution of activities. We illustrate our approach on the model of a serious game used by clinicians to monitor Alzheimer patients. We expect that such a modeling approach could provide new indications for interpreting patient performances. This paper addresses only the model definition and its suitability to check behavioral properties of interest. Indeed, this is mandatory before envisioning any clinical study.},
 author = {Elisabetta De Maria and Elisabetta De Maria and Thibaud L’Yvonnet and Thibaud l'Yvonnet and Sabine Moisan and Sabine Moisan and Jean‐Paul Rigault and Jean-Paul Rigault and Jean-Paul Rigault},
 doi = {null},
 journal = {null},
 mag_id = {2982435309},
 pmcid = {null},
 pmid = {null},
 title = {Probabilistic Activity Recognition For Serious Games With Applications In Medicine},
 year = {2019}
}

@article{Marinescu_2015,
 abstract = {null},
 author = {Raluca Marinescu and Raluca Marinescu and Cristina Seceleanu and Cristina Seceleanu and Hélène Le Guen and Helene Le Guen and Paul Pettersson and Paul Pettersson},
 doi = {10.1016/bs.adcom.2015.03.003},
 journal = {Advances in Computers},
 mag_id = {805316440},
 pmcid = {null},
 pmid = {null},
 title = {A Research Overview of Tool-Supported Model-based Testing of Requirements-based Designs},
 year = {2015}
}

@article{Martakis_2013,
 abstract = {Agile practices on requirements dependencies are a relatively unexplored topic in literature. Empirical studies on it are scarce. This research sets out to uncover concepts that practitioners in companies of various sizes across the globe and in various industries, use for dealing with requirements dependencies in their agile software projects. Concepts were revealed through online focus group research, using an adapted forum for discussion, and grounded theory to analyze the responses. Our study resulted in the following findings: (1) requirements dependencies occur in agile projects and are important to these projects' success just as this is known for `traditional' software projects'; (2) requirements dependencies (i) were considered and treated as part of risk management, (ii) were deemed a responsibility of the individual team members, and (iii) mostly did affect project planning; (3) continuous communication and collaboration - two essential features of any agile method, were found critical to mitigating the risks due to dependencies; (4) a hybrid approach to architecture between agile and plan-driven methods was perceived to yield maximum scalability and help coping with dependencies; (5) `cross-cutting concerns', a category of dependencies, were not uniformly understood in an agile context and require more research.},
 author = {Aias Martakis and Aias Martakis and Maya Daneva and Maya Daneva},
 doi = {10.1109/rcis.2013.6577679},
 journal = {null},
 mag_id = {2092351557},
 pmcid = {null},
 pmid = {null},
 title = {Handling requirements dependencies in agile projects: A focus group with agile software development practitioners},
 year = {2013}
}

@article{Mashkoor_2016,
 abstract = {The use of embedded software is advancing in modern medical devices, so does its capabilities and complexity. This paradigm shift brings many challenges such as an increased rate of medical device failures due to software faults. In this letter, we present a rigorous “correct by construction” approach for the trustworthy development of hemodialysis machines, a subclass of active medical devices. We show how informal requirements of hemodialysis machines are modeled and analyzed through a rigorous process and suggest a generalization to a larger class of active medical devices.},
 author = {Atif Mashkoor and Miklos Biro},
 doi = {10.1109/les.2015.2494459},
 journal = {IEEE Embedded Systems Letters},
 mag_id = {2288621118},
 pmcid = {null},
 pmid = {null},
 title = {Towards the Trustworthy Development of Active Medical Devices: A Hemodialysis Case Study},
 year = {2016}
}

@article{Mealy_1955,
 abstract = {The theoretical basis of sequential circuit synthesis is developed, with particular reference to the work of D. A. Huffman and E. F. Moore. A new method of synthesis is developed which emphasizes formal procedures rather than the more familiar intuitive ones. Familiarity is assumed with the use of switching algebra in the synthesis of combinational circuits.},
 author = {George H. Mealy and George H. Mealy},
 doi = {10.1002/j.1538-7305.1955.tb03788.x},
 journal = {Bell System Technical Journal},
 mag_id = {2075773711},
 pmcid = {null},
 pmid = {null},
 title = {A method for synthesizing sequential circuits},
 year = {1955}
}

@article{Mendling_2007,
 abstract = {Despite that formal and informal quality aspects are of significantimportance to business process modeling, there is only little empiricalwork reported on process model quality and its impact factors. Inthis paper we investigate understandability as a proxy for quality of processmodels and focus on its relations with personal and model characteristics.We used a questionnaire in classes at three European universitiesand generated several novel hypotheses from an exploratory data analysis.Furthermore, we interviewed practitioners to validate our findings.The results reveal that participants tend to exaggerate the differences inmodel understandability, that self-assessment of modeling competenceappears to be invalid, and that the number of arcs in models has animportant influence on understandability.},
 author = {Jan Mendling and Jan Mendling and Hajo A. Reijers and Hajo A. Reijers and Jorge Cardoso and Jorge Cardoso},
 doi = {10.1007/978-3-540-75183-0_4},
 journal = {null},
 mag_id = {1937768634},
 pmcid = {null},
 pmid = {null},
 title = {What makes process models understandable},
 year = {2007}
}

@article{Merayo_2008,
 abstract = {null},
 author = {Mercedes G. Merayo and Mercedes G. Merayo and Manuel Núñez and Manuel Núñez and Ismael Rodrı́guez and Ismael Rodríguez},
 doi = {10.1016/j.comnet.2007.10.002},
 journal = {Computer Networks},
 mag_id = {1990687584},
 pmcid = {null},
 pmid = {null},
 title = {Formal testing from timed finite state machines},
 year = {2008}
}

@article{Merayo_2018,
 abstract = {We develop a formal passive testing framework for software systems where parties communicate asynchronously. Monitors, placed in between the entities, check that a certain property holds over the observations of the interaction between users and the system under test (SUT). Due to the asynchronous nature of communications, the trace observed by the monitor might differ from the one produced by the SUT: the monitor observes inputs before they are received by the SUT and outputs are observed after they are sent by the SUT. It is necessary to take this into account in passive testing; otherwise we might obtain false positives or false negatives. In order to better assess the real causality between actions, we consider the case where each action is labelled with a timestamp giving the time when it was observed at the monitor. We also assume that we know bounds on network latency and so the timestamps allow us to determine additional causalities between actions. Our monitors are implemented as automata that take into account communications being asynchronous. Our solution checks properties against traces in polynomial time and has low storage requirements. Therefore, our proposal is suitable for real-time passive testing.},
 author = {Mercedes G. Merayo and Mercedes G. Merayo and Robert M. Hierons and Robert M. Hierons and Manuel Núñez and Manuel Núñez},
 doi = {10.1007/s00446-017-0308-0},
 journal = {Distributed Computing},
 mag_id = {2743046373},
 pmcid = {null},
 pmid = {null},
 title = {Passive testing with asynchronous communications and timestamps},
 year = {2018}
}

@article{Merayo_2018,
 abstract = {Abstract   Context: Testing usually involves the interaction of the tester with the system under test. However, there are many situations in which this interaction is not feasible and so one requires a passive approach in which the system is analysed to look for failures or unexpected behaviours. The entities of a complex system usually communicate in an asynchronous manner and this complicates the testing tasks since the observed order of events need not be the same as the order in which the events were produced. In previous work, we presented a formal passive testing theory for a single user and system communicating through an asynchronous channel. We were able to check that a trace generated by the system satisfies a given property.  Objective: This papers extends our work, for detecting potential intrusions and unexpected behaviours, to the case where many users simultaneously communicate with a central server. We evaluate the practical value of the theoretical framework with a non-trivial system.  Method: We developed a novel complete theoretical framework to analyse asynchronous systems with multiple users. All the algorithms are fully implemented. Experiments were performed over the Nextcloud platform to show the applicability of our framework. The experiments include an attack so that actual vulnerabilities could be revealed.  Results: The application of our methodology, supported by a tool fully implementing it, was able to reveal a vulnerability in the WebDAV protocol running on Nextcloud.  Conclusion: The extension of our previous work is not only useful from a theoretical point of view but also increases the applicability of the original work. We are now able to analyse systems where the interaction with different users can lead to unexpected behaviours. We have been able to find a vulnerability in a real system, showing the usefulness of our work.},
 author = {Mercedes G. Merayo and Mercedes G. Merayo and Robert M. Hierons and Robert M. Hierons and Manuel Núñez and Manuel Núñez},
 doi = {10.1016/j.infsof.2018.07.013},
 journal = {Information & Software Technology},
 mag_id = {2885528984},
 pmcid = {null},
 pmid = {null},
 title = {A tool supported methodology to passively test asynchronous systems with multiple users},
 year = {2018}
}

@article{Michael_2021,
 abstract = {To improve the state-of-theart practice of applying formal methods to cyberphysical systems, we briefly discuss the evolution of these methods and also summarize four research efforts to close the current capability gaps in their application.},
 author = {James Bret Michael and James Bret Michael and Doron Drusinsky and Doron Drusinsky and Duminda Wijesekera and Duminda Wijesekera},
 doi = {10.1109/mc.2021.3089267},
 journal = {IEEE Computer},
 mag_id = {3198638775},
 pmcid = {null},
 pmid = {null},
 title = {Formal Methods in Cyberphysical Systems},
 year = {2021}
}

@article{Mordeson_2002,
 abstract = {INTRODUCTION Sets Relations Functions Fuzzy Subsets Semigroups Finite-State Machines Finite State Automata Languages and Grammars Nondeterministic Finite-State Automata Relationships Between Languages and Automata Pushdown Automata MAX-MIN AUTOMATA Max-Min Automata General Formulation of Automata Classes of Automata Behavior of Max-Min Automata Equivalences and Homomorphisms of Max-Min Automata Reduction of Max-Min Automata Definite Max-Min Automata Reduction of Max-Min Machines Equivalences Irreducibility and Minimality Nondeterministic and Deterministic Case FUZZY MACHINES, LANGUAGES, AND GRAMMARS Max-Product Machines Equivalences Irreducibility and Minimality Max-Product Grammars and Languages Weak Regular Max-Product Grammars Weak Regular Max-Product Languages Properties of GBP Exercises FUZZY LANGUAGES AND GRAMMARS Fuzzy Languages Types of Grammars Fuzzy Context-Free Grammars Context-Free Max-Product Grammars Context-Free Fuzzy Languages On the Description of the Fuzzy Meaning of Context-Free Languages Trees and Pseudoterms Fuzzy Dendrolanguage Generating Systems Normal Form of F-CFDS Sets of Derivation Trees of Fuzzy Context-Free Grammars Fuzzy Tree Automaton Fuzzy Tree Transducer Fuzzy Meaning of Context-Free Languages PROBABILISTIC AUTOMATA AND GRAMMARS Probabilistic Automata and their Approximation e-Approximating by Nonprobability Devices e-Approximating by Finite Automata Applications The Pe Relation Fuzzy Stars Acceptors and Probabilistic Acceptors Characterizations and the Re -Relation Probabilistic and Weighted Grammars Probabilistic and Weighted Grammars of Type 3 Interrelations with Programmed and Time-variant Grammars Probabilistic Grammars and Automata Probabilistic Grammars Weakly Regular Grammars and Asynchronous Automata Type-0 Probabilistic Grammars and Probabilistic Turing Machines Context-Free Probabilistic Grammars and Pushdown Automata Realization of Fuzzy Languages by Various Automata Properties of Lk, k - 1,2,3 Further Properties of L3 ALGEBRAIC FUZZY AUTOMATA THEORY Fuzzy Finite State Machines Semigroups of Fuzzy Finite State Machines Homomorphisms Admissible Relation Fuzzy Transformation Semigroups Products of Fuzzy Finite State Machines Submachines of a Fuzzy Finite State Machine Retrievability, Separability, and Connectivity Decomposition of Fuzzy Finite State Machines Subsystems of a Fuzzy Finite State Machine Strong Subsystems Cartesian Composition of Fuzzy Finite State Machines Cartesian Composition Admissible Partitions Coverings of Products of Fuzzy Finite State Machines Associative Properties of Products Covering Properties of Products Fuzzy Semiautomaton over a Finite Group MORE ON FUZZY LANGUAGES Fuzzy Regular Languages On Fuzzy Recognizers Minimal Fuzzy Recognizers Fuzzy Recognizers and Recognizable Sets Operation on (Fuzzy) Subsets Construction of Recognizers and Recognizable Sets Accessible and Coaccessible Recognizers Complete Fuzzy Machines Fuzzy Languages on a Free Monoid Algebraic Character and Properties of Fuzzy Regular Languages Deterministic Acceptors of Regular Fuzzy Languages MINIMIZATION OF FUZZY AUTOMATA Equivalence, Reduction, and Minimization of Finite Fuzzy Automata Equivalence of Fuzzy Automata: An Algebraic Approach Reduction and Minimization of Fuzzy Automata Minimal Fuzzy Finite State Automata Behavior, Reduction, and Minimization of Finite L-Automata Matrices over a Bounded Chain Systems of Linear Equivalences over a Bounded Chain Finite L-Automata-Behavior Matrix e-Equivalence e-Irreducibility Minimization L-FUZZY AUTOMATA, GRAMMARS, AND LANGUAGES Fuzzy Recognition of Fuzzy Languages Fuzzy Languages Fuzzy Recognition by Machines Cutpoint Languages Fuzzy Languages not Fuzzy Recognized by Machines in DT2 Rational Probabilistic Events Recursive Fuzzy Languages Closure Properties Fuzzy Grammars and Recursively Enumerable Fuzzy Languages Recursively Enumerable L-Subsets Various Kind of Automata with Weights APPLICATIONS A Formulation of Fuzzy Automata and its Application as a Model of Learning Systems Formulation of Fuzzy Automata Special Cases of Fuzzy Automata Fuzzy Automata as Models of Learning Systems Applications and Simulation Results Properties of Fuzzy Automata Fractionally Fuzzy Grammars with Application to Pattern Recognition Fractionally Fuzzy Grammars A Pattern Recognition Experiment General Fuzzy Acceptors for Syntactic Pattern Recognition e-Equivalence by Inputs Fuzzy-State Automata: Their Stability and Fault Tolerance Relational Description of Automata Fuzzy-State Automata Stable and Almost Stable Behavior of Fuzzy-State Automata Fault Tolerance of Fuzzy-State Automata Clinical Monitoring with Fuzzy Automata Fuzzy Systems REFERENCES INDEX Each chapter also includes a section of exercises},
 author = {John N. Mordeson and John N. Mordeson and Davender S. Malik and Davender S. Malik},
 doi = {null},
 journal = {null},
 mag_id = {1569419053},
 pmcid = {null},
 pmid = {null},
 title = {Fuzzy Automata and Languages: Theory and Applications},
 year = {2002}
}

@article{Mraz_1999,
 abstract = {We describe one of the problems which rises when we introduce fuzzy logic into the field of cellular automata. Whatever model we take for the cellular automaton entity, we have to deal with input variables and their possible values. For our purpose we take Moore's type of automaton and we incorporate fuzzy quantities into it. At this point we are confronted with the problem of type and the set of possible values of input variables. This topic is focused upon.},
 author = {Miha Mraz and Miha Mraz and I. Lapanja and I. Lapanja and I. Lapanja and Nikolaj Zimic and Nikolaj Zimic and Jernej Virant and Jernej Virant},
 doi = {10.1109/nafips.1999.781734},
 journal = {null},
 mag_id = {1916628690},
 pmcid = {null},
 pmid = {null},
 title = {Fuzzy numbers as inputs to fuzzy automata},
 year = {1999}
}

@article{Mullen_2003,
 abstract = {The last 20 years have seen increasing interest in the use of Delphi in a wide range of health‐care applications. However, this use has been accompanied by attempts to codify and define a “true Delphi”. Many authors take a narrow view of the purpose of Delphi and/or advocate a single prescriptive approach to the conduct of a Delphi study. However, as early as 1975, Linstone and Turoff pointed to the danger of attempting to define Delphi as one would immediately encounter a study that violated that definition. Through critical examination of some of the controversies and misunderstandings that surround Delphi, this paper aims to dispel some of the myths and demonstrates the wide scope and potential of this versatile approach.},
 author = {Penelope Mullen and Penelope M. Mullen},
 doi = {10.1108/14777260310469319},
 journal = {Journal of Health Organisation and Management},
 mag_id = {2042517582},
 pmcid = {null},
 pmid = {12800279},
 title = {Delphi: myths and reality.},
 year = {2003}
}

@article{Méry_2011,
 abstract = {This paper presents a translation tool that automatically generates efficient target programming language code (C, C++, Java and C#) from Event-B formal specification related to the analysis of complex problems. This tool is a collection of plug-ins, which are used for translating Event-B formal specifications into different kinds of programming languages. The translation tool is rigorously developed with safety properties preservation. The results detailed in this paper are an architecture of the translation process, to generate a target language code from Event-B models using Event-B grammar through syntax-directed translation, code scheduling architecture and verification of an automatic generated code. The translator checks syntax and type consistency before generating the target programming language code. The translation tool has been developed as a set of Rodin plug-ins under the Eclipse development framework. An assessment of the proposed approach is given through a case study, relative to the development of a cardiac pacemaker system.},
 author = {Dominique Méry and Dominique Méry and Neeraj Kumar Singh and Neeraj Kumar Singh},
 doi = {10.1145/2069216.2069252},
 journal = {null},
 mag_id = {2089070459},
 pmcid = {null},
 pmid = {null},
 title = {Automatic code generation from event-B models},
 year = {2011}
}

@article{Méry_2013,
 abstract = {Formal methods have emerged as an alternative approach to ensuring quality and correctness of highly critical systems, overcoming limitations of traditional validation techniques such as simulation and testing. We propose a refinement-based methodology for complex medical systems design, which possesses all the required key features. A refinement-based combined approach of formal verification, model validation using a model-checker and refinement chart is proposed in this methodology for designing a high-confidence medical device. Furthermore, we show the effectiveness of this methodology for the design of a cardiac pacemaker system.},
 author = {Dominique Méry and Dominique Méry and Neeraj Kumar Singh and Neeraj Kumar Singh},
 doi = {10.1145/2406336.2406351},
 journal = {ACM Transactions in Embedded Computing Systems},
 mag_id = {2062214872},
 pmcid = {null},
 pmid = {null},
 title = {Formal Specification of Medical Systems by Proof-Based Refinement},
 year = {2013}
}

@article{Naur_1972,
 abstract = {As a contribution to programming methodology, the paper contains a detailed, step-by-step account of the considerations leading to a program for solving the 8-queens problem. The experience is related to the method of stepwise refinement and to general problem solving techniques.},
 author = {Peter Naur and Peter Naur},
 doi = {10.1007/bf01932307},
 journal = {Bit Numerical Mathematics},
 mag_id = {2020614999},
 pmcid = {null},
 pmid = {null},
 title = {An experiment on program development},
 year = {1972}
}

@article{Neary_2002,
 abstract = {A vital step in the creation of a visual language is adequate empirical evidence to support any claimed advantages. To this end, the effects of visualisation when applied to the domain of algebraic specifications are investigated through an experiment designed to compare the comprehensibility of one textual and two visual approaches. A statistical analysis of the results is presented and conclusions are drawn regarding the relative worth of the differing approaches.},
 author = {D.S. Neary and Duncan S. Neary and Martin R. Woodward and Martin R. Woodward},
 doi = {10.1006/jvlc.2001.0213},
 journal = {Journal of Visual Languages and Computing},
 mag_id = {2071625344},
 pmcid = {null},
 pmid = {null},
 title = {An Experiment to Compare the Comprehensibility of Textual and Visual Forms of Algebraic Specifications},
 year = {2002}
}

@article{Newcombe_2015,
 abstract = {Engineers use TLA+ to prevent serious but subtle bugs from reaching production.},
 author = {Chris Newcombe and Chris Newcombe and Tim Rath and Tim Rath and Timothy Andrew Rath and Fan Zhang and Fan Zhang and Bogdan Munteanu and Bogdan Munteanu and Marc Brooker and Marc John Brooker and Michael Deardeuff and Michael Benjamin Deardeuff},
 doi = {10.1145/2699417},
 journal = {Communications of The ACM},
 mag_id = {1973938051},
 pmcid = {null},
 pmid = {null},
 title = {How Amazon web services uses formal methods},
 year = {2015}
}

@article{Nison_1991,
 abstract = {Preface Acknowledgement About the author Chapter 1: Introduction Chapter 2: A Historical Background Chapter 3: Constructing the candlestick lines Chapter 4: Reversal Patterns Chapter 5: Stars Chapter 6: More reversal patterns Chapter 7: Continuation patterns Chapter 8: The magic Doji Chapter 9: Putting it all together Chapter 10: A cluster of candles Chapter 11: Candles with trend lines Chapter 12: Candles with replacement levels Chapter 13: Candles with moving averages Chapter 14: Candles with oscillators Chapter 15: Candles with volume Chapter 16: Measured moves Chapter 17: The best of the east and west: the power of convergence Glossary A: Candlestick terms and visual dictionary Glossary B: Western technical terms Bibliography Index},
 author = {Steve Nison and Steve Nison},
 doi = {null},
 journal = {null},
 mag_id = {1590184280},
 pmcid = {null},
 pmid = {null},
 title = {Japanese candlestick charting techniques : a contemporary guide to the ancient investment techniques of the Far East},
 year = {1991}
}

@article{Norman_1986,
 abstract = {null},
 author = {Donald A. Norman and Donald A. Norman},
 doi = {null},
 journal = {null},
 mag_id = {43120215},
 pmcid = {null},
 pmid = {null},
 title = {User Centered System Design},
 year = {1986}
}

@article{Nyolt_2015,
 abstract = {Model checking is well established in system design and business process modelling. Model checking ensures

and automatically proves safety and soundness of models used in day-to-day systems. However, the need for

model checking in activity recognition has not been realised. Models for activity recognition can be built by

prior knowledge. They can encode typical behaviour patterns and allow causal reasoning. As these models are

manually designed they suffer from modelling errors. To address the problem, we discuss different classes of

sensible properties and evaluate three different models for activity recognition. In all cases, modelling errors

and inconsistencies have been found.},
 author = {Martin Nyolt and Martin Nyolt and Kristina Yordanova and Kristina Yordanova and Thomas Kirste and Thomas Kirste},
 doi = {10.5220/0005275204970502},
 journal = {null},
 mag_id = {2040039886},
 pmcid = {null},
 pmid = {null},
 title = {Checking Models for Activity Recognition},
 year = {2015}
}

@article{Oinas-Kukkonen_2008,
 abstract = {A growing number of information technology systems and services are being developed to change users' attitudes or behavior or both. Despite the fact that attitudinal theories from social psychology have been quite extensively applied to the study of user intentions and behavior, these theories have basically provided checklists or rules of thumb rather than systematic design methods or methodologies to develop software solutions. This article is conceptual-theoretical by its nature. It discusses the process of designing and evaluating persuasive systems and describes what kind of content and software functionality may be found at the final product. Seven underlying postulates behind persuasive systems, ways to analyze the user and the use context, and persuasive design strategies and guidelines are highlighted. Based on the works of Fogg, the article also lists techniques for persuasive system content and functionality, describing example software requirements and implementations. Some new techniques are suggested. Moreover, a new categorization of these techniques is proposed, composing of the primary task, dialogue, system credibility, and social support categories.},
 author = {Harri Oinas-Kukkonen and Harri Oinas-Kukkonen and Marja Harjumaa and Marja Harjumaa},
 doi = {10.1007/978-3-540-68504-3_15},
 journal = {null},
 mag_id = {1493500790},
 pmcid = {null},
 pmid = {null},
 title = {A Systematic Framework for Designing and Evaluating Persuasive Systems},
 year = {2008}
}

@article{Oliveira_2009,
 abstract = {Circus specifications define both data and behavioural aspects of systems using a combination of Z and CSP constructs. Previously, a denotational semantics has been given to Circus; however, a shallow embedding of Circus in Z, in which the mapping from Circus constructs to their semantic representation as a Z specification, with yet another language being used as a meta-language, was not useful for proving properties like the refinement laws that justify the distinguishing development technique associated with Circus. This work presents a final reference for the Circus denotational semantics based on Hoare and He’s Unifying Theories of Programming (UTP); as such, it allows the proof of meta-theorems about Circus including the refinement laws in which we are interested. Its correspondence with the CSP semantics is illustrated with some examples. We also discuss the library of lemmas and theorems used in the proofs of the refinement laws. Finally, we give an account of the mechanisation of the Circus semantics and of the mechanical proofs of the refinement laws.},
 author = {Marcel Oliveira and Marcel Oliveira and Ana Cavalcanti and Ana Cavalcanti and Jim Woodcock and Jim Woodcock},
 doi = {10.1007/s00165-007-0052-5},
 journal = {Formal Aspects of Computing},
 mag_id = {1976054113},
 pmcid = {null},
 pmid = {null},
 title = {A UTP semantics for Circus},
 year = {2009}
}

@article{openalex0,
 author = {Lander E.S.},
 doi = {10.1038/35057062},
 journal = {Nature},
 note = {23772},
 title = {Initial sequencing and analysis of the human genome},
 type = {Journal},
 year = {2001}
}

@article{openalex1,
 author = {Ouzzani M.},
 doi = {10.1186/s13643-016-0384-4},
 journal = {Systematic Reviews},
 note = {16379},
 title = {Rayyan—a web and mobile app for systematic reviews},
 type = {Journal},
 year = {2016}
}

@article{openalex10,
 author = {Vandenbroucke J.P.},
 doi = {10.1371/journal.pmed.0040297},
 journal = {PLoS Medicine},
 note = {4206},
 title = {Strengthening the Reporting of Observational Studies in Epidemiology (STROBE): Explanation and Elaboration},
 type = {Journal},
 year = {2007}
}

@article{openalex100,
 author = {Moeuf A.},
 doi = {10.1080/00207543.2017.1372647},
 journal = {International Journal of Production Research},
 note = {1006},
 title = {The industrial management of SMEs in the era of Industry 4.0},
 type = {Journal},
 year = {2017}
}

@article{openalex101,
 author = {Ventsel, E.},
 doi = {10.1115/1.1483356},
 journal = {Applied Mechanics Reviews},
 note = {999},
 title = {Thin Plates and Shells: Theory, Analysis, and Applications},
 type = {Journal},
 year = {2002}
}

@article{openalex102,
 author = {Nelson H.D.},
 doi = {10.1001/jama.288.7.872},
 journal = {JAMA},
 note = {992},
 title = {Postmenopausal Hormone Replacement Therapy},
 type = {Journal},
 year = {2002}
}

@article{openalex103,
 author = {Chen C.},
 doi = {10.1371/journal.pone.0223994},
 journal = {PLoS ONE},
 note = {989},
 title = {Visualizing a field of research: A methodology of systematic scientometric reviews},
 type = {Journal},
 year = {2019}
}

@article{openalex104,
 author = {Katz I.T.},
 doi = {10.7448/ias.16.3.18640},
 journal = {Journal of the International AIDS Society},
 note = {988},
 title = {Impact of HIV‐related stigma on treatment adherence: systematic review and meta‐synthesis},
 type = {Journal},
 year = {2013}
}

@article{openalex105,
 author = {Curry L.A.},
 doi = {10.1161/circulationaha.107.742775},
 journal = {Circulation},
 note = {986},
 title = {Qualitative and Mixed Methods Provide Unique Contributions to Outcomes Research},
 type = {Journal},
 year = {2009}
}

@article{openalex106,
 author = {Gough E.},
 doi = {10.1093/cid/cir632},
 journal = {Clinical Infectious Diseases},
 note = {986},
 title = {Systematic Review of Intestinal Microbiota Transplantation (Fecal Bacteriotherapy) for Recurrent Clostridium difficile Infection},
 type = {Journal},
 year = {2011}
}

@article{openalex107,
 author = {Morris T.P.},
 doi = {10.1002/sim.8086},
 journal = {Statistics in Medicine},
 note = {986},
 title = {Using simulation studies to evaluate statistical methods},
 type = {Journal},
 year = {2019}
}

@article{openalex108,
 author = {Müllensiefen D.},
 doi = {10.1371/journal.pone.0089642},
 journal = {PLoS ONE},
 note = {976},
 title = {The Musicality of Non-Musicians: An Index for Assessing Musical Sophistication in the General Population},
 type = {Journal},
 year = {2014}
}

@article{openalex109,
 author = {Nordmann A.J.},
 doi = {10.1001/archinte.166.3.285},
 journal = {Archives of Internal Medicine},
 note = {973},
 title = {Effects of Low-Carbohydrate vs Low-Fat Diets on Weight Loss and Cardiovascular Risk Factors},
 type = {Journal},
 year = {2006}
}

@article{openalex11,
 author = {Landrigan P.J.},
 doi = {10.1016/s0140-6736(17)32345-0},
 journal = {The Lancet},
 note = {4197},
 title = {The Lancet Commission on pollution and health},
 type = {Journal},
 year = {2017}
}

@article{openalex110,
 author = {Avci O.},
 doi = {10.1016/j.ymssp.2020.107077},
 journal = {Mechanical Systems and Signal Processing},
 note = {965},
 title = {A review of vibration-based damage detection in civil structures: From traditional methods to Machine Learning and Deep Learning applications},
 type = {Journal},
 year = {2020}
}

@article{openalex111,
 author = {Friedman B.},
 doi = {10.1145/230538.230561},
 journal = {ACM transactions on office information systems},
 note = {961},
 title = {Bias in computer systems},
 type = {Journal},
 year = {1996}
}

@article{openalex112,
 author = {Devlin N.J.},
 doi = {10.1007/s40258-017-0310-5},
 journal = {Applied Health Economics and Health Policy},
 note = {954},
 title = {EQ-5D and the EuroQol Group: Past, Present and Future},
 type = {Journal},
 year = {2017}
}

@article{openalex113,
 author = {Jansen J.P.},
 doi = {10.1016/j.jval.2011.04.002},
 journal = {Value in Health},
 note = {954},
 title = {Interpreting Indirect Treatment Comparisons and Network Meta-Analysis for Health-Care Decision Making: Report of the ISPOR Task Force on Indirect Treatment Comparisons Good Research Practices: Part 1},
 type = {Journal},
 year = {2011}
}

@article{openalex114,
 author = {Ioannidis J.P.},
 doi = {10.1503/cmaj.060410},
 journal = {Canadian Medical Association Journal},
 note = {951},
 title = {The appropriateness of asymmetry tests for publication bias in meta-analyses: a large survey},
 type = {Journal},
 year = {2007}
}

@article{openalex115,
 author = {O'Connor J.P.B.},
 doi = {10.1038/nrclinonc.2016.162},
 journal = {Nature Reviews Clinical Oncology},
 note = {944},
 title = {Imaging biomarker roadmap for cancer studies},
 type = {Journal},
 year = {2016}
}

@article{openalex116,
 author = {Song F.},
 doi = {10.3310/hta14080},
 journal = {Health Technology Assessment},
 note = {938},
 title = {Dissemination and publication of research findings: an updated review of related biases},
 type = {Journal},
 year = {2010}
}

@article{openalex117,
 author = {Chmielewska B.},
 doi = {10.1016/s2214-109x(21)00079-6},
 journal = {The Lancet Global Health},
 note = {925},
 title = {Effects of the COVID-19 pandemic on maternal and perinatal outcomes: a systematic review and meta-analysis},
 type = {Journal},
 year = {2021}
}

@article{openalex118,
 author = {Goldberg S.B.},
 doi = {10.1016/j.cpr.2017.10.011},
 journal = {Clinical Psychology Review},
 note = {919},
 title = {Mindfulness-based interventions for psychiatric disorders: A systematic review and meta-analysis},
 type = {Journal},
 year = {2017}
}

@article{openalex119,
 author = {Lakens D.},
 doi = {10.1525/collabra.33267},
 journal = {Collabra Psychology},
 note = {917},
 title = {Sample Size Justification},
 type = {Journal},
 year = {2022}
}

@article{openalex12,
 author = {Hoeting J.A.},
 doi = {10.1214/ss/1009212519},
 journal = {Statistical Science},
 note = {4107},
 title = {Bayesian model averaging: a tutorial (with comments by M. Clyde, David Draper and E. I. George, and a rejoinder by the authors},
 type = {Journal},
 year = {1999}
}

@article{openalex120,
 author = {Berner E.S.},
 doi = {10.1016/j.amjmed.2008.01.001},
 journal = {The American Journal of Medicine},
 note = {910},
 title = {Overconfidence as a Cause of Diagnostic Error in Medicine},
 type = {Journal},
 year = {2008}
}

@article{openalex121,
 author = {Dent J.},
 doi = {10.1136/gut.44.2008.s1},
 journal = {Gut},
 note = {909},
 title = {An evidence-based appraisal of reflux disease management --- the Genval Workshop Report},
 type = {Journal},
 year = {1999}
}

@article{openalex122,
 author = {Anthoine E.},
 doi = {10.1186/s12955-014-0176-2},
 journal = {Health and Quality of Life Outcomes},
 note = {901},
 title = {Sample size used to validate a scale: a review of publications on newly-developed patient reported outcomes measures},
 type = {Journal},
 year = {2014}
}

@article{openalex123,
 author = {Groff D.},
 doi = {10.1001/jamanetworkopen.2021.28568},
 journal = {JAMA Network Open},
 note = {900},
 title = {Short-term and Long-term Rates of Postacute Sequelae of SARS-CoV-2 Infection},
 type = {Journal},
 year = {2021}
}

@article{openalex124,
 author = {Glaser B.G.},
 doi = {10.1177/160940690200100203},
 journal = {International Journal of Qualitative Methods},
 note = {894},
 title = {Conceptualization: On Theory and Theorizing Using Grounded Theory},
 type = {Journal},
 year = {2002}
}

@article{openalex125,
 author = {Zheng J.},
 doi = {10.1007/s40471-017-0128-6},
 journal = {Current Epidemiology Reports},
 note = {892},
 title = {Recent Developments in Mendelian Randomization Studies},
 type = {Journal},
 year = {2017}
}

@article{openalex126,
 author = {McColl A.},
 doi = {10.1136/bmj.316.7128.361},
 journal = {BMJ},
 note = {886},
 title = {General practitioners' perceptions of the route to evidence based medicine: a questionnaire survey},
 type = {Journal},
 year = {1998}
}

@article{openalex127,
 author = {O’Doherty D.},
 doi = {10.1186/s12909-018-1240-0},
 journal = {BMC Medical Education},
 note = {885},
 title = {Barriers and solutions to online learning in medical education – an integrative review},
 type = {Journal},
 year = {2018}
}

@article{openalex128,
 author = {Kiker G.A.},
 doi = {10.1897/ieam_2004a-015.1},
 journal = {Integrated Environmental Assessment and Management},
 note = {882},
 title = {Application of multicriteria decision analysis in environmental decision making},
 type = {Journal},
 year = {2005}
}

@article{openalex129,
 author = {Ross J.},
 doi = {10.1186/s13012-016-0510-7},
 journal = {Implementation Science},
 note = {879},
 title = {Factors that influence the implementation of e-health: a systematic review of systematic reviews (an update)},
 type = {Journal},
 year = {2016}
}

@article{openalex13,
 author = {Ho D.E.},
 doi = {10.1093/pan/mpl013},
 journal = {Political Analysis},
 note = {3962},
 title = {Matching as Nonparametric Preprocessing for Reducing Model Dependence in Parametric Causal Inference},
 type = {Journal},
 year = {2007}
}

@article{openalex130,
 author = {Reardon C.L.},
 doi = {10.1136/bjsports-2019-100715},
 journal = {British Journal of Sports Medicine},
 note = {877},
 title = {Mental health in elite athletes: International Olympic Committee consensus statement (2019)},
 type = {Journal},
 year = {2019}
}

@article{openalex131,
 author = {GREENHALGH T.},
 doi = {10.1111/1468-0009.12197},
 journal = {Milbank Quarterly},
 note = {877},
 title = {Achieving Research Impact Through Co‐creation in Community‐Based Health Services: Literature Review and Case Study},
 type = {Journal},
 year = {2016}
}

@article{openalex132,
 author = {Plana J.C.},
 doi = {10.1093/ehjci/jeu192},
 journal = {European Heart Journal - Cardiovascular Imaging},
 note = {868},
 title = {Expert consensus for multimodality imaging evaluation of adult patients during and after cancer therapy: a report from the American Society of Echocardiography and the European Association of Cardiovascular Imaging},
 type = {Journal},
 year = {2014}
}

@article{openalex133,
 author = {Alcácer V.},
 doi = {10.1016/j.jestch.2019.01.006},
 journal = {Engineering Science and Technology an International Journal},
 note = {867},
 title = {Scanning the Industry 4.0: A Literature Review on Technologies for Manufacturing Systems},
 type = {Journal},
 year = {2019}
}

@article{openalex134,
 author = {Chiba N.},
 doi = {10.1053/gast.1997.v112.pm9178669},
 journal = {Gastroenterology},
 note = {866},
 title = {Speed of healing and symptom relief in grade II to IV gastroesophageal reflux disease: A meta-analysis},
 type = {Journal},
 year = {1997}
}

@article{openalex135,
 author = {Wang W.},
 doi = {10.1109/access.2019.2896108},
 journal = {IEEE Access},
 note = {862},
 title = {A Survey on Consensus Mechanisms and Mining Strategy Management in Blockchain Networks},
 type = {Journal},
 year = {2019}
}

@article{openalex136,
 author = {Gad A.G.},
 doi = {10.1007/s11831-021-09694-4},
 journal = {Archives of Computational Methods in Engineering},
 note = {856},
 title = {Particle Swarm Optimization Algorithm and Its Applications: A Systematic Review},
 type = {Journal},
 year = {2022}
}

@article{openalex137,
 author = {Stanojevic S.},
 doi = {10.1183/13993003.01499-2021},
 journal = {European Respiratory Journal},
 note = {851},
 title = {ERS/ATS technical standard on interpretive strategies for routine lung function tests},
 type = {Journal},
 year = {2021}
}

@article{openalex138,
 author = {Engelman D.T.},
 doi = {10.1001/jamasurg.2019.1153},
 journal = {JAMA Surgery},
 note = {851},
 title = {Guidelines for Perioperative Care in Cardiac Surgery},
 type = {Journal},
 year = {2019}
}

@article{openalex139,
 author = {Croskerry P.},
 doi = {10.1097/acm.0b013e3181ace703},
 journal = {Academic Medicine},
 note = {846},
 title = {A Universal Model of Diagnostic Reasoning},
 type = {Journal},
 year = {2009}
}

@article{openalex14,
 author = {King G.},
 doi = {10.1093/oxfordjournals.pan.a004868},
 journal = {Political Analysis},
 note = {3936},
 title = {Logistic Regression in Rare Events Data},
 type = {Journal},
 year = {2001}
}

@article{openalex140,
 author = {Rajagopalan S.},
 doi = {10.1097/01.anes.0000296719.73450.52},
 journal = {Anesthesiology},
 note = {845},
 title = {The Effects of Mild Perioperative Hypothermia on Blood Loss and Transfusion Requirement},
 type = {Journal},
 year = {2008}
}

@article{openalex141,
 author = {Bartlett J.W.},
 doi = {10.1002/uog.5256},
 journal = {Ultrasound in Obstetrics and Gynecology},
 note = {843},
 title = {Reliability, repeatability and reproducibility: analysis of measurement errors in continuous variables},
 type = {Journal},
 year = {2008}
}

@article{openalex142,
 author = {Verbruggen F.},
 doi = {10.7554/elife.46323},
 journal = {eLife},
 note = {843},
 title = {A consensus guide to capturing the ability to inhibit actions and impulsive behaviors in the stop-signal task},
 type = {Journal},
 year = {2019}
}

@article{openalex143,
 author = {Parati G.},
 doi = {10.1097/hjh.0b013e328308da66},
 journal = {Journal of Hypertension},
 note = {837},
 title = {European Society of Hypertension guidelines for blood pressure monitoring at home: a summary report of the Second International Consensus Conference on Home Blood Pressure Monitoring},
 type = {Journal},
 year = {2008}
}

@article{openalex144,
 author = {Schumann C.M.},
 doi = {10.1523/jneurosci.1297-04.2004},
 journal = {Journal of Neuroscience},
 note = {823},
 title = {The Amygdala Is Enlarged in Children But Not Adolescents with Autism; the Hippocampus Is Enlarged at All Ages},
 type = {Journal},
 year = {2004}
}

@article{openalex145,
 author = {Muller A.E.},
 doi = {10.1016/j.psychres.2020.113441},
 journal = {Psychiatry Research},
 note = {819},
 title = {The mental health impact of the covid-19 pandemic on healthcare workers, and interventions to help them: A rapid systematic review},
 type = {Journal},
 year = {2020}
}

@article{openalex146,
 author = {O’Rourke R.A.},
 doi = {10.1016/s0735-1097(00)00831-7},
 journal = {Journal of the American College of Cardiology},
 note = {814},
 title = {American College of Cardiology/American Heart Association expert consensus document on electron-beam computed tomography for the diagnosis and prognosis of coronary artery disease},
 type = {Journal},
 year = {2000}
}

@article{openalex147,
 author = {Bradley B.T.},
 doi = {10.1016/s0140-6736(20)31305-2},
 journal = {The Lancet},
 note = {809},
 title = {Histopathology and ultrastructural findings of fatal COVID-19 infections in Washington State: a case series},
 type = {Journal},
 year = {2020}
}

@article{openalex148,
 author = {Serdyukov P.},
 doi = {10.1108/jrit-10-2016-0007},
 journal = {Journal of Research in Innovative Teaching & Learning},
 note = {801},
 title = {Innovation in education: what works, what doesn’t, and what to do about it?},
 type = {Journal},
 year = {2017}
}

@article{openalex149,
 author = {Siemieniuk R.A.},
 doi = {10.1136/bmj.m2980},
 journal = {BMJ},
 note = {801},
 title = {Drug treatments for covid-19: living systematic review and network meta-analysis},
 type = {Journal},
 year = {2020}
}

@article{openalex15,
 author = {Steyerberg E.W.},
 doi = {10.1097/ede.0b013e3181c30fb2},
 journal = {Epidemiology},
 note = {3909},
 title = {Assessing the Performance of Prediction Models},
 type = {Journal},
 year = {2009}
}

@article{openalex150,
 author = {Walt G.},
 doi = {10.1093/heapol/czn024},
 journal = {Health Policy and Planning},
 note = {791},
 title = {'Doing' health policy analysis: methodological and conceptual reflections and challenges},
 type = {Journal},
 year = {2008}
}

@article{openalex151,
 author = {Bischoff-Ferrari H.A.},
 doi = {10.1001/archinternmed.2008.600},
 journal = {Archives of Internal Medicine},
 note = {784},
 title = {Prevention of Nonvertebral Fractures With Oral Vitamin D and Dose Dependency},
 type = {Journal},
 year = {2009}
}

@article{openalex152,
 author = {Haase D.},
 doi = {10.1182/blood-2007-03-082404},
 journal = {Blood},
 note = {778},
 title = {New insights into the prognostic impact of the karyotype in MDS and correlation with subtypes: evidence from a core dataset of 2124 patients},
 type = {Journal},
 year = {2007}
}

@article{openalex153,
 author = {Annese V.},
 doi = {10.1016/j.crohns.2013.09.016},
 journal = {Journal of Crohn s and Colitis},
 note = {766},
 title = {European evidence based consensus for endoscopy in inflammatory bowel disease},
 type = {Journal},
 year = {2013}
}

@article{openalex154,
 author = {Rubino F.},
 doi = {10.1038/s41591-020-0803-x},
 journal = {Nature Medicine},
 note = {760},
 title = {Joint international consensus statement for ending stigma of obesity},
 type = {Journal},
 year = {2020}
}

@article{openalex155,
 author = {Ganatra B.},
 doi = {10.1016/s0140-6736(17)31794-4},
 journal = {The Lancet},
 note = {757},
 title = {Global, regional, and subregional classification of abortions by safety, 2010–14: estimates from a Bayesian hierarchical model},
 type = {Journal},
 year = {2017}
}

@article{openalex156,
 author = {Thangaratinam S.},
 doi = {10.1136/bmj.e2088},
 journal = {BMJ},
 note = {757},
 title = {Effects of interventions in pregnancy on maternal weight and obstetric outcomes: meta-analysis of randomised evidence},
 type = {Journal},
 year = {2012}
}

@article{openalex157,
 author = {Harutyunyan H.},
 doi = {10.1038/s41597-019-0103-9},
 journal = {Scientific Data},
 note = {757},
 title = {Multitask learning and benchmarking with clinical time series data},
 type = {Journal},
 year = {2019}
}

@article{openalex158,
 author = {Dunning J.},
 doi = {10.1016/s1569-9293(03)00191-9},
 journal = {Interactive Cardiovascular and Thoracic Surgery},
 note = {755},
 title = {Towards evidence-based medicine in cardiothoracic surgery: best BETS},
 type = {Journal},
 year = {2003}
}

@article{openalex159,
 author = {Quek T.T.},
 doi = {10.3390/ijerph16152735},
 journal = {International Journal of Environmental Research and Public Health},
 note = {750},
 title = {The Global Prevalence of Anxiety Among Medical Students: A Meta-Analysis},
 type = {Journal},
 year = {2019}
}

@article{openalex16,
 author = {Breiman L.},
 doi = {10.1214/ss/1009213726},
 journal = {Statistical Science},
 note = {3783},
 title = {Statistical Modeling: The Two Cultures (with comments and a rejoinder by the author)},
 type = {Journal},
 year = {2001}
}

@article{openalex160,
 author = {Langford B.J.},
 doi = {10.1016/j.cmi.2020.12.018},
 journal = {Clinical Microbiology and Infection},
 note = {750},
 title = {Antibiotic prescribing in patients with COVID-19: rapid review and meta-analysis},
 type = {Journal},
 year = {2021}
}

@article{openalex161,
 author = {Briggs A.H.},
 doi = {10.1177/0272989x12458348},
 journal = {Medical Decision Making},
 note = {744},
 title = {Model Parameter Estimation and Uncertainty Analysis},
 type = {Journal},
 year = {2012}
}

@article{openalex162,
 author = {Chowdhury G.G.},
 doi = {10.1002/aris.1440370103},
 journal = {Annual Review of Information Science and Technology},
 note = {743},
 title = {Natural language processing},
 type = {Journal},
 year = {2003}
}

@article{openalex163,
 author = {Baral S.},
 doi = {10.1371/journal.pmed.0040339},
 journal = {PLoS Medicine},
 note = {739},
 title = {Elevated Risk for HIV Infection among Men Who Have Sex with Men in Low- and Middle-Income Countries 2000–2006: A Systematic Review},
 type = {Journal},
 year = {2007}
}

@article{openalex164,
 author = {Wallin J.A.},
 doi = {10.1111/j.1742-7843.2005.pto_139.x},
 journal = {Basic & Clinical Pharmacology & Toxicology},
 note = {737},
 title = {Bibliometric Methods: Pitfalls and Possibilities},
 type = {Journal},
 year = {2005}
}

@article{openalex165,
 author = {Borsboom D.},
 doi = {10.1038/s43586-021-00055-w},
 journal = {Nature Reviews Methods Primers},
 note = {726},
 title = {Network analysis of multivariate data in psychological science},
 type = {Journal},
 year = {2021}
}

@article{openalex166,
 author = {Gatt A.},
 doi = {10.1613/jair.5477},
 journal = {Journal of Artificial Intelligence Research},
 note = {725},
 title = {Survey of the State of the Art in Natural Language Generation: Core tasks, applications and evaluation},
 type = {Journal},
 year = {2018}
}

@article{openalex167,
 author = {Jiménez-Luna J.},
 doi = {10.1038/s42256-020-00236-4},
 journal = {Nature Machine Intelligence},
 note = {724},
 title = {Drug discovery with explainable artificial intelligence},
 type = {Journal},
 year = {2020}
}

@article{openalex168,
 author = {Riley R.D.},
 doi = {10.1002/sim.7992},
 journal = {Statistics in Medicine},
 note = {720},
 title = {Minimum sample size for developing a multivariable prediction model: PART II ‐ binary and time‐to‐event outcomes},
 type = {Journal},
 year = {2018}
}

@article{openalex169,
 author = {Miller E.L.},
 doi = {10.1161/str.0b013e3181e7512b},
 journal = {Stroke},
 note = {710},
 title = {Comprehensive Overview of Nursing and Interdisciplinary Rehabilitation Care of the Stroke Patient},
 type = {Journal},
 year = {2010}
}

@article{openalex17,
 author = {Guidotti R.},
 doi = {10.1145/3236009},
 journal = {ACM Computing Surveys},
 note = {3752},
 title = {A Survey of Methods for Explaining Black Box Models},
 type = {Journal},
 year = {2018}
}

@article{openalex170,
 author = {Stieglitz S.},
 doi = {10.1016/j.ijinfomgt.2017.12.002},
 journal = {International Journal of Information Management},
 note = {709},
 title = {Social media analytics – Challenges in topic discovery, data collection, and data preparation},
 type = {Journal},
 year = {2017}
}

@article{openalex171,
 author = {Robinson P.D.},
 doi = {10.1183/09031936.00069712},
 journal = {European Respiratory Journal},
 note = {700},
 title = {Consensus statement for inert gas washout measurement using multiple- and single- breath tests},
 type = {Journal},
 year = {2013}
}

@article{openalex172,
 author = {Karniadakis, G.},
 doi = {10.1115/1.1483361},
 journal = {Applied Mechanics Reviews},
 note = {699},
 title = {Micro Flows: Fundamentals and Simulation},
 type = {Journal},
 year = {2002}
}

@article{openalex173,
 author = {Sim I.},
 doi = {10.1136/jamia.2001.0080527},
 journal = {Journal of the American Medical Informatics Association},
 note = {695},
 title = {Clinical Decision Support Systems for the Practice of Evidence-based Medicine},
 type = {Journal},
 year = {2001}
}

@article{openalex174,
 author = {Fanelli D.},
 doi = {10.1371/journal.pone.0010068},
 journal = {PLoS ONE},
 note = {695},
 title = {“Positive” Results Increase Down the Hierarchy of the Sciences},
 type = {Journal},
 year = {2010}
}

@article{openalex175,
 author = {Torous J.},
 doi = {10.1002/wps.20883},
 journal = {World Psychiatry},
 note = {691},
 title = {The growing field of digital psychiatry: current evidence and the future of apps, social media, chatbots, and virtual reality},
 type = {Journal},
 year = {2021}
}

@article{openalex176,
 author = {McIntosh M.J.},
 doi = {10.1177/2333393615597674},
 journal = {Global Qualitative Nursing Research},
 note = {689},
 title = {Situating and Constructing Diversity in Semi-Structured Interviews},
 type = {Journal},
 year = {2015}
}

@article{openalex177,
 author = {Peterson E.D.},
 doi = {10.1001/jama.295.16.1912},
 journal = {JAMA},
 note = {685},
 title = {Association Between Hospital Process Performance and Outcomes Among Patients With Acute Coronary Syndromes},
 type = {Journal},
 year = {2006}
}

@article{openalex178,
 author = {Preacher K.J.},
 doi = {10.1146/annurev-psych-010814-015258},
 journal = {Annual Review of Psychology},
 note = {679},
 title = {Advances in Mediation Analysis: A Survey and Synthesis of New Developments},
 type = {Journal},
 year = {2014}
}

@article{openalex18,
 author = {Bouckaert R.},
 doi = {10.1371/journal.pcbi.1006650},
 journal = {PLoS Computational Biology},
 note = {3478},
 title = {BEAST 2.5: An advanced software platform for Bayesian evolutionary analysis},
 type = {Journal},
 year = {2019}
}

@article{openalex180,
 author = {Mihaylova B.},
 doi = {10.1002/hec.1653},
 journal = {Health Economics},
 note = {677},
 title = {Review of statistical methods for analysing healthcare resources and costs},
 type = {Journal},
 year = {2010}
}

@article{openalex181,
 author = {Bishop D.V.M.},
 doi = {10.1371/journal.pone.0158753},
 journal = {PLoS ONE},
 note = {677},
 title = {CATALISE: A Multinational and Multidisciplinary Delphi Consensus Study. Identifying Language Impairments in Children},
 type = {Journal},
 year = {2016}
}

@article{openalex182,
 author = {Pan A.},
 doi = {10.2337/dc11-2055},
 journal = {Diabetes Care},
 note = {676},
 title = {Bidirectional Association Between Depression and Metabolic Syndrome},
 type = {Journal},
 year = {2012}
}

@article{openalex183,
 author = {Pollock A.},
 doi = {10.1002/14651858.cd010820.pub2},
 journal = {Cochrane library},
 note = {671},
 title = {Interventions for improving upper limb function after stroke},
 type = {Journal},
 year = {2014}
}

@article{openalex184,
 author = {Ali S.},
 doi = {10.1016/j.inffus.2023.101805},
 journal = {Information Fusion},
 note = {665},
 title = {Explainable Artificial Intelligence (XAI): What we know and what is left to attain Trustworthy Artificial Intelligence},
 type = {Journal},
 year = {2023}
}

@article{openalex185,
 author = {Coca S.},
 doi = {10.1038/sj.ki.5002729},
 journal = {Kidney International},
 note = {662},
 title = {Biomarkers for the diagnosis and risk stratification of acute kidney injury: A systematic review},
 type = {Journal},
 year = {2007}
}

@article{openalex186,
 author = {Sima V.},
 doi = {10.3390/su12104035},
 journal = {Sustainability},
 note = {660},
 title = {Influences of the Industry 4.0 Revolution on the Human Capital Development and Consumer Behavior: A Systematic Review},
 type = {Journal},
 year = {2020}
}

@article{openalex187,
 author = {Philips Z.},
 doi = {10.3310/hta8360},
 journal = {Health Technology Assessment},
 note = {659},
 title = {Review of guidelines for good practice in decision-analytic modelling in health technology assessment},
 type = {Journal},
 year = {2004}
}

@article{openalex188,
 author = {Sapatinas T.},
 doi = {10.1111/j.1467-985x.2004.298_11.x},
 journal = {Journal of the Royal Statistical Society Series A (Statistics in Society)},
 note = {657},
 title = {The Elements of Statistical Learning},
 type = {Journal},
 year = {2003}
}

@article{openalex189,
 author = {Lespérance F.},
 doi = {10.1001/jama.297.4.367},
 journal = {JAMA},
 note = {653},
 title = {Effects of Citalopram and Interpersonal Psychotherapy on Depression in Patients With Coronary Artery Disease},
 type = {Journal},
 year = {2007}
}

@article{openalex19,
 author = {Easterbrook P.},
 doi = {10.1016/0140-6736(91)90201-y},
 journal = {The Lancet},
 note = {3057},
 title = {Publication bias in clinical research},
 type = {Journal},
 year = {1991}
}

@article{openalex190,
 author = {Xu Y.},
 doi = {10.18653/v1/d15-1206},
 journal = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
 note = {653},
 title = {Classifying Relations via Long Short Term Memory Networks along Shortest Dependency Paths},
 type = {InProceedings},
 year = {2015}
}

@article{openalex191,
 author = {Aronsson G.},
 doi = {10.1186/s12889-017-4153-7},
 journal = {BMC Public Health},
 note = {652},
 title = {A systematic review including meta-analysis of work environment and burnout symptoms},
 type = {Journal},
 year = {2017}
}

@article{openalex192,
 author = {Lin X.},
 doi = {10.1161/jaha.115.002014},
 journal = {Journal of the American Heart Association},
 note = {647},
 title = {Effects of Exercise Training on Cardiorespiratory Fitness and Biomarkers of Cardiometabolic Health: A Systematic Review and Meta‐Analysis of Randomized Controlled Trials},
 type = {Journal},
 year = {2015}
}

@article{openalex193,
 author = {Kaprio J.},
 doi = {10.1007/bf02221682},
 journal = {Diabetologia},
 note = {642},
 title = {Concordance for Type 1 (insulin-dependent) and Type 2 (non-insulin-dependent) diabetes mellitus in a population-based cohort of twins in Finland},
 type = {Journal},
 year = {1992}
}

@article{openalex194,
 author = {Page M.J.},
 doi = {10.1371/journal.pmed.1002028},
 journal = {PLoS Medicine},
 note = {639},
 title = {Epidemiology and Reporting Characteristics of Systematic Reviews of Biomedical Research: A Cross-Sectional Study},
 type = {Journal},
 year = {2016}
}

@article{openalex195,
 author = {Liu X.},
 doi = {10.3390/s120811113},
 journal = {Sensors},
 note = {636},
 title = {A Survey on Clustering Routing Protocols in Wireless Sensor Networks},
 type = {Journal},
 year = {2012}
}

@article{openalex196,
 author = {Nykamp K.},
 doi = {10.1038/gim.2017.37},
 journal = {Genetics in Medicine},
 note = {636},
 title = {Sherloc: a comprehensive refinement of the ACMG–AMP variant classification criteria},
 type = {Journal},
 year = {2017}
}

@article{openalex197,
 author = {Morgan M.G.},
 doi = {10.1073/pnas.1319946111},
 journal = {Proceedings of the National Academy of Sciences},
 note = {632},
 title = {Use (and abuse) of expert elicitation in support of decision making for public policy},
 type = {Journal},
 year = {2014}
}

@article{openalex198,
 author = {Moffat K.},
 doi = {10.1016/j.resourpol.2013.11.003},
 journal = {Resources Policy},
 note = {622},
 title = {The paths to social licence to operate: An integrative model explaining community acceptance of mining},
 type = {Journal},
 year = {2013}
}

@article{openalex199,
 author = {Källander K.},
 doi = {10.2196/jmir.2130},
 journal = {Journal of Medical Internet Research},
 note = {619},
 title = {Mobile Health (mHealth) Approaches and Lessons for Increased Performance and Retention of Community Health Workers in Low- and Middle-Income Countries: A Review},
 type = {Journal},
 year = {2013}
}

@article{openalex2,
 author = {Viechtbauer W.},
 doi = {10.18637/jss.v036.i03},
 journal = {Journal of Statistical Software},
 note = {14513},
 title = {Conducting Meta-Analyses in<i>R</i>with the<b>metafor</b>Package},
 type = {Journal},
 year = {2010}
}

@article{openalex20,
 author = {Hahladakis J.N.},
 doi = {10.1016/j.jhazmat.2017.10.014},
 journal = {Journal of Hazardous Materials},
 note = {2966},
 title = {An overview of chemical additives present in plastics: Migration, release, fate and environmental impact during their use, disposal and recycling},
 type = {Journal},
 year = {2017}
}

@article{openalex21,
 author = {Alcock B.P.},
 doi = {10.1093/nar/gkz935},
 journal = {Nucleic Acids Research},
 note = {2820},
 title = {CARD 2020: antibiotic resistome surveillance with the comprehensive antibiotic resistance database},
 type = {Journal},
 year = {2019}
}

@article{openalex22,
 author = {Islam S.M.R.},
 doi = {10.1109/access.2015.2437951},
 journal = {IEEE Access},
 note = {2724},
 title = {The Internet of Things for Health Care: A Comprehensive Survey},
 type = {Journal},
 year = {2015}
}

@article{openalex23,
 author = {Atkins L.},
 doi = {10.1186/s13012-017-0605-9},
 journal = {Implementation Science},
 note = {2583},
 title = {A guide to using the Theoretical Domains Framework of behaviour change to investigate implementation problems},
 type = {Journal},
 year = {2017}
}

@article{openalex24,
 author = {Author U.},
 doi = {10.1186/1477-7525-4-79},
 journal = {Health and Quality of Life Outcomes},
 note = {2530},
 title = {Guidance for industry: patient-reported outcome measures: use in medical product development to support labeling claims: draft guidance},
 type = {Journal},
 year = {2006}
}

@article{openalex25,
 author = {Steel Z.},
 doi = {10.1093/ije/dyu038},
 journal = {International Journal of Epidemiology},
 note = {2529},
 title = {The global prevalence of common mental disorders: a systematic review and meta-analysis 1980–2013},
 type = {Journal},
 year = {2014}
}

@article{openalex26,
 author = {Whitmee S.},
 doi = {10.1016/s0140-6736(15)60901-1},
 journal = {The Lancet},
 note = {2350},
 title = {Safeguarding human health in the Anthropocene epoch: report of The Rockefeller Foundation–Lancet Commission on planetary health},
 type = {Journal},
 year = {2015}
}

@article{openalex27,
 author = {Dwivedi Y.K.},
 doi = {10.1016/j.ijinfomgt.2023.102642},
 journal = {International Journal of Information Management},
 note = {2189},
 title = {Opinion Paper: “So what if ChatGPT wrote it?” Multidisciplinary perspectives on opportunities, challenges and implications of generative conversational AI for research, practice and policy},
 type = {Journal},
 year = {2023}
}

@article{openalex28,
 author = {Dunleavy P.},
 doi = {10.1093/jopart/mui057},
 journal = {Journal of Public Administration Research and Theory},
 note = {2183},
 title = {New Public Management Is Dead--Long Live Digital-Era Governance},
 type = {Journal},
 year = {2005}
}

@article{openalex29,
 author = {Brereton P.},
 doi = {10.1016/j.jss.2006.07.009},
 journal = {Journal of Systems and Software},
 note = {2182},
 title = {Lessons from applying the systematic literature review process within the software engineering domain},
 type = {Journal},
 year = {2006}
}

@article{openalex3,
 author = {Shamseer L.},
 doi = {10.1136/bmj.g7647},
 journal = {BMJ},
 note = {11102},
 title = {Preferred reporting items for systematic review and meta-analysis protocols (PRISMA-P) 2015: elaboration and explanation},
 type = {Journal},
 year = {2015}
}

@article{openalex30,
 author = {McCambridge J.},
 doi = {10.1016/j.jclinepi.2013.08.015},
 journal = {Journal of Clinical Epidemiology},
 note = {2165},
 title = {Systematic review of the Hawthorne effect: New concepts are needed to study research participation effects},
 type = {Journal},
 year = {2013}
}

@article{openalex31,
 author = {Jordan K.M.},
 doi = {10.1136/ard.2003.011742},
 journal = {Annals of the Rheumatic Diseases},
 note = {2072},
 title = {EULAR Recommendations 2003: an evidence based approach to the management of knee osteoarthritis: Report of a Task Force of the Standing Committee for International Clinical Studies Including Therapeutic Trials (ESCISIT)},
 type = {Journal},
 year = {2003}
}

@article{openalex32,
 author = {Crowe S.},
 doi = {10.1186/1471-2288-11-100},
 journal = {BMC Medical Research Methodology},
 note = {2068},
 title = {The case study approach},
 type = {Journal},
 year = {2011}
}

@article{openalex34,
 author = {Author U.},
 doi = {10.1542/peds.2011-2654},
 journal = {PEDIATRICS},
 note = {2047},
 title = {ADHD: Clinical Practice Guideline for the Diagnosis, Evaluation, and Treatment of Attention-Deficit/Hyperactivity Disorder in Children and Adolescents},
 type = {Journal},
 year = {2011}
}

@article{openalex35,
 author = {Maas A.I.R.},
 doi = {10.1016/s1474-4422(17)30371-x},
 journal = {The Lancet Neurology},
 note = {2037},
 title = {Traumatic brain injury: integrated approaches to improve prevention, clinical care, and research},
 type = {Journal},
 year = {2017}
}

@article{openalex36,
 author = {Sert N.P.D.},
 doi = {10.1371/journal.pbio.3000411},
 journal = {PLoS Biology},
 note = {2002},
 title = {Reporting animal research: Explanation and elaboration for the ARRIVE guidelines 2.0},
 type = {Journal},
 year = {2020}
}

@article{openalex37,
 author = {Peters J.L.},
 doi = {10.1001/jama.295.6.676},
 journal = {JAMA},
 note = {1977},
 title = {Comparison of Two Methods to Detect Publication Bias in Meta-analysis},
 type = {Journal},
 year = {2006}
}

@article{openalex38,
 author = {Wallerstein N.B.},
 doi = {10.1177/1524839906289376},
 journal = {Health Promotion Practice},
 note = {1970},
 title = {Using Community-Based Participatory Research to Address Health Disparities},
 type = {Journal},
 year = {2006}
}

@article{openalex39,
 author = {Fletcher G.F.},
 doi = {10.1161/hc3901.095960},
 journal = {Circulation},
 note = {1922},
 title = {Exercise Standards for Testing and Training},
 type = {Journal},
 year = {2001}
}

@article{openalex4,
 author = {Galle P.R.},
 doi = {10.1016/j.jhep.2018.03.019},
 journal = {Journal of Hepatology},
 note = {10108},
 title = {EASL Clinical Practice Guidelines: Management of hepatocellular carcinoma},
 type = {Journal},
 year = {2018}
}

@article{openalex40,
 author = {Lex A.},
 doi = {10.1109/tvcg.2014.2346248},
 journal = {IEEE Transactions on Visualization and Computer Graphics},
 note = {1917},
 title = {UpSet: Visualization of Intersecting Sets},
 type = {Journal},
 year = {2014}
}

@article{openalex41,
 author = {Guyot P.},
 doi = {10.1186/1471-2288-12-9},
 journal = {BMC Medical Research Methodology},
 note = {1884},
 title = {Enhanced secondary analysis of survival data: reconstructing the data from published Kaplan-Meier survival curves},
 type = {Journal},
 year = {2012}
}

@article{openalex42,
 author = {Lamb C.A.},
 doi = {10.1136/gutjnl-2019-318484},
 journal = {Gut},
 note = {1876},
 title = {British Society of Gastroenterology consensus guidelines on the management of inflammatory bowel disease in adults},
 type = {Journal},
 year = {2019}
}

@article{openalex43,
 author = {Akhtar N.},
 doi = {10.1109/access.2018.2807385},
 journal = {IEEE Access},
 note = {1872},
 title = {Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey},
 type = {Journal},
 year = {2018}
}

@article{openalex44,
 author = {Hernán M.A.},
 doi = {10.1093/aje/kwv254},
 journal = {American Journal of Epidemiology},
 note = {1825},
 title = {Using Big Data to Emulate a Target Trial When a Randomized Trial Is Not Available: Table 1.},
 type = {Journal},
 year = {2016}
}

@article{openalex45,
 author = {Epstein A.E.},
 doi = {10.1161/circualtionaha.108.189742},
 journal = {Circulation},
 note = {1798},
 title = {ACC/AHA/HRS 2008 Guidelines for Device-Based Therapy of Cardiac Rhythm Abnormalities},
 type = {Journal},
 year = {2008}
}

@article{openalex46,
 author = {Dwivedi Y.K.},
 doi = {10.1016/j.ijinfomgt.2022.102542},
 journal = {International Journal of Information Management},
 note = {1769},
 title = {Metaverse beyond the hype: Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy},
 type = {Journal},
 year = {2022}
}

@article{openalex47,
 author = {Sivarajah U.},
 doi = {10.1016/j.jbusres.2016.08.001},
 journal = {Journal of Business Research},
 note = {1769},
 title = {Critical analysis of Big Data challenges and analytical methods},
 type = {Journal},
 year = {2016}
}

@article{openalex48,
 author = {Greenhalgh T.},
 doi = {10.2196/jmir.8775},
 journal = {Journal of Medical Internet Research},
 note = {1746},
 title = {Beyond Adoption: A New Framework for Theorizing and Evaluating Nonadoption, Abandonment, and Challenges to the Scale-Up, Spread, and Sustainability of Health and Care Technologies},
 type = {Journal},
 year = {2017}
}

@article{openalex49,
 author = {Rethlefsen M.L.},
 doi = {10.1186/s13643-020-01542-z},
 journal = {Systematic Reviews},
 note = {1737},
 title = {PRISMA-S: an extension to the PRISMA Statement for Reporting Literature Searches in Systematic Reviews},
 type = {Journal},
 year = {2021}
}

@article{openalex5,
 author = {Efron B.},
 doi = {10.1214/009053604000000067},
 journal = {The Annals of Statistics},
 note = {9404},
 title = {Least angle regression},
 type = {Journal},
 year = {2004}
}

@article{openalex50,
 author = {Moher D.},
 doi = {10.1016/j.jclinepi.2010.03.004},
 journal = {Journal of Clinical Epidemiology},
 note = {1697},
 title = {CONSORT 2010 Explanation and Elaboration: updated guidelines for reporting parallel group randomised trials},
 type = {Journal},
 year = {2010}
}

@article{openalex51,
 author = {Dixon-Woods M.},
 doi = {10.1186/1471-2288-6-35},
 journal = {BMC Medical Research Methodology},
 note = {1694},
 title = {Conducting a critical interpretive synthesis of the literature on access to healthcare by vulnerable groups},
 type = {Journal},
 year = {2006}
}

@article{openalex52,
 author = {Edmondson A.C.},
 doi = {10.1146/annurev-orgpsych-031413-091305},
 journal = {Annual Review of Organizational Psychology and Organizational Behavior},
 note = {1692},
 title = {Psychological Safety: The History, Renaissance, and Future of an Interpersonal Construct},
 type = {Journal},
 year = {2014}
}

@article{openalex53,
 author = {Murphy N.},
 doi = {10.3310/hta2030},
 journal = {Health Technology Assessment},
 note = {1681},
 title = {Consensus development methods, and their use in clinical guideline development.},
 type = {Journal},
 year = {1998}
}

@article{openalex54,
 author = {Bridges J.F.},
 doi = {10.1016/j.jval.2010.11.013},
 journal = {Value in Health},
 note = {1671},
 title = {Conjoint Analysis Applications in Health—a Checklist: A Report of the ISPOR Good Research Practices for Conjoint Analysis Task Force},
 type = {Journal},
 year = {2011}
}

@article{openalex55,
 author = {Brożek J.L.},
 doi = {10.1016/j.jaci.2017.03.050},
 journal = {Journal of Allergy and Clinical Immunology},
 note = {1663},
 title = {Allergic Rhinitis and its Impact on Asthma (ARIA) guidelines—2016 revision},
 type = {Journal},
 year = {2017}
}

@article{openalex56,
 author = {Levin B.},
 doi = {10.3322/ca.2007.0018},
 journal = {CA A Cancer Journal for Clinicians},
 note = {1633},
 title = {Screening and Surveillance for the Early Detection of Colorectal Cancer and Adenomatous Polyps, 2008: A Joint Guideline from the American Cancer Society, the US Multi-Society Task Force on Colorectal Cancer, and the American College of Radiology},
 type = {Journal},
 year = {2008}
}

@article{openalex57,
 author = {Valtorta N.K.},
 doi = {10.1136/heartjnl-2015-308790},
 journal = {Heart},
 note = {1607},
 title = {Loneliness and social isolation as risk factors for coronary heart disease and stroke: systematic review and meta-analysis of longitudinal observational studies},
 type = {Journal},
 year = {2016}
}

@article{openalex58,
 author = {Roberts K.B.},
 doi = {10.1542/peds.2011-1330},
 journal = {PEDIATRICS},
 note = {1583},
 title = {Urinary Tract Infection: Clinical Practice Guideline for the Diagnosis and Management of the Initial UTI in Febrile Infants and Children 2 to 24 Months},
 type = {Journal},
 year = {2011}
}

@article{openalex59,
 author = {Faragher E.B.},
 doi = {10.1136/oem.2002.006734},
 journal = {Occupational and Environmental Medicine},
 note = {1569},
 title = {The relationship between job satisfaction and health: a meta-analysis},
 type = {Journal},
 year = {2005}
}

@article{openalex6,
 author = {Dellinger R.P.},
 doi = {10.1007/s00134-012-2769-8},
 journal = {Intensive Care Medicine},
 note = {7229},
 title = {Surviving Sepsis Campaign: International Guidelines for Management of Severe Sepsis and Septic Shock, 2012},
 type = {Journal},
 year = {2013}
}

@article{openalex60,
 author = {Brignole M.},
 doi = {10.1093/eurheartj/ehy037},
 journal = {European Heart Journal},
 note = {1560},
 title = {2018 ESC Guidelines for the diagnosis and management of syncope},
 type = {Journal},
 year = {2018}
}

@article{openalex61,
 author = {Morenga L.T.},
 doi = {10.1136/bmj.e7492},
 journal = {BMJ},
 note = {1556},
 title = {Dietary sugars and body weight: systematic review and meta-analyses of randomised controlled trials and cohort studies},
 type = {Journal},
 year = {2012}
}

@article{openalex62,
 author = {Gustafsson U.O.},
 doi = {10.1007/s00268-018-4844-y},
 journal = {World Journal of Surgery},
 note = {1556},
 title = {Guidelines for Perioperative Care in Elective Colorectal Surgery: Enhanced Recovery After Surgery (ERAS<sup>®</sup>) Society Recommendations: 2018},
 type = {Journal},
 year = {2018}
}

@article{openalex63,
 author = {Blaschke T.},
 doi = {10.1016/j.isprsjprs.2013.09.014},
 journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
 note = {1552},
 title = {Geographic Object-Based Image Analysis – Towards a new paradigm},
 type = {Journal},
 year = {2013}
}

@article{openalex64,
 author = {Williamson P.R.},
 doi = {10.1186/s13063-017-1978-4},
 journal = {Trials},
 note = {1521},
 title = {The COMET Handbook: version 1.0},
 type = {Journal},
 year = {2017}
}

@article{openalex65,
 author = {Rosselló-Mora R.},
 doi = {10.1111/j.1574-6976.2001.tb00571.x},
 journal = {FEMS Microbiology Reviews},
 note = {1500},
 title = {The species concept for prokaryotes},
 type = {Journal},
 year = {2001}
}

@article{openalex66,
 author = {Artstein R.},
 doi = {10.1162/coli.07-034-r2},
 journal = {Computational Linguistics},
 note = {1495},
 title = {Inter-Coder Agreement for Computational Linguistics},
 type = {Journal},
 year = {2008}
}

@article{openalex67,
 author = {Gillen S.},
 doi = {10.1371/journal.pmed.1000267},
 journal = {PLoS Medicine},
 note = {1490},
 title = {Preoperative/Neoadjuvant Therapy in Pancreatic Cancer: A Systematic Review and Meta-analysis of Response and Resection Percentages},
 type = {Journal},
 year = {2010}
}

@article{openalex68,
 author = {Johnson F.R.},
 doi = {10.1016/j.jval.2012.08.2223},
 journal = {Value in Health},
 note = {1485},
 title = {Constructing Experimental Designs for Discrete-Choice Experiments: Report of the ISPOR Conjoint Analysis Experimental Design Good Research Practices Task Force},
 type = {Journal},
 year = {2013}
}

@article{openalex69,
 author = {Langford B.J.},
 doi = {10.1016/j.cmi.2020.07.016},
 journal = {Clinical Microbiology and Infection},
 note = {1445},
 title = {Bacterial co-infection and secondary infection in patients with COVID-19: a living rapid review and meta-analysis},
 type = {Journal},
 year = {2020}
}

@article{openalex7,
 author = {Page M.J.},
 doi = {10.1136/bmj.n160},
 journal = {BMJ},
 note = {6557},
 title = {PRISMA 2020 explanation and elaboration: updated guidance and exemplars for reporting systematic reviews},
 type = {Journal},
 year = {2021}
}

@article{openalex70,
 author = {Wang Y.},
 doi = {10.1016/j.socscimed.2019.112552},
 journal = {Social Science & Medicine},
 note = {1420},
 title = {Systematic Literature Review on the Spread of Health-related Misinformation on Social Media},
 type = {Journal},
 year = {2019}
}

@article{openalex71,
 author = {Black A.D.},
 doi = {10.1371/journal.pmed.1000387},
 journal = {PLoS Medicine},
 note = {1409},
 title = {The Impact of eHealth on the Quality and Safety of Health Care: A Systematic Overview},
 type = {Journal},
 year = {2011}
}

@article{openalex72,
 author = {Jansen W.J.},
 doi = {10.1001/jama.2015.4668},
 journal = {JAMA},
 note = {1386},
 title = {Prevalence of Cerebral Amyloid Pathology in Persons Without Dementia},
 type = {Journal},
 year = {2015}
}

@article{openalex73,
 author = {Cevik M.},
 doi = {10.1016/s2666-5247(20)30172-5},
 journal = {The Lancet Microbe},
 note = {1363},
 title = {SARS-CoV-2, SARS-CoV, and MERS-CoV viral load dynamics, duration of viral shedding, and infectiousness: a systematic review and meta-analysis},
 type = {Journal},
 year = {2020}
}

@article{openalex74,
 author = {Sonis S.T.},
 doi = {10.1002/cncr.20162},
 journal = {Cancer},
 note = {1355},
 title = {Perspectives on cancer therapy-induced mucosal injury},
 type = {Journal},
 year = {2004}
}

@article{openalex75,
 author = {Saslow D.},
 doi = {10.3322/caac.21139},
 journal = {CA A Cancer Journal for Clinicians},
 note = {1353},
 title = {American Cancer Society, American Society for Colposcopy and Cervical Pathology, and American Society for Clinical Pathology screening guidelines for the prevention and early detection of cervical cancer},
 type = {Journal},
 year = {2012}
}

@article{openalex76,
 author = {Tonelli M.},
 doi = {10.1111/j.1600-6143.2011.03686.x},
 journal = {American Journal of Transplantation},
 note = {1341},
 title = {Systematic Review: Kidney Transplantation Compared With Dialysis in Clinically Relevant Outcomes},
 type = {Journal},
 year = {2011}
}

@article{openalex77,
 author = {Slater M.},
 doi = {10.3389/frobt.2016.00074},
 journal = {Frontiers in Robotics and AI},
 note = {1286},
 title = {Enhancing Our Lives with Immersive Virtual Reality},
 type = {Journal},
 year = {2016}
}

@article{openalex78,
 author = {Alesina A.},
 doi = {10.1257/jel.53.4.898},
 journal = {Journal of Economic Literature},
 note = {1250},
 title = {Culture and Institutions},
 type = {Journal},
 year = {2015}
}

@article{openalex79,
 author = {Woods A.},
 doi = {10.1016/j.clinph.2015.11.012},
 journal = {Clinical Neurophysiology},
 note = {1227},
 title = {A technical guide to tDCS, and related non-invasive brain stimulation tools},
 type = {Journal},
 year = {2015}
}

@article{openalex8,
 author = {Tierney J.F.},
 doi = {10.1186/1745-6215-8-16},
 journal = {Trials},
 note = {5566},
 title = {Practical methods for incorporating summary time-to-event data into meta-analysis},
 type = {Journal},
 year = {2007}
}

@article{openalex80,
 author = {Rasheed A.},
 doi = {10.1109/access.2020.2970143},
 journal = {IEEE Access},
 note = {1226},
 title = {Digital Twin: Values, Challenges and Enablers From a Modeling Perspective},
 type = {Journal},
 year = {2020}
}

@article{openalex81,
 author = {Twohig-Bennett C.},
 doi = {10.1016/j.envres.2018.06.030},
 journal = {Environmental Research},
 note = {1216},
 title = {The health benefits of the great outdoors: A systematic review and meta-analysis of greenspace exposure and health outcomes},
 type = {Journal},
 year = {2018}
}

@article{openalex82,
 author = {Murray E.},
 doi = {10.1186/1741-7015-8-63},
 journal = {BMC Medicine},
 note = {1192},
 title = {Normalisation process theory: a framework for developing, evaluating and implementing complex interventions},
 type = {Journal},
 year = {2010}
}

@article{openalex83,
 author = {Boulos M.N.K.},
 doi = {10.1186/1472-6920-6-41},
 journal = {BMC Medical Education},
 note = {1181},
 title = {Wikis, blogs and podcasts: a new generation of Web-based tools for virtual collaborative clinical practice and education},
 type = {Journal},
 year = {2006}
}

@article{openalex84,
 author = {Denny J.C.},
 doi = {10.1093/bioinformatics/btq126},
 journal = {Bioinformatics},
 note = {1172},
 title = {PheWAS: demonstrating the feasibility of a phenome-wide scan to discover gene–disease associations},
 type = {Journal},
 year = {2010}
}

@article{openalex85,
 author = {Sateia M.J.},
 doi = {10.5664/jcsm.6470},
 journal = {Journal of Clinical Sleep Medicine},
 note = {1172},
 title = {Clinical Practice Guideline for the Pharmacologic Treatment of Chronic Insomnia in Adults: An American Academy of Sleep Medicine Clinical Practice Guideline},
 type = {Journal},
 year = {2017}
}

@article{openalex86,
 author = {Mosa A.S.M.},
 doi = {10.1186/1472-6947-12-67},
 journal = {BMC Medical Informatics and Decision Making},
 note = {1161},
 title = {A Systematic Review of Healthcare Applications for Smartphones},
 type = {Journal},
 year = {2012}
}

@article{openalex87,
 author = {Rudolph J.},
 doi = {10.37074/jalt.2023.6.1.9},
 journal = {Journal of Applied Learning & Teaching},
 note = {1159},
 title = {ChatGPT: Bullshit spewer or the end of traditional assessments in higher education?},
 type = {Journal},
 year = {2023}
}

@article{openalex88,
 author = {Zhang B.},
 doi = {10.1093/sleep/29.1.85},
 journal = {SLEEP},
 note = {1132},
 title = {Sex Differences in Insomnia: A Meta-Analysis},
 type = {Journal},
 year = {2006}
}

@article{openalex89,
 author = {Jamison D.T.},
 doi = {10.1016/s0140-6736(13)62105-4},
 journal = {The Lancet},
 note = {1129},
 title = {Global health 2035: a world converging within a generation},
 type = {Journal},
 year = {2013}
}

@article{openalex9,
 author = {Chan A.},
 doi = {10.1136/bmj.e7586},
 journal = {BMJ},
 note = {4848},
 title = {SPIRIT 2013 explanation and elaboration: guidance for protocols of clinical trials},
 type = {Journal},
 year = {2013}
}

@article{openalex90,
 author = {Baskerville R.L.},
 doi = {10.17705/1cais.00219},
 journal = {Communications of the Association for Information Systems},
 note = {1123},
 title = {Investigating Information Systems with Action Research},
 type = {Journal},
 year = {1999}
}

@article{openalex91,
 author = {Kapoor K.K.},
 doi = {10.1007/s10796-017-9810-y},
 journal = {Information Systems Frontiers},
 note = {1097},
 title = {Advances in Social Media Research: Past, Present and Future},
 type = {Journal},
 year = {2017}
}

@article{openalex92,
 author = {Myers S.M.},
 doi = {10.1542/peds.2007-2362},
 journal = {PEDIATRICS},
 note = {1093},
 title = {Management of Children With Autism Spectrum Disorders},
 type = {Journal},
 year = {2007}
}

@article{openalex93,
 author = {Cuomo S.},
 doi = {10.1007/s10915-022-01939-z},
 journal = {Journal of Scientific Computing},
 note = {1083},
 title = {Scientific Machine Learning Through Physics–Informed Neural Networks: Where we are and What’s Next},
 type = {Journal},
 year = {2022}
}

@article{openalex94,
 author = {Khangura S.},
 doi = {10.1186/2046-4053-1-10},
 journal = {Systematic Reviews},
 note = {1045},
 title = {Evidence summaries: the evolution of a rapid review approach},
 type = {Journal},
 year = {2012}
}

@article{openalex95,
 author = {Sijs H.V.D.},
 doi = {10.1197/jamia.m1809},
 journal = {Journal of the American Medical Informatics Association},
 note = {1041},
 title = {Overriding of Drug Safety Alerts in Computerized Physician Order Entry},
 type = {Journal},
 year = {2005}
}

@article{openalex96,
 author = {Dichev C.},
 doi = {10.1186/s41239-017-0042-5},
 journal = {International Journal of Educational Technology in Higher Education},
 note = {1039},
 title = {Gamifying education: what is known, what is believed and what remains uncertain: a critical review},
 type = {Journal},
 year = {2017}
}

@article{openalex97,
 author = {Author U.},
 doi = {10.1136/hrt.2005.079988},
 journal = {Heart},
 note = {1037},
 title = {JBS 2: Joint British Societies' guidelines on prevention of cardiovascular disease in clinical practice},
 type = {Journal},
 year = {2005}
}

@article{openalex98,
 author = {Bhimraj A.},
 doi = {10.1093/cid/ciaa478},
 journal = {Clinical Infectious Diseases},
 note = {1017},
 title = {Infectious Diseases Society of America Guidelines on the Treatment and Management of Patients With COVID-19 (April 2020)},
 type = {Journal},
 year = {2020}
}

@article{openalex99,
 author = {Oh H.},
 doi = {10.2196/jmir.7.1.e1},
 journal = {Journal of Medical Internet Research},
 note = {1015},
 title = {What Is eHealth (3): A Systematic Review of Published Definitions},
 type = {Journal},
 year = {2005}
}

@article{Osaiweran_2010,
 abstract = {Analytical Software Design (ASD) is a design approach that combines formal and empirical methods for developing mathematically verified software systems. Unlike conventional design methods, the design phase is extended with more formal techniques, so that flaws are detected earlier, thereby reducing the time needed for coding, testing, and integration. In this paper, we demonstrate formal details and concepts behind the ASD approach, report about our experience with applying ASD in industrial control applications within Philips Healthcare, and discuss results and findings gathered during this work as well as some commonly faced issues and their practical solutions.},
 author = {Ammar Osaiweran and Aah Ammar Osaiweran and Ammar Osaiweran and M. Boosten and Marcel Boosten and Mohammad Reza Mousavi and Mohammad Reza Mousavi},
 doi = {null},
 journal = {null},
 mag_id = {1570128269},
 pmcid = {null},
 pmid = {null},
 title = {Analytical software design : introduction and industrial experience report},
 year = {2010}
}

@article{Osaiweran_2013,
 abstract = {We report about experiences at Philips Healthcare with component-based development supported by formal techniques. The formal Analytical Software Design (ASD) approach of the company Verum has been incorporated into the industrial workflow. The commercial tool ASD:Suite supports both compositional verification and code generation for control components. For other components test-driven development has been used. We discuss the results of these combined techniques in a project which developed the power control service of an interventional X-ray system.},
 author = {Ammar Osaiweran and Ammar Osaiweran and Mathijs Schuts and Mathijs Schuts and Jozef Hooman and Jozef Hooman and Jacco Wesselius and J.H. Wesselius},
 doi = {10.1016/j.entcs.2013.04.005},
 journal = {Electronic Notes in Theoretical Computer Science},
 mag_id = {1966848862},
 pmcid = {null},
 pmid = {null},
 title = {Incorporating Formal Techniques into Industrial Practice: an Experience Report},
 year = {2013}
}

@article{Osaiweran_2014,
 abstract = {We report about experiences at Philips Healthcare with component-based development supported by formal techniques. The formal Analytical Software Design (ASD) approach of the company Verum has been incorporated into the industrial workflow. The commercial tool ASD:Suite supports both compositional verification and code generation for control components. For other components test-driven development has been used. We discuss the results of these combined techniques in a project which developed the power control service of an interventional X-ray system.},
 author = {Ammar Osaiweran and Ammar Osaiweran and Mathijs Schuts and Mathijs Schuts and Jozef Hooman and Jozef Hooman},
 doi = {10.1007/s10664-013-9251-2},
 journal = {Empirical Software Engineering},
 mag_id = {2081100947},
 pmcid = {null},
 pmid = {null},
 title = {Experiences with incorporating formal techniques into industrial practice},
 year = {2014}
}

@article{Pajić_2012,
 abstract = {Model-Driven Design (MDD) of cyber-physical systems advocates for design procedures that start with formal modeling of the real-time system, followed by the model's verification at an early stage. The verified model must then be translated to a more detailed model for simulation-based testing and finally translated into executable code in a physical implementation. As later stages build on the same core model, it is essential that models used earlier in the pipeline are valid approximations of the more detailed models developed downstream. The focus of this effort is on the design and development of a model translation tool, UPP2SF, and how it integrates system modeling, verification, model-based WCET analysis, simulation, code generation and testing into an MDD based framework. UPP2SF facilitates automatic conversion of verified timed automata-based models (in UPPAAL) to models that may be simulated and tested (in Simulink/State flow). We describe the design rules to ensure the conversion is correct, efficient and applicable to a large class of models. We show how the tool enables MDD of an implantable cardiac pacemaker. We demonstrate that UPP2SF preserves behaviors of the pacemaker model from UPPAAL to State flow. The resultant State flow chart is automatically converted into C and tested on a hardware platform for a set of requirements.},
 author = {Miroslav Pajić and Miroslav Pajic and Zhihao Jiang and Zhihao Jiang and Insup Lee and Insup Lee and Oleg Sokolsky and Oleg Sokolsky and Rahul Mangharam and Rahul Mangharam},
 doi = {10.1109/rtas.2012.25},
 journal = {null},
 mag_id = {1995146104},
 pmcid = {null},
 pmid = {null},
 title = {From Verification to Implementation: A Model Translation Tool and a Pacemaker Case Study},
 year = {2012}
}

@article{Pervez_2014,
 abstract = {Fast Health Interoperable Resources (FHIR) is the recently proposed standard from HL7. Its distinguishing features include the user friendly implementation, support of built-in terminologies and for widely-used web standards. Given the safety-critical nature of FHIR, the rigorous analysis of e-health systems using the FHIR is a dire need since they are prone to failures. As a first step towards this direction, we propose to use probabilistic model checking, i.e., a formal probabilistic analysis approach, to assess the reliability of a typical e-health system used in hospitals based on the FHIR standard. In particular, we use the PRISM model checker to analyze the Markov Decision Process (MDP) and Continuous Time Markov Chain (CTMC) models to assess the failure probabilities of the overall system.},
 author = {Usman Pervez and Usman Pervez and Osman Hasan and Osman Hasan and Khalid Latif and Khalid Latif and Sofiène Tahar and Sofiène Tahar and Amjad Gawanmeh and Amjad Gawanmeh and Mohamed Salah Hamdi and Mohamed Hamdi},
 doi = {10.1109/healthcom.2014.7001811},
 journal = {null},
 mag_id = {2089014229},
 pmcid = {null},
 pmid = {null},
 title = {Formal reliability analysis of a typical FHIR standard based e-Health system using PRISM},
 year = {2014}
}

@article{Petersen_2015,
 abstract = {null},
 author = {Kai Petersen and Kai Petersen and Sairam Vakkalanka and Sairam Vakkalanka and Ludwik Kuźniarz and Ludwik Kuzniarz},
 doi = {10.1016/j.infsof.2015.03.007},
 journal = {Information & Software Technology},
 mag_id = {1999798506},
 pmcid = {null},
 pmid = {null},
 title = {Guidelines for conducting systematic mapping studies in software engineering : An update},
 year = {2015}
}

@article{Plagge_2007,
 abstract = {We present the architecture and implementation of the proz tool to validate high-level Z specifications. The tool was integrated into prob, by providing a translation of Z into B and by extending the kernel of prob to accommodate some new syntax and data types. We describe the challenge of going from the tool friendly formalism B to the more specification-oriented formalism Z, and show how many Z specifications can be systematically translated into B. We describe the extensions, such as record types and free types, that had to be added to the kernel to support a large subset of Z. As a side-effect, we provide a way to animate and model check records in prob. By incorporating proz into prob, we have inherited many of the recent extensions developed for B, such as the integration with CSP or the animation of recursive functions. Finally, we present a successful industrial application, which makes use of this fact, and where proz was able to discover several errors in Z specifications containing higher-order recursive functions.},
 author = {Daniel Plagge and Daniel Plagge and Michaël Leuschel and Michael Leuschel},
 doi = {10.1007/978-3-540-73210-5_25},
 journal = {null},
 mag_id = {1491364274},
 pmcid = {null},
 pmid = {null},
 title = {Validating Z specifications using the PROB animator and model checker},
 year = {2007}
}

@article{Raffo_2005,
 abstract = {null},
 author = {David Raffo and David Raffo},
 doi = {10.1016/j.infsof.2005.09.004},
 journal = {Information & Software Technology},
 mag_id = {2036734551},
 pmcid = {null},
 pmid = {null},
 title = {Software project management using PROMPT: A hybrid metrics, modeling and utility framework},
 year = {2005}
}

@article{Rasch_2003,
 abstract = {One of the main advantages of the UML is its possibility to model different views on a system using a range of diagram types. The various diagrams can be used to specify different aspects, and their combination makes up the complete system description. This does, however, always pose the question of consistency: it may very well be the case that the designer has specified contradictory requirements which can never be fulfilled together.},
 author = {Holger Rasch and Holger Rasch and Heike Wehrheim and Heike Wehrheim},
 doi = {10.1007/978-3-540-39958-2_16},
 journal = {null},
 mag_id = {1489862800},
 pmcid = {null},
 pmid = {null},
 title = {Checking Consistency in UML Diagrams: Classes and State Machines},
 year = {2003}
}

@article{Reinhartz-Berger_2014,
 abstract = {Feature modeling is a common way to present and manage variability of software and systems. As a prerequisite for effective variability management is comprehensible representation, the main aim of this paper is to investigate difficulties in understanding feature models. In particular, we focus on the comprehensibility of feature models as expressed in Common Variability Language (CVL), which was recommended for adoption as a standard by the Architectural Board of the Object Management Group. Using an experimental approach with participants familiar and unfamiliar with feature modeling, we analyzed comprehensibility in terms of comprehension score, time spent to complete tasks, and perceived difficulty of different feature modeling constructs. The results showed that familiarity with feature modeling did not influence the comprehension of mandatory, optional, and alternative features, although unfamiliar modelers perceived these elements more difficult than familiar modelers. OR relations were perceived as difficult regardless of the familiarity level, while constraints were significantly better understood by familiar modelers. The time spent to complete tasks was higher for familiar modelers.},
 author = {Iris Reinhartz-Berger and Iris Reinhartz-Berger and Kathrin Figl and Kathrin Figl and Øystein Haugen and Øystein Haugen},
 doi = {10.1007/978-3-319-11653-2_31},
 journal = {null},
 mag_id = {230555665},
 pmcid = {null},
 pmid = {null},
 title = {Comprehending Feature Models Expressed in CVL},
 year = {2014}
}

@article{Reinhartz-Berger_2014,
 abstract = {Software Product Line Engineering (SPLE) deals with developing artifacts that capture the common and variable aspects of software product families. Domain models are one kind of such artifacts. Being developed in early stages, domain models need to specify commonality and variability and guide the reuse of the artifacts in particular software products. Although different modeling methods have been proposed to manage and support these activities, the assessment of these methods is still in an inceptive stage. In this work, we examined the comprehensibility of domain models specified in ADOM, a UML-based SPLE method. In particular, we conducted a controlled experiment in which 116 undergraduate students were required to answer comprehension questions regarding a domain model that was equipped with explicit reuse guidance and/or variability specification. We found that explicit specification of reuse guidance within the domain model helped understand the model, whereas explicit specification of variability increased comprehensibility only to a limited extent. Explicit specification of both reuse guidance and variability often provided intermediate results, namely, results that were better than specification of variability without reuse guidance, but worse than specification of reuse guidance without variability. All these results were perceived in different UML diagram types, namely, use case, class, and sequence diagrams and for different commonality-, variability-, and reuse-related aspects.},
 author = {Iris Reinhartz-Berger and Iris Reinhartz-Berger and Arnon Sturm and Arnon Sturm},
 doi = {10.1007/s10664-012-9234-8},
 journal = {Empirical Software Engineering},
 mag_id = {2098419850},
 pmcid = {null},
 pmid = {null},
 title = {Comprehensibility of UML-based software product line specifications},
 year = {2014}
}

@article{Rivera_2017,
 abstract = {Event-B is a modelling language and a formal methods approach for correct construction of software. This paper presents our work on code generation for Event-B, including the definition of a syntactic translation from Event-B to JML-annotated Java programs, the implementation of the translation as the EventB2Java tool, and two case studies on the use of EventB2Java. The first case study is on implementing an Android application with the aid of the EventB2Java tool, and the second on testing an Event-B specification of the Tokeneer security-critical system. Additionally, we have benchmarked our EventB2Java tool against two other Java code generators for Event-B.},
 author = {Victor M. Rivera and Victor Rivera and Néstor Cataño and Néstor Cataño and Tim Wahls and Tim Wahls and Camilo Rueda and Camilo Rueda},
 doi = {10.1007/s10009-015-0381-2},
 journal = {International Journal on Software Tools for Technology Transfer},
 mag_id = {308378599},
 pmcid = {null},
 pmid = {null},
 title = {Code generation for Event-B},
 year = {2017}
}

@article{Rohmer_2013,
 abstract = {From exploring planets to cleaning homes, the reach and versatility of robotics is vast. The integration of actuation, sensing and control makes robotics systems powerful, but complicates their simulation. This paper introduces a versatile, scalable, yet powerful general-purpose robot simulation framework called V-REP. The paper discusses the utility of a portable and flexible simulation framework that allows for direct incorporation of various control techniques. This renders simulations and simulation models more accessible to a general-public, by reducing the simulation model deployment complexity. It also increases productivity by offering built-in and ready-to-use functionalities, as well as a multitude of programming approaches. This allows for a multitude of applications including rapid algorithm development, system verification, rapid prototyping, and deployment for cases such as safety/remote monitoring, training and education, hardware control, and factory automation simulation.},
 author = {Eric Rohmer and Eric Rohmer and Surya P. N. Singh and Surya P. N. Singh and Surya Prakash Singh and Surya P. N. Singh and Marc Freese and Marc Freese},
 doi = {10.1109/iros.2013.6696520},
 journal = {null},
 mag_id = {2082511574},
 pmcid = {null},
 pmid = {null},
 title = {V-REP: A versatile and scalable robot simulation framework},
 year = {2013}
}

@article{Sadigh_2014,
 abstract = {We address the problem of formally verifying quantitative properties of driver models. We first propose a novel stochastic model of the driver behavior based on Convex Markov Chains, i.e., Markov chains in which the transition probabilities are only known to lie in convex uncertainty sets. This formalism captures the intrinsic uncertainty in estimating transition probabilities starting from experimentally-collected data. We then formally verify properties of the model expressed in probabilistic computation tree logic (PCTL). Results show that our approach can correctly predict quantitative information about driver behavior depending on her state, e.g., whether he or she is attentive or distracted. Copyright © 2014, Association for the Advancement of Artificial Intelligence. All rights reserved.},
 author = {Dorsa Sadigh and Dorsa Sadigh and Katherine Driggs-Campbell and Katherine Driggs-Campbell and Alberto Puggelli and Alberto Puggelli and Wenchao Li and Wenchao Li and Victor Shia and Victor Shia and Růžena Bajcsy and Ruzena Bajcsy and Alberto Sangiovanni‐Vincentelli and Alberto Sangiovanni-Vincentelli and S. Shankar Sastry and Shankar Sastry and Sanjit A. Seshia and Sanjit A. Seshia},
 doi = {null},
 journal = {null},
 mag_id = {2319542951},
 pmcid = {null},
 pmid = {null},
 title = {Data-Driven Probabilistic Modeling and Verification of Human Driver Behavior},
 year = {2014}
}

@article{Said_2009,
 abstract = {UML-B is a `UML-like' graphical front end for Event-B that provides support for object-oriented modelling concepts. In particular, UML-B supports class diagrams and state machines, concepts that are not explicitly supported in plain Event-B. In Event-B, refinement is used to relate system models at different abstraction levels. The same abstraction-refinement concepts can also be applied in UML-B. This paper introduces the notions of refined classes and refined state machines to enable refinement of classes and state machines in UML-B. Together with these notions, a technique for moving an event between classes to facilitate abstraction is also introduced. Our work makes explicit the structures of class and state machine refinement in UML-B. The UML-B drawing tool and Event-B translator are extended to support the new refinement concepts. A case study of an auto teller machine (ATM) is presented to demonstrate application and effectiveness of refined classes and refined state machines.},
 author = {Mar Yah Said and Mar Yah Said and Michael Butler and Michael Butler and Colin Snook and Colin Snook},
 doi = {10.1007/978-3-642-05089-3_37},
 journal = {null},
 mag_id = {1521713193},
 pmcid = {null},
 pmid = {null},
 title = {Language and Tool Support for Class and State Machine Refinement in UML-B},
 year = {2009}
}

@article{Said_2015,
 abstract = {UML-B is a `UML-like' graphical front-end for Event-B that provides support for object-oriented and state machine modelling concepts, which are not available in Event-B. In particular, UML-B includes class diagram and state machine diagram editors with automatic generation of corresponding Event-B. In Event-B, refinement is used to relate system models at different abstraction levels. The same refinement concepts are also applicable in UML-B but require special consideration due to the higher-level modelling concepts. In previous work, we described a case study to introduce support for refinement in UML-B. We now provide a more complete presentation of the technique of refinement in UML-B including a formalisation of the refinement rules and a definition of the extensions to the abstract syntax of UML-B notation. The provision of gluing invariants to discharge the proof obligations associated with a refinement is a significant step in providing verifiable models. We discuss and compare two approaches for constructing gluing invariants in the context of UML-B refinement.},
 author = {Mar Yah Said and Mar Yah Said and Michael Butler and Michael Butler and Colin Snook and Colin Snook},
 doi = {10.1007/s10270-013-0391-z},
 journal = {Software and Systems Modeling},
 mag_id = {1986788631},
 pmcid = {null},
 pmid = {null},
 title = {A method of refinement in UML-B},
 year = {2015}
}

@article{Schmaltz_2008,
 abstract = {Conformance testing for labeled transition systems starts with defining when an implementation conforms to its specification. One of the formal theories for model-based testing uses the implementation relation  ioco for this purpose. A peculiar aspect of  ioco is to consider the absence of outputs as an observable action, named quiescence. Recently a number of real-time extensions of  ioco have been proposed in the literature. Quiescence and the observation of arbitrary delays are issues when defining such extensions. We present two new timed implementation relations and show their relation with existing ones. Based on these new definitions and using several examples, we show the subtle differences, and the consequences that small modifications in the definitions can have on the resulting relations. Moreover, we present conditions under which some of these implementation relations coincide. The notion of M-quiescence, i.e., if outputs occur in a system they occur before a delay M, turns out to be important in these conditions.},
 author = {Julien Schmaltz and Julien Schmaltz and Jan Tretmans and Jan Tretmans},
 doi = {10.1007/978-3-540-85778-5_18},
 journal = {null},
 mag_id = {1586681669},
 pmcid = {null},
 pmid = {null},
 title = {On Conformance Testing for Timed Systems},
 year = {2008}
}

@article{SCOPUSAbdilahiS.A.202414,
 author = {Abdilahi S.A.},
 doi = {10.1177/20503121241259862},
 journal = {SAGE Open Medicine},
 note = {1},
 title = {Epidemiology of stunting in children aged 6–59 months, an unresolved chronic nutritional problem in Ethiopia: A systematic review and meta-analysis},
 type = {Journal},
 year = {2024}
}

@article{SCOPUSArmfieldN.R.20151,
 author = {Armfield N.R.},
 doi = {10.1016/j.ijmedinf.2015.06.006},
 journal = {International Journal of Medical Informatics},
 note = {114},
 title = {The clinical use of Skype-For which patients, with which problems and in which settings? A snapshot review of the literature},
 type = {Journal},
 year = {2015}
}

@article{SCOPUSDiasC.R.20175,
 author = {Dias C.R.},
 doi = {10.1016/j.jbi.2017.10.004},
 journal = {Journal of Biomedical Informatics},
 note = {13},
 title = {Qualitative review of usability problems in health information systems for radiology},
 type = {Journal},
 year = {2017}
}

@article{SCOPUSGlennyA.M.20050,
 author = {Glenny A.M.},
 doi = {10.3310/hta9260},
 journal = {Health Technology Assessment},
 note = {525},
 title = {Indirect comparisons of competing interventions},
 type = {Journal},
 year = {2005}
}

@article{SCOPUSGuoJ.202219,
 author = {Guo J.},
 doi = {10.1136/bmjopen-2021-060249},
 journal = {BMJ Open},
 note = {0},
 title = {Effect of tibial transverse transport on chronic lower extremity angiopathy: a protocol for a systematic review and meta-analysis},
 type = {Journal},
 year = {2022}
}

@article{SCOPUSHaberA.C.20226,
 author = {Haber A.C.},
 doi = {10.1093/bib/bbac440},
 journal = {Briefings in Bioinformatics},
 note = {9},
 title = {Open tools for quantitative anonymization of tabular phenotype data: literature review},
 type = {Journal},
 year = {2022}
}

@article{SCOPUSHarkess-MurphyE.202416,
 author = {Harkess-Murphy E.},
 doi = {10.1136/bmjopen-2024-090202},
 journal = {BMJ open},
 note = {0},
 title = {Trauma-informed family carer education and practical skills training in dementia: a systematic scoping review protocol},
 type = {Journal},
 year = {2024}
}

@article{SCOPUSKaforauL.S.K.202113,
 author = {Kaforau L.S.K.},
 doi = {10.1136/bmjopen-2020-042423},
 journal = {BMJ Open},
 note = {2},
 title = {Prevalence and risk factors of adverse birth outcomes in the Pacific Island region: A scoping review protocol},
 type = {Journal},
 year = {2021}
}

@article{SCOPUSKasoA.W.202521,
 author = {Kaso A.W.},
 doi = {10.1186/s12962-025-00620-0},
 journal = {Cost Effectiveness and Resource Allocation},
 note = {0},
 title = {Willingness to join community-based health insurance and associated factors among households in Ethiopian: a systematic review and meta-analysis},
 type = {Journal},
 year = {2025}
}

@article{SCOPUSMenonP.20219,
 author = {Menon P.},
 doi = {10.1136/bmjopen-2020-044456},
 journal = {BMJ Open},
 note = {4},
 title = {Risk factors associated with quad bike crashes: A protocol for systematic review of observational studies},
 type = {Journal},
 year = {2021}
}

@article{SCOPUSSinhaM.K.202211,
 author = {Sinha M.K.},
 doi = {10.1136/bmjopen-2020-047821},
 journal = {BMJ Open},
 note = {3},
 title = {Exercise dose-response relationship with heart rate variability in individuals with overweight and obesity: protocol for a systematic review and meta-analysis of randomised controlled trials},
 type = {Journal},
 year = {2022}
}

@article{SCOPUSTalmorG.202112,
 author = {Talmor G.},
 doi = {10.1177/0003489421990149},
 journal = {Annals of Otology, Rhinology and Laryngology},
 note = {3},
 title = {Vocal Fold Motion Impairment Following Chemotherapy Administration: Case Reports and Review of the Literature},
 type = {Journal},
 year = {2021}
}

@article{SCOPUSVaabenN.202515,
 author = {Vaaben N.},
 doi = {10.1111/aas.14591},
 journal = {Acta Anaesthesiologica Scandinavica},
 note = {0},
 title = {Clinical leadership education—A scoping review protocol},
 type = {Journal},
 year = {2025}
}

@article{SCOPUSVanHooffM.20142,
 author = {Van Hooff M.},
 doi = {10.1371/journal.pone.0104226},
 journal = {PLoS ONE},
 note = {48},
 title = {The Nijmegen decision tool for chronic low back pain. Development of a clinical decision tool for secondary or tertiary spine care specialists},
 type = {Journal},
 year = {2014}
}

@article{SCOPUSWungBuhA.202110,
 author = {Wung Buh A.},
 doi = {10.1136/bmjopen-2020-043042},
 journal = {BMJ Open},
 note = {4},
 title = {Effects of implementing Pressure Ulcer Prevention Practice Guidelines (PUPPG) in the prevention of pressure ulcers among hospitalised elderly patients: A systematic review protocol},
 type = {Journal},
 year = {2021}
}

@article{SCOPUSXuK.20218,
 author = {Xu K.},
 doi = {10.1136/bmjopen-2021-049039},
 journal = {BMJ Open},
 note = {5},
 title = {Effects of moxibustion on reproduction and metabolism of polycystic ovary syndrome: A protocol for meta-analysis and systematic review},
 type = {Journal},
 year = {2021}
}

@article{SCOPUSYaoQ.20157,
 author = {Yao Q.},
 doi = {10.1136/bmjopen-2015-007704},
 journal = {BMJ Open},
 note = {9},
 title = {Acupuncture for patients with chronic Urticaria: A systematic review protocol},
 type = {Journal},
 year = {2015}
}

@article{SCOPUSZhaiJ.20164,
 author = {Zhai J.},
 doi = {10.1016/j.jclinepi.2016.02.023},
 journal = {Journal of Clinical Epidemiology},
 note = {15},
 title = {Reporting of core items in hierarchical Bayesian analysis for aggregating N-of-1 trials to estimate population treatment effects is suboptimal},
 type = {Journal},
 year = {2016}
}

@article{Serrador_2015,
 abstract = {null},
 author = {Pedro Serrador and Pedro Serrador and Jeffrey K. Pinto and Jeffrey K. Pinto},
 doi = {10.1016/j.ijproman.2015.01.006},
 journal = {International Journal of Project Management},
 mag_id = {2051526268},
 pmcid = {null},
 pmid = {null},
 title = {Does Agile work? - A quantitative analysis of agile project success},
 year = {2015}
}

@article{Shafique_2015,
 abstract = {Model-based testing (MBT) is about testing a software system using a model of its behaviour. To benefit fully from MBT, automation support is required. The goal of this systematic review is determining the current state of the art of prominent MBT tool support where we focus on tools that rely on state-based models. We automatically searched different source of information including digital libraries and mailing lists dedicated to the topic. Precisely defined criteria are used to compare selected tools and comprise support for test adequacy and coverage criteria, level of automation for various testing activities and support for the construction of test scaffolding. Simple adequacy criteria are supported but not advanced ones; data(-flow) criteria are seldom supported; support for creating test scaffolding varies a great deal. The results of this review should be of interest to a wide range of stakeholders: software companies interested in selecting the most appropriate MBT tool for their needs; organizations willing to invest into creating MBT tool support; researchers interested in setting research directions.},
 author = {Muhammad Shafique and Muhammad Shafique and Yvan Labiche and Yvan Labiche},
 doi = {10.1007/s10009-013-0291-0},
 journal = {International Journal on Software Tools for Technology Transfer},
 mag_id = {2066672703},
 pmcid = {null},
 pmid = {null},
 title = {A systematic review of state-based test tools},
 year = {2015}
}

@article{Shull_2007,
 abstract = {This book gathers chapters from some of the top international empirical software engineering researchers focusing on the practical knowledge necessary for conducting, reporting and using empirical methods in software engineering. Topics and features include guidance on how to design, conduct and report empirical studies. The volume also provides information across a range of techniques, methods and qualitative and quantitative issues to help build a toolkit applicable to the diverse software development contexts},
 author = {Forrest Shull and Forrest Shull and Janice Singer and Janice Singer and Dag I. K. Sjøberg and Dag I. K. Sjøberg},
 doi = {10.1007/978-1-84800-044-5},
 journal = {null},
 mag_id = {1564623840},
 pmcid = {null},
 pmid = {null},
 title = {Guide to Advanced Empirical Software Engineering},
 year = {2007}
}

@article{Sifakis_2013,
 abstract = {The monograph advocates rigorous system design as a coherent and accountable model-based process leading from requirements to correct implementations. It presents the current state of the art in system design, discusses its limitations, and identifies possible avenues for overcoming them.A rigorous system design flow is defined as a formal accountable and iterative process composed of steps, and based on four principles: (1) separation of concerns; (2) component-based construction; (3) semantic coherency; and (4) correctness-by-construction. The combined application of these principles allows the definition of a methodology clearly identifying where human intervention and ingenuity are needed to resolve design choices, as well as activities that can be supported by tools to automate tedious and error-prone tasks. An implementable system model is progressively derived by source-to-source automated transformations in a single host component-based language rooted in well-defined semantics. Using a single modeling language throughout the design flow enforces semantic coherency. Correct-by-construction techniques allow well-known limitations of a posteriori verification to be overcome and ensure accountability. It is possible to explain, at each design step, which among the requirements are satisfied and which may not be satisfied.The presented view for rigorous system design has been amply implemented in the BIP (Behavior, Interaction, Priority) component framework and substantiated by numerous experimental results showing both its relevance and feasibility.The monograph concludes with a discussion advocating a systemcentric vision for computing, identifying possible links with other disciplines, and emphasizing centrality of system design.},
 author = {Joseph Sifakis and Joseph Sifakis},
 doi = {10.1561/1000000034},
 journal = {Foundations and Trends in Electronic Design Automation},
 mag_id = {1975146975},
 pmcid = {null},
 pmid = {null},
 title = {Rigorous System Design},
 year = {2013}
}

@article{Simalatsar_2014,
 abstract = {Nowadays medical software is tightly coupled with medical devices that perform patient state monitoring and lately even some basic treatment procedures. Medical guidelines (GLs) can be seen as specification of a medical system which requires their computer-interpretable representation of medical GLs. Until now most of the medical GLs are often represented in a textual format and therefore often suffer from such structural problems as incompleteness, inconsistencies, ambiguity and redundancy, which makes the translation process to the machine-interpretable language more complicated. Computer-based interpretation of GLs can improve the quality of protocols as well as the quality of medical service. Several GLs formal representation methods have been presented recently. Only some of them enable automatic formal verification by introducing an additional translation path to the existing model checking environments. However, if a verified property fails it is difficult to trace back the result needed to change the model. Moreover, these formalisms provide the notion of time mostly in terms of actions order. In this paper we preset the application of a well-know formal behaviour representation approach of embedded systems design domain to medical GLs interpretation. We use Timed Automata extended with Tasks (TAT) and TIMES toolbox to represent medical GLs as a system behaviour in a computer interpretable form. We discuss the verification issues with the help of the anticancer drug imatinib case study.},
 author = {Alena Simalatsar and Alena Simalatsar and Wenqi You and Wenqi You and Verena Gotta and Verena Gotta and Verena Gotta and Nicolas Widmer and Nicolas Widmer and Giovanni De Micheli and Giovanni De Micheli},
 doi = {10.1142/s0218213014600033},
 journal = {International Journal on Artificial Intelligence Tools},
 mag_id = {2061553015},
 pmcid = {null},
 pmid = {null},
 title = {Representation of Medical Guidelines with a Computer Interpretable Model},
 year = {2014}
}

@article{Skulkittiyut_2010,
 abstract = {Intelligent Space (IS), a kind of intelligent home, is one of the popular of the applying RT into our daily life. In intelligent space environment, human behavior is one of the meaningful information for interpretation of our intention and needs. In this paper, we proposed the human posture recognition approach which focuses on a top-view vision. A top-view vision enables our system to observe the whole area with single camera and also avoid horizontal obstacle. Moreover, a human posture feature which based on DFT (Discrete-time Fourier Transform) coefficients of log-polar histogram has been proposed. We have categorized human behavior to two types. The first one is a stationary behavior which consists of standing, sitting and lying. The second one is a moving behavior.},
 author = {Weerachai Skulkittiyut and Skulkittiyut Weerachai and Makoto Mizukawa and Makoto Mizukawa},
 doi = {10.1109/iccas.2010.5669768},
 journal = {null},
 mag_id = {1567036401},
 pmcid = {null},
 pmid = {null},
 title = {Human behavior recognition via top-view vision for intelligent space},
 year = {2010}
}

@article{Slissenko_2008,
 abstract = {We describe a prototype of a simulator for reactive timed abstract state machines (ASM) that checks whether the generated runs verify a requirements specification represented as a formula of a First Order Timed Logic (FOTL). The simulator deals with ASM with continuous or discrete time. The time constraints are linear inequalities. It can treat two semantics, one with instantaneous actions and another one with delayed actions, the delays being bounded and non-deterministic.},
 author = {Anatol Slissenko and Anatol Slissenko and Pavel Vasilyev and Pavel Vasilyev and Pavel Vasilyev},
 doi = {null},
 journal = {Journal of Universal Computer Science},
 mag_id = {138509690},
 pmcid = {null},
 pmid = {null},
 title = {Simulation of Timed Abstract State Machines with Predicate Logic Model-Checking.},
 year = {2008}
}

@article{Smith_1999,
 abstract = {1 Introduction.- 1.1 Motivation.- 1.2 Classes.- 1.3 Objects.- 1.4 Inheritance.- 1.5 Polymorphism.- 1.6 Case Study: Tetris.- 2 Semantic Basis.- 2.1 Object Identity.- 2.1.1 Types and values.- 2.1.2 Forward declaration.- 2.1.3 Self-reference.- 2.2 Objects.- 2.2.1 Objects vs. object identities.- 2.2.2 Forward declaration revisited.- 2.3 Modularity and Compositionality.- 2.3.1 Object coupling.- 2.3.2 Object aliasing.- 2.3.3 Object containment.- 3 Syntactic Constructs.- 3.1 Class Definitions.- 3.2 Visibility Lists.- 3.3 Inherited Classes.- 3.3.1 Cancellation and redefinition of features.- 3.4 Local Definitions.- 3.4.1 Basic types.- 3.4.2 Axiomatic definitions.- 3.4.3 Abbreviation definitions.- 3.4.4 Free types.- 3.5 State Schemas.- 3.6 Initial State Schemas.- 3.7 Operations.- 3.7.1 Operation schemas.- 3.7.2 Operation promotions.- 3.7.3 Operation operators.- 3.7.4 Distributed operators.- 3.7.5 Recursion.- 3.8 Predicates.- 3.8.1 Boolean-valued expressions.- 3.8.2 Promoted initial state predicates.- 3.9 Expressions.- 3.9.1 Class names.- 3.9.2 Polymorphism.- 3.9.3 Class union.- 3.9.4 Object containment.- 3.9.5 Promoted attributes.- 3.9.6 Self.- 4 Language Definition.- 4.1 Meta-Functions.- 4.2 Global Paragraphs.- 4.3 Class Paragraphs.- 4.4 Operation Expressions.- 4.5 Predicates.- 4.6 Expressions.- 5 Concurrent Systems.- 5.1 Aggregation.- 5.2 Synchronization.- 5.3 Communication.- 5.4 Nondeterminism.- 5.5 Case Study: Hearts.- 6 Concrete Syntax.- 6.1 Specifications.- 6.2 Global Paragraphs.- 6.3 Class Paragraphs.- 6.4 Operation Expressions.- 6.5 Schema Expressions.- 6.6 Declarations.- 6.7 Predicates.- 6.8 Expressions.},
 author = {Graeme Smith and Graeme Smith},
 doi = {null},
 journal = {null},
 mag_id = {1564637213},
 pmcid = {null},
 pmid = {null},
 title = {The Object-Z specification language},
 year = {1999}
}

@article{Snook_2001,
 abstract = {The recognised deficiency in the level of empirical investigation of software engineering methods is particularly acute in the area of formal methods, where reports about their usefulness vary widely. We interviewed several formal methods users about the use of formal methods and their impact on various aspects of software engineering including the effects on the company, its products and its development processes as well as pragmatic issues such as scalability, understandability and tool support. The interviews are a first stage of empirical assessment. Future work will investigate some of the issues raised using formal experimentation and case studies.},
 author = {Colin Snook and Colin Snook and Rachel Harrison and Rachel Harrison},
 doi = {10.1016/s0950-5849(00)00166-x},
 journal = {Information & Software Technology},
 mag_id = {2060921624},
 pmcid = {null},
 pmid = {null},
 title = {Practitioners' views on the use of formal methods: an industrial survey by structured interview},
 year = {2001}
}

@article{Snook_2004,
 abstract = {null},
 author = {Colin Snook and Colin Snook and Rachel Harrison and Rachel Harrison},
 doi = {10.1016/j.infsof.2004.04.003},
 journal = {Information & Software Technology},
 mag_id = {2000538740},
 pmcid = {null},
 pmid = {null},
 title = {Experimental comparison of the comprehensibility of a Z specification and its implementation in Java},
 year = {2004}
}

@article{Snook_2006,
 abstract = {The emergence of the UML as a de facto standard for object-oriented modeling has been mirrored by the success of the B method as a practically useful formal modeling technique. The two notations have much to offer each other. The UML provides an accessible visualization of models facilitating communication of ideas but lacks formal precise semantics. B, on the other hand, has the precision to support animation and rigorous verification but requires significant effort in training to overcome the mathematical barrier that many practitioners perceive. We utilize a derivation of the B notation as an action and constraint language for the UML and define the semantics of UML entities via a translation into B. Through the UML-B profile we provide specializations of UML entities to support model refinement. The result is a formally precise variant of UML that can be used for refinement based, object-oriented behavioral modeling. The design of UML-B has been guided by industrial applications.},
 author = {Colin Snook and Colin Snook and Michael Butler and Michael Butler},
 doi = {10.1145/1125808.1125811},
 journal = {ACM Transactions on Software Engineering and Methodology},
 mag_id = {1985804037},
 pmcid = {null},
 pmid = {null},
 title = {UML-B: Formal modeling and design aided by UML},
 year = {2006}
}

@article{Stump_2014,
 abstract = {We introduce StarExec, a public web-based service built to facilitate the experimental evaluation of logic solvers, broadly understood as automated tools based on formal reasoning. Examples of such tools include theorem provers, SAT and SMT solvers, constraint solvers, model checkers, and software verifiers. The service, running on a compute cluster with 380 processors and 23 terabytes of disk space, is designed to provide a single piece of storage and computing infrastructure to logic solving communities and their members. It aims at reducing duplication of effort and resources as well as enabling individual researchers or groups with no access to comparable infrastructure. StarExec allows community organizers to store, manage and make available benchmark libraries; competition organizers to run logic solver competitions; and community members to do comparative evaluations of logic solvers on public or private benchmark problems.},
 author = {Aaron Stump and Aaron Stump and Geoff Sutcliffe and Geoff Sutcliffe and Geoffrey Sutcliffe and Cesare Tinelli and Cesare Tinelli},
 doi = {10.1007/978-3-319-08587-6_28},
 journal = {null},
 mag_id = {333834217},
 pmcid = {null},
 pmid = {null},
 title = {Starexec: A cross-community infrastructure for logic solving},
 year = {2014}
}

@article{Sun_2009,
 abstract = {Recent development on distributed systems has shown that a variety of fairness constraints (some of which are only recently defined) play vital roles in designing self-stabilizing population protocols. Current practice of system analysis is, however, deficient under fairness. In this work, we present PAT, a toolkit for flexible and efficient system analysis under fairness. A unified algorithm is proposed to model check systems with a variety of fairness effectively in two different settings. Empirical evaluation shows that PAT complements existing model checkers in terms of fairness. We report that previously unknown bugs have been revealed using PAT against systems functioning under strong global fairness.},
 author = {Jun Sun and Jun Sun and Yang Liu and Yang Liu and Yang Liu and Jin Song Dong and Jin Song Dong and Jun Pang and Jun Pang},
 doi = {10.1007/978-3-642-02658-4_59},
 journal = {null},
 mag_id = {1515421996},
 pmcid = {null},
 pmid = {null},
 title = {PAT: Towards Flexible Verification under Fairness},
 year = {2009}
}

@article{Szegedy_2015,
 abstract = {We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
 author = {Christian Szegedy and Christian Szegedy and Wei Liu and Wei Liu and Yangqing Jia and Yangqing Jia and Pierre Sermanet and Pierre Sermanet and Scott Reed and Scott Reed and Dragomir Anguelov and Dragomir Anguelov and Dumitru Erhan and Dumitru Erhan and Vincent Vanhoucke and Vincent Vanhoucke and Andrew Rabinovich and Andrew Rabinovich},
 doi = {10.1109/cvpr.2015.7298594},
 journal = {null},
 mag_id = {2097117768},
 pmcid = {null},
 pmid = {null},
 title = {Going deeper with convolutions},
 year = {2015}
}

@article{Sztipanovits_2012,
 abstract = {System integration is the elephant in the china store of large-scale cyber-physical system (CPS) design. It would be hard to find any other technology that is more undervalued scientifically and at the same time has bigger impact on the presence and future of engineered systems. The unique challenges in CPS integration emerge from the heterogeneity of components and interactions. This heterogeneity drives the need for modeling and analyzing cross-domain interactions among physical and computational/networking domains and demands deep understanding of the effects of heterogeneous abstraction layers in the design flow. To address the challenges of CPS integration, significant progress needs to be made toward a new science and technology foundation that is model based, precise, and predictable. This paper presents a theory of composition for heterogeneous systems focusing on stability. Specifically, the paper presents a passivity-based design approach that decouples stability from timing uncertainties caused by networking and computation. In addition, the paper describes cross-domain abstractions that provide effective solution for model-based fully automated software synthesis and high-fidelity performance analysis. The design objectives demonstrated using the techniques presented in the paper are group coordination for networked unmanned air vehicles (UAVs) and high-confidence embedded control software design for a quadrotor UAV. Open problems in the area are also discussed, including the extension of the theory of compositional design to guarantee properties beyond stability, such as safety and performance.},
 author = {János Sztipanovits and Janos Sztipanovits and Xenofon Koutsoukos and Xenofon Koutsoukos and Gábor Karsai and Gabor Karsai and Nicholas Kottenstette and Nicholas Kottenstette and Panos J. Antsaklis and Panos J. Antsaklis and Vijay Gupta and Vijay Gupta and Bill Goodwine and Bill Goodwine and John S. Baras and John S. Baras and Shige Wang and Shige Wang and Shige Wang},
 doi = {10.1109/jproc.2011.2161529},
 journal = {null},
 mag_id = {2005140205},
 pmcid = {null},
 pmid = {null},
 title = {Toward a Science of Cyber–Physical System Integration},
 year = {2012}
}

@article{Tahir_2015,
 abstract = {The Field of Formal Methods (FM) is a growing field that uses mathematical notations for making accurate and unambiguous specifications, so that in the early phases of software development discrepancies and errors are identified. By using the popular kind of formal methods i.e. VDM, the quality of design, system specification and verification of software can be improved effectively. In this paper, formal specification of an e-Health system is described by using VDM-SL. VDM-SL is a popular formal method used for the specification of software and hardware systems. Formal methods are not only applicable for creating and verifying specifications of mission critical systems, but also for the business and commercial systems, we can use formal methods. This paper is a motivation to use formal methods for creating specifications, validating and verifying specifications of software systems. In this way, development time, testing and maintenance cost in building software is greatly reduced.},
 author = {Hafız Muhammad Tahir and Hafiz Muhammad Tahir and Muhammad Adnan and Muhammad Nadeem and Muhammad Nadeem and Nazir Ahmad Zafar and Nazir Ahmad Zafar},
 doi = {10.1109/nsec.2015.7396346},
 journal = {null},
 mag_id = {2280145101},
 pmcid = {null},
 pmid = {null},
 title = {Specifying electronic health system with vienna development method specification language},
 year = {2015}
}

@article{Teije_2006,
 abstract = {Objectives: During the last decade, evidence-based medicine has given rise to an increasing number of medical practice guidelines and protocols. However, the work done on developing and distributing protocols outweighs the efforts on guaranteeing their quality. Indeed, anomalies like ambiguity and incompleteness are frequent in medical protocols. Recent efforts have tried to address the problem of protocol improvement, but they are not sufficient since they rely on informal processes and notations. Our objective is to improve the quality of medical protocols. Approach: The solution we suggest to the problem of quality improvement of protocols consists in the utilisation of formal methods. It requires the definition of an adequate protocol representation language, the development of techniques for the formal analysis of protocols described in that language and, more importantly, the evaluation of the feasibility of the approach based on the formalisation and verification of real-life medical protocols. For the first two aspects we rely on earlier work from the fields of knowledge representation and formal methods. The third aspect, i.e. the evaluation of the use of formal methods in the quality improvement of protocols, constitutes our main objective. The steps with which we have carried out this evaluation are the following: (1) take two real-life reference protocols which cover a wide variety of protocol characteristics; (2) formalise these reference protocols; (3) check the formalisation for the verification of interesting protocol properties; and (4) determine how many errors can be uncovered in this way. Results: Our main results are: a consolidated formal language to model medical practice protocols; two protocols, each both modelled and formalised; a list of properties that medical protocols should satisfy; verification proofs for these protocols and properties; and perspectives of the potentials of this approach. Our results have been evaluated by a panel of medical experts, who judged that the problems we detected in the protocols with the help of formal methods were serious and should be avoided. Conclusions: We have succeeded in demonstrating the feasibility of formal methods for improving medical protocols.},
 author = {Annette ten Teije and Annette ten Teije and Mar Marcos and Mar Marcos and Michel Balser and Michel Balser and Joyce van Croonenborg and Joyce van Croonenborg and Christoph Duelli and Christoph Duelli and Frank van Harmelen and Frank van Harmelen and Peter Lucas and Peter J. F. Lucas and Silvia Miksch and Silvia Miksch and Wolfgang Reif and Wolfgang Reif and Kitty Rosenbrand and Kitty Rosenbrand and Andreas Seyfang and Andreas Seyfang},
 doi = {10.1016/j.artmed.2005.10.006},
 journal = {Artificial Intelligence in Medicine},
 mag_id = {2028652935},
 pmcid = {null},
 pmid = {16376061},
 title = {Improving medical protocols by formal methods},
 year = {2006}
}

@article{Tran_2015,
 abstract = {Serious Games offer a new way to older adults to improve various abilities such as the vision, the balance or the memory. However, cognitive impairment causes a lot of difficulties to them when actively practicing these games. Their engagement and motivation are reduced rapidly when encountering successive problems without any help. In this paper, we present an approach to assist older adults in Serious Game using an interactive system. Three groups of players with different cognitive impairments Mnesic Plaint, MCI and Alzheimer have been tested with the system in a concentration-based game. As the experimental results, the players performed a high performance when playing with the assistance of the system, especially among Alzheimer group. The future work aims to perform this approach with a larger population and explore other factors which can influence on the players' motivation.},
 author = {Minh Khue Phan Tran and Minh Khue Phan Tran and François Brémond and Francois Bremond and Robert Pierpoint and Philippe Robert},
 doi = {10.1007/978-3-319-40216-1_30},
 journal = {null},
 mag_id = {2276258890},
 pmcid = {null},
 pmid = {null},
 title = {Assistance for Older Adults in Serious Game Using an Interactive System},
 year = {2015}
}

@article{Ujjwal_2018,
 abstract = {We propose a system design for pedestrian detection by leveraging the power of multiple convolutional layers explicitly. We quantify the effect of different convolutional layers on the detection of pedestrians of varying scales and occlusion level. We show that earlier convolutional layers are better at handling small-scale and partially occluded pedestrians. We take cue from these conclusions and propose a pedestrian detection system design based on Faster-RCNN which leverages multiple convolutional layers by late fusion. In our design, we introduce height-awareness in the loss function to make the network emphasize on pedestrian heights which are misclassified during the training process. The proposed system design achieves a log-average miss-rate of 9.25% on the caltech-reasonable dataset. This is within 1.5% of the current state-of-art approach, while being a more compact system.},
 author = {. Ujjwal and Ujjwal and Aziz Dziri and Aziz Dziri and Aziz Dziri and Aziz Dziri and Bertrand Leroy and Bertrand Leroy and Bertrand Leroy and Bertrand Leroy and François Brémond and Francois Bremond},
 doi = {10.1109/avss.2018.8639083},
 journal = {null},
 mag_id = {2901701820},
 pmcid = {null},
 pmid = {null},
 title = {Late Fusion of Multiple Convolutional Layers for Pedestrian Detection},
 year = {2018}
}

@article{undefined_null,
 abstract = {null},
 author = {},
 doi = {10.1016/c2009-0-20452-1},
 journal = {null},
 mag_id = {4240175633},
 pmcid = {null},
 pmid = {null},
 title = {Industrial Applications of Formal Methods to Model, Design and Analyze Computer Systems},
 year = {null}
}

@article{Utting_2006,
 abstract = {This book gives a practical introduction to model-based testing, showing how to write models for testing purposes and how to use model-based testing tools to generate test suites. It is aimed at testers and software developers who wish to use model-based testing, rather than at tool-developers or academics.

The book focuses on the mainstream practice of functional black-box testing and covers different styles of models, especially transition-based models (UML state machines) and pre/post models (UML/OCL specifications and B notation). The steps of applying model-based testing are demonstrated on examples and case studies from a variety of software domains, including embedded software and information systems. 

From this book you will learn:

* The basic principles and terminology of model-based testing
* How model-based testing differs from other testing processes
* How model-based testing fits into typical software lifecycles such as agile methods and the Unified Process
* The benefits and limitations of model-based testing, its cost effectiveness and how it can reduce time-to-market
* A step-by-step process for applying model-based testing
* How to write good models for model-based testing
* How to use a variety of test selection criteria to control the tests that are generated from your models
* How model-based testing can connect to existing automated test execution platforms such as Mercury Test Director, Java JUnit, and proprietary test execution environments

* Presents the basic principles and terminology of model-based testing
* Shows how model-based testing fits into the software lifecycle, its cost-effectiveness, and how it can reduce time to market
* Offers guidance on how to use different kinds of modeling techniques, useful test generation strategies, how to apply model-based testing techniques to real applications using case studies},
 author = {Mark Utting and Mark Utting and Bruno Legeard and Bruno Legeard},
 doi = {null},
 journal = {null},
 mag_id = {1608087281},
 pmcid = {null},
 pmid = {null},
 title = {Practical Model-Based Testing: A Tools Approach},
 year = {2006}
}

@article{Valiron_2015,
 abstract = {The Quipper language offers a unified general-purpose programming framework for quantum computation.},
 author = {Benoît Valiron and Benoît Valiron and Neil J. Ross and Neil J. Ross and Peter Selinger and Peter Selinger and D. Scott Alexander and D. Scott Alexander and Jonathan M. Smith and Jonathan M. Smith},
 doi = {10.1145/2699415},
 journal = {Communications of The ACM},
 mag_id = {1944374408},
 pmcid = {null},
 pmid = {null},
 title = {Programming the quantum future},
 year = {2015}
}

@article{Vidal_2011,
 abstract = {Research highlights? Describing, defining, and understanding better project complexity and its measures. ? Building up a generic project complexity framework. ? Performing a Delphi study about project complexity factors. ? Building up an AHP-based multi-criteria evaluation of project complexity. ? Exploring the sensitivity of such a measure. Project complexity is ever growing and needs to be understood, analysed and measured better to assist modern project management. The overall ambition of this paper is therefore to define a measure of project complexity in order to assist decision-making, notably when analysing several projects in a portfolio, or when studying different areas of a project. A synthesised literature review on existing complexity measures is firstly proposed in order to highlight their limitations. Then, we identify the multiple aspects of project complexity thanks to the construction and refinement of a project complexity framework thanks to an international Delphi study. We then propose a multi-criteria approach to project complexity evaluation, underlining the benefits of such an approach. In order to solve properly this multi-criteria problem, we first conduct a critical state of the art on multi-criteria methodologies. We then argue for the use of the Analytic Hierarchy Process. In the end, this tool permits to define a relative project complexity measure, which can notably assist decision-making. Complexity scales and subscales are defined in order to highlight the most complex alternatives and their principal sources of complexity within the set of criteria and sub-criteria which exist in the hierarchical structure. Finally, a case study within a start-up firm in the entertainment industry (musicals production) is performed. Conclusions, limitations and perspectives of research are given in the end.},
 author = {Ludovic‐Alexandre Vidal and Ludovic-Alexandre Vidal and Franck Marle and Franck Marle and Jean-Claude Bocquet and Jean-Claude Bocquet},
 doi = {10.1016/j.eswa.2010.10.016},
 journal = {Expert Systems With Applications},
 mag_id = {2014399712},
 pmcid = {null},
 pmid = {null},
 title = {Using a Delphi process and the Analytic Hierarchy Process (AHP) to evaluate the complexity of projects},
 year = {2011}
}

@article{Voas_2016,
 abstract = {Seven experts weigh in on the current use and practice of formal methods in cybersecurity.},
 author = {Jeffrey Voas and Jeffrey Voas and Kim Schaffer and Kim Schaffer},
 doi = {10.1109/mc.2016.131},
 journal = {IEEE Computer},
 mag_id = {2367331181},
 pmcid = {null},
 pmid = {null},
 title = {Insights on Formal Methods in Cybersecurity},
 year = {2016}
}

@article{Vrigkas_2015,
 abstract = {Recognizing human activities from video sequences or still images is a challenging task due to problems such as background clutter, partial occlusion, changes in scale, viewpoint, lighting, and appearance. Many applications, including video surveillance systems, human-computer interaction, and robotics for human behavior characterization, require a multiple activity recognition system. In this work, we provide a detailed review of recent and state-of-the-art research advances in the field of human activity classification. We propose a categorization of human activity methodologies and discuss their advantages and limitations. In particular, we divide human activity classification methods into two large categories according to whether they use data from different modalities or not. Then, each of these categories is further analyzed into sub-categories, which reflect how they model human activities and what type of activities they are interested in. Moreover, we provide a comprehensive analysis of the existing, publicly available human activity classification datasets and examine the requirements for an ideal human activity recognition dataset. Finally, we report the characteristics of future research directions and present some open issues on human activity recognition.},
 author = {Michalis Vrigkas and Michalis Vrigkas and Christophoros Nikou and Christophoros Nikou and Ioannis A. Kakadiaris and Ioannis A. Kakadiaris},
 doi = {10.3389/frobt.2015.00028},
 journal = {Frontiers in Robotics and AI},
 mag_id = {2180635266},
 pmcid = {null},
 pmid = {null},
 title = {A Review of Human Activity Recognition Methods},
 year = {2015}
}

@article{Weyers_2012,
 abstract = {Controlling and observing complex systems is central to the study of human–machine interaction. In our understanding, there is much to be gained from integrating formal modeling and analysis, including the reconfiguration of user interfaces, with the development of user interfaces with high usability. To this end, we introduce a new approach to modeling and reconfiguration of user interfaces jointly with a newly developed set of tools for interactive and visual creation and automatic transformation of user interfaces' interaction logic to a formal language based on Petri nets. Reconfiguration will be embedded into a process for adapting user interfaces to the user's cognitive representation of the controlled system. This process involves practicing the use of a given user interface, adapting it to the user's needs through reconfiguration, and applying the resulting adaptations to the formally defined interaction logic. An evaluation study confirms that this process reduces errors in interaction.},
 author = {Benjamin Weyers and Benjamin Weyers and Dina Burkolter and Dina Burkolter and Wolfram Luther and Wolfram Luther and Annette Kluge and Annette Kluge},
 doi = {10.1080/10447318.2011.654199},
 journal = {International Journal of Human-computer Interaction},
 mag_id = {2047630267},
 pmcid = {null},
 pmid = {null},
 title = {Formal Modeling and Reconfiguration of User Interfaces for Reduction of Errors in Failure Handling of Complex Systems},
 year = {2012}
}

@article{Weyers_2017,
 abstract = {This book provides a comprehensive collection of methods and approaches for using formal methods within Human-Computer Interaction (HCI) research, the use of which is a prerequisite for usability and user-experience (UX) when engineering interactive systems. World-leading researchers present methods, tools and techniques to design and develop reliable interactive systems, offering an extensive discussion of the current state-of-the-art with case studies which highlight relevant scenarios and topics in HCI as well as presenting current trends and gaps in research and future opportunities and developments within this emerging field. The Handbook of Formal Methods in Human-Computer Interaction is intended for HCI researchers and engineers of interactive systems interested in facilitating formal methods into their research or practical work.},
 author = {Benjamin Weyers and Benjamin Weyers and Judy Bowen and Judy Bowen and Alan Dix and Alan Dix and Philippe Palanque and Philippe Palanque},
 doi = {10.1007/978-3-319-51838-1},
 journal = {null},
 mag_id = {2608006100},
 pmcid = {null},
 pmid = {null},
 title = {The Handbook of Formal Methods in Human-Computer Interaction},
 year = {2017}
}

@article{Wieringa_2014,
 abstract = {This book provides guidelines for practicing design science in the fields of information systems and software engineering research. A design process usually iterates over two activities: first designing an artifact that improves something for stakeholders and subsequently empirically investigating the performance of that artifact in its context. This validation in context is a key feature of the book - since an artifact is designed for a context, it should also be validated in this context. The book is divided into five parts. Part I discusses the fundamental nature of design science and its artifacts, as well as related design research questions and goals. Part II deals with the design cycle, i.e. the creation, design and validation of artifacts based on requirements and stakeholder goals. To elaborate this further, Part III presents the role of conceptual frameworks and theories in design science. Part IV continues with the empirical cycle to investigate artifacts in context, and presents the different elements of research problem analysis, research setup and data analysis. Finally, Part V deals with the practical application of the empirical cycle by presenting in detail various research methods, including observational case studies, case-based and sample-based experiments and technical action research. These main sections are complemented by two generic checklists, one for the design cycle and one for the empirical cycle. The book is written for students as well as academic and industrial researchers in software engineering or information systems. It provides guidelines on how to effectively structure research goals, how to analyze research problems concerning design goals and knowledge questions, how to validate artifact designs and how to empirically investigate artifacts in context and finally how to present the results of the design cycle as a whole.},
 author = {Roel Wieringa and Roel Wieringa},
 doi = {null},
 journal = {null},
 mag_id = {49973284},
 pmcid = {null},
 pmid = {null},
 title = {Design Science Methodology for Information Systems and Software Engineering},
 year = {2014}
}

@article{Wing_1990,
 abstract = {Formal methods used in developing computer systems (i.e. mathematically based techniques for describing system properties) are defined, and their role is delineated. Formal specification languages, which provide the formal method's mathematical basis, are examined. Certain pragmatic concerns about formal methods and their users, uses, and characteristics are discussed. Six well-known or commonly used formal methods are illustrated by simple examples. They are Z, VDM, Larch, temporal logic, CSP, and transition axioms. >},
 author = {Jeannette M. Wing and Jeannette M. Wing},
 doi = {10.1109/2.58215},
 journal = {IEEE Computer},
 mag_id = {2170486381},
 pmcid = {null},
 pmid = {null},
 title = {A specifier's introduction to formal methods},
 year = {1990}
}

@article{Woodcock_2009,
 abstract = {Formal methods use mathematical models for analysis and verification at any part of the program life-cycle. We describe the state of the art in the industrial use of formal methods, concentrating on their increasing use at the earlier stages of specification and design. We do this by reporting on a new survey of industrial use, comparing the situation in 2009 with the most significant surveys carried out over the last 20 years. We describe some of the highlights of our survey by presenting a series of industrial projects, and we draw some observations from these surveys and records of experience. Based on this, we discuss the issues surrounding the industrial adoption of formal methods. Finally, we look to the future and describe the development of a Verified Software Repository, part of the worldwide Verified Software Initiative. We introduce the initial projects being used to populate the repository, and describe the challenges they address.},
 author = {Jim Woodcock and Jim Woodcock and Peter Gorm Larsen and Peter Gorm Larsen and Juan Bicarregui and Juan Bicarregui and JS Fitzgerald and John Fitzgerald},
 doi = {10.1145/1592434.1592436},
 journal = {ACM Computing Surveys},
 mag_id = {2150189917},
 pmcid = {null},
 pmid = {null},
 title = {Formal methods: Practice and experience},
 year = {2009}
}

@article{Xiong_2010,
 abstract = {null},
 author = {Xijiao Xiong and Xijiao Xiong and Jing Liu and Jing Liu and Zuohua Ding and Zuohua Ding},
 doi = {10.1016/j.entcs.2010.08.050},
 journal = {Electronic Notes in Theoretical Computer Science},
 mag_id = {2045170496},
 pmcid = {null},
 pmid = {null},
 title = {Design and Verification of a Trustable Medical System},
 year = {2010}
}

@article{Zadeh_1965,
 abstract = {A fuzzy set is a class of objects with a continuum of grades of membership. Such a set is characterized by a membership (characteristic) function which assigns to each object a grade of membership ranging between zero and one. The notions of inclusion, union, intersection, complement, relation, convexity, etc., are extended to such sets, and various properties of these notions in the context of fuzzy sets are established. In particular, a separation theorem for convex fuzzy sets is proved without requiring that the fuzzy sets be disjoint.},
 author = {L.A. Zadeh and L. A. Zadeh},
 doi = {10.1016/s0019-9958(65)90241-x},
 journal = {Information and control},
 mag_id = {4211007335},
 pmcid = {null},
 pmid = {null},
 title = {Fuzzy sets},
 year = {1965}
}

@article{Zhang_2019,
 abstract = {Although widely used in many applications, accurate and efficient human action recognition remains a challenging area of research in the field of computer vision. Most recent surveys have focused on narrow problems such as human action recognition methods using depth data, 3D-skeleton data, still image data, spatiotemporal interest point-based methods, and human walking motion recognition. However, there has been no systematic survey of human action recognition. To this end, we present a thorough review of human action recognition methods and provide a comprehensive overview of recent approaches in human action recognition research, including progress in hand-designed action features in RGB and depth data, current deep learning-based action feature representation methods, advances in human–object interaction recognition methods, and the current prominent research topic of action detection methods. Finally, we present several analysis recommendations for researchers. This survey paper provides an essential reference for those interested in further research on human action recognition.},
 author = {Hong Bo Zhang and Hong-Bo Zhang and Yi Xiang Zhang and Yi Xiang Zhang and Yi-Xiang Zhang and Bineng Zhong and Bineng Zhong and Qing Lei and Qing Lei and Lijie Yang and Lijie Yang and Ji Xiang Du and Ji-Xiang Du and Ji Xiang Du and Duan Sheng Chen and Duansheng Chen and Duan Sheng Chen},
 doi = {10.3390/s19051005},
 journal = {Sensors},
 mag_id = {2917819557},
 pmcid = {null},
 pmid = {30818796},
 title = {A Comprehensive Survey of Vision-Based Human Action Recognition Methods.},
 year = {2019}
}

@article{Zheng_2013,
 abstract = {Cardiovascular disease (CVD) has become the leading cause of human deaths today. In order to combat this disease, many professionals are using mobile electrocardiogram (ECG) remote monitoring system. While using mobile ECG systems, most of the cardiac anomalies can be observed, especially when serious myocardial ischemia, heart failure, and malignant arrhythmia occur. Thus, ECG anomaly detection and analysis have attracted more and more attention in the clinical and research communities. Currently, the existing solutions of ECG automatic detection and analysis technologies are challenged by an accuracy requirement. Based on this motivation, we propose a novel Multi-Resolution Support Vector Machine (MR-SVM) algorithm to detect ECG waveform anomaly. This proposal is tested in our WE-CARE (a Wearable Efficient telecardiology system) project. Clinical trials and experimental results show that the algorithm can successfully extract original QRS complex waves and T waves regardless of noise magnitude and distinguish the ST segment morphological anomalies. Compared with European standard ST-T database, our solution can achieve the average T wave recognition accuracy rate of 97.5% and ST anomaly detection accuracy rate of 93%.},
 author = {Qian Zheng and Qian Zheng and Chao Chen and Chao Chen and Zhinan Li and Zhinan Li and Anpeng Huang and Anpeng Huang and Bingli Jiao and Bingli Jiao and Xiaohui Duan and Xiaohui Duan and Linzhen Xie and Linzhen Xie},
 doi = {10.1109/brc.2013.6487453},
 journal = {null},
 mag_id = {2068023570},
 pmcid = {null},
 pmid = {null},
 title = {A novel multi-resolution SVM (MR-SVM) algorithm to detect ECG signal anomaly in WE-CARE project},
 year = {2013}
}

@article{Zuccon_2013,
 abstract = {In the field of information retrieval (IR), researchers and practitioners are often faced with a demand for valid approaches to evaluate the performance of retrieval systems. The Cranfield experiment paradigm has been dominant for the in-vitro evaluation of IR systems. Alternative to this paradigm, laboratory-based user studies have been widely used to evaluate interactive information retrieval (IIR) systems, and at the same time investigate users' information searching behaviours. Major drawbacks of laboratory-based user studies for evaluating IIR systems include the high monetary and temporal costs involved in setting up and running those experiments, the lack of heterogeneity amongst the user population and the limited scale of the experiments, which usually involve a relatively restricted set of users. In this paper, we propose an alternative experimental methodology to laboratory-based user studies. Our novel experimental methodology uses a crowdsourcing platform as a means of engaging study participants. Through crowdsourcing, our experimental methodology can capture user interactions and searching behaviours at a lower cost, with more data, and within a shorter period than traditional laboratory-based user studies, and therefore can be used to assess the performances of IIR systems. In this article, we show the characteristic differences of our approach with respect to traditional IIR experimental and evaluation procedures. We also perform a use case study comparing crowdsourcing-based evaluation with laboratory-based evaluation of IIR systems, which can serve as a tutorial for setting up crowdsourcing-based IIR evaluations.},
 author = {Guido Zuccon and Guido Zuccon and Teerapong Leelanupab and Teerapong Leelanupab and Stewart Whiting and Stewart Whiting and Emine Yılmaz and Emine Yilmaz and Joemon M. Jose and Joemon M. Jose and Leif Azzopardi and Leif Azzopardi},
 doi = {10.1007/s10791-012-9206-z},
 journal = {Information Retrieval},
 mag_id = {2089350162},
 pmcid = {null},
 pmid = {null},
 title = {Crowdsourcing interactions: using crowdsourcing for evaluating interactive information retrieval systems},
 year = {2013}
}

@article{Özay_2017,
 abstract = {null},
 author = {Necmiye Özay and Necmiye Ozay and Paulo Tabuada and Paulo Tabuada},
 doi = {10.1007/s10626-017-0246-9},
 journal = {Discrete Event Dynamic Systems},
 mag_id = {2606707698},
 pmcid = {null},
 pmid = {null},
 title = {Guest editorial: special issue on formal methods in control},
 year = {2017}
}
