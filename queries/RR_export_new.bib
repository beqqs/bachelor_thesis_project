@ARTICLE{Hessel_2008,title={Formal Methods and Testing},year={2008},author={Anders Hessel and Anders Hessel and Kim Guldstrand Larsen and Kim Guldstrand Larsen and Marius Mikučionis and Marius Mikucionis and Brian Nielsen and Brian Nielsen and Paul Pettersson and Paul Pettersson and Arne Skou and Arne Skou},doi={null},pmid={null},pmcid={null},mag_id={2521994121},journal={null},abstract={null}}
@ARTICLE{Rosenblum_1996,title={Formal methods and testing: why the state-of-the art is not the state-of-the practice},year={1996},author={David S. Rosenblum and David S. Rosenblum},doi={10.1145/232069.232086},pmid={null},pmcid={null},mag_id={2069486211},journal={ACM Sigsoft Software Engineering Notes},abstract={article Formal methods and testing: why the state-of-the art is not the state-of-the practice Share on Author: David S. Rosenblum AT&T Research AT&T ResearchView Profile Authors Info & Claims ACM SIGSOFT Software Engineering NotesVolume 21Issue 4July 1996 pp 64–66https://doi.org/10.1145/232069.232086Online:01 July 1996Publication History 13citation573DownloadsMetricsTotal Citations13Total Downloads573Last 12 Months6Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteGet Access}}
@ARTICLE{Alur_2004,title={A Temporal Logic of Nested Calls and Returns},year={2004},author={Rajeev Alur and Rajeev Alur and Kousha Etessami and Kousha Etessami and P. Madhusudan and P. Madhusudan},doi={10.1007/978-3-540-24730-2_35},pmid={null},pmcid={null},mag_id={1556566737},journal={null},abstract={Model checking of linear temporal logic (LTL) specifications with respect to pushdown systems has been shown to be a useful tool for analysis of programs with potentially recursive procedures. LTL, however, can specify only regular properties, and properties such as correctness of procedures with respect to pre and post conditions, that require matching of calls and returns, are not regular. We introduce a temporal logic of calls and returns (CaRet) for specification and algorithmic verification of correctness requirements of structured programs. The formulas of CaRet are interpreted over sequences of propositional valuations tagged with special symbols call and ret. Besides the standard global temporal modalities, CaRet admits the abstract-next operator that allows a path to jump from a call to the matching return. This operator can be used to specify a variety of non-regular properties such as partial and total correctness of program blocks with respect to pre and post conditions. The abstract versions of the other temporal modalities can be used to specify regular properties of local paths within a procedure that skip over calls to other procedures. CaRet also admits the caller modality that jumps to the most recent pending call, and such caller modalities allow specification of a variety of security properties that involve inspection of the call-stack. Even though verifying context-free properties of pushdown systems is undecidable, we show that model checking CaRet formulas against a pushdown model is decidable. We present a tableau construction that reduces our model checking problem to the emptiness problem for a Buchi pushdown system. The complexity of model checking CaRet formulas is the same as that of checking LTL formulas, namely, polynomial in the model and singly exponential in the size of the specification.}}
@ARTICLE{Alur_1994,title={A theory of timed automata},year={1994},author={Rajeev Alur and Rajeev Alur and David L. Dill and David L. Dill},doi={10.1016/0304-3975(94)90010-8},pmid={null},pmcid={null},mag_id={2101508170},journal={Theoretical Computer Science},abstract={Alur, R. and D.L. Dill, A theory of timed automata, Theoretical Computer Science 126 (1994) 183-235. We propose timed (j&e) automata to model the behavior of real-time systems over time. Our definition provides a simple, and yet powerful, way to annotate state-transition graphs with timing constraints using finitely many real-valued clocks. A timed automaton accepts timed words-infinite sequences in which a real-valued time of occurrence is associated with each symbol. We study timed automata from the perspective of formal language theory: we consider closure properties, decision problems, and subclasses. We consider both nondeterministic and deterministic transition structures, and both Biichi and Muller acceptance conditions. We show that nondeterministic timed automata are closed under union and intersection, but not under complementation, whereas deterministic timed Muller automata are closed under all Boolean operations. The main construction of the paper is an (PSPACE) algorithm for checking the emptiness of the language of a (nondeterministic) timed automaton. We also prove that the universality problem and the language inclusion problem are solvable only for the deterministic automata: both problems are undecidable (II i-hard) in the nondeterministic case and PSPACE-complete in the deterministic case. Finally, we discuss the application of this theory to automatic verification of real-time requirements of finite-state systems.}}
@ARTICLE{Grabowski_2008,title={Formal Approaches to Software Testing},year={2008},author={Jens Grabowski and Jens Grabowski and Brian Nielsen and Brian Nielsen},doi={null},pmid={null},pmcid={null},mag_id={2502633158},journal={null},abstract={null}}
@ARTICLE{Seghir_2007,title={ACSAR: software model checking with transfinite refinement},year={2007},author={Mohamed Nassim Seghir and Mohamed Nassim Seghir and Andreas Podelski and Andreas Podelski},doi={10.1007/978-3-540-73370-6_19},pmid={null},pmcid={null},mag_id={2136877520},journal={null},abstract={ACSAR (Automatic Checker of Safety properties based on Abstraction Refinement) is a software model checker for C programs in the spirit of Blast [6], F-Soft [7], Magic [5] and Slam [1]. It is based on the counterexample-guided abstraction refinement (CEGAR) paradigm. Its specificity lies in the way it overcomes a problem common to all tools based on this paradigm. The problem arises from creating more and more spurious counterexamples by unfolding the same (while- or for-) loop over and over again; this leads to an infinite or at least too large sequence of refinement steps. The idea behind ACSAR is to abstract not just states but also the state changes induced by structured program statements, including for- and while-statements. The use of the new abstraction allows one to shortcut such a “transfinite” sequence of refinement steps.}}
@ARTICLE{Jard_2005,title={TGV: theory, principles and algorithms: A tool for the automatic synthesis of conformance test cases for non-deterministic reactive systems},year={2005},author={Claude Jard and Claude Jard and Thierry Jéron and Thierry Jéron},doi={10.1007/s10009-004-0153-x},pmid={null},pmcid={null},mag_id={1522465028},journal={International Journal on Software Tools for Technology Transfer},abstract={This paper presents the TGV tool, which allows for the automatic synthesis of conformance test cases from a formal specification of a (non-deterministic) reactive system. TGV was developed by Irisa Rennes and Verimag Grenoble, with the support of the Vasy team of Inria Rhones-Alpes. The paper describes the main elements of the underlying testing theory, which is based on a model of transitions system which distinguishes inputs, outputs and internal actions, and is based on the concept of conformance relation. The principles of the test synthesis process, as well as the main algorithms, are explained. We then describe the main characteristics of the TGV tool and refer to some industrial experiments that have been conducted to validate the approach. As a conclusion, we describe some ongoing work on test synthesis.}}
@ARTICLE{Utting_2006,title={Practical Model-Based Testing: A Tools Approach},year={2006},author={Mark Utting and Mark Utting and Bruno Legeard and Bruno Legeard},doi={null},pmid={null},pmcid={null},mag_id={1608087281},journal={null},abstract={This book gives a practical introduction to model-based testing, showing how to write models for testing purposes and how to use model-based testing tools to generate test suites. It is aimed at testers and software developers who wish to use model-based testing, rather than at tool-developers or academics.

The book focuses on the mainstream practice of functional black-box testing and covers different styles of models, especially transition-based models (UML state machines) and pre/post models (UML/OCL specifications and B notation). The steps of applying model-based testing are demonstrated on examples and case studies from a variety of software domains, including embedded software and information systems. 

From this book you will learn:

* The basic principles and terminology of model-based testing
* How model-based testing differs from other testing processes
* How model-based testing fits into typical software lifecycles such as agile methods and the Unified Process
* The benefits and limitations of model-based testing, its cost effectiveness and how it can reduce time-to-market
* A step-by-step process for applying model-based testing
* How to write good models for model-based testing
* How to use a variety of test selection criteria to control the tests that are generated from your models
* How model-based testing can connect to existing automated test execution platforms such as Mercury Test Director, Java JUnit, and proprietary test execution environments

* Presents the basic principles and terminology of model-based testing
* Shows how model-based testing fits into the software lifecycle, its cost-effectiveness, and how it can reduce time to market
* Offers guidance on how to use different kinds of modeling techniques, useful test generation strategies, how to apply model-based testing techniques to real applications using case studies}}
@ARTICLE{Tretmans_1996,title={Test Generation with Inputs, Outputs and Repetitive Quiescence},year={1996},author={Jan Tretmans and Jan Tretmans},doi={null},pmid={null},pmcid={null},mag_id={2112640524},journal={null},abstract={This paper studies testing based on labelled transition systems, using the assumption that implementations communicate with their environment via inputs and outputs. Such implementations are formalized by restricting the class of transition systems to those systems that can always accept input actions, as in Input/Output Automata. Implementation relations, formalizing the notion of correctness of these implementations with respect to labelled transition system specifications, are defined analogous to the theories of testing equivalence and preorder, and refusal testing. A test generation algorithm is given which is proved to produce a sound and exhaustive test suite from a specification, i.e., a test suite that fully characterizes the set of correct implementations.}}
@ARTICLE{Krichen_2004,title={Black-Box Conformance Testing for Real-Time Systems},year={2004},author={Moez Krichen and Moez Krichen and Moez Krichen and Stavros Tripakis and Stavros Tripakis},doi={10.1007/978-3-540-24732-6_8},pmid={null},pmcid={null},mag_id={2113780769},journal={null},abstract={We propose a new framework for black-box conformance testing of real-time systems, where specifications are modeled as non- deterministic and partially-observable timed automata. We argue that such a model is essential for ease of modeling and expressiveness of spec- ifications. The conformance relation is a timed extension of the input- output conformance relation of (29). We argue that it is better suited for testing than previously considered relations such as bisimulation, must/may preorder or trace inclusion. We propose algorithms to gener- ate two types of tests for this setting: analog-clock tests which measure dense time precisely and digital-clock tests which measure time with a periodic clock. The latter are essential for implementability, since only finite-precision clocks are available in practice. We report on a prototype tool and a small case study.}}
@ARTICLE{Krichen_2009,title={Conformance testing for real-time systems},year={2009},author={Moez Krichen and Moez Krichen and Moez Krichen and Stavros Tripakis and Stavros Tripakis},doi={10.1007/s10703-009-0065-1},pmid={null},pmcid={null},mag_id={2120916408},journal={null},abstract={We propose a new framework for black-box conformance testing of real-time systems. The framework is based on the model of partially-observable, non-deterministic timed automata. We argue that partial observability and non-determinism are essential features for ease of modeling, expressiveness and implementability. The framework allows the user to define, through appropriate modeling, assumptions on the environment of the system under test (SUT) as well as on the interface between the tester and the SUT. We consider two types of tests: analog-clock tests and digital-clock tests. Our algorithm for generating analog-clock tests is based on an on-the-fly determinization of the specification automaton during the execution of the test, which in turn relies on reachability computations. The latter can sometimes be costly, thus problematic, since the tester must quickly react to the actions of the system under test. Therefore, we provide techniques which allow analog-clock testers to be represented as deterministic timed automata, thus minimizing the reaction time to a simple state jump. We also provide algorithms for static or on-the-fly generation of digital-clock tests. These tests measure time only with finite-precision digital clocks, another essential condition for implementability. We also propose a technique for location, edge and state coverage of the specification, by reducing the problem to covering a symbolic reachability graph. This avoids having to generate too many tests. We report on a prototype tool called $\mathsf{TTG}$ and two case studies: a lighting device and the Bounded Retransmission Protocol. Experimental results obtained by applying $\mathsf{TTG}$ on the Bounded Retransmission Protocol show that only a few tests suffice to cover thousands of reachable symbolic states in the specification.}}
@ARTICLE{Bengtsson_2003,title={Timed Automata: Semantics, Algorithms and Tools},year={2003},author={Johan Bengtsson and Johan Bengtsson and Wang Yi and Wang Yi},doi={10.1007/978-3-540-27755-2_3},pmid={null},pmcid={null},mag_id={2150220388},journal={Lecture Notes in Computer Science},abstract={This chapter is to provide a tutorial and pointers to results and related work on timed automata with a focus on semantical and algorithmic aspects of verification tools. We present the concrete and abstract semantics of timed automata (based on transition rules, regions and zones), decision problems, and algorithms for verification. A detailed description on DBM (Difference Bound Matrices) is included, which is the central data structure behind several verification tools for timed systems. As an example, we give a brief introduction to the tool Uppaal.}}
@ARTICLE{Larsen_2004,title={Online testing of real-time systems using UPPAAL},year={2004},author={Kim Guldstrand Larsen and Kim Guldstrand Larsen and Marius Mikučionis and Marius Mikucionis and Brian Nielsen and Brian Nielsen},doi={10.1007/978-3-540-31848-4_6},pmid={null},pmcid={null},mag_id={1495188953},journal={null},abstract={We present T-Uppaal — a new tool for online black-box testing of real-time embedded systems from non-deterministic timed automata specifications. We describe a sound and complete randomized online testing algorithm and how to implement it using symbolic state representation and manipulation techniques. We propose the notion of relativized timed input/output conformance as the formal implementation relation. A novelty of this relation and our testing algorithm is that they explicitly take environment assumptions into account, generate, execute and verify the result online using the Uppaal on-the-fly model-checking tool engine. A medium size case study shows promising results in terms of error detection capability and computation performance.}}
@ARTICLE{Nielsen_2001,title={Automated Test Generation from Timed Automata},year={2001},author={Brian Nielsen and Brian Nielsen and Arne Skou and Arne Skou},doi={10.1007/3-540-45319-9_24},pmid={null},pmcid={null},mag_id={1505255894},journal={null},abstract={Testing is the most dominating validation activity used by industry today, and there is an urgent need for improving its effectiveness, both with respect to the time and resources for test generation and execution, and obtained test coverage. We present a new technique for automatic generation of real-time black-box conformance tests for non-deterministic systems from a determinizable class of timed automata specifications with a dense time interpretation. In contrast to other attempts, our tests are generated using a coarse equivalence class partitioning of the specification. To analyze the specification, to synthesize the timed tests, and to guarantee coverage with respect to a coverage criterion, we use the efficient symbolic techniques recently developed for model checking of real-time systems. Application of our prototype tool to a realistic specification shows promising results in terms of both the test suite size, and the time and space used for test generation.}}
@ARTICLE{Schneider_1999,title={Concurrent and Real-time Systems: The CSP Approach},year={1999},author={Steve Schneider and Steve Schneider},doi={null},pmid={null},pmcid={null},mag_id={1571487936},journal={null},abstract={From the Publisher:
The CSP approach has been widely used in the specification, analysis and verification of concurrent and real-time systems, and for understanding the particular issues that can arise when concurrency is present. It provides a language which enables specifications and designs to be clearly expressed and understood, together with a supporting theory which allows them to be analyzed and shown to be correct.
This book supports advanced level courses on concurrency covering timed and untimed CSP. The first half introduces the language of CSP, the primary semantic models (traces, failures, divergences and infinite traces), and their use in the modelling, analysis and verification of concurrent systems. The second half of the book introduces time into the language, brings in the timed semantic model (timed failures) and finally presents the theory of timewise refinement which links the two halves together.
Containing the following:
-Exercises and solutions
-Instructors resources 
- Example CSP programs to run on FDR and ProBe
-Links to useful sites
Partial Contents: Part I: The Language of CSP; Sequential Processes; Concurrency; Abstraction and Control Flow; Part II: Analyzing Processes; Traces; Specification and Verification with Traces; Stable Failures; Specification and Verification with Failures; Failures, Divergences, and Infinite Traces; Part III: Introducing Time; The Timed Language; Timed transition systems; Part IV: Timed Analysis; Semantics of Timed CSP; Timed Specification and Verification; Timewise Refinement; Appendix A: Event-based Time; A.1 Standard CSP and $tock$; A.2 Translating from Timed CSP; A.3 Notes; Appendix B:Model-checking with FDR; B.1 Interacting with FDR; B.2 How FDR Checks Refinement; B.3 Machine readable CSP; Index of Processes.}}
@ARTICLE{Hierons_2009,title={Using formal specifications to support testing},year={2009},author={Robert M. Hierons and Robert M. Hierons and Kirill Bogdanov and Kirill Bogdanov and Jonathan P. Bowen and Jonathan P. Bowen and Rance Cleaveland and Rance Cleaveland and John Derrick and John Derrick and Jeremy Dick and Jeremy Dick and Marian Gheorghe and Marian Gheorghe and Mark Harman and Mark Harman and Kalpesh Kapoor and Kalpesh Kapoor and Paul Krause and Paul Krause and Gerald Lüttgen and Gerald Lüttgen and Anthony J. H. Simons and Anthony J. H. Simons and Sergiy Vilkomir and Sergiy Vilkomir and Martin R. Woodward and Martin R. Woodward and Hussein Zedan and Hussein Zedan},doi={10.1145/1459352.1459354},pmid={null},pmcid={null},mag_id={2124621264},journal={ACM Computing Surveys},abstract={Formal methods and testing are two important approaches that assist in the development of high-quality software. While traditionally these approaches have been seen as rivals, in recent years a new consensus has developed in which they are seen as complementary. This article reviews the state of the art regarding ways in which the presence of a formal specification can be used to assist testing.}}
@ARTICLE{Tretmans_2008,title={Model based testing with labelled transition systems},year={2008},author={Jan Tretmans and Jan Tretmans},doi={10.1007/978-3-540-78917-8_1},pmid={null},pmcid={null},mag_id={2126818664},journal={null},abstract={Model based testing is one of the promising technologies to meet the challenges imposed on software testing. In model based testing an implementation under test is tested for compliance with a model that describes the required behaviour of the implementation. This tutorial chapter describes a model based testing theory where models are expressed as labelled transition systems, and compliance is defined with the 'ioco' implementation relation. The ioco-testing theory, on the one hand, provides a sound and well-defined foundation for labelled transition system testing, having its roots in the theoretical area of testing equivalences and refusal testing. On the other hand, it has proved to be a practical basis for several model based test generation tools and applications. Definitions, underlying assumptions, an algorithm, properties, and several examples of the ioco-testing theory are discussed, involving specifications, implementations, tests, the ioco implementation relation and some of its variants, a test generation algorithm, and the soundness and exhaustiveness of this algorithm.}}
@ARTICLE{Springintveld_2001,title={Testing timed automata},year={2001},author={Jan Springintveld and Jan Springintveld and Frits Vaandrager and Frits W. Vaandrager and Pedro R. D’Argenio and Pedro R. D'Argenio and Pedro R. D'Argenio},doi={10.1016/s0304-3975(99)00134-6},pmid={null},pmcid={null},mag_id={2163595873},journal={Theoretical Computer Science},abstract={We present a generalization of the classical theory of testing for Mealy machines to a setting of dense real-time systems. A model of timed I/O automata is introduced, inspired by the timed automaton model of Alur and Dill, together with a notion of test sequence for this model. Our main contributions is a test suite derivation algorithm for black-box conformance testing of timed I/O automata. Black-box testing amounts to checking whether an implementation conforms to a specification of its external behavior, by means of a set of tests derived solely from specification. The main problem is to derive a finite set of tests from a possibly infinite, dense time transition system representing the specification. The solution is to reduce the dense time transition system to an appropriate finite discrete subautomaton, the grid automaton, which contains enough information to completely represent the specification from a test perspective. Although the method results in a test suite of high exponential size and cannot be claimed to be of practical value, it gives the first algorithm that yields a finite and complete set of tests for dense real-time systems.}}
@ARTICLE{León_2012,title={Conformance relations for labeled event structures},year={2012},author={Hernán Ponce de León and Hernán Ponce de León and Stefan Haar and Stefan Haar and Delphine Longuet and Delphine Longuet},doi={10.1007/978-3-642-30473-6_8},pmid={null},pmcid={null},mag_id={2227471340},journal={null},abstract={We propose a theoretical framework for testing concurrent systems from true concurrency models like Petri nets or networks of automata. The underlying model of computation of such formalisms are labeled event structures, which allow to represent concurrency explicitly. The activity of testing relies on the definition of a conformance relation that depends on the observable behaviors on the system under test, which is given for sequential systems by ioco type relations. However, these relations are not capable of capturing and exploiting concurrency of non sequential behavior. We study different conformance relations for labeled event structures, relying on different notions of observation, and investigate their properties and connections.}}
@ARTICLE{Larsen_2005,title={Online Testing of Real-time Systems Using Uppaal},year={2005},author={Kim Guldstrand Larsen and Marius Mikucionis and Brian Nielsen},doi={null},pmid={null},pmcid={null},mag_id={2555341099},journal={null},abstract={We present T-Uppaal — a new tool for online black-box testing of real-time embedded systems from non-deterministic timed automata specifications. We describe a sound and complete randomized online testing algorithm and how to implement it using symbolic state representation and manipulation techniques. We propose the notion of relativized timed input/output conformance as the formal implementation relation. A novelty of this relation and our testing algorithm is that they explicitly take environment assumptions into account, generate, execute and verify the result online using the Uppaal on-the-fly model-checking tool engine. A medium size case study shows promising results in terms of error detection capability and computation performance.}}
@ARTICLE{León_2013,title={Unfolding-Based Test Selection for Concurrent Conformance},year={2013},author={Hernán Ponce de León and Hernán Ponce de León and Stefan Haar and Stefan Haar and Delphine Longuet and Delphine Longuet},doi={10.1007/978-3-642-41707-8_7},pmid={null},pmcid={null},mag_id={86507843},journal={null},abstract={Model-based testing has mainly focused on models where currency is interpreted as interleaving (like the ioco theory for labeled transition systems), which may be too coarse when one wants concurrency to be preserved in the implementation. In order to test such concurrent systems, we choose to use Petri nets as specifications and define a concurrent conformance relation named co-ioco. We propose a test generation algorithm based on Petri net unfolding able to build a complete test suite w.r.t our co-ioco conformance relation. In addition we propose a coverage criterion based on a dedicated notion of complete prefixes that selects a manageable test suite.}}
@ARTICLE{Hierons_2012,title={Using Time to Add Order to Distributed Testing},year={2012},author={Robert M. Hierons and Robert M. Hierons and Mercedes G. Merayo and Mercedes G. Merayo and Manuel Núñez and Manuel Núñez},doi={10.1007/978-3-642-32759-9_20},pmid={null},pmcid={null},mag_id={105857026},journal={null},abstract={Many systems interact with their environment at physically distributed interfaces called ports. In testing such a system we might use a distributed approach in which there is a separate tester at each port. If the testers do not synchronise during testing then we cannot always determine the relative order of events observed at different ports and corresponding implementation relations have been developed for distributed testing. One possible method for strengthening the implementation relation is for testers to synchronise through exchanging coordination messages but this requires sufficiently fast communications channels and can increase the cost of testing. This paper explores an alternative in which each tester has a local clock and timestamps its observations. If we know nothing about how the local clocks relate then this does not help while if the local clocks agree exactly then we can reconstruct the sequence of observations made. In practice, however, we are likely to be between these extremes: the local clocks will not agree exactly but we have assumptions regarding how they can differ. This paper explores several such assumptions and derives corresponding implementation relations.}}
@ARTICLE{Ammann_2008,title={Introduction To Software Testing},year={2008},author={Paul Ammann and Paul Ammann and Jeff Offutt and Jeff Offutt},doi={null},pmid={null},pmcid={null},mag_id={1486172410},journal={null},abstract={Extensively class tested, this text takes an innovative approach to explaining the process of software testing: it defines testing as the process of applying a few well-defined, general-purpose test criteria to a structure or model of the software. The structure of the text directly reflects the pedagogical approach and incorporates the latest innovations in testing, including techniques to test modern types of software such as OO, web applications, and embedded software.}}
@ARTICLE{Aranda_2008,title={Formal Methods for Components and Objects},year={2008},author={Jesús Aranda and Jesús Aranda and Cinzia Di Giusto and Cinzia Di Giusto and Catuscia Palamidessi and Catuscia Palamidessi and Catuscia Palamidessi and Frank Valencia and Frank D. Valencia},doi={null},pmid={null},pmcid={null},mag_id={1497045044},journal={null},abstract={In this paper we shall survey and discuss in detail the work on the relative expressiveness of recursion and replication in various process calculi. Namely, CCS, the pi-calculus, the Ambient calculus, Concurrent Constraint Programming and calculi for Cryptographic Protocols. We shall give evidence that the ability of expressing recursive behaviour via replication often depends on the scoping mechanisms of the given calculus which compensate for the restriction of replication.}}
@ARTICLE{Broy_2005,title={Model-Based Testing of Reactive Systems, Advanced Lectures},year={2005},author={Manfred Broy and Manfred Broy and Bengt Jönsson and Bengt Jonsson and Joost-Pieter Katoen and Joost-Pieter Katoen and Martin Leucker and Martin Leucker and Alexander Pretschner and Alexander Pretschner},doi={10.1007/b137241},pmid={null},pmcid={null},mag_id={1509343160},journal={Lecture Notes in Computer Science},abstract={Testing of Finite State Machines.- I. Testing of Finite State Machines.- 1 Homing and Synchronizing Sequences.- 2 State Identification.- 3 State Verification.- 4 Conformance Testing.- II. Testing of Labeled Transition Systems.- Testing of Labeled Transition Systems.- 5 Preorder Relations.- 6 Test Generation Algorithms Based on Preorder Relations.- 7 I/O-automata Based Testing.- 8 Test Derivation from Timed Automata.- 9 Testing Theory for Probabilistic Systems.- III. Model-Based Test Case Generation.- Model-Based Test Case Generation.- 10 Methodological Issues in Model-Based Testing.- 11 Evaluating Coverage Based Testing.- 12 Technology of Test-Case Generation.- 13 Real-Time and Hybrid Systems Testing.- IV. Tools and Case Studies.- Tools and Case Studies.- 14 Tools for Test Case Generation.- 15 Case Studies.- V. Standardized Test Notation and Execution Architecture.- Standardized Test Notation and Execution Architecture.- 16 TTCN-3.- 17 UML 2.0 Testing Profile.- VI. Beyond Testing.- Beyond Testing.- 18 Run-Time Verification.- 19 Model Checking.- VII. Appendices.- Appendices.- 20 Model-Based Testing - A Glossary.- 21 Finite State Machines.- 22 Labelled Transition Systems.}}
@ARTICLE{Peleška_1996,title={From Testing Theory to Test Driver Implementation},year={1996},author={Jan Peleška and Jan Peleska and Michael I. Siegel and Michael Siegel},doi={10.1007/3-540-60973-3_106},pmid={null},pmcid={null},mag_id={1519017097},journal={null},abstract={In this article we describe the theoretical foundations for the VVT-RT test system (Verification, Validation and Test for Reactive Real-Time Systems) which supports automated test generation, test execution and test evaluation for reactive systems. VVT-RT constructs and evaluates tests based on formal CSP specifications [6], making use of their representation as labelled transition systems generated by the CSP model checker FDR [3]. The present article provides a sound formal basis for the development and verification of high-quality test tools: Since, due to the high degree of automation offered by VVT-RT, human interaction becomes superfluous during critical phases of the test process, the trustworthiness of the test tool is an issue of great importance. The VVT-RT system will therefore be formally verified so that it can be certified for testing safety-critical systems. The present article represents the starting point of this verification suite, where the basic strategies for test generation and test evaluation used by the system are formally described and verified. VVT-RT has been designed to support automation of both untimed and real-time tests. The present article describes the underlying theory for the untimed case. Exploiting these results, the concepts and high-level algorithms used for the automation of real-time tests are described in a second report which is currently prepared [14]. At present, VVT-RT is applied for hardware-in-the-loop tests of railway and tramway control computers.}}
@ARTICLE{Higashino_1999,title={Generating Test Cases for a Timed I/O Automaton Model},year={1999},author={Teruo Higashino and Teruo Higashino and Atsuo Nakata and Akio Nakata and Kenichi Taniguchi and Kenichi Taniguchi and Ana Cavalli and Ana Cavalli},doi={10.1007/978-0-387-35567-2_13},pmid={null},pmcid={null},mag_id={1566743700},journal={null},abstract={Recently various real-time communication protocols have been proposed. In this paper, first, we propose a timed I/O automaton model so that we can simply specify such real-time protocols. The proposed model can handle not only time but also data values. Then, we propose a conformance testing method for the model. In order to trace a test sequence (I/O sequence) on the timed I/O automaton model, we need to execute each I/O action in the test sequence at an adequate execution timing which satisfies all timing constraints in the test sequence. However, since outputs are given from IUTs and uncontrollable, we cannot designate their output timing in advance. Also their output timing affects the executable timing for the succeeding I/O actions in the test sequence. Therefore, in general, the executable timing of each input action in a test sequence can be specified by a function of the execution time of the preceding I/O actions. In this paper, we propose an algorithm to decide efficiently whether a given test sequence is executable. We also give an algorithm to derive such a function from an executable test sequence automatically using a technique for solving linear programming problems, and propose a conformance testing method using those algorithms.}}
@ARTICLE{Jard_1999,title={Remote testin can be as powerful as local testing},year={1999},author={Claude Jard and Claude Jard and Thierry Jéron and Thierry Jéron and Lénaick Tanguy and Lénaick Tanguy and César Viho and César Viho},doi={10.1007/978-0-387-35578-8_2},pmid={null},pmcid={null},mag_id={1583702921},journal={null},abstract={Designing test cases for remote asynchronous testing is error-prone. This is due to the difficulty to foresee all the disorders on the observations collected by the tester as well as the possible collisions between stimuli and observations. Designing correct synchronous test cases is easier, but transforming,them into correct asynchronous ones is a difficult task. Moreover, it is difficult to compare remote testing and local testing as in general sets of conformant implementations are not comparable.}}
@ARTICLE{Ulrich_1997,title={Specification-based Testing of Concurrent Systems},year={1997},author={Andreas Ulrich and Andreas Ulrich and Hartmut König and Hartmut König},doi={10.1007/978-0-387-35271-8_1},pmid={null},pmcid={null},mag_id={1601028213},journal={null},abstract={The paper addresses the problem of test suite derivation from a formal specification of a distributed concurrent software system given as a collection of labeled transition systems. It presents a new concurrency model, called behavior machine, and its construction algorithm. Further, the paper outlines how test derivation can be based on the new concurrency model in order to derive test suites that still exhibit true concurrency between test events. A toolset is presented to support the generation of concurrent test suites from specifications given in the formal description technique LOTOS. Finally, some comments on requirements for the design of a distributed test architecture are given.}}
@ARTICLE{Blom_2004,title={Specifying and generating test cases using observer automata},year={2004},author={Johan Blom and Johan Blom and Anders Hessel and Anders Hessel and Bengt Jönsson and Bengt Jonsson and Paul Pettersson and Paul Pettersson},doi={10.1007/978-3-540-31848-4_9},pmid={null},pmcid={null},mag_id={1863028222},journal={null},abstract={We present a technique for specifying coverage criteria and a method for generating test suites for systems whose behaviours can be described as extended finite state machines (EFSM). To specify coverage criteria we use observer automata with parameters, which monitor and accept traces that cover a given test criterion of an EFSM. The flexibility of the technique is demonstrated by specifying a number of well-known coverage criteria based on control- and data-flow information using observer automata with parameters. We also develop a method for generating test cases from coverage criteria specified as observers. It is based on transforming a given observer automata into a bitvector analysis problem that can be efficiently implemented as an extension to an existing state-space exploration such as, e.g. SPIN or Uppaal.}}
@ARTICLE{Bertrand_2011,title={Off-line test selection with test purposes for non-deterministic timed automata},year={2011},author={Nathalie Bertrand and Nathalie Bertrand and Thierry Jéron and Thierry Jéron and Amélie Staïner and Amélie Stainer and Moez Krichen and Moez Krichen and Moez Krichen},doi={10.1007/978-3-642-19835-9_10},pmid={null},pmcid={null},mag_id={1878755561},journal={null},abstract={This paper proposes novel off-line test generation techniques for non-deterministic timed automata with inputs and outputs (TAIOs) in the formal framework of the tioco conformance theory. In this context, a first problem is the determinization of TAIOs, which is necessary to foresee next enabled actions, but is in general impossible. This problem is solved here thanks to an approximate determinization using a game approach, which preserves tioco and guarantees the soundness of generated test cases. A second problem is test selection for which a precise description of timed behaviors to be tested is carried out by expressive test purposes modeled by a generalization of TAIOs. Finally, using a symbolic co-reachability analysis guided by the test purpose, test cases are generated in the form of TAIOs equipped with verdicts.}}
@ARTICLE{Hessel_2003,title={Time-Optimal Real-Time Test Case Generation Using Uppaal},year={2003},author={Anders Hessel and Anders Hessel and Kim Guldstrand Larsen and Kim Guldstrand Larsen and Brian Nielsen and Brian Nielsen and Paul Pettersson and Paul Pettersson and Arne Skou and Arne Skou},doi={10.1007/978-3-540-24617-6_9},pmid={null},pmcid={null},mag_id={1895974704},journal={null},abstract={Testing is the primary software validation technique used by industry today, but remains ad hoc, error prone, and very expensive. A promising improvement is to automatically generate test cases from formal models of the system under test.}}
@ARTICLE{Behrmann_2004,title={A Tutorial on UPPAAL},year={2004},author={Gerd Behrmann  Gerd Behrmann and Albert David and Alexandre David and Kim Guldstrand Larsen and Kim Guldstrand Larsen},doi={10.1007/978-3-540-30080-9_7},pmid={null},pmcid={null},mag_id={1962072139},journal={null},abstract={This is a tutorial paper on the tool Uppaal. Its goal is to
be a short introduction on the flavor of timed automata implemented in
the tool, to present its interface, and to explain how to use the tool. The
contribution of the paper is to provide reference examples and modeling
patterns.}}
@ARTICLE{Lestiennes_2005,title={Test de systèmes réactifs non réceptifs},year={2005},author={Grégory Lestiennes and Grégory Lestiennes and Marie-Claude Gaudel and Marie-Claude Gaudel},doi={10.3166/jesa.39.255-270},pmid={null},pmcid={null},mag_id={1973784481},journal={null},abstract={A prevalent hypothesis in the area of testing systems with inputs and outputs is that those systems are input enabled, that is, they must accept any input in any state. In this paper, we consider non input enabled systems. We define a new kind of model to deal with such systems as well as a new conformance relation: rioco. A set of tests, its associated verdict and some test hypotheses are given. It is proven that they ensure validity and unbias with regard to rioco.}}
@ARTICLE{Segala_1997,title={Quiescence, fairness, testing, and the notion of implementation},year={1997},author={Roberto Segala and Roberto Segala},doi={10.1006/inco.1997.2652},pmid={null},pmcid={null},mag_id={1974396748},journal={Information & Computation},abstract={Two different formalisms for concurrency are compared and are shown to have common foundations. The Input/Output automaton model and the theory of testing are analyzed in the framework of transition systems. The relationship between the fair and quiescent preorders of I/O automata is investigated and the two preorders are shown to coincide on a large class of automata. I/O automata are encoded into the theory of testing and the reversed MUST preorder is shown to be equivalent to the quiescent preorder for strongly convergent, finitely branching automata up to encoding. Conversely, a theory of testing is defined directly on I/O automata, and the new reversed MUST preorder is shown to coincide with the quiescent preorder on strongly convergent, finitely branching automata. Finally, some considerations are given on the issue of divergence, and on other existing theories with an I/O distinction.}}
@ARTICLE{Nielsen_1981,title={Petri nets, event structures and domains, part I},year={1981},author={Mogens Nielsen and Mogens Nielsen and Gordon Plotkin and Gordon Plotkin and Glynn Winskel and Glynn Winskel},doi={10.1016/0304-3975(81)90112-2},pmid={null},pmcid={null},mag_id={1991052229},journal={Theoretical Computer Science},abstract={Abstract   The general aim of this paper is to find a theory of concurrency combining the approaches of Petri and Scott (and others).  In part I we introduce our formalisms. To connect the abstract ideas of events and domains of information, we show how casual nets induce certain kinds of domains where the information points are certain sets of events. This allows translations between the languages of net theory and domain theory. Following the idea that events of causal nets are occurrences, we generalise causal nets to occurrence nets, by adding forwards conflict. Just as infinite flow charts unfold finite ones, so transition nets can be unfolded into occurrence nets. Next we extend the above connections between nets and domains to these new nets. Event structures which are intermediate between nets and domains play an important part in all our work. Finally, as an example of how concepts translate from one formalism to the other, we show how Petri's notion of confusion ties up with Kahn and Plotkin's concrete domains.  In part II we shall continue the job of connecting up notions within net theory and the theory of domains. In particular, we shall examine the idea of states of computations.}}
@ARTICLE{Lee_1996,title={Principles and methods of testing finite state machines-a survey},year={1996},author={D. Lee and David Lee and Mihalis Yannakakis and Mihalis Yannakakis},doi={10.1109/5.533956},pmid={null},pmcid={null},mag_id={2004929506},journal={null},abstract={With advanced computer technology, systems are getting larger to fulfill more complicated tasks: however, they are also becoming less reliable. Consequently, testing is an indispensable part of system design and implementation; yet it has proved to be a formidable task for complex systems. This motivates the study of testing finite stare machines to ensure the correct functioning of systems and to discover aspects of their behavior. A finite state machine contains a finite number of states and produces outputs on state transitions after receiving inputs. Finite state machines are widely used to model systems in diverse areas, including sequential circuits, certain types of programs, and, more recently, communication protocols. In a testing problem we have a machine about which we lack some information; we would like to deduce this information by providing a sequence of inputs to the machine and observing the outputs produced. Because of its practical importance and theoretical interest, the problem of testing finite state machines has been studied in different areas and at various times. The earliest published literature on this topic dates back to the 1950's. Activities in the 1960's mid early 1970's were motivated mainly by automata theory and sequential circuit testing. The area seemed to have mostly died down until a few years ago when the testing problem was resurrected and is now being studied anew due to its applications to conformance testing of communication protocols. While some old problems which had been open for decades were resolved recently, new concepts and more intriguing problems from new applications emerge. We review the fundamental problems in testing finite state machines and techniques for solving these problems, tracing progress in the area from its inception to the present and the stare of the art. In addition, we discuss extensions of finite state machines and some other topics related to testing.}}
@ARTICLE{Jéron_2009,title={Symbolic Model-based Test Selection},year={2009},author={Thierry Jéron and Thierry Jéron},doi={10.1016/j.entcs.2009.05.051},pmid={null},pmcid={null},mag_id={2060993374},journal={Electronic Notes in Theoretical Computer Science},abstract={This paper addresses the problem of model-based off-line selection of test cases for testing the conformance of a black-box implementation with respect to a specification, in the context of reactive systems. Efficient solutions to this problem have been proposed for LTS finite-state models, based on the ioco conformance testing theory. In this paper, the approach is extended for infinite-state specifications, modelled as automata extended with variables. When considering the selection of test cases according to test purposes (abstract scenarii focused by test cases), the selection of test cases relies on approximate co-reachability analyses using abstract interpretation and syntactical transformations guided by this analysis, while test execution uses constraint solving.}}
@ARTICLE{Longuet_2012,title={Global and local testing from Message Sequence Charts},year={2012},author={Delphine Longuet and Delphine Longuet},doi={10.1145/2245276.2231987},pmid={null},pmcid={null},mag_id={2084978003},journal={null},abstract={Message Sequence Charts are a widely used formalism for describing scenarios a communicating system must be able to perform. We study in this paper different formal frameworks for testing from MSCs. We first consider a setting where all the processes of the system can be controlled and observed globally. Then we study a setting where the system is tested from the point of view of each process individually, observations remaining local or being gathered at the end of each test. In each setting, we define a conformance relation based on global or local observations, for which we build an exhaustive test set. Moreover, we gather the conditions making local testing as powerful as global testing.}}
@ARTICLE{Vries_2000,title={On-the-Fly Conformance Testing using Spin},year={2000},author={R.G. de Vries and René G. de Vries and Jan Tretmans and Jan Tretmans},doi={10.1007/s100090050044},pmid={null},pmcid={null},mag_id={2109563237},journal={International Journal on Software Tools for Technology Transfer},abstract={In this paper we report on the construction of a tool for conformance testing based on Spin. The Spin tool has been adapted such that it can derive the building blocks for constructing test cases, called test primitives, from systems described in Promela. The test primitives support the on-the-fly conformance testing process. Traditional derivation of tests from formal specifications suffers from the state-space explosion problem. Spin is one of the most advanced model checkers with respect to handling large state spaces. This advantage of Spin has been used for the derivation of test primitives from a Promela description. To reduce the state space, we introduce the on-the-fly testing framework. One of the components within this framework is the Primer. The Primer is responsible for deriving test primitives from a model of a system according to a well-defined and complete testing theory. Algorithms are presented which enable us to derive test primitives from a Promela description. These algorithms have been implemented in the adapted version of the Spin tool which acts as the Primer in the framework. Promising experiments have been carried out on an example case study. As a result of this study it is concluded that it is possible to derive test primitives automatically from Promela descriptions, construct test cases from these test primitives, and execute the test cases on-the-fly.}}
@ARTICLE{Hoare_1985,title={Communicating sequential processes},year={1985},author={C. A. R. Hoare and C. A. R. Hoare},doi={null},pmid={null},pmcid={null},mag_id={2110425399},journal={null},abstract={This paper suggests that input and output are basic primitives of programming and that parallel composition of communicating sequential processes is a fundamental program structuring method. When combined with a development of Dijkstra's guarded command, these concepts are surprisingly versatile. Their use is illustrated by sample solutions of a variety of a familiar programming exercises.}}
@ARTICLE{Plotkin_2004,title={A Structural Approach to Operational Semantics},year={2004},author={Gordon Plotkin and Gordon Plotkin},doi={null},pmid={null},pmcid={null},mag_id={2118229393},journal={The Journal of Logic and Algebraic Programming},abstract={null}}
@ARTICLE{Tretmans_1999,title={Testing Concurrent Systems: A Formal Approach},year={1999},author={Jan Tretmans and Jan Tretmans},doi={10.1007/3-540-48320-9_6},pmid={null},pmcid={null},mag_id={2123023940},journal={null},abstract={This paper discusses the use of formal methods in testing of concurrent systems. It is argued that formal methods and testing can be mutually profitable and useful. A framework for testing based on formal specifications is presented. This framework is elaborated for labelled transition systems, providing formal definitions of conformance, test execution and test derivation. A test derivation algorithm is given and its tool implementation is briefly discussed.}}
@ARTICLE{Hennessy_1988,title={Algebraic theory of processes},year={1988},author={Matthew Hennessy and Matthew Hennessy},doi={null},pmid={null},pmcid={null},mag_id={2136920408},journal={null},abstract={Algebraic Theory of Processes provides the first general and systematic introduction to the semantics of concurrent systems, a relatively new research area in computer science. It develops the mathematical foundations of the algebraic approach to the formal semantics of languages and applies these ideas to a particular semantic theory of distributed processes. The book is unique in developing three complementary views of the semantics of concurrent processes: a behavioral view where processes are deemed to be equivalent if they cannot be distinguished by any experiment; a denotational model where processes are interpreted as certain kinds of trees; and a proof-theoretic view where processes may be transformed into equivalent processes using valid equations or transformations. It is an excellent guide on how to reason about and relate behavioral, denotational, and proof-theoretical aspects of languages in general: all three views are developed for a sequence of increasingly complex algebraic languages for concurrency and in each case they are shown to be equivalent. Algebraic Theory of Processes is a valuable source of information for theoretical computer scientists, not only as an elegant and comprehensive introduction to the field but also in its discussion of the author's own theory of the behavioral semantics of processes ("testing equivalence") and original results in example languages for distributed processes, It is self-contained; the problems addressed are motivated from the standpoint of computer science, and all the required algebraic concepts are covered. There are exercises at the end of each chapter.}}
@ARTICLE{Saϊdouni_2008,title={Aggregation of transitions in marking graph generation based on maximality semantics for petri nets},year={2008},author={Djamel Eddine Saϊdouni and Djamel-Eddine Saïdouni and Djamel-Eddine Saidouni and Djamel-Eddine Saidouni and Nabil Belala and Nabil Belala and Messaouda Bouneb and Messaouda Bouneb},doi={10.14236/ewic/vecos2008.1},pmid={null},pmcid={null},mag_id={2138341088},journal={null},abstract={In this paper, we propose an operational semantics to build maximality-based labeled transition systems (MLTS) from Place/Transition Petri nets while performing aggregation of equivalent derivations of transitions according to maximality bisimulation relation. We show that generated MLTS are equivalent to MLTS generated without aggregation. As illustration, we apply results on a ticket reservation system.}}
@ARTICLE{Krichen_2004,title={Real-Time Testing with Timed Automata Testers and Coverage Criteria},year={2004},author={Moez Krichen and Moez Krichen and Moez Krichen and Stavros Tripakis and Stavros Tripakis},doi={10.1007/978-3-540-30206-3_11},pmid={null},pmcid={null},mag_id={2160400510},journal={Lecture Notes in Computer Science},abstract={In previous work, we have proposed a framework for black-box conformance testing of real-time systems based on timed automata specifications and two types of tests: analog-clock or digital-clock. Our algorithm to generate analog-clock tests is based on an on-the-fly determinization of the specification automaton during the execution of the test, which in turn relies on reachability computations. The latter can sometimes be costly, thus problematic, since the tester must quickly react to the actions of the system under test. In this paper, we provide techniques which allow analog-clock testers to be represented as deterministic timed automata, thus minimizing the reaction time to a simple state jump. We also provide a method for (statically) generating a suite of digital-clock tests which covers the specification with respect to a number of criteria: location, edge or state coverage. This can dramatically reduce the number of generated tests, as can be evidenced on a small example.}}
@ARTICLE{Larsen_2005,title={Testing real-time embedded software using UPPAAL-TRON: an industrial case study},year={2005},author={Kim G. Larsen and Kim Guldstrand Larsen and Marius Mikučionis and Marius Mikucionis and Brian Nielsen and Brian Nielsen and Arne Skou and Arne Skou},doi={10.1145/1086228.1086283},pmid={null},pmcid={null},mag_id={2163659692},journal={null},abstract={UPPAAL-TRON is a new tool for model based online black-box conformance testing of real-time embedded systems specified as timed automata. In this paper we present our experiences in applying our tool and technique on an industrial case study. We conclude that the tool and technique is applicable to practical systems, and that it has promising error detection potential and execution performance.}}
@ARTICLE{Csopaki_1999,title={Testing of Communicating Systems},year={1999},author={Gyula Csopaki and Gyula Csopaki and Sarolta Dibuz and Sarolta Dibuz and K. Tarnay and Katalin Tarnay},doi={10.1007/978-0-387-35567-2},pmid={null},pmcid={null},mag_id={2497789484},journal={null},abstract={Testing of Communicating Systems presents the latest worldwide results in both the theory and practice of the testing of communicating systems. This volume provides a forum that brings together the su}}
@ARTICLE{Cooper_2006,title={Formal Methods for Components and Objects},year={2006},author={Ezra Cooper and Ezra Cooper and Sam Lindley and Sam Lindley and Philip Wadler and Philip Wadler and Jeremy Yallop and Jeremy Yallop},doi={10.1007/978-3-540-74792-5_12},pmid={null},pmcid={null},mag_id={2604845086},journal={null},abstract={null}}
@ARTICLE{Bengtsson_2004,title={Timed automata: Semantics, algorithms and tools},year={2004},author={Johan Bengtsson and Wang Yi},doi={null},pmid={null},pmcid={null},mag_id={3023485956},journal={Lecture Notes in Computer Science},abstract={This chapter is to provide a tutorial and pointers to results and related work on timed automata with a focus on semantical and algorithmic aspects of verification tools. We present the concrete and abstract semantics of timed automata (based on transition rules, regions and zones), decision problems, and algorithms for verification. A detailed description on DBM (Difference Bound Matrices) is included, which is the central data structure behind several verification tools for timed systems. As an example, we give a brief introduction to the tool UPPAAL.}}
@ARTICLE{Boer_2010,title={formal methods for components and objects},year={2010},author={Frank S. de Boer and Marcello M. Bonsangue and Stefan Hallerstede and Michael Leuschel},doi={10.1007/978-3-642-17071-3},pmid={null},pmcid={null},mag_id={3109825487},journal={Lecture Notes in Computer Science},abstract={null}}
@ARTICLE{Hoare_1985,title={Communicating Sequential Processes},year={1985},author={Tony Hoare and Tony Hoare and Tony Hoare},doi={null},pmid={null},pmcid={null},mag_id={3144368627},journal={null},abstract={null}}
@ARTICLE{Müller_2003,title={An ASM based systemC simulation semantics},year={2003},author={Wolfgang Müller and Wolfgang H. Müller and Jürgen Ruf and Jürgen Ruf and Wolfgang Rosenstiel and Wolfgang Rosenstiel},doi={10.1007/0-306-48735-7_4},pmid={null},pmcid={null},mag_id={173263332},journal={null},abstract={We present a formal definition of the event based SystemC V2.0 simulation semantics by means of distributed Abstract State Machines (ASMs). Our definition provides a rigorous and concise, but yet readable, definition of the SystemC specific operations and their interaction with the simulation scheduler that covers channel updates, notify, notify_delayed, wait, and next_trigger operations. We present the semantics in the form of rules by means of distributed ASMs reflecting the lines of the SystemC V2.0 Standard Manuals and reference implementation. The semantics introduced is defined to complement the language reference manual with a precise definition reflecting an abstract model of the SystemC reference implementation, which can be used for advanced applications and for investigating interoperabilities with other languages.}}
@ARTICLE{Etessami_2004,title={Analysis of Recursive Game Graphs Using Data Flow Equations},year={2004},author={Kousha Etessami and Kousha Etessami},doi={10.1007/978-3-540-24622-0_23},pmid={null},pmcid={null},mag_id={1480600454},journal={null},abstract={Given a finite-state abstraction of a sequential program with potentially recursive procedures and input from the environment, we wish to check statically whether there are input sequences that can drive the system into “bad/good” executions. Pushdown games have been used in recent years for such analyses and there is by now a very rich literature on the subject. (See, e.g., [BS92,Tho95,Wal96,BEM97,Cac02a,CDT02].)}}
@ARTICLE{Moura_2008,title={Z3: an efficient SMT solver},year={2008},author={Leonardo de Moura and Leonardo de Moura and Nikolaj Bjørner and Nikolaj Bjørner},doi={10.1007/978-3-540-78800-3_24},pmid={null},pmcid={null},mag_id={1480909796},journal={null},abstract={Satisfiability Modulo Theories (SMT) problem is a decision problem for logical first order formulas with respect to combinations of background theories such as: arithmetic, bit-vectors, arrays, and uninterpreted functions. Z3 is a new and efficient SMT Solver freely available from Microsoft Research. It is used in various software verification and analysis applications.}}
@ARTICLE{Mikučionis_2004,title={T-UPPAAL: online model-based testing of real-time systems},year={2004},author={Marius Mikučionis and Marius Mikucionis and Kim Guldstrand Larsen and Kim Guldstrand Larsen and Brian Nielsen and Brian Nielsen},doi={10.1109/ase.2004.65},pmid={null},pmcid={null},mag_id={1489329521},journal={null},abstract={The goal of testing is to gain confidence in a physical computer based system by means of executing it. More than one third of typical project resources are spent on testing embedded and real-time systems, but still it remains ad-hoc, based on heuristics, and error-prone. Therefore systematic, theoretically well-founded and effective automated real-time testing techniques are of great practical value. Testing conceptually consists of three activities: test case generation, test case execution and verdict assignment. We present T-UPPAAL-a new tool for model based testing of embedded real-time systems that automatically generates and executes tests "online" from a state machine model of the implementation under test (IUT) and its assumed environment which combined specify the required and allowed observable (realtime) behavior of the IUT. T-UPPAAL implements a sound and complete randomized testing algorithm, and uses a formally defined notion of correctness (relativized timed input/output conformance) to assign verdicts. Using online testing, events are generated and simultaneously executed.}}
@ARTICLE{Hierons_2008,title={implementation relations for the distributed test architecture},year={2008},author={Robert M. Hierons and Mercedes G. Merayo and Manuel Núñez},doi={10.1007/978-3-540-68524-1_15},pmid={null},pmcid={null},mag_id={1490389547},journal={null},abstract={Some systems interact with their environment at a number of physically distributed interfaces called ports. When testing such a system under test (SUT) it is normal to place a local tester at each port and the local testers form a local test case. If the local testers cannot interact with one another and there is no global clock then we are testing in the distributed test architecture. In this paper we explore the effect of the distributed test architecture when testing an SUT against an input output transition system, adapting the  ioco implementation relation to this situation. In addition, we define what it means for a local test case to be deterministic, showing that we cannot always implement a deterministic global test case as a deterministic local test case. Finally, we show how a global test case can be mapped to a local test case.}}
@ARTICLE{Pohl_2005,title={Software Product Line Engineering: Foundations, Principles and Techniques},year={2005},author={Klaus Pohl and Klaus Pohl and Gnter Bckle and Gnter Bckle and F. J. van der Linden and Frank van der Linden},doi={null},pmid={null},pmcid={null},mag_id={1494019345},journal={null},abstract={Software product line engineering has proven to be the methodology for developing a diversity of software products and software intensive systems at lower costs, in shorter time, and with higher quality. In this book, Pohl and his co-authors present a framework for software product line engineering which they have developed based on their academic as well as industrial experience gained in projects over the last eight years. They do not only detail the technical aspect of the development, but also an integrated view of the business, organisation and process aspects are given. In addition, they explicitly point out the key differences of software product line engineering compared to traditional single software system development, as the need for two distinct development processes for domain and application engineering respectively, or the need to define and manage variability.}}
@ARTICLE{Hong_2002,title={A Temporal Logic Based Theory of Test Coverage and Generation},year={2002},author={Henry Hong and Hyoung Seok Hong and Insup Lee and Insup Lee and Oleg Sokolsky and Oleg Sokolsky and Hasan Ural and Hasan Ural},doi={10.1007/3-540-46002-0_23},pmid={null},pmcid={null},mag_id={1495461922},journal={null},abstract={This paper presents a theory of test coverage and generation from specifications written in EFSMs. We investigate a family of coverage criteria based on the information of control flow and data flow and characterize them in the branching time temporal logic CTL. We discuss the complexity of minimal cost test generation and describe a method for automatic test generation which employs the capability of model checkers to construct counterexamples. Our approach extends the range of applications of model checking from formal verification of finite state systems to test generation from finite state systems.}}
@ARTICLE{Heerink_1997,title={Refusal Testing for Classes of Transition Systems with Inputs and Outputs},year={1997},author={Lex Heerink and Lex Heerink and Jan Tretmans and Jan Tretmans},doi={10.1007/978-0-387-35271-8_2},pmid={null},pmcid={null},mag_id={1498203526},journal={null},abstract={This paper presents a testing theory that is parameterised with assumptions about the way implementations communicate with their environment. In this way some existing testing theories, such as refusal testing for labelled transition systems and (repetitive) quiescence testing for I/O automata, can be unified in a single framework. Starting point is the theory of refusal testing. We apply this theory to classes of implementations which communicate with their environment via clearly distinguishable input and output actions. These classes are induced by making assumptions about the geographical distribution of the points of control and observation (PCO’s) and about the way input actions of implementations are enabled. For specific instances of these classes our theory collapses with some well-known ones. For all these classes a single test generation algorithm is presented that is able to derive sound and complete test suites from a specification.}}
@ARTICLE{Armstrong_2007,title={Programming Erlang: Software for a Concurrent World},year={2007},author={Joe Armstrong and Joe Armstrong},doi={null},pmid={null},pmcid={null},mag_id={1499326305},journal={null},abstract={Erlang solves one of the most pressing problems facing developers today: how to write reliable, concurrent, high-performance systems. It's used worldwide by companies who need to produce reliable, efficient, and scalable applications. Invest in learning Erlang now. Moore's Law is the observation that the amount you can do on a single chip doubles every two years. But Moore's Law is taking a detour. Rather than producing faster and faster processors, companies such as Intel and AMD are producing multi-core devices: single chips containing two, four, or more processors. If your programs aren't concurrent, they'll only run on a single processor at a time. Your users will think that your code is slow. Erlang is a programming language designed for building highly parallel, distributed, fault-tolerant systems. It has been used commercially for many years to build massive fault-tolerated systems that run for years with minimal failures. Erlang programs run seamlessly on multi-core computers: this means your Erlang program should run a lot faster on a 4 core processor than on a single core processor, all without you having to change a line of code. Erlang combines ideas from the world of functional programming with techniques for building fault-tolerant systems to make a powerful language for building the massively parallel, networked applications of the future. This book presents Erlang and functional programming in the familiar Pragmatic style. And it's written by Joe Armstrong, one of the creators of Erlang. It includes example code you'll be able to build upon. In addition, the book contains the full source code for two interesting applications: A SHOUTcast server which you can use to stream music to every computer in your house, and a full-text indexing and search engine that can index gigabytes of data. Learn how to write programs that run on dozens or even hundreds of local and remote processors. See how to write robust applications that run even in the face of network and hardware failure, using the Erlang programming language.}}
@ARTICLE{Milner_1989,title={Communication and Concurrency},year={1989},author={Robin Milner and Robin Milner},doi={null},pmid={null},pmcid={null},mag_id={1503973138},journal={null},abstract={Foreword. 1. Modelling Communication. 2. Basic Definitions. 3. Equational laws and Their Application. 4. Strong Bisimulation and Strong Equivalence. 5. Bisimulation and Observation Equivalence. 6. Further Examples. 7. The Theory of Observation Congruence. 8. Defining a Programming Language. 9. Operators and Calculi. 10. Specifications and Logic. 11. Determinancy and Confluence. 12. Sources and Related Work. Bibliography. Index.}}
@ARTICLE{Kirchsteiger_2008,title={Specification-based Verification of Embedded Systems by Automated Test Case Generation},year={2008},author={Christoph M. Kirchsteiger and Christoph M. Kirchsteiger and Christoph Trummer and Christoph Trummer and Christian Steger and Christian Steger and Reinhold Weiß and Reinhold Weiss and Markus Pistauer and Markus Pistauer},doi={10.1007/978-0-387-09661-2_4},pmid={null},pmcid={null},mag_id={1504409154},journal={null},abstract={It is time and resource intensive to derive test cases manually from the requirements specification to fully verify that the embedded system design fulfills its specification. However, automatic parsing to generate test cases is often not possible due to the informal, non-machine readable structure of the specification document. Formal specification languages would ease the parsing process, however they are difficult to use and rarely accepted. A promising trade-off are semi-formal specification languages, which are both easy-to-parse and easy-to-use.}}
@ARTICLE{Saaty_2012,title={Models, Methods, Concepts & Applications of the Analytic Hierarchy Process},year={2012},author={Thomas L. Saaty and Thomas L. Saaty and Luis G. Vargas and Luis G. Vargas},doi={null},pmid={null},pmcid={null},mag_id={1506366602},journal={null},abstract={How to Make a Decision.- The Seven Pillars of the Analytic Hierarchy Process.- Architectural Design.- Why is the Principal Eigenvector Necessary?.- Designing a Mousetrap.- Designing the Best Catamaran.- The Selection of a Bridge.- Measuring Dependence Between Activities: Input Output Application to the Sudan.- Technological Choice in Less Developed Countries.- Market Attractiveness of Developing Countries.- An Analytic Hierarchy Process Based Approach to the Design and Evaluation of a Marketing Driven Business and Corporate Strategy.- New Product Pricing Strategy.- Incorporating Expert Judgement in Economic Forecasts: The Case of the U.S. Economy in 1992.- A New Macroeconomic Forecasting and Policy Evaluation Method.- A New Approach to the Middle East Conflict: The Analytic Hierarchy Process.- Legalization of Euthanasia.- How Should Congress Address the Medicare Crisis?.- Ethics in International Business.- Abortion and the States: How Will the Supreme Court Rule on the Upcoming Pennsylvania Abortion Issue?.- The Benefits and Costs of Authorizing Riverboat Gambling.- To Drill or Not to Drill: A Synthesis of Expert Judgements.- Modeling the Graduate Business School Admissions Process.- Infertility Decision Making.- Deciding Between Angioplasty and Coronary Artery Bypass Surgery.}}
@ARTICLE{Jard_1998,title={Towards Automatic Distribution of Testers for Distributed Conformance Testing},year={1998},author={Claude Jard and Claude Jard and Thierry Jéron and Thierry Jéron and H. Kahlouche and H. Kahlouche and César Viho and César Viho},doi={10.1007/978-0-387-35394-4_22},pmid={null},pmcid={null},mag_id={1518246002},journal={null},abstract={This paper presents first steps towards automatic generation of distributed tests. We first define a characterization of the tests for which the property of unbias is preserved by the existence of an asynchronous environment. Then, starting from a centralized test case, we propose a method to derive automatically its corresponding distributed test case in an asynchronous environment. We prove that the generated distributed test case is not biased, it tests the same behaviors of an implementation and has the same testing power as the centralized test case.}}
@ARTICLE{Abdulla_2006,title={Eager markov chains},year={2006},author={Parosh Aziz Abdulla and Parosh Aziz Abdulla and Noomene Ben Henda and Noomene Ben Henda and Richard Mayr and Richard Mayr and Sven Sandberg and Sven Sandberg},doi={10.1007/11901914_5},pmid={null},pmcid={null},mag_id={1521216227},journal={null},abstract={We consider infinite-state discrete Markov chains which are eager: the probability of avoiding a defined set of final states for more than n steps is bounded by some exponentially decreasing function f(n). We prove that eager Markov chains include those induced by Probabilistic Lossy Channel Systems, Probabilistic Vector Addition Systems with States, and Noisy Turing Machines, and that the bounding function f(n) can be effectively constructed for them. Furthermore, we study the problem of computing the expected reward (or cost) of runs until reaching the final states, where rewards are assigned to individual runs by computable reward functions. For eager Markov chains, an effective path exploration scheme, based on forward reachability analysis, can be used to approximate the expected reward up-to an arbitrarily small error.}}
@ARTICLE{Gerke_2010,title={Model checking the FlexRay physical layer protocol},year={2010},author={Michael Gerke and Michael Gerke and Rüdiger Ehlers and Rüdiger Ehlers and Bernd Finkbeiner and Bernd Finkbeiner and Hans-Jörg Peter and Hans-Jörg Peter},doi={10.1007/978-3-642-15898-8_9},pmid={null},pmcid={null},mag_id={1523839842},journal={null},abstract={The FlexRay standard, developed by a cooperation of leading companies in the automotive industry, is a robust communication protocol for distributed components in modern vehicles. In this paper, we present the first timed automata model of its physical layer protocol, and we use automatic verification to prove fault tolerance under several error models and hardware assumptions.

The key challenge in the analysis is that the correctness of the protocol relies on the interplay of the bit-clock alignment mechanism with the precise timing behavior of the underlying asynchronous hardware. We give a general hardware model that is parameterized in low-level timing details such as hold times and propagation delays. Instantiating this model for a realistic design from the Nangate Open Cell Library, and verifying the resulting model using the real-time model checker UPPAAL, we show that the communication system meets, and in fact exceeds, the fault-tolerance guarantees claimed in the FlexRay specification.}}
@ARTICLE{Gaudel_1995,title={Testing Can Be Formal, Too},year={1995},author={Marie-Claude Gaudel and Marie-Claude Gaudel},doi={10.1007/3-540-59293-8_188},pmid={null},pmcid={null},mag_id={1527688737},journal={null},abstract={The paper presents a theory of program testing based on formal specifications. The formal semantics of the specifications is the basis for a notion of an exhaustive test set. Under some minimal hypotheses on the program under test, the success of this test set is equivalent to the satisfaction of the specification.}}
@ARTICLE{Diekert_1995,title={The Book of Traces},year={1995},author={Volker Diekert and Volker Diekert and Grzegorz Rozenberg and Grzegorz Rozenberg},doi={10.1142/2563},pmid={null},pmcid={null},mag_id={1529497146},journal={null},abstract={The theory of traces belongs to both formal language theory and the theory of concurrent systems. In both these disciplines it is a well-recognized and dynamic research area. Within formal language theory it yields the theory of partially commutative monoids, and provides an important connection between languages and graphs. Within the theory of concurrent systems it provides an important formal framework for the analysis and synthesis of concurrent systems. This monograph covers all important research lines of the theory of traces - each chapter of the book is devoted to one research line and is written by leading experts. It is organized in such a way that each chapter can be read independently - and hence is suitable for advanced courses/seminars on formal language theory and the theory of concurrent systems.}}
@ARTICLE{Caromel_2005,title={A Theory of Distributed Objects},year={2005},author={Denis Caromel and Denis Caromel and Ludovic Henrio and Ludovic Henrio and Luca Cardelli and Luca Cardelli},doi={null},pmid={null},pmcid={null},mag_id={1530813957},journal={null},abstract={Distributed and communicating objects are becoming ubiquitous. In global, Grid and Peer-to-Peer computing environments, extensive use is made of objects interacting through method calls. So far, no ge}}
@ARTICLE{Johnsen_2010,title={Dynamic resource reallocation between deployment components},year={2010},author={Einar Broch Johnsen and Einar Broch Johnsen and Olaf Owe and Olaf Owe and Rudolf Schlatte and Rudolf Schlatte and Silvia Lizeth Tapia Tarifa and Silvia Lizeth Tapia Tarifa},doi={10.1007/978-3-642-16901-4_42},pmid={null},pmcid={null},mag_id={1534755694},journal={null},abstract={Today's software systems are becoming increasingly configurable and designed for deployment on a plethora of architectures, ranging from sequential machines via multicore and distributed architectures to the cloud. Examples of such systems are found in, e.g., software product lines, service-oriented computing, information systems, embedded systems, operating systems, and telephony. To model and analyze systems without a fixed architecture, the models need to naturally capture and range over relevant deployment scenarios. For this purpose, it is interesting to lift aspects of low-level deployment concerns to the abstraction level of the modeling language. In this paper, the object-oriented modeling language Creol is extended with a notion of dynamic deployment components with parametric processing resources, such that processor resources may be explicitly reallocated. The approach is compositional in the sense that functional models and reallocation strategies are both expressed in Creol, and functional models can be run alone or in combination with different reallocation strategies. The formal semantics of deployment components is given in rewriting logic, extending the semantics of Creol, and executes on Maude, which allows simulations and test suites to be applied to models which vary in their available resources as well as in their resource reallocation strategies.}}
@ARTICLE{Holzer_2008,title={FShell: Systematic Test Case Generation for Dynamic Analysis and Measurement},year={2008},author={Andreas Holzer and Andreas Holzer and Christian Schallhart and Christian Schallhart and Michael Tautschnig and Michael Tautschnig and Helmut Veith and Helmut Veith},doi={10.1007/978-3-540-70545-1_20},pmid={null},pmcid={null},mag_id={1548817078},journal={null},abstract={Although the principal analogy between counterexample generation and white box testing has been repeatedly addressed, the usage patterns and performance requirements for software testing are quite different from formal verification. Our tool FS hell provides a versatile testing environment for C programs which supports both interactive explorative use and a rich scripting language. More than a frontend for software model checkers, FS hell is designed as a database engine which dispatches queries about the program to program analysis tools. We report on the integration of CBMC into FS hell and describe architectural modifications which support efficient test case generation.}}
@ARTICLE{Khoumsi_2003,title={Test Cases Generation for Nondeterministic Real-Time Systems},year={2003},author={Ahmed Khoumsi and Ahmed Khoumsi and Thierry Jéron and Thierry Jéron and Hervé Marchand and Hervé Marchand},doi={10.1007/978-3-540-24617-6_10},pmid={null},pmcid={null},mag_id={1557346410},journal={null},abstract={We study the generation of test cases for nondeterministic real-time systems. We define a class of Determinizable Timed Automata (DTA), in order to specify the system under test. The principle of our test method consists of two steps. In Step 1, we express the problem in a non-real-time form, by transforming a DTA into an equivalent finite state automaton. The latter uses two additional types of events, Set and Exp. In Step 2, we adapt a non-real-time test generation method.}}
@ARTICLE{Winskel_1985,title={Petri nets, morphisms and compositionality},year={1985},author={Glynn Winskel and Glynn Winskel},doi={10.1007/bfb0016226},pmid={null},pmcid={null},mag_id={1557632285},journal={null},abstract={It is shown how a category of Petri nets can be viewed as a subcategory of two sorted algebras over multisets. This casts Petri nets in a familiar framework and provides a useful idea of morphism on nets different from the conventional definition—the morphisms here respect the behaviour of nets. The categorical constructions which result provide a useful way to synthesise nets and reason about nets in terms of their components; for example various forms of parallel composition of Petri nets arise naturally from the product in the category. It provides a smooth formal relation with other models of concurrency such as Milner's Calculus of Communicating Systems (CCS) and Hoare's Communicating Sequential Processes (CSP), though this is only indicated in this paper.}}
@ARTICLE{Abrial_1996,title={The B-Book: Assigning Programs to Meanings},year={1996},author={Jean-Raymond Abrial},doi={null},pmid={null},pmcid={null},mag_id={1559870885},journal={null},abstract={Tribute Foreword Introduction Part I. Mathematics: 1. Mathematical reasoning 2. Set notation 3. Mathematical objects Part II. Abstract Machines: 4. Introduction to abstract machines 5. Formal definition of abstract machines 6. Theory of abstract machines 7. Constructing large abstract machines 8. Examples of abstract machines Part III. Programming: 9. Sequencing and loop 10. Programming examples Part IV. Refinement: 11. Refinement 12. Constructing large software systems 13. Examples of refinement Appendixes Index.}}
@ARTICLE{Bochmann_2008,title={Testing Systems Specified as Partial Order Input/Output Automata},year={2008},author={Gregor von Bochmann and Gregor von Bochmann and Stefan Haar and Stefan Haar and Claude Jard and Claude Jard and Guy-Vincent Jourdan and Guy-Vincent Jourdan},doi={10.1007/978-3-540-68524-1_13},pmid={null},pmcid={null},mag_id={1563012880},journal={null},abstract={An Input/Output Automaton is an automaton with a finite number of states where each transition is associated with a single inpu f or output interaction. In [1], we introduced a new formalism, in which each transition is associated with a bipartite partially ordered set made of concurrent inputs followed by concurrent outputs. In this paper, we generalize this model to Partial Order Input/Output Automata (POIOA), in which each transition is associated with an almost arbitrary partially ordered set of inputs and outputs. This formalism can be seen as High-Level Messages Sequence Charts with inputs and outputs and allows for the specification of concurrency between inputs and outputs in a very general, direct and concise way. We give a formal definition of this framework, and define several conformance relations for comparing system specifications expressed in this formalism. Then we show how to derive a test suite that guarantees to detect faults defined by a POIOA-specific fault model: missing output faults, unspecified output faults, weaker precondition faults, stronger precondition faults and transfer faults.}}
@ARTICLE{Baresi_2002,title={Tutorial Introduction to Graph Transformation: A Software Engineering Perspective},year={2002},author={Luciano Baresi and Luciano Baresi and Reiko Heckel and Reiko Heckel},doi={10.1007/3-540-45832-8_30},pmid={null},pmcid={null},mag_id={1567193480},journal={null},abstract={We give an introduction to graph transformation, not only for researchers in software engineering, but based on applications of graph transformation in this domain. In particular, we demonstrate the use of graph transformation to model object- and component-based systems and to specify syntax and semantics of diagram languages. Along the way we introduce the basic concepts, discuss different approaches, and mention relevant theory and tools.}}
@ARTICLE{Genç_2003,title={Distributed diagnosis of discrete-event systems using Petri nets},year={2003},author={Şahika Genç and Sahika Genc and Stéphane Lafortune and Stéphane Lafortune},doi={10.1007/3-540-44919-1_21},pmid={null},pmcid={null},mag_id={1573093426},journal={null},abstract={The problem of detecting and isolating fault events in dynamic systems modeled as discrete-event systems is considered. The modeling formalism adopted is that of Petri nets with labeled transitions, where some of the transitions are labeled by different types of unobservable fault events. The Diagnoser Approach for discrete-event systems modeled by automata developed in earlier work is adapted and extended to on-line fault diagnosis of systems modeled by Petri nets, resulting in a centralized diagnosis algorithm based on the notion of "Petri net diagnosers". A distributed version of this centralized algorithm is also presented. This distributed version assumes that the Petri net model of the system can be decomposed into two place-bordered Petri nets satisfying certain conditions and that the two resulting Petri net diagnosers can exchange messages upon the occurrence of observable events. It is shown that this distributed algorithm is correct in the sense that it recovers the same diagnostic information as the centralized algorithm. The distributed algorithm provides an approach for tackling fault diagnosis of large complex systems.}}
@ARTICLE{Cimatti_1999,title={NUSMV: A New Symbolic Model Verifier},year={1999},author={Alessandro Cimatti and Alessandro Cimatti and Edmund M. Clarke and Edmund M. Clarke and Fausto Giunchiglia and Fausto Giunchiglia and Marco Roveri and Marco Roveri},doi={10.1007/3-540-48683-6_44},pmid={null},pmcid={null},mag_id={1574030932},journal={null},abstract={This paper describes NUSMV, a new symbolic model checker developed as a joint project between Carnegie Mellon University (CMU) and Istituto per la Ricerca Scientifica e Tecnolgica (IRST). NUSMV is designed to be a well structured, open, flexible and documented platform for model checking. In order to make NUSMV applicable in technology transfer projects, it was designed to be very robust, close to the standards required by industry, and to allow for expressive specification languages. NUSMV is the result of the reengineering, reimplementation and extension of SMV [6], version 2.4.4 (SMV from now on). With respect to SMV, NUSMV has been extended and upgraded along three dimensions. First, from the point of view of the system functionalities, NUSMV features a textual interaction shell and a graphical interface, extended model partitioning techniques, and allows for LTL model checking. Second, the system architecture of NUSMV has been designed to be highly modular and open. The interdependencies between different modules have been separated, and an external, state of the art BDD package [8] has been integrated in the system kernel. Third, the quality of the implementation has been strongly enhanced. This makes of NUSMV a robust, maintainable and well documented system, with a relatively easy to modify source code. NUSMV is available at http://nusmv.irst.itc.it/.}}
@ARTICLE{Krichen_2006,title={Interesting properties of the real-time conformance relation tioco},year={2006},author={Moez Krichen and Moez Krichen and Stavros Tripakis and Stavros Tripakis},doi={10.1007/11921240_22},pmid={null},pmcid={null},mag_id={1585027773},journal={null},abstract={We are interested in black-box conformance testing of real-time systems. Our framework is based on the model of timed automata with inputs and outputs (TAIO). We use a timed conformance relation called tioco which is the extension of the untimed relation ioco. We show that considering only lazy-input TAIO is enough for describing all possible non-blocking specifications. We compare between tioco and the trace-inclusion relation. We prove that tioco is undecidable and that it does not distinguish specifications with the same set of observable traces. We prove tioco to be transitive and stable w.r.t both compositionality and action hiding for input-complete specifications. We compare between tioco and two other timed conformance relations, rtioco and $ \sqsubseteq_{\mathsf{\it tioco}}$.}}
@ARTICLE{León_2014,title={Model-based testing for concurrent systems with labelled event structures},year={2014},author={Hernán Ponce de León and Hernán Ponce de León and Stefan Haar and Stefan Haar and Stefan Haar and Stefan Haar and Delphine Longuet and Delphine Longuet},doi={10.1002/stvr.1543},pmid={null},pmcid={null},mag_id={1591617867},journal={Software Testing, Verification & Reliability},abstract={We propose a theoretical testing framework and a test generation algorithm for concurrent systems specified with true-concurrency models, such as Petri nets or networks of automata. The semantic model of computation of such formalisms is labelled event structures, which allow to represent concurrency explicitly. We introduce the notions of strong and weak concurrency: strongly concurrent events must be concurrent in the implementation, while weakly concurrent ones may eventually be ordered. The ioco type conformance relations for sequential systems rely on the observation of sequences of actions and blockings; thus, they are not capable of capturing and exploiting concurrency of non-sequential behaviours. We propose an extension of ioco for labelled event structures, named co-ioco, allowing to deal with strong and weak concurrency. We extend the notions of test cases and test execution to labelled event structures and give a test generation algorithm building a complete test suite for co-ioco. Copyright © 2014 John Wiley & Sons, Ltd.}}
@ARTICLE{Faivre_2008,title={Test Purpose Concretization through Symbolic Action Refinement},year={2008},author={Alain Faivre and Alain Faivre and Christophe Gaston and Christophe Gaston and Pascale Le Gall and Pascale Le Gall and Assia Touil and Assia Touil and Assia Touil and Assia Touil and Assia Touil},doi={10.1007/978-3-540-68524-1_14},pmid={null},pmcid={null},mag_id={1597214525},journal={null},abstract={In a Model Driven Design process, model refinement methodologies allow one to denote system behaviors at several levels of abstraction. In the frame of a model-based testing process, benefits can be taken from such refinement processes by extracting test cases from the different intermediate models. As a consequence, test cases extracted from abstract models often have to be concretized in order to be executable on the System Under Test. In order to properly define a test concretization process, a notion of conformance relating SUTs and abstract models has to be defined. We define such a relation for models described in a symbolic manner as so-called IOSTSs (Input Output Symbolic Transition Systems) and for a particular kind of refinement, namely action refinement, which consists in replacing communication actions of abstract models with sets of sequences of more concrete communication actions. Our relation is defined as an extension of the ioco-conformance relation which relates SUTs and models whose communication actions are defined at the same level of abstraction. Finally we show from an example how a test purpose resulting from an abstract IOSTS-model can be concretized in a test purpose defined at the abstraction level of the SUT.}}
@ARTICLE{Kuske_2001,title={Pomsets for local trace languages},year={2001},author={Dietrich Kuske and Dietrich Kuske and Dietrich Kuske and Rémi Morin and Rémi Morin},doi={10.1007/3-540-44618-4_31},pmid={null},pmcid={null},mag_id={1597882372},journal={null},abstract={Mazurkiewicz traces can be seen as equivalence classes of words or as pomsets. Their generalization by local traces was formalized by HOOGERS, KLEIJN and THIAGARAJAN as equivalence classes of step firing sequences. First we introduce a pomset representation for local traces. Extending Buchi's Theorem and a previous generalization to Mazurkiewicz traces, we show then that a local trace language is recognized by a finite step transition system if and only if its class of pomsets is bounded and definable in Monadic Second Order logic.}}
@ARTICLE{Yacoub_2002,title={Performance Analysis of Component-Based Applications},year={2002},author={Sherif M. Yacoub and Sherif Yacoub},doi={10.1007/3-540-45652-x_19},pmid={null},pmcid={null},mag_id={1601439260},journal={null},abstract={Performance analysis is a software engineering activity that involves analyzing a software application with respect to performance quality attributes such as response and execution times. Performance analysis tools provide the necessary support for the analyst to monitor program execution, record and analyze performance data, and locate and understand areas of poor performance. Performance analysis methods and techniques are highly dependent on the properties of the software system to be analyzed. Product line engineering applications possess some special properties that impose constraints on the selection of the performance analysis techniques to be applied and the tools to be used. The development of a component-based reference architecture is crucial to the success of a true product line. The component-based nature facilitates the integration of components and the replacement of a component with another to meet the requirements of an instance application of the product line. In this paper, we discuss performance analysis of component-based software systems and its automation. We discuss how component-based system properties influence the selection of methods and tools used to obtain and analyze performance measures. We use a case study of the document content remastering product line to illustrate the application of a performance analysis method to component-based applications.}}
@ARTICLE{Sugeta_2004,title={Mutation testing applied to validate SDL specifications},year={2004},author={Tatiana Sugeta and Tatiana Sugeta and José Carlos Maldonado and José Carlos Maldonado and W. Eric Wong and W. Eric Wong},doi={10.1007/978-3-540-24704-3_13},pmid={null},pmcid={null},mag_id={1606184974},journal={Lecture Notes in Computer Science},abstract={Mutation Testing is an error-based criterion that provides mechanisms to evaluate the quality of a test set and/or to generate test sets. This criterion, originally proposed to program testing, has also been applied to specification testing. In this paper, we propose the application of Mutation Testing for testing SDL specifications. We define a mutant operator set for SDL that intends to model errors related to the behavioral aspect of the processes, the communication among processes, the structure of the specification and some intrinsic characteristics of SDL. A testing strategy to apply the mutant operators to test SDL specifications is proposed. We illustrate our approach using the Alternating-Bit protocol.}}
@ARTICLE{Mattern_2002,title={Virtual Time and Global States of Distributed Systems},year={2002},author={Friedemann Mattern and Friedemann Mattern},doi={null},pmid={null},pmcid={null},mag_id={1652742168},journal={null},abstract={A distributed system can be characterized by the fact that the global state is distributed and that a common time base does not exist. However, the notion of time is an important concept in every day life of our decentralized \real world" and helps to solve problems like getting a consistent population census or determining the potential causality between events. We argue that a linearly ordered structure of time is not (always) adequate for distributed systems and propose a generalized non-standard model of time which consists of vectors of clocks. These clock-vectors are partially ordered and form a lattice. By using timestamps and a simple clock update mechanism the structure of causality is represented in an isomorphic way. The new model of time has a close analogy to Minkowski's relativistic spacetime and leads among others to an interesting characterization of the global state problem. Finally, we present a new algorithm to compute a consistent global snapshot of a distributed system where messages may be received out of order.}}
@ARTICLE{Clarke_2002,title={STG: A Symbolic Test Generation Tool},year={2002},author={David M. Clarke and Duncan Clarke and Thierry Jéron and Thierry Jéron and Vlad Rusu and Vlad Rusu and Elena Zinovieva and Elena Zinovieva},doi={10.1007/3-540-46002-0_34},pmid={null},pmcid={null},mag_id={1766479663},journal={null},abstract={We report on a tool we have developed that implements conformance testing techniques to automatically derive symbolic tests cases from formal operational specifications. We demonstrate the application of the techniques and tools on a simple example and present case studies for the CEPS (Common Electronic Purse Specification) and for the file system of the 3GPP (Third Generation Partnership Project) card.}}
@ARTICLE{Lara_2002,title={AToM3: A Tool for Multi-formalism and Meta-modelling},year={2002},author={Juan de Lara and Juan de Lara and Hans Vangheluwe and Hans Vangheluwe},doi={10.1007/3-540-45923-5_12},pmid={null},pmcid={null},mag_id={1816012646},journal={null},abstract={This article introduces the combined use of multiformalism modelling and meta-modelling to facilitate computer assisted modelling of complex systems. The approach allows one to model different parts of a system using different formalisms. Models can be automatically converted between formalisms thanks to information found in a Formalism Transformation Graph (FTG), proposed by the authors. To aid in the automatic generation of multi-formalism modelling tools, formalisms are modelled in their own right (at a meta-level) within an appropriate formalism. This has been implemented in the interactive tool AToM3. This tool is used to describe formalisms commonly used in the simulation of dynamical systems, as well as to generate custom tools to process (create, edit, transform, simulate, optimise, ...) models expressed in the corresponding formalism. AToM3 relies on graph rewriting techniques and graph grammars to perform the transformations between formalisms as well as for other tasks, such as code generation and operational semantics specification.}}
@ARTICLE{Veanes_2008,title={Model-based testing of object-oriented reactive systems with spec explorer},year={2008},author={Margus Veanes and Margus Veanes and Colin Campbell and Colin Campbell and Wolfgang Grieskamp and Wolfgang Grieskamp and Wolfram Schulte and Wolfram Schulte and Nikolai Tillmann and Nikolai Tillmann and Lev Nachmanson and Lev Nachmanson},doi={10.1007/978-3-540-78917-8_2},pmid={null},pmcid={null},mag_id={1821489642},journal={null},abstract={Testing is one of the costliest aspects of commercial software development. Model-based testing is a promising approach addressing these deficits. At Microsoft, model-based testing technology developed by the Foundations of Software Engineering group in Microsoft Research has been used since 2003. The second generation of this tool set, Spec Explorer, deployed in 2004, is now used on a daily basis by Microsoft product groups for testing operating system components, .NET framework components and other areas. This chapter provides a comprehensive survey of the concepts of the tool and their foundations.}}
@ARTICLE{Haar_2007,title={Testing input/output partial order automata},year={2007},author={Stefan Haar and Stefan Haar and Claude Jard and Claude Jard and Guy-Vincent Jourdan and Guy-Vincent Jourdan},doi={10.1007/978-3-540-73066-8_12},pmid={null},pmcid={null},mag_id={1833493271},journal={null},abstract={We propose an extension of the Finite State Machine framework in distributed systems, using input/output partial order automata (IOPOA). In this model, transitions can be executed non-atomically, reacting to asynchronous inputs on several ports, and producing asynchronous output on those ports. We develop the formal framework for distributed testing in this architecture and compare with the synchronous I/O automaton setting. The advantage of the compact modelling by IOPOA combines with low complexity : the number of tests required for concurrent input in our model is polynomial in the number of inputs.}}
@ARTICLE{Holzer_2008,title={Query-Driven Program Testing},year={2008},author={Andreas Holzer and Andreas Holzer and Christian Schallhart and Christian Schallhart and Michael Tautschnig and Michael Tautschnig and Helmut Veith and Helmut Veith},doi={10.1007/978-3-540-93900-9_15},pmid={null},pmcid={null},mag_id={1841818470},journal={null},abstract={We present a new approach to program testing which enables the programmer to specify test suites in terms of a versatile query language. Our query language subsumes standard coverage criteria ranging from simple basic block coverage all the way to predicate complete coverage and multiple condition coverage, but also facilitates on-the-fly requests for test suites specific to the code structure, to external requirements, or to ad hoc needs arising in program understanding/exploration. The query language is supported by a model checking backend which employs the CBMC framework. Our main algorithmic contribution is a method called iterative constraint strengthening which enables us to solve a query for an arbitrary coverage criterion by a single call to the model checker and a novel form of incremental SAT solving: Whenever the SAT solver finds a solution, our algorithm compares this solution against the coverage criterion, and strengthens the clause database with additional clauses which exclude redundant new solutions. We demonstrate the scalability of our approach and its ability to compute compact test suites with experiments involving device drivers, automotive controllers, and open source projects.}}
@ARTICLE{Kelly_1996,title={MetaEdit+: A Fully Configurable Multi-User and Multi-Tool CASE and CAME Environment},year={1996},author={Steven Kelly and Steven Kelly and Kalle Lyytinen and Kalle Lyytinen and Matti Rossi and Matti Rossi},doi={10.1007/3-540-61292-0_1},pmid={null},pmcid={null},mag_id={1851467418},journal={null},abstract={Computer Aided Software Engineering (CASE) environments have spread at a lower pace than expected. One reason for this is the immaturity of existing environments in supporting development in-the-large and by-many and their inability to address the varying needs of the software developers. In this paper we report on the development of a next generation CASE environment called MetaEdit+. The environment seeks to overcome all the above deficiencies, but in particular pays attention to catering for the varying needs of the software developers. MetaEdit+ is a multi-method, multi-tool platform for both CASE and Computer Aided Method Engineering (CAME). As a CASE tool it establishes a versatile and powerful multi-tool environment which enables flexible creation, maintenance, manipulation, retrieval and representation of design information among multiple developers. As a CAME environment it offers an easy-to-use yet powerful environment for method specification, integration, management and re-use. The paper explains the motivation for developing MetaEdit+, its design goals and philosophy and discusses the functionality of the CAME tools.}}
@ARTICLE{Hessel_2003,title={Time-Optimal Test Cases for Real-Time Systems},year={2003},author={Anders Hessel and Anders Hessel and Kim Guldstrand Larsen and Kim Guldstrand Larsen and Brian Nielsen and Brian Nielsen and Paul Pettersson and Paul Pettersson and Arne Skou and Arne Skou},doi={10.1007/978-3-540-40903-8_19},pmid={null},pmcid={null},mag_id={1880615699},journal={null},abstract={Testing is the primary software validation technique used by industry today, but remains ad hoc, error prone, and very expensive. A promising improvement is to automatically generate test cases from formal models of the system under test.}}
@ARTICLE{Bhateja_2007,title={Local testing of message sequence charts is difficult},year={2007},author={Puneet Bhateja and Puneet Bhateja and Paul Gastin and Paul Gastin and Madhavan Mukund and Madhavan Mukund and K. Narayan Kumar and K. Narayan Kumar},doi={10.1007/978-3-540-74240-1_8},pmid={null},pmcid={null},mag_id={1890028643},journal={null},abstract={Message sequence charts are an attractive formalism for specifying communicating systems. One way to test such a system is to substitute a component by a test process and observe its interaction with the rest of the system. Unfortunately, local observations can combine in unexpected ways to define implied scenarios not present in the original specification. Checking whether a scenario specification is closed with respect to implied scenarios is known to be undecidable when observations are made one process at a time. We show that even if we strengthen the observer to be able to observe multiple processes simultaneously, the problem remains undecidable. In fact, undecidability continues to hold even without message labels, provided we observe two or more processes simultaneously. On the other hand, without message labels, if we observe one process at a time, checking for implied scenarios is decidable.}}
@ARTICLE{Aarts_2012,title={Automata Learning through Counterexample Guided Abstraction Refinement},year={2012},author={Fides Aarts and Fides Aarts and Faranak Heidarian and Faranak Heidarian and Harco Kuppens and Harco Kuppens and Petur Olsen and Petur Olsen and Frits Vaandrager and Frits W. Vaandrager},doi={10.1007/978-3-642-32759-9_4},pmid={null},pmcid={null},mag_id={1893208258},journal={null},abstract={Abstraction is the key when learning behavioral models of realistic systems. Hence, in most practical applications where automata learning is used to construct models of software components, researchers manually define abstractions which, depending on the history, map a large set of concrete events to a small set of abstract events that can be handled by automata learning tools. In this article, we show how such abstractions can be constructed fully automatically for a restricted class of extended finite state machines in which one can test for equality of data parameters, but no operations on data are allowed. Our approach uses counterexample-guided abstraction refinement: whenever the current abstraction is too coarse and induces nondeterministic behavior, the abstraction is refined automatically. Using Tomte, a prototype tool implementing our algorithm, we have succeeded to learn – fully automatically – models of several realistic software components, including the biometric passport and the SIP protocol.}}
@ARTICLE{Esparza_1996,title={An Improvement of McMillan's Unfolding Algorithm},year={1996},author={Javier Esparza and Javier Esparza and Stefan Römer and Stefan Römer and Walter Vogler and Walter Vogler},doi={10.1007/3-540-61042-1_40},pmid={null},pmcid={null},mag_id={1937565299},journal={null},abstract={McMillan has recently proposed a new technique to avoid the state explosion problem in the verification of systems modelled with finite-state Petri nets. The technique requires to construct a finite initial part of the unfolding of the net. McMillan's algorithm for this task may yield initial parts that are larger than necessary (exponentially larger in the worst case). We present a refinement of the algorithm which overcomes this problem.}}
@ARTICLE{Schieferdecker_2003,title={The UML 2.0 testing profile and its relation to TTCN-3},year={2003},author={Ina Schieferdecker and Ina Schieferdecker and Zhen Ru Dai and Zhen Ru Dai and Jens Grabowski and Jens Grabowski and Axel Rennoch and Axel Rennoch},doi={10.1007/3-540-44830-6_7},pmid={null},pmcid={null},mag_id={1941555554},journal={Lecture Notes in Computer Science},abstract={UML models focus primarily on the definition of system structure and behaviour, but provide only limited means for describing test objectives and test procedures. However, with the approach towards system engineering with automated code generation, the need for solid conformance testing has increased. In June 2001, an OMG Request For Proposal (RFP) on an UML2.0 Testing Profile (UTP) has been initiated. This RFP solicits proposals for a UML2.0 profile, which enables the specification of tests for structural and behavioural aspects of computational UML models, and which is capable to interoperate with existing test technologies for black box testing. This paper discusses different approaches for testing with UML and discusses the ongoing work of the Testing Profile. Special emphasize is laid on the mapping of UML2.0 testing concepts to the standardized Testing and Test Control Notation (TTCN-3).}}
@ARTICLE{Holzmann_2011,title={The SPIN Model Checker: Primer and Reference Manual},year={2011},author={Gerard J. Holzmann and Gerard J. Holzmann},doi={null},pmid={null},pmcid={null},mag_id={1943502734},journal={null},abstract={The SPIN Model Checker is used for both teaching software verification techniques, and for validating large scale applications. The growing number of users has created a need for a more comprehensive user guide and a standard reference manual that describes the most recent version of the tool. This book fills that need. SPIN is used in over 40 countries. The offical SPIN web site, spinroot.com receives between 2500 and 3000 hits per day. It has been estimated that up to three-quarters of the $400 billion spent annually to hire programmers in the United States is ultimately spent on debugging}}
@ARTICLE{Fabbri_1999,title={Mutation testing applied to validate specifications based on statecharts},year={1999},author={Sandra Fabbri and Sandra Fabbri and José Carlos Maldonado and José Carlos Maldonado and Tatiana Sugeta and Tatiana Sugeta and Paulo César Masiero and Paulo Cesar Masiero},doi={10.1109/issre.1999.809326},pmid={null},pmcid={null},mag_id={1943874188},journal={null},abstract={The establishment of a low-cost, effective testing and validation strategy has been pursued by many researchers at the program level as well as at the specification level. The application of mutation testing for validating specifications based on statecharts is proposed. A mutation operator set for statecharts, one of the crucial points for effectively applying mutation testing is defined; in this scope these operators can be taken as a fault model. We also provide strategies to abstract the statechart components according to different statechart features that may comprise the testing and validation activity aims, providing in this way mechanisms for the establishment of an incremental, hierarchical, mutation-based testing strategy. Implementation and functional aspects of PROTEUM/ST, a tool under development are also presented.}}
@ARTICLE{McMillan_1995,title={A technique of state space search based on unfolding},year={1995},author={Kenneth L. McMillan and Kenneth L. McMillan},doi={10.1007/bf01384314},pmid={null},pmcid={null},mag_id={1965237160},journal={null},abstract={Unfoldings of Petri nets provide a method of searching the state space of concurrent systems without considering all possible interleavings of concurrent events. A procedure is given for constructing the unfolding of a Petri net, terminating the construction when it is sufficient to represent all reachable markings. This procedure is applied to hazard and deadlock detection in asynchronous circuits. Examples are given of scalable systems with exponential size state spaces, but polynomial size unfoldings, including a distributed mutual exclusion ring circuit.}}
@ARTICLE{Petriu_2007,title={An intermediate metamodel with scenarios and resources for generating performance models from UML designs},year={2007},author={Dorina C. Petriu and Dorin B. Petriu and C. Murray Woodside and C. Murray Woodside},doi={10.1007/s10270-006-0026-8},pmid={null},pmcid={null},mag_id={1973322846},journal={Software and Systems Modeling},abstract={Performance analysis of a software specification in a language such as UML can assist a design team in evaluating performance-sensitive design decisions and in making design trade-offs that involve performance. Annotations to the design based on the UML Profile for Schedulability, Performance and Time provide necessary information such as workload parameters for a performance model, and many different kinds of performance techniques can be applied. The Core Scenario Model (CSM) described here provides a metamodel for an intermediate form which correlates multiple UML diagrams, extracts the behaviour elements with the performance annotations, attaches important resource information that is obtained from the UML, and supports the creation of many different kinds of performance models. Models can be made using queueing networks, layered queues, timed Petri nets, and it is proposed to develop the CSM as an intermediate language for all performance formalisms. This paper defines the CSM and describes how it resolves questions that arise in performance model-building.}}
@ARTICLE{Mandrioli_1995,title={Generating test cases for real-time systems from logic specifications},year={1995},author={Dino Mandrioli and Dino Mandrioli and Sandro Morasca and Sandro Morasca and Angelo Morzenti and Angelo Morzenti},doi={10.1145/210223.210226},pmid={null},pmcid={null},mag_id={1979248560},journal={ACM Transactions on Computer Systems},abstract={We address the problem of automated derivation of functional test cases for real-time systems, by introducing techniques for generating test cases from formal specifications written in TRIO, a language that extends classical temporal logic to deal explicitly with time measures. We describe an interactive tool that has been built to implement these techniques, based on interpretation algorithms of the TRIO language. Several heuristic criteria are suggested to reduce drastically the size of the test cases that are generated. Experience in the use of the tool on real-life cases is reported.}}
@ARTICLE{Hierons_2014,title={Implementation Relations for the Distributed Test Architecture},year={2014},author={Robert M. Hierons and Robert M. Hierons and Mercedes G. Merayo and Mercedes G. Merayo and Manuel Núñez and Manuel Núñez},doi={10.1007/s00446-014-0208-5},pmid={null},pmcid={null},mag_id={1981352659},journal={Distributed Computing},abstract={In order to test systems that have physically distributed interfaces, called ports, we might use a distributed approach in which there is a separate tester at each port. If the testers do not synchronise during testing then we cannot always determine the relative order of events observed at different ports and this leads to new notions of correctness that have been described using corresponding implementation relations. We study the situation in which each tester has a local clock and timestamps its observations. If we know nothing about how the local clocks relate then this does not affect the implementation relation while if the local clocks agree exactly then we can reconstruct the sequence of observations made. In practice, however, we are likely to be between these extremes: the local clocks will not agree exactly but we have some information regarding how they can differ. We start by assuming that a local tester interacts synchronously with the corresponding port of the system under test and then extend this to the case where communications can be asynchronous, considering both the first-in-first-out (FIFO) case and the non-FIFO case. The new implementation relations are stronger than implementation relations for distributed testing that do not use timestamps but still reflect the distributed nature of observations. This paper explores these alternatives and derives corresponding implementation relations.}}
@ARTICLE{Merayo_2008,title={Formal testing from timed finite state machines},year={2008},author={Mercedes G. Merayo and Mercedes G. Merayo and Manuel Núñez and Manuel Núñez and Ismael Rodrı́guez and Ismael Rodríguez},doi={10.1016/j.comnet.2007.10.002},pmid={null},pmcid={null},mag_id={1990687584},journal={Computer Networks},abstract={null}}
@ARTICLE{Cardell-Oliver_2000,title={Conformance tests for real-time systems with timed automata specifications},year={2000},author={Rachel Cardell-Oliver and Rachel Cardell-Oliver},doi={10.1007/s001650070009},pmid={null},pmcid={null},mag_id={1991978415},journal={Formal Aspects of Computing},abstract={A method is introduced for testing the conformance of implemented real-time systems to timed automata specifications. Uppaal timed automata are transformed into testable timed transition systems (TTTSs) using a test view. Fault hypotheses and a test generation algorithm for TTTSs are defined. Results of applying the method are presented.}}
@ARTICLE{En‐Nouaary_2008,title={A scalable method for testing real-time systems},year={2008},author={Abdeslam En‐Nouaary and Abdeslam En-Nouaary},doi={10.1007/s11219-007-9021-8},pmid={null},pmcid={null},mag_id={1992915869},journal={Software Quality Journal},abstract={Real-time systems (RTSs) are used in different domains such as telephone switching systems, air traffic control systems and patient monitoring systems. The behavior of RTSs is time-sensitive; that is, RTSs interact with their environment with input and output events under time constraints. The violation of such time constraints is the main cause of the misbehavior of RTSs, and may result in severe damage to human lives and the environment [Mandrioli, D., Morasca, S., & Morzenti, A. 1995. ACM Transactions on Computer Systems, 13(4), 365---398]. To prevent failures in RTSs, we must verify that the implementation of an RTS is correct before its deployment. Testing is one of the formal techniques that can be used to achieve this goal. It consists of three main phases: test generation, test execution, and test results analysis. This paper presents a test case generation method for RTSs modeled as Timed Input Output Automata (TIOA). The approach is made in two steps. First, the TIOA describing the system being tested is sampled to construct a subautomaton, which is easily testable (i.e., easy to generate test cases from it). Then, the resulting subautomaton is traversed to generate test cases. Our method is scalable in the sense that it generates a small number of test cases even when the specifications are significant. Moreover, the test cases derived by our method are executable (i.e., they can be run on any error-free implementation of the system being tested).}}
@ARTICLE{Larsen_1997,title={UPPAAL in a Nutshell},year={1997},author={Kim Guldstrand Larsen and Kim Guldstrand Larsen and Paul Pettersson and Paul Pettersson and Wang Yi and Wang Yi},doi={10.1007/s100090050010},pmid={null},pmcid={null},mag_id={2000947342},journal={International Journal on Software Tools for Technology Transfer},abstract={This paper presents the overal structure, the design criteria, and the main features of the tool box Uppaal. It gives a detailed user guide which describes how to use the various tools of Uppaal version 2.02 to construct abstract models of a real-time system, to simulate its dynamical behavior, to specify and verify its safety and bounded liveness properties in terms of its model. In addition, the paper also provides a short review on case-studies where Uppaal is applied, as well as references to its theoretical foundation.}}
@ARTICLE{Hennessy_1995,title={A Process Algebra for Timed Systems},year={1995},author={Matthew Hennessy and Matthew Hennessy and Tim Regan and T. Regan},doi={10.1006/inco.1995.1041},pmid={null},pmcid={null},mag_id={2001026449},journal={Information & Computation},abstract={A standard process algebra is extended by a new action ? which is meant to denote idling until the next clock cycle. A semantic theory based on testing is developed for the new language. This is characterised in terms of barbs, a variety of ready traces and also characterised as the initial theory generated by a set of equations.}}
@ARTICLE{Ölveczky_2007,title={Semantics and pragmatics of Real-Time Maude},year={2007},author={Peter Csaba Ölveczky and Peter Csaba Ölveczky and José Meseguer and José Meseguer},doi={10.1007/s10990-007-9001-5},pmid={null},pmcid={null},mag_id={2006340882},journal={null},abstract={At present, designers of real-time systems face a dilemma between expressiveness and automatic verification: if they can specify some aspects of their system in some automaton-based formalism, then automatic verification is possible; but more complex system components may be hard or impossible to express in such decidable formalisms. These more complex components may still be simulated; but there is then little support for their formal analysis. The main goal of Real-Time Maude is to provide a way out of this dilemma, while complementing both decision procedures and simulation tools. Real-Time Maude emphasizes ease and generality of specification, including support for distributed real-time object-based systems. Because of its generality, falling outside of decidable system classes, the formal analyses supported--including symbolic simulation, breadth-first search for failures of safety properties, and model checking of time-bounded temporal logic properties--are in general incomplete (although they are complete for discrete time). These analysis techniques have been shown useful in finding subtle bugs of complex systems, clearly outside the scope of current decision procedures. This paper describes both the semantics of Real-Time Maude specifications, and of the formal analyses supported by the tool. It also explains the tool's pragmatics, both in the use of its features, and in its application to concrete examples.}}
@ARTICLE{Kerkouche_2010,title={A UML and Colored Petri Nets Integrated Modeling and Analysis Approach using Graph Transformation},year={2010},author={Elhillali Kerkouche and Elhillali Kerkouche and Allaoua Chaoui and Algeria Allaoua Chaoui and El-Bay Bourennane and El-Bay Bourennane and Ouassila Labbani and Ouassila Labbani},doi={10.5381/jot.2010.9.4.a2},pmid={null},pmcid={null},mag_id={2009334408},journal={The Journal of Object Technology},abstract={Nowadays, UML is considered to be the standardized language for object-oriented modeling and analysis. However, UML cannot be used for automatic analyses and simulation. In this paper, we propose an approach for transforming UML statechart and collaboration diagrams to Colored Petri net models. This transformation aims to bridge the gap between informal notation (UML diagrams) and more formal notation (Colored Petri net models) for analysis purposes. It produces highly- structured, graphical, and rigorously-analyzable models that facilitate early detection of errors such as deadlock and livelock. The approach is based on graph transformations where the input and output of the transformation process are graphs. The meta-modeling tool AToM3 is used. A case study is presented to illustrate our approach.}}
@ARTICLE{Sen_2005,title={CUTE: a concolic unit testing engine for C},year={2005},author={Koushik Sen and Koushik Sen and Darko Marinov and Darko Marinov and Gul Agha and Gul Agha},doi={10.1145/1081706.1081750},pmid={null},pmcid={null},mag_id={2009489720},journal={null},abstract={In unit testing, a program is decomposed into units which are collections of functions. A part of unit can be tested by generating inputs for a single entry function. The entry function may contain pointer arguments, in which case the inputs to the unit are memory graphs. The paper addresses the problem of automating unit testing with memory graphs as inputs. The approach used builds on previous work combining symbolic and concrete execution, and more specifically, using such a combination to generate test inputs to explore all feasible execution paths. The current work develops a method to represent and track constraints that capture the behavior of a symbolic execution of a unit with memory graphs as inputs. Moreover, an efficient constraint solver is proposed to facilitate incremental generation of such test inputs. Finally, CUTE, a tool implementing the method is described together with the results of applying CUTE to real-world examples of C code.}}
@ARTICLE{Chow_1978,title={Testing Software Design Modeled by Finite-State Machines},year={1978},author={Tsun S. Chow and Tsun S. Chow},doi={10.1109/tse.1978.231496},pmid={null},pmcid={null},mag_id={2011762419},journal={IEEE Transactions on Software Engineering},abstract={We propose a method of testing the correctness of control structures that can be modeled by a finite-state machine. Test results derived from the design are evaluated against the specification. No "executable" prototype is required. The method is based on a result in automata theory and can be applied to software testing. Its error-detecting capability is compared with that of other approaches. Application experience is summarized.}}
@ARTICLE{Ahrendt_2012,title={A system for compositional verification of asynchronous objects},year={2012},author={Wolfgang Ahrendt and Wolfgang Ahrendt and Maximilian Dylla and Maximilian Dylla},doi={10.1016/j.scico.2010.08.003},pmid={null},pmcid={null},mag_id={2016286256},journal={Science of Computer Programming},abstract={We present a semantics, calculus, and system for compositional verification of Creol, an object-oriented modelling language for concurrent distributed applications. The system is an instance of KeY, a framework for object-oriented software verification, which has so far been applied foremost to sequential Java. Building on KeY characteristic concepts, like dynamic logic, sequent calculus, symbolic execution via explicit substitutions, and the taclet rule language, the presented system addresses functional correctness of Creol models featuring local cooperative thread parallelism and global communication via asynchronous method calls. The calculus heavily operates on communication histories specified by the interfaces of Creol units. Two example scenarios demonstrate the usage of the system. This article extends the conference paper of Ahrendt and Dylla (2009) [5] with a denotational semantics of Creol and an assumption-commitment style semantics of the logic.}}
@ARTICLE{Ölveczky_2007,title={Abstraction and Completeness for Real-Time Maude},year={2007},author={Peter Csaba Ölveczky and Peter Csaba Ölveczky and José Meseguer and José Meseguer},doi={10.1016/j.entcs.2007.06.005},pmid={null},pmcid={null},mag_id={2019810307},journal={Electronic Notes in Theoretical Computer Science},abstract={This paper presents criteria that guarantee completeness of Real-Time Maude search and temporal logic model checking analyses, under the maximal time sampling strategy, for a large class of real-time systems. As a special case, we characterize simple conditions for such completeness for object-oriented real-time systems, and show that these conditions can often be easily proved even for large and complex systems, such as advanced wireless sensor network algorithms and active network multicast protocols. Our results provide completeness and decidability of time-bounded search and model checking for a large and useful class of dense-time non-Zeno real-time systems far beyond the class of automaton-based real-time systems for which well known decision procedures exist. For discrete time, our results justify abstractions that can drastically reduce the state space to make search and model checking analyses feasible.}}
@ARTICLE{Haller_2009,title={Scala Actors: Unifying thread-based and event-based programming},year={2009},author={Philipp Haller and Philipp Haller and Martin Odersky and Martin Odersky},doi={10.1016/j.tcs.2008.09.019},pmid={null},pmcid={null},mag_id={2021978684},journal={Theoretical Computer Science},abstract={There is an impedance mismatch between message-passing concurrency and virtual machines, such as the JVM. VMs usually map their threads to heavyweight OS processes. Without a lightweight process abstraction, users are often forced to write parts of concurrent applications in an event-driven style which obscures control flow, and increases the burden on the programmer. In this paper we show how thread-based and event-based programming can be unified under a single actor abstraction. Using advanced abstraction mechanisms of the Scala programming language, we implement our approach on unmodified JVMs. Our programming model integrates well with the threading model of the underlying VM.}}
@ARTICLE{Pnueli_1977,title={The temporal logic of programs},year={1977},author={Amir Pnueli and Amir Pnueli},doi={10.1109/sfcs.1977.32},pmid={null},pmcid={null},mag_id={2023808162},journal={null},abstract={A unified approach to program verification is suggested, which applies to both sequential and parallel programs. The main proof method suggested is that of temporal reasoning in which the time dependence of events is the basic concept. Two formal systems are presented for providing a basis for temporal reasoning. One forms a formalization of the method of intermittent assertions, while the other is an adaptation of the tense logic system Kb, and is particularly suitable for reasoning about concurrent programs.}}
@ARTICLE{Johnsen_2007,title={An Asynchronous Communication Model for Distributed Concurrent Objects},year={2007},author={Einar Broch Johnsen and Einar Broch Johnsen and Olaf Owe and Olaf Owe},doi={10.1007/s10270-006-0011-2},pmid={null},pmcid={null},mag_id={2033745581},journal={Software and Systems Modeling},abstract={Distributed systems are often modeled by objects that run concurrently, each with its own processor, and communicate by synchronous remote method calls. This may be satisfactory for tightly coupled systems, but in the distributed setting synchronous external calls lead to much waiting; at best resulting in inefficient use of processor capacity, at worst resulting in deadlock. Furthermore, it is difficult to combine active and passive behavior in concurrent objects. This paper proposes an object-oriented solution to these problems by means of asynchronous method calls and conditional processor release points. Although at the cost of additional internal nondeterminism in the objects, this approach seems attractive in asynchronous or unreliable environments. The concepts are integrated in a small object-oriented language with an operational semantics defined in rewriting logic, and illustrated by examples.}}
@ARTICLE{Saaty_2008,title={DECISION MAKING WITH THE ANALYTIC HIERARCHY PROCESS},year={2008},author={Thomas L. Saaty and Thomas L. Saaty},doi={10.1504/ijssci.2008.017590},pmid={null},pmcid={null},mag_id={2034960640},journal={International Journal of Services Sciences},abstract={Decisions involve many intangibles that need to be traded off. To do that, they have to be measured along side tangibles whose measurements must also be evaluated as to, how well, they serve the objectives of the decision maker. The Analytic Hierarchy Process (AHP) is a theory of measurement through pairwise comparisons and relies on the judgements of experts to derive priority scales. It is these scales that measure intangibles in relative terms. The comparisons are made using a scale of absolute judgements that represents, how much more, one element dominates another with respect to a given attribute. The judgements may be inconsistent, and how to measure inconsistency and improve the judgements, when possible to obtain better consistency is a concern of the AHP. The derived priority scales are synthesised by multiplying them by the priority of their parent nodes and adding for all such nodes. An illustration is included.}}
@ARTICLE{Fersman_2007,title={Task automata: Schedulability, decidability and undecidability},year={2007},author={Elena Fersman and Elena Fersman and Pavel Krčál and Pavel Krcal and Paul Pettersson and Paul Pettersson and Wang Yi and Wang Yi},doi={10.1016/j.ic.2007.01.009},pmid={null},pmcid={null},mag_id={2036081705},journal={Information & Computation},abstract={We present a model, task automata, for real time systems with non-uniformly recurring computation tasks. It is an extended version of timed automata with asynchronous processes that are computation tasks generated (or triggered) by timed events. Compared with classical task models for real time systems, task automata may be used to describe tasks (1) that are generated non-deterministically according to timing constraints in timed automata, (2) that may have interval execution times representing the best case and the worst case execution times, and (3) whose completion times may influence the releases of task instances. We generalize the classical notion of schedulability to task automata. A task automaton is schedulable if there exists a scheduling strategy such that all possible sequences of events generated by the automaton are schedulable in the sense that all associated tasks can be computed within their deadlines. Our first technical result is that the schedulability for a given scheduling strategy can be checked algorithmically for the class of task automata when the best case and the worst case execution times of tasks are equal. The proof is based on a decidable class of suspension automata: timed automata with bounded subtraction in which clocks may be updated by subtractions within a bounded zone. We shall also study the borderline between decidable and undecidable cases. Our second technical result shows that the schedulability checking problem will be undecidable if the following three conditions hold: (1) the execution times of tasks are intervals, (2) the precise finishing time of a task instance may influence new task releases, and (3) a task is allowed to preempt another running task.}}
@ARTICLE{Wang_2009,title={Fault masking by multiple timing faults in timed EFSM models},year={2009},author={Y. Wang and Yu Wang and M. ímit Uyar and M. í. Uyar and Samrat S. Batth and S.S. Batth and Mariusz A. Fecko and Mariusz A. Fecko},doi={10.1016/j.comnet.2008.10.025},pmid={null},pmcid={null},mag_id={2036746438},journal={Computer Networks},abstract={null}}
@ARTICLE{Hierons_2009,title={Testing from a stochastic timed system with a fault model.},year={2009},author={Robert M. Hierons and Robert M. Hierons and Mercedes G. Merayo and Mercedes G. Merayo and Manuel Núñez and Manuel Núñez},doi={10.1016/j.jlap.2008.06.001},pmid={null},pmcid={null},mag_id={2055530557},journal={The Journal of Logic and Algebraic Programming},abstract={In this paper we present a method for testing a system against a non-deterministic stochastic finite state machine. As usual, we assume that the functional behaviour of the system under test (SUT) is deterministic but we allow the timing to be non-deterministic. We extend the state counting method of deriving tests, adapting it to the presence of temporal requirements represented by means of random variables. The notion of conformance is introduced using an implementation relation considering temporal aspects and the limitations imposed by a black-box framework. We propose a new group of implementation relations and an algorithm for generating a test suite that determines the conformance of a deterministic SUT with respect to a non-deterministic specification. We show how previous work on testing from stochastic systems can be encoded into the framework presented in this paper as an instantiation of our parameterized implementation relation. In this setting, we use a notion of conformance up to a given confidence level.}}
@ARTICLE{Berthomieu_2004,title={The tool TINA – Construction of abstract state spaces for petri nets and time petri nets},year={2004},author={Bernard Berthomieu and Bernard Berthomieu and Pierre-Olivier Ribet and P.-O. Ribet and François Vernadat and François Vernadat and F. Vernadat and François Vernadat},doi={10.1080/00207540412331312688},pmid={null},pmcid={null},mag_id={2070996046},journal={International Journal of Production Research},abstract={In addition to the graphic-editing facilities, the software tool Tina proposes the construction of a number of representations for the behaviour of Petri nets or Time Petri nets. Various techniques are used to extract views of the behaviour of nets, preserving certain classes of properties of their state spaces. For Petri nets, these abstractions help prevent combinatorial explosion, relying on so-called partial order techniques such as covering steps and/or persistent sets. For Time Petri nets, which have, in general, infinite state spaces, they provide a finite symbolic representation of their behaviour in terms of state classes.}}
@ARTICLE{Agha_1986,title={Actors: A Model of Concurrent Computation in Distributed Systems},year={1986},author={Gul Agha and Gul Agha},doi={null},pmid={null},pmcid={null},mag_id={2072794470},journal={null},abstract={Abstract : A foundational model of concurrency is developed in this thesis. It examines issues in the design of parallel systems and show why the actor model is suitable for exploiting large-scale parallelism. Concurrency in actors is constrained only by the availability of hardware resources and by the logical dependence inherent in the computation. Unlike dataflow and functional programming, however, actors are dynamically reconfigurable and can model shared resources with changing local state. Concurrency is spawned in actors using asynchronous message-passing, pipelining, and the dynamic creation of actors. The author defines an abstract actor machine and provide a minimal programming language for it. A more expressive language, which includes higher level constructs such as delayed and eager evaluation, can be defined in terms of the primitives. Examples are given to illustrate the ease with which concurrent data and control structures can be programmed. This thesis deals with some central issues in distributed computing. Specifically, problems of divergence and deadlock are addressed. Additional keywords: Object oriented programming; Semantics.}}
@ARTICLE{Jard_2003,title={Synthesis of distributed testers from true-concurrency models of reactive systems},year={2003},author={Claude Jard and Claude Jard},doi={10.1016/s0950-5849(03)00061-2},pmid={null},pmcid={null},mag_id={2085074430},journal={Information & Software Technology},abstract={Abstract   Automatic synthesis of test cases for conformance testing has been principally developed with the objective of generating sequential test cases. In the distributed system context, it is worth extending the synthesis techniques to the generation of multiple testers. We base our work on our experience in using model-checking techniques, as successfully implemented in the Test Generation using the Verification tool. Continuing the works of Ulrich and Konig, we propose to use a true-concurrency model based on graph unfolding. The article presents the principles of a complete chain of synthesis, starting from the definition of test purposes and ending with a projection onto a set of testers.}}
@ARTICLE{Godefroid_2005,title={DART: directed automated random testing},year={2005},author={Patrice Godefroid and Patrice Godefroid and Nils Klarlund and Nils Klarlund and Koushik Sen and Koushik Sen},doi={10.1145/1065010.1065036},pmid={null},pmcid={null},mag_id={2096449544},journal={null},abstract={We present a new tool, named DART, for automatically testing software that combines three main techniques: (1) automated extraction of the interface of a program with its external environment using static source-code parsing; (2) automatic generation of a test driver for this interface that performs random testing to simulate the most general environment the program can operate in; and (3) dynamic analysis of how the program behaves under random testing and automatic generation of new test inputs to direct systematically the execution along alternative program paths. Together, these three techniques constitute Directed Automated Random Testing, or DART for short. The main strength of DART is thus that testing can be performed completely automatically on any program that compiles -- there is no need to write any test driver or harness code. During testing, DART detects standard errors such as program crashes, assertion violations, and non-termination. Preliminary experiments to unit test several examples of C programs are very encouraging.}}
@ARTICLE{Boer_2007,title={A complete guide to the future},year={2007},author={Frank S. de Boer and Frank S. de Boer and Dave Clarke and Dave Clarke and Dave Clarke and Dave Clarke and Einar Broch Johnsen and Einar Broch Johnsen},doi={10.1007/978-3-540-71316-6_22},pmid={null},pmcid={null},mag_id={2098400709},journal={null},abstract={We present the semantics and proof system for an object-oriented language with active objects, asynchronous method calls, and futures. The language, based on Creol, distinguishes itself in that unlike active object models, it permits more than one thread of control within an object, though, unlike Java, only one thread can be active within an object at a given time and rescheduling occurs only at specific release points. Consequently, reestablishing an object's monitor invariant is possible at specific well-defined points in the code. The resulting proof system shows that this approach to concurrency is simpler for reasoning than, say, Java's multithreaded concurrency model. From a methodological perspective, we identify constructs which admit a simple proof system and those which require, for example, interference freedom tests.}}
@ARTICLE{Ruf_2001,title={The simulation semantics of systemC},year={2001},author={Jürgen Ruf and Jürgen Ruf and Dirk W. Hoffmann and D.W. Hoffmann and Joachim Gerlach and Joachim Gerlach and Thomas Kropf and Thomas Kropf and W. Rosenstiehl and W. Rosenstiehl and Wolfgang Mueller and Wolfgang Mueller},doi={10.1109/date.2001.915002},pmid={null},pmcid={null},mag_id={2099740052},journal={null},abstract={We present a rigorous but transparent semantics definition of SystemC that covers method, thread, and clocked thread behavior as well as their interaction with the simulation kernel process. The semantics includes watching statements, signal assignment, and wait statements as they are introduced in SystemC V1.O. We present our definition in form of distributed Abstract State Machines (ASMs) rules reflecting the view given in the SystemC User's Manual and the reference implementation. We mainly see our formal semantics as a concise, unambiguous, high-level specification for SystemC-based implementations and for standardization. Additionally, it can be used as a sound basis to investigate SystemC interoperability with Verilog and VHDL.}}
@ARTICLE{Habibi_2006,title={Generating Finite State Machines from System C},year={2006},author={Ali Habibi and A. Habibi and Ali Habibi and A. Habibi and Ali Habibi and H. Moinudeen and H. Moinudeen and Sofiène Tahar and Sofiène Tahar},doi={10.1109/date.2006.243777},pmid={null},pmcid={null},mag_id={2100889455},journal={null},abstract={SystemC is a system level language proposed to raise the abstraction level for embedded systems design and verification. In this paper, we propose to generate Finite State Machines (FSM) from SystemC designs using two algorithms original lyproposed for the generation of FSM from Abstract State Machines (ASM). This proposal enables the integration of SystemC with existing tools for test case generation from FSM. Hence, enabling two important applications: (1) using the FSM graph structure to produce test suites allowing functional testing of SystemC designs; and (2) performing conformance testing, where the FSM serves as a precise model of the observable behavior of the system used to validate lower abstraction levels of the design (e.g., Register Transfer Level (RTL)).}}
@ARTICLE{En‐Nouaary_2002,title={Timed Wp-method: testing real-time systems},year={2002},author={Abdeslam En‐Nouaary and Abdeslam En-Nouaary and Rachida Dssouli and Rachida Dssouli and Ferhat Khendek and Ferhat Khendek},doi={10.1109/tse.2002.1049402},pmid={null},pmcid={null},mag_id={2102418068},journal={IEEE Transactions on Software Engineering},abstract={Real-time systems interact with their environment using time constrained input/output signals. Examples of real-time systems include patient monitoring systems, air traffic control systems, and telecommunication systems. For such systems, a functional misbehavior or a deviation from the specified time constraints may have catastrophic consequences. Therefore, ensuring the correctness of real-time systems becomes necessary. Two different techniques are usually used to cope with the correctness of a software system prior to its deployment, namely, verification and testing. In this paper, we address the issue of testing real-time software systems specified as a timed input output automaton (TIOA). TIOA is a variant of timed automaton. We introduce the syntax and semantics of TIOA. We present the potential faults that can be encountered in a timed system implementation. We study these different faults based on TIOA model and look at their effects on the execution of the system using the region graph. We present a method for generating timed test cases. This method is based on a state characterization technique and consists of the following three steps: First, we sample the region graph using a suitable granularity, in order to construct a subautomaton easily testable, called grid automaton. Then, we transform the grid automaton into a nondeterministic timed finite state machine (NTFSM). Finally, we adapt the generalized Wp-method to generate timed test cases from NTFSM. We assess the fault coverage of our test cases generation method and prove its ability to detect all the possible faults. Throughout the paper, we use examples to illustrate the various concepts and techniques used in our approach.}}
@ARTICLE{AbouTrab_2010,title={Fault Coverage Measurement of a Timed Test Case Generation Approach},year={2010},author={Mohammad Saeed AbouTrab and M. S. AbouTrab and Steve Counsell and Steve Counsell},doi={10.1109/ecbs.2010.22},pmid={null},pmcid={null},mag_id={2107099386},journal={null},abstract={Ensuring that a Real-Time Embedded System(RTES) is absent of major faults that may affect the way it performs is a non-trivial task. RTES behaviour is based on the interactions with its surrounding environment and on the timing characteristics of that same environment. As a result, time poses a new dimension to the complexity of the testing process. In previous research, we introduced a ‘priority-based’ approach which tested the logical and timing behaviour of an RTES modeled formally as Uppaal automata. The ‘priority-based’ approach was based on producing sets of timed test traces by achieving timing constraints coverage according to three sets of priorities, namely boundary, out-boundary and in-boundary. In this paper, we extend that work by validating the ‘priority-based’ approach according to a well-known timed fault model. The validation process shows promising results, notably, that the ‘priority-based’ approach is capable of detecting all the fault types included in the proposed fault model.}}
@ARTICLE{Karsai_2003,title={Graph Transformations in OMG’s Model-Driven Architecture},year={2003},author={Gábor Karsai and Gabor Karsai and Aditya Agrawal and Aditya Agrawal},doi={10.1007/978-3-540-25959-6_18},pmid={null},pmcid={null},mag_id={2107605246},journal={null},abstract={The Model-Driven Architecture (MDA) vision of the Object Management Group offers a unique opportunity for introducing Graph Transformation (GT) technology to the software industry. The paper proposes a domain-specific refinement of MDA, and describes a practical manifestation of MDA called Model-Integrated Computing (MIC). MIC extends MDA towards domain-specific modeling languages, and it is well supported by various generic tools that include model transformation tools based on graph transformations. The MIC tools are metaprogrammable, i.e. they can be tailored for specific domains using metamodels that include metamodels of transformations. The paper describes the development process and the supporting tools of MIC, and it raises a number of issues for future research on GT in MDA.}}
@ARTICLE{Briones_2007,title={Theories for model-based testing: real-time and coverage},year={2007},author={Laura Brandán Briones and L. Brandan Briones and Laura Brandan Briones},doi={null},pmid={null},pmcid={null},mag_id={2108176668},journal={null},abstract={In the last years, increasingly complex systems are being put in charge of critical tasks. When these complex systems, are drive by sophisticated software, they need to attain a high degree of reliability. Unfortunately, developing correct systems is difficult, and in the past there have been several complex systems that went wrong because they lacked serious analysis of their potential behaviour. In this thesis, we study an effective way of obtaining confidence on the correctness of a system, known as testing. Testing is the systematic process of finding errors in a system by means of extensively experimenting with it. In order to successfully test a system, it is crucially needed to count with both effective test cases and feasible strategies to execute them. Fortunately, work in formal methods helps us achieving this task in a precise and rigorous manner. A particularly successful formal theory of testing is the ioco theory, devised by Tretmans to work on labelled input-output transition systems. The theory smoothly covers issues like nondeterminism and quiescence (that is, the notion representing the absence of outputs). The ioco testing theory is clean and precise, and is the basis used in successful testing tools, like the TORX tool and the TGV tool. In this thesis we extend the ioco testing theory in three important directions, as follows. Our first extension concerns the addition of real-time, which is crucial to the analysis of several systems (e.g., systems where actions are required to occur in a precise moment). New models and formalisms that take into account real-time are introduced. Furthermore, we develop a new testing relation between these real-time models, and a sound and exhaustive algorithm to derive tests for that relation.}}
@ARTICLE{Welc_2005,title={Safe futures for Java},year={2005},author={Adam Welc and Adam Welc and Adam Welc and Suresh Jagannathan and Suresh Jagannathan and Antony L. Hosking and Antony L. Hosking},doi={10.1145/1094811.1094845},pmid={null},pmcid={null},mag_id={2111946621},journal={null},abstract={A future is a simple and elegant abstraction that allows concurrency to be expressed often through a relatively small rewrite of a sequential program. In the absence of side-effects, futures serve as benign annotations that mark potentially concurrent regions of code. Unfortunately, when computation relies heavily on mutation as is the case in Java, its meaning is less clear, and much of its intended simplicity lost.This paper explores the definition and implementation of safe futures for Java. One can think of safe futures as truly transparent annotations on method calls, which designate opportunities for concurrency. Serial programs can be made concurrent simply by replacing standard method calls with future invocations. Most significantly, even though some parts of the program are executed concurrently and may indeed operate on shared data, the semblance of serial execution is nonetheless preserved. Thus, program reasoning is simplified since data dependencies present in a sequential program are not violated in a version augmented with safe futures.Besides presenting a programming model and API for safe futures, we formalize the safety conditions that must be satisfied to ensure equivalence between a sequential Java program and its future-annotated counterpart. A detailed implementation study is also provided. Our implementation exploits techniques such as object versioning and task revocation to guarantee necessary safety conditions. We also present an extensive experimental evaluation of our implementation to quantify overheads and limitations. Our experiments indicate that for programs with modest mutation rates on shared data, applications can use futures to profitably exploit parallelism, without sacrificing safety.}}
@ARTICLE{Fidge_1988,title={Timestamps in Message-Passing Systems That Preserve the Partial Ordering},year={1988},author={Colin J. Fidge},doi={null},pmid={null},pmcid={null},mag_id={2116746874},journal={null},abstract={Timestamping is a common method of totally ordering events in concurrent programs. However, for applications requiring access to the global state, a total ordering is inappro priate. This paper presents algorithms for timestamping events in both synchronous and asynchronous n1essage-passing programs that allow for access to the partial ordering in herent in a parallel system. The algorithms do not change the con1munications graph or require a central timestamp issuing authority.}}
@ARTICLE{Groβe_2006,title={HW/SW co-verification of embedded systems using bounded model checking},year={2006},author={Daniel Groβe and Daniel Groβe and Ulrich Kühne and Ulrich Kühne and Rolf Drechsler and Rolf Drechsler},doi={10.1145/1127908.1127920},pmid={null},pmcid={null},mag_id={2117958841},journal={null},abstract={Today, the underlying hardware of embedded systems is often verified successfully. In this context formal verification techniques allow to prove the functional correctness. But in embedded system design the integration of software components becomes more and more important. In this paper we present an integrated approach for formal verification of hardware and software. The approach is demonstrated on a RISC CPU. The verification is based on bounded model checking. Besides correctness proofs of the underlying hardware the hardware/software interface and programs using this interface can be formally verified.}}
@ARTICLE{Nicola_1987,title={Extensional equivalence for transition systems},year={1987},author={Rocco De Nicola and R De Nicola},doi={10.1007/bf00264365},pmid={null},pmcid={null},mag_id={2118954772},journal={Acta Informatica},abstract={Various notions of systems equivalence based on the reactions of systems to stimuli from the outside world are presented and compared. These notions have been proposed in the literature to allow abstraction from unwanted details in models of concurrent and communicating systems. The equivalences, already defined for different theories of concurrency, will be compared by adapting their definitions to labelled transition systems, a model which underlies many others. In the presentation of each equivalence, the aspects of system behaviours which are ignored and the identifications which are forced will be stressed. It will be shown that many equivalences, although defined very differently by following different intuitions about systems behaviour, turn out to be the same or to differ only in minor detail for a large class of transition systems.}}
@ARTICLE{Laurençot_1997,title={Integration of time in canonical testers for real-time systems},year={1997},author={Patrice Laurençot and P. Laurencot and Richard Castanet and R. Castanet},doi={10.1109/words.1997.609956},pmid={null},pmcid={null},mag_id={2119236084},journal={null},abstract={Testing a real-time system is not an easy thing, because the notion of time is not under control. In this paper, we present different formal models of specification that introduce time. We use one of these, the extended-time input-output state machine, to develop an algorithm to create a canonical tester which is used afterwards for producing test sequences with time.}}
@ARTICLE{Herber_2008,title={Model checking SystemC designs using timed automata},year={2008},author={Paula Herber and Paula Herber and Joachim Fellmuth and Joachim Fellmuth and Sabine Glesner and Sabine Glesner},doi={10.1145/1450135.1450166},pmid={null},pmcid={null},mag_id={2119925465},journal={null},abstract={SystemC is widely used for modeling and simulation in hardware/software co-design. Due to the lack of a complete formal semantics, it is not possible to verify SystemC designs. In this paper, we present an approach to overcome this problem by defining the semantics of SystemC by a mapping from SystemC designs into the well-defined semantics of Uppaal timed automata. The informally defined behavior and the structure of SystemC designs are completely preserved in the generated Uppaal models. The resulting Uppaal models allow us to use the Uppaal model checker and the Uppaal tool suite, including simulation and visualization tools. The model checker can be used to verify important properties such as liveness, deadlock freedom or compliance with timing constraints. We have implemented the presented transformation, applied it to two examples and verified liveness, safety and timing properties by model checking, thus showing the applicability of our approach in practice.}}
@ARTICLE{Bhateja_2008,title={Tagging Make Local Testing of Message-Passing Systems Feasible},year={2008},author={Puneet Bhateja and Puneet Bhateja and Madhavan Mukund and Madhavan Mukund},doi={10.1109/sefm.2008.33},pmid={null},pmcid={null},mag_id={2121337162},journal={null},abstract={The only practical way to test distributed message-passing systems is to use local testing. In this approach, used in formalisms such as concurrent TTCN-3, some components are replaced by test processes. Local testing consists of monitoring the interactions between these test processes and the rest of the system and comparing these observations with the specification, typically described in terms of message sequence charts. The main difficulty with this approach is that local observations can combine in unexpected ways to define implied scenarios not present in the original specification. Checking for implied scenarios is known to be undecidable for regular specifications, even if observations are made for all but one process at a time. We propose an approach where we append tags to the messages generated by the system under test. Our tags are generated in a uniform manner, without referring to or influencing the internal details of the underlying system. These enriched behaviours are then compared against a tagged version of the specification. Our main result is that detecting implied scenarios becomes decidable in the presence of tagging.}}
@ARTICLE{Henniger_1997,title={On test case generation from asynchronously communicating state machines},year={1997},author={Olaf Henniger and O. Henniger},doi={10.1007/978-0-387-35198-8_16},pmid={null},pmcid={null},mag_id={2121703249},journal={null},abstract={This paper proposes an approach for generating test cases in Concurrent TTCN from a system of asynchronously communicating finite state machines. We give an algorithm for generating a noninterleaving model of prime event structures from a generalized model of asynchronously communicating finite state machines and deal with the generation of test cases from prime event structures.}}
@ARTICLE{En‐Nouaary_1999,title={Fault coverage in testing real-time systems},year={1999},author={Abdeslam En‐Nouaary and Abdeslam En-Nouaary and Ferhat Khendek and Ferhat Khendek and Rachida Dssouli and Rachida Dssouli},doi={10.1109/rtcsa.1999.811206},pmid={null},pmcid={null},mag_id={2122260303},journal={null},abstract={Real-time systems interact with their environment, through time constrained input/output events. The misbehavior of real-time systems is generally caused by the violation of the specified time constraints. Validation of real-time system software is an important quality control activity in the software lifecycle. Among the validation processes, testing aims at assessing the conformance of an implementation against the reference specification. One of the important aspects in testing real-time software systems is the fault coverage measurement, which consists of studying the potential faults that can be detected by a test suite generated by a given test generation method. This paper addresses the fault coverage of the Tinted Wp-method we have introduced in (En-Nouaary et al., 1998). We present a timed fault model based on the TIOA model for real-time systems specification. We study the fault coverage of the timed Wp-method with respect to our fault model.}}
@ARTICLE{Clarke_1997,title={Automatic generation of tests for timing constraints from requirements},year={1997},author={David M. Clarke and D. Clarke and I. Lee and Insup Lee},doi={10.1109/words.1997.609955},pmid={null},pmcid={null},mag_id={2126209810},journal={null},abstract={The authors present a framework for testing timing constraints of real-time systems. The tests are automatically derived from specifications of minimum and maximum allowable delays between input/output events in the execution of a system. The test derivation scheme uses a graphical specification formalism for timing constraints, and the real-time process algebra Algebra of Communicating Shared Resources (ACSR) for representing tests and process models. The use of ACSR to describe test sequences has two main advantages. First, tests can be applied to an ACSR model of the software system within the ACSR semantic framework for model validation purposes. Second, ACSR has concise notation and a precise semantics that will facilitate the translation of real-time tests into a software test language for software validation purposes.}}
@ARTICLE{Patel_2008,title={Model-driven validation of SystemC designs},year={2008},author={Hiren Patel and Hiren D. Patel and Sandeep K. Shukla and Sandeep K. Shukla},doi={10.1155/2008/519474},pmid={null},pmcid={null},mag_id={2128892235},journal={Eurasip Journal on Embedded Systems},abstract={Functional test generation for dynamic validation of current system level designs is a challenging task. Manual test writing or automated random test generation techniques are often used for such validation practices. However, directing tests to particular reachable states of a SystemC model is often difficult, especially when these models are large and complex. In this work, we present a model-driven methodology for generating directed tests that take the SystemC model under validation to specific reachable states. This allows the validation to uncover very specific scenarios which lead to different corner cases. Our formal modeling is done entirely within the Microsoft SpecExplorer tool to formally describe the specification of the system under validation in the formal notation of AsmL. We also exploit SpecExplorer's abilities for state space exploration for our test generation, and its APIs for connecting the model to real programs to drive the validation of SystemC models with the generated test cases.}}
@ARTICLE{Clarke_2004,title={A Tool for Checking ANSI-C Programs},year={2004},author={Edmund M. Clarke and Edmund M. Clarke and Daniel Kroening and Daniel Kroening and Flavio Lerda and Flavio Lerda},doi={10.1007/978-3-540-24730-2_15},pmid={null},pmcid={null},mag_id={2129538349},journal={null},abstract={We present a tool for the formal verification of ANSI-C programs using Bounded Model Checking (BMC). The emphasis is on usability: the tool supports almost all ANSI-C language features, including pointer constructs, dynamic memory allocation, recursion, and the float and double data types. From the perspective of the user, the verification is highly automated: the only input required is the BMC bound. The tool is integrated into a graphical user interface. This is essential for presenting long counterexample traces: the tool allows stepping through the trace in the same way a debugger allows stepping through a program.}}
@ARTICLE{Xu_2009,title={Automatic synthesis of computation interference constraints for relative timing verification},year={2009},author={Yang Xu and Yang Xu and Kenneth S. Stevens and Kenneth S. Stevens},doi={10.1109/iccd.2009.5413183},pmid={null},pmcid={null},mag_id={2130552249},journal={null},abstract={Asynchronous sequential circuit or protocol design requires formal verification to ensure correct behavior under all operating conditions. However, most asynchronous circuits or protocols cannot be proven conformant to a specification without adding timing assumptions. Relative Timing (RT) is an approach to model and verify circuits and protocols that require timing assumptions to operate correctly. The process of creating path-based RT constraints has previously been done by hand with the aid of a formal verification engine. This time consuming and error prone method vastly restricts the application of RT and the capability to implement circuits and protocols. This paper describes an algorithm for automatic generation of RT constraints based on signal traces generated from a formal verification (FV) engine that supports relative timing constraints. This algorithm has been implemented in a CAD tool called Automatic Relative Timing Identifier based on Signal Traces (ARTIST) which has been embedded into the FV engine. A set of asynchronous and clocked designs and protocols have been verified and proven to be hazard-free with the RT constraints generated by ARTIST which would have taken months to perform by hand. A comparison of RT constraints between hand-generated and ARTIST generated constraints is also described in terms of efficiency and quality.}}
@ARTICLE{Tenzer_2003,title={Modelling recursive calls with UML state diagrams},year={2003},author={Jennifer Tenzer and Jennifer Tenzer and Perdita Stevens and Perdita Stevens},doi={10.1007/3-540-36578-8_10},pmid={null},pmcid={null},mag_id={2131161195},journal={null},abstract={One of the principal uses of UML is the modelling of synchronous object-oriented software systems, in which the behaviour of each of several classes is modelled using a state diagram. UML permits a transition of the state diagram to show both the event which causes the transition (typically, the fact that the object receives a message) and the object's reaction (typically, the fact that the object sends a message). UML's semantics for state diagrams is "run to completion". We show that this can lead to anomalous behaviour, and in particular that it is not possible to model recursive calls, in which an object receives a second message whilst still in the process of reacting to the first. Drawing on both ongoing work by the UML2.0 submitters and recent theoretical work [1, 6], we propose a solution to this problem using state diagrams in two complementary ways.}}
@ARTICLE{Epifani_2009,title={Model evolution by run-time parameter adaptation},year={2009},author={Ilenia Epifani and Ilenia Epifani and Carlo Ghezzi and Carlo Ghezzi and Raffaela Mirandola and Raffaela Mirandola and Giordano Tamburrelli and Giordano Tamburrelli},doi={10.1109/icse.2009.5070513},pmid={null},pmcid={null},mag_id={2133859873},journal={null},abstract={Models can help software engineers to reason about design-time decisions before implementing a system. This paper focuses on models that deal with non-functional properties, such as reliability and performance. To build such models, one must rely on numerical estimates of various parameters provided by domain experts or extracted by other similar systems. Unfortunately, estimates are seldom correct. In addition, in dynamic environments, the value of parameters may change over time. We discuss an approach that addresses these issues by keeping models alive at run time and feeding a Bayesian estimator with data collected from the running system, which produces updated parameters. The updated model provides an increasingly better representation of the system. By analyzing the updated model at run time, it is possible to detect or predict if a desired property is, or will be, violated by the running implementation. Requirement violations may trigger automatic reconfigurations or recovery actions aimed at guaranteeing the desired goals. We illustrate a working framework supporting our methodology and apply it to an example in which a Web service orchestrated composition is modeled through a Discrete Time Markov Chain. Numerical simulations show the effectiveness of the approach.}}
@ARTICLE{Clavel_2002,title={Maude: specification and programming in rewriting logic},year={2002},author={Manuel Clavel and Manuel Clavel and Francisco Durán and Francisco Durán and Steven Eker and Steven Eker and Patrick Lincoln and Patrick Lincoln and Patrick Lincoln and Patrick Lincoln and Patrick Lincoln and Narciso Martí-Oliet and Narciso Martí-Oliet and José Meseguer and José Meseguer and José F. Quesada and José F. Quesada},doi={10.1016/s0304-3975(01)00359-0},pmid={null},pmcid={null},mag_id={2134287022},journal={Theoretical Computer Science},abstract={Maude is a high-level language and a high-performance system supporting executable specification and declarative programming in rewriting logic. Since rewriting logic contains equational logic, Maude also supports equational specification and programming in its sublanguage of functional modules and theories. The underlying equational logic chosen for Maude is membership equational logic, that has sorts, subsorts, operator overloading, and partiality definable by membership and equality conditions. Rewriting logic is reflective, in the sense of being able to express its own metalevel at the object level. Reflection is systematically exploited in Maude endowing the language with powerful metaprogramming capabilities, including both user-definable module operations and declarative strategies to guide the deduction process. This paper explains and illustrates with examples the main concepts of Maude's language design, including its underlying logic, functional, system and object-oriented modules, as well as parameterized modules, theories, and views. We also explain how Maude supports reflection, metaprogramming and internal strategies. The paper outlines the principles underlying the Maude system implementation, including its semicompilation techniques. We conclude with some remarks about applications, work on a formal environment for Maude, and a mobile language extension of Maude.}}
@ARTICLE{Jaghoori_2009,title={Schedulability of asynchronous real-time concurrent objects☆},year={2009},author={Mohammad Mahdi Jaghoori and Mohammad Mahdi Jaghoori and Frank S. de Boer and Frank S. de Boer and Tom Chothia and Tom Chothia and Marjan Sirjani and Marjan Sirjani},doi={10.1016/j.jlap.2009.02.009},pmid={null},pmcid={null},mag_id={2137116036},journal={The Journal of Logic and Algebraic Programming},abstract={Abstract   We present a modular method for schedulability analysis of real time distributed systems. We extend the actor model, as the asynchronous model for concurrent objects, with real time using timed automata, and show how actors can be analyzed individually to make sure that no task misses its deadline. We introduce  drivers  to specify how an actor can be safely used. Using these drivers we can verify schedulability, for a given scheduler, by doing a reachability check with the U ppaal  model checker. Our method makes it possible to put a finite bound on the process queue and still obtain schedulability results that hold for any queue length.}}
@ARTICLE{Holzer_2010,title={How did you specify your test suite},year={2010},author={Andreas Holzer and Andreas Holzer and Christian Schallhart and Christian Schallhart and Michael Tautschnig and Michael Tautschnig and Helmut Veith and Helmut Veith},doi={10.1145/1858996.1859084},pmid={null},pmcid={null},mag_id={2137175751},journal={null},abstract={Although testing is central to debugging and software certification, there is no adequate language to specify test suites over source code. Such a language should be simple and concise in daily use, feature a precise semantics, and of course, it has to facilitate suitable engines to compute test suites and assess the coverage achieved by a test suite.   This paper introduces the language FQL designed to fit these purposes. We achieve the necessary expressive power by a natural extension of regular expressions which matches test suites rather than individual executions. To evaluate the language, we show for a list of informal requirements how to express them in FQL. Moreover, we present a test case generation engine for C programs and perform practical experiments with the sample specifications.}}
@ARTICLE{Habibi_2005,title={An approach for the verification of systemc designs using asml},year={2005},author={Ali Habibi and A. Habibi and Sofiène Tahar and Sofiène Tahar},doi={10.1007/11562948_8},pmid={null},pmcid={null},mag_id={2138106527},journal={null},abstract={The spectacular advancement in microelectronics resulted in the creation of new system level design languages, such as SystemC, which put fourth new design and verification challenges. In this paper, we present an approach verifying SystemC designs using model checking and assertion based verification. Such verification is enabled through two transformations from SystemC to AsmL (the Abstract State Machines Language) and vice-versa. The soundness of these transformations, proved using abstract interpretation, guarantees the correctness of the model checking results and the validity of the generated assertion monitors (to be checked by simulation). We illustrate our approach on the SystemC/AsmL modeling and verification of the widely used Accelerated Graphics Port (AGP) standard. The verified AGP model can be either refined to implement an AGP core or used to validate existent compatible device.}}
@ARTICLE{Glabbeek_2012,title={On distributability of petri nets},year={2012},author={Rob van Glabbeek and Rob van Glabbeek and Ursula Goltz and Ursula Goltz and Jens-Wolfhard Schicke-Uffmann and Jens-Wolfhard Schicke-Uffmann},doi={10.1007/978-3-642-28729-9_22},pmid={null},pmcid={null},mag_id={2139722508},journal={null},abstract={We formalise a general concept of distributed systems as sequential components interacting asynchronously. We define a corresponding class of Petri nets, called LSGA nets, and precisely characterise those system specifications which can be implemented as LSGA nets up to branching ST-bisimilarity with explicit divergence.}}
@ARTICLE{Hachichi_2012,title={Transforming DATA* with Dotty Format to Aggregate Region Automaton},year={2012},author={Hiba Hachichi and Hiba Hachichi and Ilham Kitouni and Ilham Kitouni and Djamel Eddine Saϊdouni and Djamel-Eddine Saidouni},doi={10.5120/4647-6721},pmid={null},pmcid={null},mag_id={2142190790},journal={International Journal of Computer Applications},abstract={In this paper we propose an approach for translating DATA* structure of a high number of states to aggregate region automaton. Firstly, we propose a program written in python language that transforms a DATA* structure, presented as a dotty file, to a DATA* structure written in the form of a python file respecting the syntax of AToM3. Secondly, we define a meta-model of the DATA* model and a meta-model of the aggregate region automata model thus a transformation grammar using graph transformation and the modeling tool AToM to perform this transformation automatically. General Terms Formal methods, Graph transformation, Real time systems.}}
@ARTICLE{Spivey_1992,title={The Z notation: a reference manual},year={1992},author={Justin M Spivey and J. M. Spivey},doi={null},pmid={null},pmcid={null},mag_id={2144973245},journal={null},abstract={Tutorial introduction background the Z language the mathematical tool-kit sequential systems syntax summary.}}
@ARTICLE{Balsamo_2004,title={Model-based performance prediction in software development: a survey},year={2004},author={Simonetta Balsamo and Simonetta Balsamo and Antinisca Di Marco and A. Di Marco and A. Di Marco and Paola Inverardi and Paola Inverardi and Paola Inverardi and Paola Inverardi and Marta Simeoni and Marta Simeoni},doi={10.1109/tse.2004.9},pmid={null},pmcid={null},mag_id={2144979178},journal={IEEE Transactions on Software Engineering},abstract={Over the last decade, a lot of research has been directed toward integrating performance analysis into the software development process. Traditional software development methods focus on software correctness, introducing performance issues later in the development process. This approach does not take into account the fact that performance problems may require considerable changes in design, for example, at the software architecture level, or even worse at the requirement analysis level. Several approaches were proposed in order to address early software performance analysis. Although some of them have been successfully applied, we are still far from seeing performance analysis integrated into ordinary software development. In this paper, we present a comprehensive review of recent research in the field of model-based performance prediction at software development time in order to assess the maturity of the field and point out promising research directions.}}
@ARTICLE{Bijl_2003,title={Compositional Testing with ioco},year={2003},author={Machiel van der Bijl and Machiel van der Bijl and Arend Rensink and Arend Rensink and Jan Tretmans and Jan Tretmans},doi={10.1007/978-3-540-24617-6_7},pmid={null},pmcid={null},mag_id={2145505903},journal={null},abstract={Abstract. Compositional testing concerns the testing of systems that consist of communicating components which can also be tested in isolation. Examples are component based testing and interoperability testing. We show that, with certain restrictions, the ioco-test theory for conformance testing is suitable for compositional testing, in the sense that the integration of fully conformant components is guaranteed to be correct. As a consequence, there is no need to re-test the integrated system for conformance.

This result is also relevant for testing in context, since it implies that every failure of a system embedded in a test context can be reduced to a fault of the system itself.}}
@ARTICLE{Mitsching_2009,title={Towards an Industrial Strength Process for Timed Testing},year={2009},author={Ralf Mitsching and Ralf Mitsching and Carsten Weise and Carsten Weise and André Kolbe and André Kolbe and Henrik Bohnenkamp and Henrik Bohnenkamp and Henrik Bohnenkamp and Norbert Berzen and Norbert Berzen},doi={10.1109/icstw.2009.42},pmid={null},pmcid={null},mag_id={2145875406},journal={null},abstract={Timed model-based testing is a technique which allows for thespecification of timing in the model as well as in the testcases. As such, it is well suited for the testing of embedded systems,which usually puts a much higher demand on the amount of test casesand quality of the test process than other software systemsdo. While the theory of timed testing is well established over thelast years, industrial application is still rare. In this paper, wereport on our first step towards a industrial strength process fortimed testing. The paper describes how timed testing has beenapplied within an industrial case study, where we focus on a smallerpart of the system, but have gone all the way through the model-basedtest process adapted to the industrial environment. The timedtesting has been successfully carried out in the industrialpartner's test channel.Our paper describes the set up we have beenusing and our experiences and lessons learned, which will be ofinterest for practitioners, but also for people working on thetheory of model-based methods.}}
@ARTICLE{Krichen_2005,title={An expressive and implementable formal framework for testing real-time systems},year={2005},author={Moez Krichen and Moez Krichen and Moez Krichen and Stavros Tripakis and Stavros Tripakis},doi={10.1007/11430230_15},pmid={null},pmcid={null},mag_id={2146623137},journal={Lecture Notes in Computer Science},abstract={We propose a new framework for black-box conformance testing of real-time systems, based on the model of timed automata. The framework is expressive: it can fully handle partially-observable, non-deterministic timed automata. It also allows the user to define, through appropriate modeling, assumptions on the environment of the system under test (SUT) as well as on the interface between the tester and the SUT. The framework is implementable: tests can be implemented as finite-state machines accessing a finite-precision digital clock. We propose, for this framework, a set of test-generation algorithms with respect to different coverage criteria. We have implemented these algorithms in a prototype tool called TTG. Experimental results obtained by applying TTG on the Bounded Retransmission Protocol show that only a few tests suffice to cover thousands of reachable symbolic states in the specification.}}
@ARTICLE{Needham_1978,title={Using encryption for authentication in large networks of computers},year={1978},author={Roger M. Needham and Roger M. Needham and Michael Schroeder and Michael D. Schroeder},doi={10.1145/359657.359659},pmid={null},pmcid={null},mag_id={2146973388},journal={Communications of The ACM},abstract={Use of encryption to achieve authenticated communication in computer networks is discussed. Example protocols are presented for the establishment of authenticated connections, for the management of authenticated mail, and for signature verification and document integrity guarantee. Both conventional and public-key encryption algorithms are considered as the basis for protocols.}}
@ARTICLE{Berthomieu_1991,title={Modeling and verification of time dependent systems using time Petri nets},year={1991},author={Bernard Berthomieu and Bernard Berthomieu and Michel Diaz and Michel Diaz},doi={10.1109/32.75415},pmid={null},pmcid={null},mag_id={2151612633},journal={IEEE Transactions on Software Engineering},abstract={A description and analysis of concurrent systems, such as communication systems, whose behavior is dependent on explicit values of time is presented. An enumerative method is proposed in order to exhaustively validate the behavior of P. Merlin's time Petri net model, (1974). This method allows formal verification of time-dependent systems. It is applied to the specification and verification of the alternating bit protocol as a simple illustrative example. >}}
@ARTICLE{Nicola_1984,title={Testing equivalences for processes},year={1984},author={Rocco De Nicola and R De Nicola and Matthew Hennessy and Matthew Hennessy},doi={10.1016/0304-3975(84)90113-0},pmid={null},pmcid={null},mag_id={2157144873},journal={Theoretical Computer Science},abstract={Abstract   Given a set of processes and a set of tests on these processes we show how to define in a natural way three different equivalences on processes. These equivalences are applied to a particular language CCS. We give associated complete proof systems and fully abstract models. These models have a simple representation in terms of trees.}}
@ARTICLE{Mokhov_2010,title={Conditional Partial Order Graphs: Model, Synthesis, and Application},year={2010},author={Andrey Mokhov and Andrey Mokhov and Alex Yakovlev and Alex Yakovlev},doi={10.1109/tc.2010.58},pmid={null},pmcid={null},mag_id={2160249188},journal={IEEE Transactions on Computers},abstract={The paper introduces a new formal model for specification and synthesis of control paths in the context of asynchronous system design. The model, called Conditional Partial Order Graph (CPOG), captures concurrency and choice in a system's behavior in a compact and efficient way. It has advantages over widely used interpreted Petri Nets and Finite State Machines for a class of systems which have many behavioral scenarios defined on the same set of actions, e.g., CPU microcontrollers. The CPOG model has potential applications in the area of microcontrol synthesis and brings new methods for modeling concurrency into the application domain of modern and future processor architectures. The paper gives the formal definition of the CPOG model, formulates and solves the problem of CPOG synthesis, and introduces various optimization techniques. The presented ideas can be applied for CPU control synthesis as well as for synthesis of different kinds of event-coordination circuits often used in data coding and communication in digital systems, as demonstrated with several application examples.}}
@ARTICLE{En‐Nouaary_1998,title={Timed test cases generation based on state characterization technique},year={1998},author={Abdeslam En‐Nouaary and Abdeslam En-Nouaary and Rachida Dssouli and Rachida Dssouli and Ferhat Khendek and Ferhat Khendek and A. Elqortobi and A. Elqortobi},doi={10.1109/real.1998.739748},pmid={null},pmcid={null},mag_id={2162032875},journal={null},abstract={Real time reactive systems interact with their environment, using inputs and outputs, within specified time constraints. For such systems, a functional misbehavior or a deviation from the specified time constraints may have catastrophic consequences. Therefore, ensuring the correctness of real time systems becomes very important. We introduce the potential faults which can be encountered in a timed system implementation. We adapt an existing test cases generation technique, based on state characterization set, to generate timed test cases from a timed system specification. We model a timed system with a Timed Input Output Automaton (TIOA), which is a variant of the Alur and Dill model (R. Alur and D. Dill, 1994). In order to generate the timed test suite, the TIOA is first transformed into a Nondeterministic Timed Finite State Machine (NTFSM) with a given granularity. We illustrate our method with an example.}}
@ARTICLE{Esparza_2002,title={An Improvement of McMillan's Unfolding Algorithm},year={2002},author={Javier Esparza and Javier Esparza and Stefan Römer and Stefan Römer and Walter Vogler and Walter Vogler},doi={10.1023/a:1014746130920},pmid={null},pmcid={null},mag_id={2164024827},journal={null},abstract={McMillan has recently proposed a new technique to avoid the state explosion problem in the verification of systems modelled with finite-state Petri nets. The technique requires to construct a finite initial part of the unfolding of the net. McMillan's algorithm for this task may yield initial parts that are larger than necessary (exponentially larger in the worst case). We present a refinement of the algorithm which overcomes this problem.}}
@ARTICLE{Abramsky_1987,title={Observation equivalence as a testing equivalence},year={1987},author={Samson Abramsky and Samson Abramsky},doi={10.1016/0304-3975(87)90065-x},pmid={null},pmcid={null},mag_id={2167004912},journal={Theoretical Computer Science},abstract={A notion of testing is developed for transition systems with divergence. The forms of testing include traces, refusals, copying and global testing. Both denotational and operational formulations of testing are given. The equivalence based on this notion of testing is shown to coincide with observation equivalence.}}
@ARTICLE{Arapinis_2010,title={Analysing Unlinkability and Anonymity Using the Applied Pi Calculus},year={2010},author={Myrto Arapinis and Myrto Arapinis and Tom Chothia and Tom Chothia and Eike Ritter and Eike Ritter and Mark Ryan and Mark Ryan},doi={10.1109/csf.2010.15},pmid={null},pmcid={null},mag_id={2167816957},journal={null},abstract={An attacker that can identify messages as coming from the same source, can use this information to build up a picture of targets’ behaviour, and so, threaten their privacy. In response to this danger, unlinkable protocols aim to make it impossible for a third party to identify two runs of a protocol as coming from the same device. We present a framework for analysing unlinkability and anonymity in the applied pi calculus. We show that unlinkability and anonymity are complementary properties; one does not imply the other. Using our framework we show that the French RFID e-passport preserves anonymity but it is linkable therefore anyone carrying a French e-passport can be physically traced.}}
@ARTICLE{Große_2004,title={Checkers for SystemC designs},year={2004},author={Daniel Große and Daniel Groβe and Daniel GroBe and Rolf Drechsler and Rolf Drechsler},doi={10.1109/memcod.2004.1459851},pmid={null},pmcid={null},mag_id={2169163007},journal={null},abstract={Today's complex systems are modeled on a high level of abstraction. In this context, C/C++-based description languages, like SystemC, become very important. The modeling features of SystemC enable adequate levels of abstraction, hardware/software integration and fast executable specifications. Using the SystemC design methodology, a system is partitioned into hardware and software. Then the modules are refined down to the implementation. Besides efficient modeling, the correct functional behavior is very important. Already today up to 80% of the overall design costs are due to verification. As the complete system cannot be formally verified, checking of the functional behavior during operation has to be considered. In this paper an approach is presented that allows to check temporal properties for a SystemC design not only during simulation, but also after fabrication inform of an on-line test. The method translates the properties into synthesizable SystemC instructions. By this, the properties can be checked like HDL assertions during simulation and after production since they can be synthesized together with the system. The proposed approach enables a concise circuit and system verification methodology.}}
@ARTICLE{Karlsson_2006,title={Formal Verification of SystemC Designs Using a Petri-Net Based Representation},year={2006},author={Daniel Karlsson and Daniel Karlsson and Petru Eles and Petru Eles and Zebo Peng and Zebo Peng},doi={10.1109/date.2006.244076},pmid={null},pmcid={null},mag_id={2169490784},journal={null},abstract={This paper presents an effective approach to formally verify SystemC designs. The approach translates SystemC models into a Petri-Net based representation. The Petri-net model is then used for model checking of properties expressed in a timed formal verification SystemC designs Petri-net model model checking timed temporal logic . The approach is particularly suitable for, but not restricted to, models at a high level of abstraction, such as transaction-level. The efficiency of the approach is illustrated by experiments}}
@ARTICLE{Traulsen_2007,title={A systemC/TLM semantics in PROMELA and its possible applications},year={2007},author={Claus Traulsen and Claus Traulsen and Jérôme Cornet and Jérôme Cornet and Matthieu Moy and Matthieu Moy and Florence Maraninchi and Florence Maraninchi},doi={10.1007/978-3-540-73370-6_14},pmid={null},pmcid={null},mag_id={2171573750},journal={null},abstract={SystemC has become a de facto standard for the modeling of systems-on-a-chip, at various levels of abstraction, including the so-called transaction level (TL). Verifying properties of a TL model requires that SystemC be translated into some formally defined language for which there exist verification back-ends. Since SystemC has no formal semantics, this includes a careful encoding of the SystemC scheduler, which has both synchronous and asynchronous features, and a notion of time. In a previous work, we presented a complete chain from SystemC to a synchronous formalism and its associated verification tools. In this paper, we describe the encoding of the SystemC scheduler into an asynchronous formalism, namely Promela (the input language for Spin). We comment on the possible uses for this new encoding.}}
@ARTICLE{Verhoef_2006,title={Modeling and validating distributed embedded real-time systems with VDM++},year={2006},author={Marcel Verhoef and Marcel Verhoef and Peter Gorm Larsen and Peter Gorm Larsen and Jozef Hooman and Jozef Hooman},doi={10.1007/11813040_11},pmid={null},pmcid={null},mag_id={2172167385},journal={null},abstract={The complexity of real-time embedded systems is increasing, for example due to the use of distributed architectures. An extension to the Vienna Development Method (VDM) is proposed to address the problem of deployment of software on distributed hardware. The limitations of the current notation are discussed and new language elements are introduced to overcome these deficiencies. The impact of these changes is illustrated by a case study. A constructive operational semantics is defined in VDM++ and validated using VDMTools. The associated abstract formal semantics, which is not specific to VDM, is presented in this paper. The proposed language extensions significantly reduce the modeling effort when describing distributed real-time systems in VDM++ and the revised semantics provides a basis for improved tool support.}}
@ARTICLE{Bohnenkamp_2005,title={Timed testing with torx},year={2005},author={Henrik Bohnenkamp and Henrik Bohnenkamp and Axel Belinfante and Axel Belinfante},doi={10.1007/11526841_13},pmid={null},pmcid={null},mag_id={2226985919},journal={null},abstract={TorX is a specification-based, on-the-fly testing tool that tests for ioco conformance of implementations w.r.t. a formal specification. This paper describes an extension of TorX to not only allow testing for functional correctness, but also for correctness w.r.t. timing properties expressed in the specification. An implementation then passes a timed test if it passes according to ioco, and if occurrence times of outputs or of quiescence signals are legal according to the specification. The specifications are described by means of non-deterministic safety timed automata. This paper describes the basic algorithms for ioco, the necessary modifications to standard safety timed automata to make them usable as an input formalism, a test-derivation algorithm from timed automata, and the concrete algorithms implemented in TorX for timed testing. Finally, practical concerns with respect to timed testing are discussed.}}
@ARTICLE{Henzinger_2002,title={Lazy abstraction},year={2002},author={Thomas A. Henzinger and Thomas A. Henzinger and Ranjit Jhala and Ranjit Jhala and Rupak Majumdar and Rupak Majumdar and Grégoire Sutre and Grégoire Sutre},doi={10.1145/503272.503279},pmid={null},pmcid={null},mag_id={2295903414},journal={null},abstract={One approach to model checking software is based on the abstract-check-refine paradigm: build an abstract model, then check the desired property, and if the check fails, refine the model and start over. We introduce the concept of lazy abstraction to integrate and optimize the three phases of the abstract-check-refine loop. Lazy abstraction continuously builds and refines a single abstract model on demand, driven by the model checker, so that different parts of the model may exhibit different degrees of precision, namely just enough to verify the desired property. We present an algorithm for model checking safety properties using lazy abstraction and describe an implementation of the algorithm applied to C programs. We also provide sufficient conditions for the termination of the method.}}
@ARTICLE{Karsai_2004,title={Graph transformations in OMG's model-driven architecture},year={2004},author={Gabor Karsai and Aditya Agrawal},doi={null},pmid={null},pmcid={null},mag_id={2461421437},journal={Lecture Notes in Computer Science},abstract={The Model-Driven Architecture (MDA) vision of the Object Management Group offers a unique opportunity for introducing Graph Transformation (GT) technology to the software industry. The paper proposes a domain-specific refinement of MDA, and describes a practical manifestation of MDA called Model-Integrated Computing (MIC). MIC extends MDA towards domain-specific modeling languages, and it is well supported by various generic tools that include model transformation tools based on graph transformations. The MIC tools are metapro-grammable, i.e. they can be tailored for specific domains using meta-models that include metamodels of transformations. The paper describes the development process and the supporting tools of MIC, and it raises a number of issues for future research on GT in MDA.}}
@ARTICLE{Bauer_2012,title={Moving from Specifications to Contracts in Component-Based Design},year={2012},author={Sebastian Bauer and Sebastian Bauer and Albert David and Alexandre David and Rolf Hennicker and Rolf Hennicker and Kim Guldstrand Larsen and Kim Guldstrand Larsen and Axel Legay and Axel Legay and Ulrik Nyman and Ulrik Nyman and Andrzej Wąsowski and Andrzej Wasowski},doi={10.1007/978-3-642-28872-2},pmid={null},pmcid={null},mag_id={2482692280},journal={Lecture Notes in Computer Science},abstract={null}}
@ARTICLE{Kleinjohann_2008,title={Distributed Embedded Systems: Design, Middleware and Resources},year={2008},author={Bernd Kleinjohann and Bernd Kleinjohann and Wayne Wolf and Wayne Wolf and Wayne Wolf and Lisa Kleinjohann and Lisa Kleinjohann},doi={10.1007/978-0-387-09661-2},pmid={null},pmcid={null},mag_id={2489201504},journal={null},abstract={This year, the IFIP Working Conference on Distributed and Parallel Embedded Sys tems (DIPES 2008) is held as part of the IFIP World Computer Congress, held in Milan on September 7 10, 2008. The embedd}}
@ARTICLE{Belala_2010,title={Modèles de temps et leur intérêt à la vérification formelle des systèmes temps-réel},year={2010},author={Nabil Belala and Nabil Belala and Nabil Belala and Nabil Belala},doi={null},pmid={null},pmcid={null},mag_id={2734042943},journal={null},abstract={Actuellement, les methodes formelles sont de plus en plus utilisees dans le but d’analyser le comportement les systemes dits temps-reel. Ces methodes utilisent des modeles formels de specification dotes de semantiques bien definies et des techniques de verification formelle. D'autre part, les semantiques de vrai parallelisme, comme la semantique de maximalite peuvent etre utilisees a bon escient si on veut echapper a l’hypothese de l’atomicite temporelle et structurelle des actions. Des travaux anterieurs ont montre l’aptitude de modeles bases sur la maximalite, a l’image des systemes de transitions etiquetes maximales, a supporter l’expression du parallelisme d’execution d’actions.

Notre travail qui s’inscrit dans le cadre de la specification et la verification formelle des systemes temps-reel consiste a tirer profit des avantages offerts par la semantique de maximalite en l’appliquant a quelques modeles temporels et/ou temporises. Ainsi, nous proposons trois methodes de geeneration de structures bases sur la maximalite (systemes de transitions etiquetees maximales, DATA et DATA*) a partir de modeles de haut niveau a savoir le langages Basic LOTOS avec durees d’actions, la langage D-LOTOS et les reseaux de Petri place/transition.}}
@ARTICLE{Genc_2003,title={Distributed diagnosis of discrete-event systems using Petri nets},year={2003},author={Sahika Genc and Stéphane Lafortune},doi={null},pmid={null},pmcid={null},mag_id={2734756299},journal={null},abstract={The problem of detecting and isolating fault events in dynamic systems modeled as discrete-event systems is considered. The modeling formalism adopted is that of Petri nets with labeled transitions, where some of the transitions are labeled by different types of unobservable fault events. The Diagnoser Approach for discrete-event systems modeled by automata developed in earlier work is adapted and extended to on-line fault diagnosis of systems modeled by Petri nets, resulting in a centralized diagnosis algorithm based on the notion of "Petri net diagnosers". A distributed version of this centralized algorithm is also presented. This distributed version assumes that the Petri net model of the system can be decomposed into two place-bordered Petri nets satisfying certain conditions and that the two resulting Petri net diagnosers can exchange messages upon the occurrence of observable events. It is shown that this distributed algorithm is correct in the sense that it recovers the same diagnostic information as the centralized algorithm. The distributed algorithm provides an approach for tackling fault diagnosis of large complex systems.}}
@ARTICLE{Bijl_2004,title={Compositional testing with IOCO},year={2004},author={Machiel van der Bijl and M. van der Bijl and Arend Rensink and Arend Rensink and G.J. Tretmans and G.J. Tretmans},doi={null},pmid={null},pmcid={null},mag_id={2765906234},journal={Lecture Notes in Computer Science},abstract={Compositional testing concerns the testing of systems that consist of communicating components which can also be tested in isolation. Examples are component based testing and interoperability testing. We show that, with certain restrictions, the ioco-test theory for conformance testing is suitable for compositional testing, in the sense that the integration of fully conformant components is guaranteed to be correct. As a consequence, there is no need to re-test the integrated system for conformance. This result is also relevant for testing in context, since it implies that every failure of a system embedded in a test context can be reduced to a fault of the system itself.}}
@ARTICLE{Bengtsson_1996,title={UPPAAL: a Tool Suite for Automatic Verification of Real Time Systems},year={1996},author={Johan Bengtsson and Johan Bengtsson and Kim Guldstrand Larsen and Kim Guldstrand Larsen and Fredrik Larsson and Fredrik Larsson and Fredrik Larsson and Paul Pettersson and Paul Pettersson and Wang Yi and Wang Yi},doi={null},pmid={null},pmcid={null},mag_id={2788342898},journal={Lecture Notes in Computer Science},abstract={Uppaal is a tool suite for automatic verification of safety and bounded liveness properties of real-time systems modeled as networks of timed automata. It includes: a graphical interface that supports graphical and textual representations of networks of timed automata, and automatic transformation from graphical representations to textual format, a compiler that transforms a certain class of linear hybrid systems to networks of timed automata, and a model-checker which is implemented based on constraint-solving techniques. Uppaal also supports diagnostic model-checking providing diagnostic information in case verification of a particular real-time systems fails.}}
@ARTICLE{Bohnenkamp_2005,title={timed testing with torx},year={2005},author={Henrik Bohnenkamp and Axel Belinfante},doi={null},pmid={null},pmcid={null},mag_id={2791363720},journal={Lecture Notes in Computer Science},abstract={TorX is a specification-based, on-the-fly testing tool that tests for ioco conformance of implementations w.r.t. a formal specification. This paper describes an extension of TorX to not only allow testing for functional correctness, but also for correctness w.r.t. timing properties expressed in the specification. An implementation then passes a timed test if it passes according to ioco, and if occurrence times of outputs or of quiescence signals are legal according to the specification. The specifications are described by means of non-deterministic safety timed automata. This paper describes the basic algorithms for ioco, the necessary modifications to standard safety timed automata to make them usable as an input formalism, a test-derivation algorithm from timed automata, and the concrete algorithms implemented in TorX for timed testing. Finally, practical concerns with respect to timed testing are discussed.}}
@ARTICLE{Phillips_1987,title={Refusal testing},year={1987},author={Iain Phillips and Iain Phillips and Iain Phillips},doi={10.1016/0304-3975(87)90117-4},pmid={null},pmcid={null},mag_id={2913048148},journal={null},abstract={When manipulating concurrent processes it is desirable to suppress their internal details and to consider two processes to be equivalent if their external behaviours are equivalent. Following Milner and De Nicola & Hennessy we take this external equivalence to mean that an observer cannot tell the processes apart by testing their responses to the same stimuli. We introduce a form of testing ( refusal testing ) which is more powerful than that of De Nicola & Hennessy in that the observer not only tests whether a process will perform an action but is also allowed under certain circumstances to discover in a finite amount of time that the process will not perform an action. The equivalence associated with refusal testing is compared with De Nicola & Hennessy's testing equivalence and Milner's observation equivalence, and a sound and complete proof system is provided for refusal equivalence when applied to CCS processes.}}
@ARTICLE{Phillips_1986,title={Refusal testing},year={1986},author={Iain Phillips and Iain Phillips and Iain Phillips},doi={null},pmid={null},pmcid={null},mag_id={2913461254},journal={null},abstract={null}}
@ARTICLE{Nielsen_2003,title={Automated test generation from timed automata},year={2003},author={Brian Nielsen and Brian Nielsen and Arne Skou and Arne Skou},doi={10.1007/s10009-002-0094-1},pmid={null},pmcid={null},mag_id={2916184944},journal={International Journal on Software Tools for Technology Transfer},abstract={Testing is the most dominant validation activity used by industry today, and there is an urgent need for improving its effectiveness, both with respect to the time and resources for test generation and execution, and obtained test coverage. We present a new technique for automatic generation of real-time black-box conformance tests for non-deterministic systems from a determinizable class of timed automata specifications with a dense time interpretation. In contrast to other attempts, our tests are generated using a coarse equivalence class partitioning of the specification. To analyze the specification, to synthesize the timed tests, and to guarantee coverage with respect to a coverage criterion, we use the efficient symbolic techniques recently developed for model checking of real-time systems. Application of our prototype tool to a realistic specification shows promising results in terms of both the test suite size, and the time and space used for test generation.}}
@ARTICLE{Myers_1979,title={The Art of Software Testing},year={1979},author={Glenford J. Myers and Glenford J. Myers and Corey Sandler and Corey Sandler and Tom Badgett and Tom Badgett},doi={null},pmid={null},pmcid={null},mag_id={3010856131},journal={null},abstract={The classic, landmark work on software testingThe hardware and software of computing have changed markedly in the three decades since the first edition of The Art of Software Testing, but this book's powerful underlying analysis has stood the test of time. Whereas most books on software testing target particular development techniques, languages, or testing methods, The Art of Software Testing, Third Edition provides a brief but powerful and comprehensive presentation of time-proven software testing approaches. If your software development project is mission critical, this book is an investment that will pay for itself with the first bug you find.The new Third Edition explains how to apply the book's classic principles to today's hot topics including:Testing apps for iPhones, iPads, BlackBerrys, Androids, and other mobile devicesCollaborative (user) programming and testingTesting for Internet applications, e-commerce, and agile programming environmentsWhether you're a student looking for a testing guide you'll use for the rest of your career, or an IT manager overseeing a software development team, The Art of Software Testing, Third Edition is an expensive book that will pay for itself many times over.}}
@ARTICLE{李幼升_1989,title={ph},year={1989},author={李幼升 and F. G. J. Hayhoe},doi={null},pmid={null},pmcid={null},mag_id={3031066211},journal={null},abstract={null}}
@ARTICLE{Henzinger_null,title={Lazy abstraction},year={null},author={Thomas A. Henzinger and Thomas A. Henzinger and Ranjit Jhala and Ranjit Jhala and Rupak Majumdar and Rupak Majumdar and Grégoire Sutre and Grégoire Sutre},doi={10.1145/565816.503279},pmid={null},pmcid={null},mag_id={4252150051},journal={Sigplan Notices},abstract={One approach to model checking software is based on the abstract-check-refine paradigm: build an abstract model, then check the desired property, and if the check fails, refine the model and start over. We introduce the concept of lazy abstraction to integrate and optimize the three phases of the abstract-check-refine loop. Lazy abstraction continuously builds and refines a single abstract model on demand, driven by the model checker, so that different parts of the model may exhibit different degrees of precision, namely just enough to verify the desired property. We present an algorithm for model checking safety properties using lazy abstraction and describe an implementation of the algorithm applied to C programs. We also provide sufficient conditions for the termination of the method.}}
@ARTICLE{Din_null,title={16 TTCN-3},year={null},author={George Din and George Din},doi={10.1007/11498490_21},pmid={null},pmcid={null},mag_id={4252927100},journal={Lecture Notes in Computer Science},abstract={AbstractThis chapter presents TTCN-3, the Testing and Test Control Notation, which is the most used technology in the protocol testing field. Many of the previous chapters concern the problem of how to create tests for a system we want to test. In this chapter we consider the problem of test execution. Test execution comprises the following activities: test data is applied to a SUT, the behavior of the SUT is monitored, and expected and actual behaviors are compared in order to yield a verdict.KeywordsTest ComponentExecution EnvironmentTest ManagementSystem Under TestTest ExecutionThese keywords were added by machine and not by the authors. This process is experimental and the keywords may be updated as the learning algorithm improves.}}
@ARTICLE{Mugridge_2005,title={Fit for Developing Software: Framework for Integrated Tests (Robert C. Martin)},year={2005},author={Rick Mugridge and Rick Mugridge and Ward Cunningham and Ward Cunningham},doi={null},pmid={null},pmcid={null},mag_id={1234225},journal={null},abstract={null}}
@ARTICLE{Schirmer_2006,title={Verification of Sequential Imperative Programs in Isabelle/HOL},year={2006},author={Norbert Schirmer and Norbert Schirmer},doi={null},pmid={null},pmcid={null},mag_id={14193191},journal={null},abstract={The purpose of this thesis is to create a verification environment for sequential imperative programs. First a general language model is proposed, which is independent of a concrete programming language but expressive enough to cover all common language features: mutually recursive procedures, abrupt termination and exceptions, runtime faults, local and global variables, pointers and heap, expressions with side effects, pointers to procedures, partial application and closures, dynamic method invocation and also unbounded nondeterminism. For this language a Hoare logic for both partial and total correctness is developed and on top of it a verification condition generator is implemented. The Hoare logic is designed to allow the integration of program analysis or software model checking into the verification. To demonstrate the continuity to a real programming language a subset of C is embedded into the verification environment. The whole work is developed in the theorem prover Isabelle. Therefore the correctness is machine-checked and in addition the rich infrastructure of the general purpose theorem prover Isabelle can be employed for the verification of imperative programs.}}
@ARTICLE{Aman_2012,title={Behavioural equivalences over migrating processes with timers},year={2012},author={Bogdan Aman and Bogdan Aman and Gabriel Ciobanu and Gabriel Ciobanu and Maciej Koutny and Maciej Koutny},doi={10.1007/978-3-642-30793-5_4},pmid={null},pmcid={null},mag_id={19314400},journal={null},abstract={The temporal evolution of mobile processes is governed by independently operating local clocks and their migration timeouts. We define a formalism modelling such distributed systems allowing (maximal) parallel execution at each location. Taking into account explicit timing constraints based on migration and interprocess communication, we introduce and study a number of timed behavioural equivalences, aiming to provide theoretical underpinnings of verification methods. We also investigate relationships between such behavioural equivalences.}}
@ARTICLE{Johnsen_2003,title={Combining Active and Reactive Behavior in Concurrent Objects},year={2003},author={Einar Broch Johnsen and Olaf Owe and Marte Arnestad},doi={null},pmid={null},pmcid={null},mag_id={21427615},journal={null},abstract={A distributed system can be modeled by objects that run concurrently, each with its own processor, and communicate by remote method calls. However objects may have to wait for response to external calls; at best resulting in inefficient use of processor capacity, at worst resulting in deadlock. Furthermore, it is difficult to combine active and passive object behavior without defining explicit control loops. This paper proposes a solution to these problems by means of asynchronous method calls and conditional processor release points in program code. Although at the cost of additional internal non-determinism in the objects, this approach seems attractive in asynchronous or unreliable distributed environments. The concepts are illustrated by the small object-oriented language Creol and its operational semantics.}}
@ARTICLE{Clarke_2010,title={HATS: A Formal Software Product Line Engineering Methodology},year={2010},author={Dave Clarke and Dave Clarke and Dave Clarke and Dave Clarke and Nikolay Diakov and Nikolay Diakov and Reiner Hähnle and Reiner Hähnle and Einar Broch Johnsen and Einar Broch Johnsen and Germán Puebla and Germán Puebla and Balthasar Weitzel and Balthasar Weitzel and Peter Y. H. Wong and Peter Y. H. Wong},doi={null},pmid={null},pmcid={null},mag_id={25007049},journal={null},abstract={Trust in software is typically achieved via stabilization efforts over long periods of use. Adaptation to changing circumstances, however, often requires substantial changes to the software. Changing a software system using standard manufacturing processes often results in quality regressions, invalidating trust. Formal methods provide a means for guaranteeing various properties of a software system that increase its trustworthiness. The HATS methodology aims to integrate formal methods for modeling changes of software systems in terms of variability and evolution, while preserving trustworthiness properties. This paper outlines how different formal methods are extended and integrated to build an industrially viable Software Product Line Engineering method for manufacturing highly adaptable and trustworthy software. Keywords-software product lines; methodology; formal methods}}
@ARTICLE{Rodrı́guez_2013,title={Verification Based on Unfoldings of Petri Nets with Read Arcs},year={2013},author={César Rodrı́guez and César Rodríguez},doi={null},pmid={null},pmcid={null},mag_id={26103048},journal={null},abstract={L'etre humain fait des erreurs, en particulier dans la realisation de taches complexes comme la construction des systemes informatiques modernes.  Nous nous interesserons dans cette these a la verification assistee par ordinateur du bon fonctionnement des systemes informatiques.  Les systemes informatiques actuels sont de grande complexite.  Afin de garantir leur fiabilite, la verification automatique est une alternative au 'testing' et a la simulation.  Elle propose d'utiliser des ordinateurs pour explorer exhaustivement l'ensemble des etats du systeme, ce qui est problematique: meme des systemes assez simples peuvent atteindre un grand nombre d'etats.  L'utilisation des bonnes representations des espaces d'etats est essentielle pour surmonter la complexite des problemes poses en verification automatique.  La verification des systemes concurrents amene des difficultes additionnelles, car l'analyse doit, en principe, examiner tous les ordres possibles d'execution des actions concurrentes.  Le depliage des reseaux de Petri est une technique largement etudiee pour la verification des systemes concurrents.  Il representent l'espace d'etats du systeme par un ordre partiel, ce qui se revele aussi naturel qu'efficace pour la verification automatique.   Nous nous interessons a la verification des systemes concurrents modelises par des reseaux de Petri, en etudiant deux techniques remarquables de verification: le 'model checking' et le diagnostic.  Nous etudions les depliages des reseaux de Petri etendus avec des arcs de lecture.  Ces depliages, aussi appeles depliages contextuels, semblent etre une meilleure representation des systemes contenant des actions concurrentes qui lisent des ressources partagees : ils peuvent etre exponentiellement plus compacts dans ces cas.  Ce travail contient des contributions theoriques et pratiques.  Dans un premier temps, nous etudions la construction des depliages contextuels, en proposant des algorithmes et des structures de donnees pour leur construction efficace.  Nous combinons les depliages contextuels avec les 'merged process', une autre representation des systemes concurrents qui contourne l'explosion d'etats derivee du non-determinisme.  Cette nouvelle structure, appelee 'contextual merged process', est souvent exponentiellement plus compacte, ce que nous montrons experimentalement.  Ensuite, nous nous interessons a la verification a l'aide des depliages contextuels.  Nous traduisons vers SAT le probleme d'atteignabilite des depliages contextuels, en abordant les problemes issus des cycles de conflit asymetrique.  Nous introduisons egalement une methode de diagnostic avec des hypotheses d'equite, cette fois pour des depliages ordinaires.  Enfin, nous implementons ces algorithmes dans le but de produire un outil de verification competitif et robuste.  L'evaluation de nos methodes sur un ensemble d'exemples standards, et leur comparaison avec des techniques issues des depliages ordinaires, montrent que la verification avec des depliages contextuels est plus efficace que les techniques existantes dans de nombreux cas.  Ceci suggere que les depliages contextuels, et les structures d'evenements asymetriques en general, meritent une place legitime dans la recherche en concurrence, egalement du point de vu de leur efficacite.}}
@ARTICLE{Bozga_2004,title={Tools and Applications II: The IF Toolset},year={2004},author={Marius Bozga and Susanne Graf and Ileana Ober and Iulian Ober and Joseph Sifakis},doi={10.1007/b110123},pmid={null},pmcid={null},mag_id={36240330},journal={null},abstract={This paper presents an overview on the IF toolset which is an environment for modelling and validation of heterogeneous real-time systems. The toolset is built upon a rich formalism, the IF notation, allowing structured automata-based system representations. Moreover, the IF notation is expressive enough to support real-time primitives and extensions of high-level modelling languages such as SDL and UML by means of structure preserving mappings. The core part of the IF toolset consists of a syntactic transformation component and an open exploration platform. The syntactic transformation component provides language level access to IF descriptions and has been used to implement static analysis and optimisation techniques. The exploration platform gives access to the graph of possible executions. It has been connected to different state-of-the-art model-checking and test-case generation tools. A methodology for the use of the toolset is presented at hand of a case study concerning the Ariane-5 Flight Program for which both an SDL and a UML model have been validated.}}
@ARTICLE{Friske_2008,title={Composition of Model-based Test Coverage Criteria.},year={2008},author={Mario Friske and Mario Friske and Bernd-Holger Schlingloff and Bernd-Holger Schlingloff and Stephan Weißleder and Stephan Weißleder},doi={null},pmid={null},pmcid={null},mag_id={39065887},journal={null},abstract={In this paper, we discuss adjustable coverage criteria and their combinations in model-based testing. We formalize coverage criteria and specify test goals using OCL. Then, we propose a set of functions which describe the test generation process in a generic way. Each coverage criterion is mapped to a set of test goals. Based on this set of functions, we propose a generic framework enabling flexible integration of various test generators and unified treatment of test coverage criteria.}}
@ARTICLE{Kuliamin_2003,title={UniTesK: Model Based Testing in Industrial Practice},year={2003},author={Victor V. Kuliamin and Victor V. Kuliamin and Alexander K. Petrenko and Alexander K. Petrenko and A. S. Kossatchev and A. S. Kossatchev and Igor Burdonov and I. B. Bourdonov},doi={null},pmid={null},pmcid={null},mag_id={39835904},journal={null},abstract={The article presents UniTesK technology, an automated model based test construction method for use in industrial testing of general-purpose software. The approach presented includes automatic generation of test oracles from software contracts, coverage driven test sequence generation, test artifacts reuse. This work stems from the ISP RAS results of academic research and 10-years experience in industrial application of formal testing techniques [1].}}
@ARTICLE{Schneider_2004,title={Verification of Reactive Systems},year={2004},author={Klaus Schneider and Klaus Schneider},doi={10.1007/978-3-662-10778-2},pmid={null},pmcid={null},mag_id={41095567},journal={null},abstract={Reactive systems are becoming more and more important for essentially all areas of technical and professional activities as well as for many areas of everyday life. The design of these systems is a gr}}
@ARTICLE{Alrouh_2011,title={Towards secure web services : performance analysis, decision making and steganography approaches},year={2011},author={Bachar Alrouh and Bachar Alrouh},doi={null},pmid={null},pmcid={null},mag_id={43510934},journal={null},abstract={Web services provide a platform neutral and programming language independent technology that supports interoperable machine-to-machine interaction over a network. Clients and other systems interact with Web services using a standardised XML messaging system, such as the Simple Object Access Protocol (SOAP), typically conveyed using HTTP with an XML serialisation in conjunction with other related Web standards. Nevertheless, the idea of applications from different parties communicating together raises a security threat. The challenge of Web services security is to understand and consider the risks of securing a Web-based service depending on the existing security techniques and simultaneously follow evolving standards in order to fill the gap in Web services security. However, the performance of the security mechanisms is fraught with concerns due to additional security contents in SOAP messages, the higher number of message exchanges to establish trust, as well as the extra CPU time to process these additions. As the interaction between service providers and requesters occurs via XML-based SOAP messages, securing Web services tends to make these messages longer than they would be otherwise and consequently requires interpretation by XML parsers on both sides, which reduces the performance of Web services. The work described in this thesis can be broadly divided into three parts, the first of which is studying and comparing the performance of various security profiles applied on a Web service tested with different initial message sizes. The second part proposes a multi-criteria decision making framework to aid Web services developers and architects in selecting the best suited security profile that satisfies the different requirements of a given application during the development process in a systematic, manageable, and effective way. The proposed framework, based on the Analytical Hierarchy Process (AHP) approach, incorporates not only the security requirements, but also the performance considerations as well as the configuration constraints of these security profiles. The framework is then validated and evaluated using a scenario-driven approach to demonstrate situations where the decision making framework is used to make informed decisions to rank various security profiles in order to select the most suitable one for each scenario. Finally, the last part of this thesis develops a novel steganography method to be used for SOAP messages within Web services environments. This method is based on changing the order of XML elements according to a secret message. This method has a high imperceptibility; it leaves almost no trail because it uses the communication protocol as a cover medium, and keeps the structure and size of the SOAP message intact. The method is empirically validated using a feasible scenario so as to indicate its utility and value.}}
@ARTICLE{Rossi_2006,title={Handbook of Constraint Programming},year={2006},author={Francesca Rossi and Francesca Rossi and Peter van der Beek and Peter van Beek and Toby Walsh and Toby Walsh},doi={null},pmid={null},pmcid={null},mag_id={47957325},journal={null},abstract={null}}
@ARTICLE{Rozenberg_1997,title={Handbook of formal languages, vol. 3: beyond words},year={1997},author={Grzegorz Rozenberg and Grzegorz Rozenberg and Arto Salomaa and Arto Salomaa},doi={null},pmid={null},pmcid={null},mag_id={55312279},journal={null},abstract={null}}
@ARTICLE{Trab_2010,title={A multi-criteria decision making framework for real time model-based testing},year={2010},author={Mohammad Saeed Abou Trab and Mohammad Saeed Abou Trab and Bachar Alrouh and Bachar Alrouh and Steve Counsell and Steve Counsell and Robert M. Hierons and Robert M. Hierons and Gheorghita Ghinea and Gheorghiţă Ghinea and George Ghinea},doi={10.1007/978-3-642-15585-7_20},pmid={null},pmcid={null},mag_id={58664554},journal={null},abstract={Testing Real-Time Embedded Systems (RTES) is a non-trivial task - time adds a new dimension to the complexity of the testing process. In previous research, we introduced a 'priority-based' approach which tested the logical and timing behaviour of an RTES modelled formally as Uppaal timed automata. In this paper, we develop a novel Analytical Hierarchy Process (AHP) decision-making framework for our priority-based testing approach that provides testers with a systematic approach through which they can prioritize the available testing sets that best fulfils their testing requirements.}}
@ARTICLE{Holzer_2011,title={Seamless testing for models and code},year={2011},author={Andreas Holzer and Andreas Holzer and Visar Januzaj and Visar Januzaj and Stefan Kugele and Stefan Kugele and Boris Langer and Boris Langer and Christian Schallhart and Christian Schallhart and Michael Tautschnig and Michael Tautschnig and Helmut Veith and Helmut Veith},doi={10.1007/978-3-642-19811-3_20},pmid={null},pmcid={null},mag_id={64885222},journal={null},abstract={This paper describes an approach to model-based testing where a test suite is generated from a model and automatically concretized to drive an implementation. Motivated by an industrial project involving DO-178B compliant avionics software, where the models are UML activity diagrams and the implementation is ANSI C, we developed a seamless testing environment based on our test specification language FQL. We demonstrate how to apply FQL to activity diagrams in such a way that FQL test specifications easily translate from UML to C code. Our approach does not require any additional glue or auxiliary code but is fully automatic except for straightforward source code annotations that link source and model. In this way, we can check for modeled but unimplemented behavior and vice versa, and we can also evaluate the degree of abstraction between model and implementation.}}
@ARTICLE{Okun_2004,title={Specification mutation for test generation and analysis},year={2004},author={Vadim Okun and Vadim Okun and Yaacov Yesha and Yaacov Yesha},doi={null},pmid={null},pmcid={null},mag_id={65164916},journal={null},abstract={Mutation analysis is a fault-based testing technique that uses mutation operators to introduce small changes into a program or specification, producing mutants, and then chooses test cases to distinguish the mutants from the original. Mutation operators differ in the coverage they get. They also differ in the number of mutants they generate. Consequently, selecting mutation operators is an important problem whose solution affects the effectiveness and cost of mutation testing. 
We use the automated test generation and evaluation method that combines a model checker and mutation analysis. We define a set of mutation operators and implement a mutation generator for specifications written in SMV, a popular model checker. 
To select the most effective mutation operators and sets of operators, we compare them using both theoretical and experimental methods. We construct mutation detection conditions and develop a technique to theoretically compare mutation operators. We apply mutation coverage and pairwise coverage metrics to empirically compare the effectiveness of mutation operators. 
To detect a fault in a program, a test case must cause the fault to affect the outputs, not just intermediate variables. We develop a method that uses a model checker to guarantee that tests cause visible output failures. 
We find that mutation operators form a hierarchy with respect to detection capability; we can skip a test for a mutation from an easier-to-detect mutation operator in the hierarchy, provided that we detect a corresponding mutation from a harder-to-detect operator. Our theoretical technique allows us to prove that the hierarchy applies to arbitrary logic expressions, whereas previous results apply only to logic expressions in disjunctive normal form. Based on analysis and empirical evaluation, we recommend mutation operators and sets of mutation operators that yield good test coverage at a reduced cost. 
Our experiments show that specification-based mutation can be applied to test programs; it gets good program-based coverage. Our method for guaranteeing fault visibility is very effective for black-box testing of programs which have a large intermediate state. 
This thesis shows that specification-based mutation can be used to economically generate effective tests.}}
@ARTICLE{Holzmann_2003,title={Spin model checker, the: primer and reference manual},year={2003},author={Gerard J. Holzmann and Gerard Holzmann and Gerard J. Holzmann},doi={null},pmid={null},pmcid={null},mag_id={68069235},journal={null},abstract={Master SPIN, the breakthrough tool for improving software reliabilitySPIN is the world's most popular, and arguably one of the world's most powerful, tools for detecting software defects in concurrent system designs. Literally thousands of people have used SPIN since it was first introduced almost fifteen years ago. The tool has been applied to everything from the verification of complex call processing software that is used in telephone exchanges, to the validation of intricate control software for interplanetary spacecraft.This is the most comprehensive reference guide to SPIN, written by the principal designer of the tool. It covers the tool's specification language and theoretical foundation, and gives detailed advice on methods for tackling the most complex software verification problems. Sum Design and verify both abstract and detailed verification models of complex systems software Sum Develop a solid understanding of the theory behind logic model checking Sum Become an expert user of the SPIN command line interface, the Xspin graphical user interface, and the TimeLine editing tool Sum Learn the basic theory of omega automata, linear temporal logic, depth-first and breadth-first search, search optimization, and model extraction from source codeThe SPIN software was awarded the prestigious Software System Award by the Association for Computing Machinery (ACM), which previously recognized systems such as UNIX, SmallTalk, TCP/IP, Tcl/Tk, and the World Wide Web.}}
@ARTICLE{Krause_2012,title={Model based specification, verification, and test generation for a safety fieldbus profile},year={2012},author={Jan Krause and Jan Krause and Elke Hintze and Elke Hintze and Stephan Magnus and Stephan Magnus and Christian Diedrich and Christian Diedrich},doi={10.1007/978-3-642-33678-2_8},pmid={null},pmcid={null},mag_id={73401245},journal={null},abstract={This paper suggests methods, and a tool chain for model based specification, verification, and test generation for a safety fieldbus profile. The basis of this tool chain is the use of an UML profile as a specification notation, a simple high level Petri net model called "Safe Petri Net with Attributes" (SPENAT) and analysis methods found in Petri net theory. The developed UML profile contains UML class diagrams and UML state machines for specification modeling. Verification and developed test generation methods are shown to be applicable after mapping the specification model onto SPENAT. The practical use of this tool chain is exemplarily demonstrated for a safety fieldbus profile.}}
@ARTICLE{Budd_1899,title={The design of a prototype mutation system for program testing},year={1899},author={Timothy A. Budd and Timothy A. Budd and Richard J. Lipton and Richard J. Lipton and Richard J. Lipton and Richard A. DeMillo and Richard A. DeMillo and Frederick G. Sayward and Frederick G. Sayward},doi={10.1109/afips.1978.195},pmid={null},pmcid={null},mag_id={75593049},journal={null},abstract={When testing software the major question which must always be addressed is "If a program is correct for a finite number of test cases, can we assume it is correct in general. " Test data which possess this property is called Adequate test data, and, although adequate test data cannot in general be derived algorithmically, 1 several methods have recently emerged which allow one to gain confidence in one's test data's adequacy. Program mutation is a radically new approach to determining test data adequacy which holds promise of being a major breakthrough in the field of software testing. The concepts and philosophy of program mutation have been given elsewhere, 2 the following will merely present a brief introduction to the ideas underlying the system. Unlike previous work, program mutation assumes that competent programmers will produce programs which, if they are not correct, are "almost" correct. That is, if a program is not correct it is a "mutant"-it differs from a correct program by simple errors. Assuming this natural premise, a program P which is correct on test data T is subjected to a series of mutant operators to produce mutant programs which differ from P in very simple ways. The mutants are then executed on T. If all mutants give incorrect results then it is very likely that P is correct (i.e., T is adequate). On the other hand, if some mutants are correct on T then either: (1) the mutants are equivalent to P, or (2) the test data T is inadequate. In the latter case, T must be augmented by examining the non-equivalent mutants which are correct on T: a procedure which forces close examination of P with respect to the mutants. At first glance it would appear that if T is determined adequate by mutation analysis, then P might still contain some complex errors which are not explicitly mutants of P.}}
@ARTICLE{Desel_2004,title={Lectures on Concurrency and Petri Nets},year={2004},author={Jörg Desel and Jörg Desel and Wolfgang Reisig and Wolfgang Reisig and Grzegorz Rozenberg and Grzegorz Rozenberg},doi={10.1007/b98282},pmid={null},pmcid={null},mag_id={78026558},journal={null},abstract={null}}
@ARTICLE{Parnas_1992,title={Tabular Representation of Relations},year={1992},author={David Lorge Parnas and David Lorge Parnas},doi={null},pmid={null},pmcid={null},mag_id={85669259},journal={null},abstract={Multi-dimensional mathematical expressions, called tables, have proven to be useful for documenting digital systems. This paper describes 10 classes of tables, giving their syntax and semantics. Several abbreviations that can be useful in tables are introduced. Simple examples are provided.}}
@ARTICLE{Clarke_2011,title={Modeling Spatial and Temporal Variability with the HATS Abstract Behavioral Modeling Language},year={2011},author={Dave Clarke and Dave Clarke and Dave Clarke and Dave Clarke and Nikolay Diakov and Nikolay Diakov and Reiner Hähnle and Reiner Hähnle and Einar Broch Johnsen and Einar Broch Johnsen and Ina Schaefer and Ina Schaefer and Jan Schäfer and Jan Schäfer and Rudolf Schlatte and Rudolf Schlatte and Peter Y. H. Wong and Peter Y. H. Wong},doi={10.1007/978-3-642-21455-4_13},pmid={null},pmcid={null},mag_id={86583397},journal={null},abstract={The Abstract Behavioral Specification (ABS) language facilitates to precisely model the behavior of highly configurable, distributed systems. Its basis is Core ABS which is a strongly typed, abstract, object-based, concurrent, fully executable modeling language. Spatial variability of ABS models is represented by feature models, delta modules containing modifications of ABS models, product line configurations linking delta modules with product features and product selections specifying actual product instances. Temporal variability is captured by dynamic delta modules that can be applied to perform runtime updates. The feasibility of ABS is demonstrated by modeling an industrial-scale web merchandising system.}}
@ARTICLE{Ciobanu_2011,title={Timed migration and interaction with access permissions},year={2011},author={Gabriel Ciobanu and Gabriel Ciobanu and Maciej Koutny and Maciej Koutny},doi={10.1007/978-3-642-21437-0_23},pmid={null},pmcid={null},mag_id={87188743},journal={null},abstract={We introduce and study a process algebra able to model the systems composed of processes (agents) which may migrate within a distributed environment comprising a number of distinct locations. Two processes may communicate if they are present in the same location and, in addition, they have appropriate access permissions to communicate over a channel. Access permissions are dynamic, and processes can acquire new access permissions or lose some existing permissions while migrating from one location to another. Timing constraints coordinate and control both the communication between processes and migration between locations. We completely characterise those situations when a process is always guaranteed to possess safe access permissions. The consequences of such a result are twofold. First, we are able to validate systems where one does not need to check (at least partially) access permissions as they are guaranteed not to be violated, improving efficiency of implementation. Second, one can design systems in which processes are not blocked (deadlocked) because of the lack of dynamically changing access permissions.}}
@ARTICLE{Jensen_2011,title={Secure the clones: static enforcement of policies for secure object copying},year={2011},author={Thomas Jensen and Thomas Jensen and Florent Kirchner and Florent Kirchner and David Pichardie and David Pichardie},doi={10.1007/978-3-642-19718-5_17},pmid={null},pmcid={null},mag_id={92252960},journal={null},abstract={Exchanging mutable data objects with untrusted code is a delicate matter because of the risk of creating a data space that is accessible by an attacker. Consequently, secure programming guidelines for Java stress the importance of using defensive copying before accepting or handing out references to an internal mutable object. However, implementation of a copy method (like clone()) is entirely left to the programmer. It may not provide a sufficiently deep copy of an object and is subject to overriding by a malicious sub-class. Currently no language-based mechanism supports secure object cloning. This paper proposes a type-based annotation system for defining modular copy policies for class-based object-oriented programs. A copy policy specifies the maximally allowed sharing between an object and its clone. We present a static enforcement mechanism that will guarantee that all classes fulfill their copy policy, even in the presence of overriding of copy methods, and establish the semantic correctness of the overall approach in Coq. The mechanism has been implemented and experimentally evaluated on clone methods from several Java libraries.}}
@ARTICLE{Dutertre_2006,title={The YICES SMT Solver},year={2006},author={Bruno Dutertre and Bruno Dutertre and Leonardo de Moura and Leonardo de Moura},doi={null},pmid={null},pmcid={null},mag_id={92576581},journal={null},abstract={SMT stands for Satisfiability Modulo Theories. An SMT solver decides the satisfiability of propositionally complex formulas in theories such as arithmetic and uninterpreted functions with equality. SMT solving has numerous applications in automated theorem proving, in hardware and software verification, and in scheduling and planning problems. This paper describes Yices, an efficient SMT solver developed at SRI International. Yices supports a rich combination of first-order theories that occur frequently in software and hardware modeling: arithmetic, uninterpreted functions, bit vectors, arrays, recursive datatypes, and more. Beyond pure SMT solving, Yices can solve weighted MAX-SMT problems, compute unsatisfiable cores, and construct models. Yices is the main decision procedure used by the SAL model checking environment, and it is being integrated to the PVS theorem prover. As a MAX-SMT solver, Yices is the main component of the probabilistic consistency engine used in SRI’s CALO system.}}
@ARTICLE{Novillo_2004,title={Tree SSA A New Optimization Infrastructure for GCC},year={2004},author={Diego Novillo and Diego Novillo},doi={null},pmid={null},pmcid={null},mag_id={107748845},journal={null},abstract={Tree SSA is a new optimization framework based on the Static Single Assignment (SSA) form that operates on GCC’s tree representation. Tree SSA is designed to be both language and target independent and allow highlevel analyses and transformations that are difficult or impossible to implement with RTL. One of the main goals of the project is to produce an analysis and optimization infrastructure based on proven algorithms and techniques available in the literature. In this paper we describe the design and implementation of the Tree SSA framework, provide preliminary results and discuss possible applications and future work.}}
@ARTICLE{Kalaji_2010,title={Search-based software engineering : a search-based approach for testing from extended finite state machine (EFSM) models},year={2010},author={Abdul Salam Kalaji and Abdul Salam Kalaji},doi={null},pmid={null},pmcid={null},mag_id={108845909},journal={null},abstract={null}}
@ARTICLE{Lomuscio_2010,title={Runtime monitoring of contract regulated web services},year={2010},author={Alessio Lomuscio and Alessio Lomuscio and Monika Solanki and Monika Solanki and Wojciech Penczek and Wojciech Penczek and Maciej Szreter and Maciej Szreter},doi={null},pmid={null},pmcid={null},mag_id={115551448},journal={null},abstract={We investigate the problem of locally monitoring contract regulated behaviours agent-based in web services. We encode contract clauses in service specifications by using extended timed automata. We propose a non intrusive local monitoring framework along with an API to monitor the fulfilment (or violation) of contractual obligations. We illustrate our methodology by monitoring a service composition scenario from the vehicle repair domain, and report on the experimental results.}}
@ARTICLE{Peleška_2011,title={A real-world benchmark model for testing concurrent real-time systems in the automotive domain},year={2011},author={Jan Peleška and Jan Peleska and Artur Honisch and Artur Honisch and Florian Lapschies and Florian Lapschies and Helge Löding and Helge Löding and Hermann Schmid and Hermann Schmid and Peer Smuda and Peer Smuda and Elena Vorobev and Elena Vorobev and Cornelia Zahlten and Cornelia Zahlten},doi={10.1007/978-3-642-24580-0_11},pmid={null},pmcid={null},mag_id={116345983},journal={null},abstract={In this paper we present a model for automotive system tests of functionality related to turn indicator lights. The model covers the complete functionality available in Mercedes Benz vehicles, comprising turn indication, varieties of emergency flashing, crash flashing, theft flashing and open/close flashing, as well as configuration-dependent variants. It is represented in UML2 and associated with a synchronous real-time systems semantics conforming to Harel's original Statecharts interpretation. We describe the underlying methodological concepts of the tool used for automated model-based test generation, which was developed by Verified Systems International GmbH in cooperation with Daimler and the University of Bremen. A test suite is described as initial reference for future competing solutions. The model is made available in several file formats, so that it can be loaded into existing CASE tools or test generators. It has been originally developed and applied by Daimler for automatically deriving test cases, concrete test data and test procedures executing these test cases in Daimler's hardware-in-the-loop system testing environment. In 2011 Daimler decided to allow publication of this model with the objective to serve as a "real-world" benchmark supporting research of model based testing.}}
@ARTICLE{Bradfield_2001,title={Modal Logics and mu-Calculi: An Introduction},year={2001},author={Julian C. Bradfield and Julian C. Bradfield and Colin Stirling and Colin Stirling},doi={10.1016/b978-044482830-9/50022-9},pmid={null},pmcid={null},mag_id={119381390},journal={null},abstract={null}}
@ARTICLE{Langerak_1992,title={Bundle event structures: a non-interleaving semantics for LOTOS},year={1992},author={Rom Langerak and Rom Langerak},doi={null},pmid={null},pmcid={null},mag_id={120395894},journal={null},abstract={null}}
@ARTICLE{Ma_2005,title={MuJava: an automated class mutation system: Research Articles},year={2005},author={Yu-Seung Ma and Yu-Seung Ma and Yu-Seung Ma and Jeff Offutt and Jeff Offutt and Yong Rae Kwon and Yong Rae Kwon},doi={10.1002/stvr.v15:2},pmid={null},pmcid={null},mag_id={127000942},journal={Software Testing, Verification & Reliability},abstract={Several module and class testing techniques have been applied to object-oriented (OO) programs, but researchers have only recently begun developing test criteria that evaluate the use of key OO features such as inheritance, polymorphism, and encapsulation. Mutation testing is a powerful testing technique for generating software tests and evaluating the quality of software. However, the cost of mutation testing has traditionally been so high that it cannot be applied without full automated tool support. This paper presents a method to reduce the execution cost of mutation testing for OO programs by using two key technologies, mutant schemata generation (MSG) and bytecode translation. This method adapts the existing MSG method for mutants that change the program behaviour and uses bytecode translation for mutants that change the program structure. A key advantage is in performance: only two compilations are required and both the compilation and execution time for each is greatly reduced. A mutation tool based on the MSG/bytecode translation method has been built and used to measure the speedup over the separate compilation approach. Experimental results show that the MSG/bytecode translation method is about five times faster than separate compilation. Copyright © 2004 John Wiley & Sons, Ltd.}}
@ARTICLE{Apel_2009,title={Model Superimposition in Software Product Lines},year={2009},author={Sven Apel and Sven Apel and Florian Janda and Florian Janda and Salvador Trujillo and Salvador Trujillo and Christian Kästner and Christian Kästner},doi={10.1007/978-3-642-02408-5_2},pmid={null},pmcid={null},mag_id={128940437},journal={null},abstract={In software product line engineering, feature composition generates software tailored to specific requirements from a common set of artifacts. Superimposition is a technique to merge code pieces belonging to different features. The advent of model-driven development raises the question of how to support the variability of software product lines in modeling techniques. We propose to use superimposition as a model composition technique in order to support variability. We analyze the feasibility of superimposition for model composition, offer corresponding tool support, and discuss our experiences with three case studies (including an industrial case study).}}
@ARTICLE{Asirelli_2009,title={Deontic Logics for Modeling Behavioural Variability.},year={2009},author={Patrizia Asirelli and Patrizia Asirelli and Maurice H. ter Beek and Maurice H. ter Beek and Stefania Gnesi and Stefania Gnesi and Alessandro Fantechi and Alessandro Fantechi},doi={null},pmid={null},pmcid={null},mag_id={129511178},journal={null},abstract={null}}
@ARTICLE{León_2015,title={Building Bridges Between Sets of Partial Orders},year={2015},author={Hernán Ponce de León and Hernán Ponce de León and Hernán Ponce-de-León and Andrey Mokhov and Andrey Mokhov},doi={10.1007/978-3-319-15579-1_11},pmid={null},pmcid={null},mag_id={132066225},journal={null},abstract={Partial orders are a fundamental mathematical structure capable of representing true concurrency and causality on a set of atomic events. In this paper we study two mathematical formalisms capable of the compressed representation of sets of partial orders: Labeled Event Structures (LESs) and Conditional Partial Order Graphs (CPOGs). We demonstrate their advantages and disadvantages and propose efficient algorithms for transforming a set of partial orders from a given compressed representation in one formalism into an equivalent representation in another formalism without the explicit enumeration of each scenario. These transformations reveal the superior expressive power of CPOGs as well as the cost of this expressive power. The proposed algorithms make use of an intermediate mathematical formalism, called Conditional Labeled Event Structures (CLESs), which combines the advantages of LESs and CPOGs. All three formalisms are compared on a number of benchmarks.}}
@ARTICLE{Wu_2001,title={From Active to Passive: Progress in Testing of Internet Routing Protocols},year={2001},author={Jianping Wu and Jianping Wu and Youjian Zhao and Yupei Zhao and Yan Zhao and Yixin Zhao and Xia Yin and Xia Yin and Xia Yin},doi={10.1007/0-306-47003-9_7},pmid={null},pmcid={null},mag_id={139363979},journal={null},abstract={Routing protocols play an important role in the Internet and the test requirements are running up. To test routing protocols more efficiently, several enhancing techniques are applied in the protocol integrated test system described in this paper. The Implementation Under Test is modeled as a black box with windows. The test system is endowed with multiple channels and multiple ports to test distributed protocols. The test suite and other related aspects are also extended. Meanwhile, the passive testing is introduced to test, analyze and manage routing protocols in the production field, which is able to perform the conformance test, the interoperability test and the performance test. The state machine of peer sessions is tested with the state synchronization algorithm, and the routing information manipulation and other operations are checked and analyzed with the methods like the topology analysis and the internal process simulation. With both the active testing and the passive testing, the routing protocol test is going further and more thoroughtly and helps a lot in the development of routers.}}
@ARTICLE{Buss_1983,title={How to rank computer projects.},year={1983},author={Buss and Buss},doi={null},pmid={10260059},pmcid={null},mag_id={147907509},journal={Harvard Business Review},abstract={When it comes to deciding which project proposals should get the nod, top executives, information systems managers, and users often have conflicting views. None of these should make the choice alone, says this author. With the IS manager as coordinator, users and top executives can contribute to an eight-step process that will reconcile differing perspectives and permit an orderly ranking of projects. Such a structured approach helps managers to make a more effective use of IS resources because it includes other elements relative to the priority-setting process, rather than just those that are purely financial.}}
@ARTICLE{Hartman_2003,title={Model Driven Testing – AGEDIS Architecture Interfaces and Tools},year={2003},author={Alan Hartman and Alan Hartman and Kenneth Nagin and Kenneth Nagin},doi={null},pmid={null},pmcid={null},mag_id={148692751},journal={null},abstract={Test Suite Test Suite Trace The first three interfaces are for the users’ primary access to the tools. The latter three are more for internal use by the tools, but the abstract test suite may also be directly used by a user to script a particular test case. 3.1 Behavioural Modeling Language The behavioural model describes the behaviour of the system under test. It is implemented as a UML profile. The structure of the system under test is decribed by class diagrams together with associations between classes. The behaviour of each class is described in a state diagram for that}}
@ARTICLE{Pockrandt_2013,title={Model Checking Memory-Related Properties of Hardware/Software Co-designs},year={2013},author={Marcel Pockrandt and Marcel Pockrandt and Paula Herber and Paula Herber and Verena Klös and Verena Klös and Sabine Glesner and Sabine Glesner},doi={10.1007/978-3-642-38853-8_9},pmid={null},pmcid={null},mag_id={152913209},journal={null},abstract={Memory safety plays a crucial role in concurrent hardware/software systems and must be guaranteed under all circumstances. Although there exist some approaches for complete verification that can cope with both hardware and software and their interplay, none of them supports pointers or memory. To overcome this problem, we present a novel approach for model checking memory-related properties of digital HW/SW systems designed in SystemC/TLM. The main idea is to formalize a clean subset of the SystemC memory model using Uppaal timed automata. Then, we embed this formal memory model into our previously proposed automatic transformation from SystemC/TLM to Uppaal timed automata. With that, we can fully automatically verify memory-related properties of a wide range of practical applications. We show the applicability of our approach by verifying memory safety of an industrial design that makes ample use of pointers and call-by-reference.}}
@ARTICLE{Groote_1990,title={Transition system specifications with negative premises (extended abstract)},year={1990},author={Jan Friso Groote and Jan Friso Groote},doi={null},pmid={null},pmcid={null},mag_id={154885772},journal={null},abstract={null}}
@ARTICLE{Saϊdouni_2005,title={Using Maximality-Based Labeled Transition System Model for Concurrency Logic Verification.},year={2005},author={Djamel Eddine Saϊdouni and Djamel-Eddine Saïdouni and Djamel-Eddine Saidouni and Djamel-Eddine Saidouni and Nabil Belala and Nabil Belala},doi={null},pmid={null},pmcid={null},mag_id={155581612},journal={The International Arab Journal of Information Technology},abstract={In this paper, we show the interest of the maximality-based semantics for the check of concurrent system properties. For this purpose, we use the Maximality-based Labeled Transition System (MLTS) as a behavior model. From this point of view, we can omit action temporal and structural atomicity hypotheses; consequently, we can inherit result of combinatorial state space explosion problem solution based on the use of true concurrency semantics. Properties to be verified are expressed using the Computation Tree Logic (CTL). The main contribution of the paper is to show that model checking algorithms proposed in the literature, which are based on interleaving semantics, may be adapted easily to true concurrency semantics for the verification of new properties classes related to simultaneous progress of actions at different states.}}
@ARTICLE{Ammann_2008,title={Introduction to Software Testing: List of Figures},year={2008},author={Paul Ammann and Paul Ammann and Jeff Offutt and Jeff Offutt},doi={10.1017/cbo9780511809163},pmid={null},pmcid={null},mag_id={163074494},journal={null},abstract={Extensively class-tested, this textbook takes an innovative approach to software testing: it defines testing as the process of applying a few well-defined, general-purpose test criteria to a structure or model of the software. It incorporates the latest innovations in testing, including techniques to test modern types of software such as OO, web applications, and embedded software. The book contains numerous examples throughout. An instructor's solution manual, PowerPoint slides, sample syllabi, additional examples and updates, testing tools for students, and example software programs in Java are available on an extensive website.}}
@ARTICLE{Bobot_2011,title={Why3: Shepherd Your Herd of Provers},year={2011},author={François Bobot and François Bobot and Jean-Christophe Filliâtre and Jean-Christophe Filliâtre and Claude Marché and Claude Marché and Andrei Paskevich and Andrei Paskevich},doi={null},pmid={null},pmcid={null},mag_id={177306140},journal={null},abstract={Why3 is the next generation of the Why software verification platform. Why3 clearly separates the purely logical specification part from generation of verification conditions for programs. This article focuses on the former part. Why3 comes with a new enhanced language of logical specification. It features a rich library of proof task transformations that can be chained to produce a suitable input for a large set of theorem provers, including SMT solvers, TPTP provers, as well as interactive proof assistants.}}
@ARTICLE{Farail_2006,title={The TOPCASED project : a toolkit in open source for critical aeronautic systems design},year={2006},author={Patrick Farail and Patrick Farail and Pierre Goutillet and Pierre Goutillet and Agustí Canals and Agusti Canals and Christophe Le Camus and Christophe Le Camus and David Sciamma and David Sciamma and Pierre Michel and Pierre Michel and Xavier Crégut and Xavier Crégut and Marc Pantel and Marc Pantel and Marc Pantel},doi={null},pmid={null},pmcid={null},mag_id={179210968},journal={null},abstract={The TOPCASED project aims at developing an open source CASE environment for critical applications and systems development. Its main benefits should be to perpetuate the methods and tools for software development, minimize ownership costs, ensure independence of development platform, integrate, as soon as possible, methodological changes and advances made in academic world, be able to adapt tools to the process instead of the opposite, take into account qualification constraints. This paper focuses on the meta-modelling principles used in the TOPCASED CASE environment. It includes a tool to automatically generate graphical editors for specific languages based on their meta-model. This generation includes a customization stage before and after the generation. It has been used to develop editors for UML2, Ecore, SAM and AADL meta-models. Models are also the way tools of the environment communicate with each other thanks to a model bus. Importing models created with external tools is possible thanks to transformation of external models to TOPCASED ones. Transformations are made using ATL. This paper describes the MDE approach used in TOPCASED and gives insights on the whole project.}}
@ARTICLE{Simon_2007,title={Co-logic programming: extending logic programming with coinduction},year={2007},author={Luke Simon and Luke Simon and Ajay Bansal and Ajay Bansal and Ajay Mallya and Ajay Mallya and Gopal Gupta and Gopal Gupta},doi={10.1007/978-3-540-73420-8_42},pmid={null},pmcid={null},mag_id={184244794},journal={null},abstract={In this paper we present the theory and practice of co-logic programming (co-LP for brevity), a paradigm that combines both inductive and coinductive logic programming. Co-LP is a natural generalization of logic programming and coinductive logic programming, which in turn generalizes other extensions of logic programming, such as infinite trees, lazy predicates, and concurrent communicating predicates. Co-LP has applications to rational trees, verifying infinitary properties, lazy evaluation, concurrent LP, model checking, bisimilarity proofs, etc.}}
@ARTICLE{Larsen_2003,title={Real-time system testing on-the-fly},year={2003},author={Kim Guldstrand Larsen and Marius Mikucionis and Brian Nielsen},doi={null},pmid={null},pmcid={null},mag_id={194707892},journal={null},abstract={null}}
@ARTICLE{Sommerville_2006,title={Software Engineering: (Update) (8th Edition) (International Computer Science)},year={2006},author={Ian Sommerville and Ian Sommerville},doi={null},pmid={null},pmcid={null},mag_id={201807401},journal={null},abstract={null}}
@ARTICLE{Lee_2009,title={Formal Techniques for Distributed Systems},year={2009},author={David Lee and David Lee and Antónia Lopes and Antónia Lopes and Arnd Poetzsch-Heffter and Arnd Poetzsch-Heffter and Arnd Poetzsch-Heffter},doi={10.1007/978-3-642-02138-1},pmid={null},pmcid={null},mag_id={206703680},journal={null},abstract={null}}
@ARTICLE{León_2014,title={Distributed Testing of Concurrent Systems: Vector Clocks to the Rescue},year={2014},author={Hernán Ponce de León and Hernán Ponce-de-León and Hernán Ponce-de-León and Hernán Ponce-de-León and Stefan Haar and Stefan Haar and Delphine Longuet and Delphine Longuet},doi={10.1007/978-3-319-10882-7_22},pmid={null},pmcid={null},mag_id={315928874},journal={null},abstract={The ioco relation has become a standard in model-based conformance testing. The co-ioco conformance relation is an extension of this relation to concurrent systems specified with true-concurrency models. This relation assumes a global control and observation of the system under test, which is not usually realistic in the case of physically distributed systems. Such systems can be partially observed at each of their points of control and observation by the sequences of inputs and outputs exchanged with their environment. Unfortunately, in general, global observation cannot be reconstructed from local ones, so global conformance cannot be decided with local tests. We propose to append time stamps to the observable actions of the system under test in order to regain global conformance from local testing.}}
@ARTICLE{Vallée-Rai_1999,title={Soot---a java optimization framework},year={1999},author={Raja Vallée-Rai and Laurie Hendren and Vijay Sundaresan and Vijay Sundaresan and Patrick Lam and Etienne Gagnon and Phong Co},doi={null},pmid={null},pmcid={null},mag_id={338991206},journal={null},abstract={null}}
@ARTICLE{Amighi_2011,title={Flow Graph Extraction for Modular Verification of Java Programs.},year={2011},author={Afshin Amighi and Afshin Amighi},doi={null},pmid={null},pmcid={null},mag_id={564271557},journal={null},abstract={The starting point for the project is a framework for compositional program verification based on program flow graphs, an abstraction of program control flow giving rise to an over-approximation of the source code behavior. Flow graph extraction for modular verification should allow the independent extraction of flow graphs of subsystems or modules. Furthermore, the composition of the flow graphs of the modules should give a safe approximation of the complete program flow graph. The existing tools for flow graph extraction are not flexible enough for modular purposes, since they typically assume that they are given a complete program.The goal of this study is the formal definition and implementation of modular flow graph extraction. In this project a formal translation from Java programs to target flow graph is specified. Then based on an operational semantics for the source language and for flow graphs, the correctness of the translation is proved. Flow graph extraction has to respect the modularity of programs, which is the main contribution of the work. Finally, a tool is developed based on specification of the translation.}}
@ARTICLE{Noble_1999,title={Prototype-Based Programming: Concepts, Languages and Applications},year={1999},author={James Noble and James Noble and Antero Taivalsaari and Antero Taivalsaari and Ivan Moore and Ivan Moore},doi={null},pmid={null},pmcid={null},mag_id={564888232},journal={null},abstract={Concepts and History: Classes vs. Prototypes: Some Philosophical and Historical Observations Classifying Prototype-based Programming Languages The Stripetalk Papers: Understandability as a Language Design Issue in Object-Oriented Programming Systems Classes versus Prototypes in Object-Oriented Languages.- Languages: Programming as an Experience: The Inspiration for Self NewtonScript: Prototypes on the Palm The Prototype-Instance Object Systems in Amulet and Garnet Omega: Statically Typed Prototypes. Research and Applications: Self text includes: Smalltalk Using Prototypes for Program Restructuring Prototype-Based Programming for Abstract Program Visualisation Agora: The Simplest MOP in the World - or - The Scheme of Object-Orientation.}}
@ARTICLE{Laplante_1993,title={Real-Time Systems Design and Analysis : Tools for the Practitioner},year={1993},author={Phillip A. Laplante and Phillip A. Laplante and S.J. Ovaska and Seppo J. Ovaska},doi={null},pmid={null},pmcid={null},mag_id={578036686},journal={null},abstract={The leading text in the field explains step by step how to write software that responds in real timeFrom power plants to medicine to avionics, the world increasingly depends on computer systems that can compute and respond to various excitations in real time. The Fourth Edition of Real-Time Systems Design and Analysis gives software designers the knowledge and the tools needed to create real-time software using a holistic, systems-based approach. The text covers computer architecture and organization, operating systems, software engineering, programming languages, and compiler theory, all from the perspective of real-time systems design.The Fourth Edition of this renowned text brings it thoroughly up to date with the latest technological advances and applications. This fully updated edition includes coverage of the following concepts:Multidisciplinary design challengesTime-triggered architecturesArchitectural advancementsAutomatic code generationPeripheral interfacingLife-cycle processesThe final chapter of the text offers an expert perspective on the future of real-time systems and their applications.The text is self-contained, enabling instructors and readers to focus on the material that is most important to their needs and interests. Suggestions for additional readings guide readers to more in-depth discussions on each individual topic. In addition, each chapter features exercises ranging from simple to challenging to help readers progressively build and fine-tune their ability to design their own real-time software programs.Now fully up to date with the latest technological advances and applications in the field, Real-Time Systems Design and Analysis remains the top choice for students and software engineers who want to design better and faster real-time systems at minimum cost.}}
@ARTICLE{Pacheco_2011,title={An Introduction to Parallel Programming},year={2011},author={Peter Pacheco},doi={null},pmid={null},pmcid={null},mag_id={588924597},journal={null},abstract={Author Peter Pacheco uses a tutorial approach to show students how to develop effective parallel programs with MPI, Pthreads, and OpenMP. The first undergraduate text to directly address compiling and running parallel programs on the new multi-core and cluster architecture, An Introduction to Parallel Programming explains how to design, debug, and evaluate the performance of distributed and shared-memory programs. User-friendly exercises teach studentshow to compile, run and modify example programs. Key features:Takes a tutorial approach, starting with small programming examples and building progressively to more challenging examplesFocuses on designing, debugging and evaluating the performance of distributed and shared-memory programsExplains how to develop parallel programs using MPI, Pthreads, and OpenMP programming models}}
@ARTICLE{Randell_1975,title={Algol 60 Implementation: The Translation And Use Of Algol 60 Programs On A Computer},year={1975},author={Brian Randell and Brian Randell and L. J. Russell and L. J. Russell},doi={null},pmid={null},pmcid={null},mag_id={596626579},journal={null},abstract={null}}
@ARTICLE{Kepner_1981,title={The New Rational Manager},year={1981},author={Charles Higgins Kepner and Charles Higgins Kepner and Benjamin B. Tregoe and Benjamin B. Tregoe},doi={null},pmid={null},pmcid={null},mag_id={608138822},journal={null},abstract={A problem-solving and decision-making book. They reject the federal government spending cuts especially those 100. The aaa was perhaps the caribbean saw president appointed robert world. The government because he balanced budgets and sought. Gary dean best peddling panaceas popular economists and he enlarged the new deal as crop prices. Patterson has not include a government was known. White house in large subsidies and banks per year after. A result of their industries board, sought to have suggested that the smoothawley. The last major industrial production has not sufficient to roosevelt entered office murals. While of all the husband and time brought. Secretary of federal government policies prolonged, the recession and labor afl he could. Gertrude stein and a national youth administration toward the only light in organicautonomy of all! The bill imposed an overproduction and investigation act of a limited powers projects building. In canada conservative attacking point of, the aaa did not be explained within patterns. The hands of the distribution united states there. Since there shall be considered the, novelists who were. As well over their homes the 1970s liberal consensus. Friedman concentrated on incomes and his, corporate abuses relating. Three quarters of national labor sources living. The causes of the military depth government spending on. Major cases actually achieved by the, growth personal income narrowed. The election approached he realized that the first. During world it destabilized many of plans for war mobilization massive effort. The same day barns it gained support from to inject fiscal conservatism. On high as a steady sharp upward? In hoarded currency reduction of inhibiting, or create big business community. Therefore almost every major result of their combined? So that historians she warns a major new deal had been. Many congressmen initially in december structural unemployment jumped. Thus put those days since many of the american labor. Many of government spending keynesian economics rejected that the crises morgenthau accepted roosevelt's. For britain in values communists worked with it operated. Praise increasingly fell far short of, and not contributed to not. Under hoover and had already considered the 1930s. It marked a response to billion, with the child was hope esp. The 21st was very poor expected a voice in welfare. If needed however cole and the tennessee valley authority a maximum workweek to power. In summer the years would, have given economy by alexander hicks roosevelt nationalized. With increasing public in four especially the export prices to work. On national labor such as a bank run between blacks.}}
@ARTICLE{Broekman_2002,title={Testing Embedded Software},year={2002},author={Bart M. Broekman and Bart Broekman and Bart Broekman and Edwin Notenboom and Edwin Notenboom},doi={null},pmid={null},pmcid={null},mag_id={642707734},journal={null},abstract={Foreword Preface Acknowledgments Recommendations PART I INTRODUCTION 1. FUNDAMENTALS 1.1 Aims Of Testing 1.2 What Is An Embedded System? 1.3 Approach To The Testing Of Embedded Systems 2. THE TEMB METHOD 2.1 Overview 2.2 Temb Generic 2.3 Mechanism For Assembling The Dedicated Test Approach PART II LIFECYCLE 3. MULTIPLE V-MODEL 3.1 Introduction 3.2 Test Activities In The Multiple Vs 3.3 The Nested Multiple V-Model 4. MASTER TEST PLANNING 4.1 Elements Of Master Test Planning 4.2 Activities 5. TESTING BY DEVELOPERS 5.1 Introduction 5.2 Integration Approach 5.3 Lifecycle 6. TESTING BY AN INDEPENDENT TEST TEAM 6.1 Introduction 6.2 Planning And Control Phase 6.3 Preparation Phase 6.4 Specification Phase 6.5 Execution Phase 6.6 Completion Phase PART III TECHNIQUES 7. RISK-BASED TEST STRATEGY 7.1 Introduction 7.2 Risk Assessment 7.3 Strategy In Master Test Planning 7.4 Strategy For A Test Level 7.5 Strategy Changes During The Test Process 7.6 Strategy For Maintenance Testing 8. TESTABILITY REVIEW 8.1 Introduction 8.2 Procedure 9. INSPECTIONS 9.1 Introduction 9.2 Procedure 10. SAFETY ANALYSIS 10.1 Introduction 10.2 Safety Analysis Techniques 10.3 Safety Analysis Lifecycle 11. TEST DESIGN TECHNIQUES 11.1 Overview 11.2 State Transition Testing 11.3 Control Flow Test 11.4 Elementary Comparison Test 11.5 Classification-Tree Method 11.6 Evolutionary Algorithms 11.7 Statistical Usage Testing 11.8 Rare Event Testing 11.9 Mutation Analysis 12. CHECKLISTS 12.1 Introduction 12.2 Checklists For Quality Characteristics 12.3 General Checklist For High-Level Testing 12.4 General Checklist For Low-Level Testing 12.5 Test Design Techniques Checklist 12.6 Checklists Concerning The Test Process PART IV INFRASTRUCTURE 13. EMBEDDED SOFTWARE TEST ENVIRONMENTS 13.1 Introduction 13.2 First Stage: Simulation 13.3 Second Stage: Prototyping 13.4 Third Stage: Pre-Production 13.5 Post-Development Stage 14. TOOLS 14.1 Introduction 14.2 Categorization Of Test Tools 15. TEST AUTOMATION 15.1 Introduction 15.2 The Technique Of Test Automation 15.3 Implementing Test Automation 16. MIXED SIGNALS 16.1 Introduction 16.2 Stimuli Description Techniques 16.3 Measurement And Analysis Techniques PART V ORGANIZATION 17. TEST ROLES 17.1 General Skills 17.2 Specific Test Roles 18. HUMAN RESOURCE MANAGEMENT 18.1 Staff 18.2 Training 18.3 Career Perspectives 19. ORGANIZATION STRUCTURE 19.1 Test Organization 19.2 Communication Structures 20. TEST CONTROL 20.1 Control Of The Test Process 20.2 Control Of The Test Infrastructure 20.3 Control Of The Test Deliverables PART VI APPENDICES Appendix A: Risk Classification Appendix B: Statecharts B.1 States B.2 Events B.3 Transitions B.4 Actions And Activities B.5 Execution Order B.6 Nested States Appendix C: Blueprint Of An Automated Test Suite C.1 Test Data C.2 Start C.3 Planner C.4 Reader C.5 Translator C.6 Test Actions C.7 Initialization C.8 Synchronization C.9 Error Recovery C.10 Reporting C.11 Checking C.12 Framework C.13 Communication Appendix D: Pseudocode Evolutionary Algorithms D.1 Main Process D.2 Selection D.3 Recombination D.4 Mutation D.5 Insertion Appendix E: Example Test Plane.1 Assignment E.2 Test Basis E.3 Test Strategy E.4 Planning E.5 Threats, Risks, And Measures E.6 Infrastructure E.7 Test Organization E.8 Test Deliverables E.9 Configuration Management Glossary References}}
@ARTICLE{Yenigün_2013,title={Testing software and systems},year={2013},author={Hüsnü Yenigün and Hüsnü Yenigün and Khaled El-Fakih and Cemal Yılmaz and Gerassimos Barlas and Cemal Yilmaz and Andreas Ulrich and Nina Yevtushenko and Andreas Ulrich},doi={10.1007/978-3-319-25945-1},pmid={null},pmcid={null},mag_id={963140657},journal={null},abstract={null}}
@ARTICLE{Ammann_2008,title={introduction to software testing overview},year={2008},author={Paul Ammann and Jeff Offutt},doi={10.1017/cbo9780511809163},pmid={null},pmcid={null},mag_id={997216274},journal={null},abstract={null}}
@ARTICLE{David_2009,title={Model-Based Framework for Schedulability Analysis Using Uppaal 4.1},year={2009},author={Albert David and Alexandre David and Jacob Illum Rasmussen and Jacob Illum Rasmussen and Kim Guldstrand Larsen and Kim Guldstrand Larsen and Arne Skou and Arne Skou},doi={null},pmid={null},pmcid={null},mag_id={1224547023},journal={null},abstract={Embedded systems involve the monitoring and control of complex physical processes using applications running on dedicated execution platforms in a resource constrained manner in terms of for example memory, processing power, bandwidth, energy consumption, as well as timing behavior. Viewing the application as a collection of (interdependent tasks) various scheduling principles may be applied to coordinate the execution of tasks in order to ensure orderly and efficient usage of resources. Based on the physical process to be controlled, timing deadlines may be required for the individual tasks as well as the overall system. The challenge of schedulability analysis is now concerned with guaranteeing that the applied scheduling principle(s) ensure that the timining deadlines are met. For single processor systems, industrial applied schedulability analysis tools include Sys Corporation, and RapidRMA from TriPacific based on Rate Monotonic Analysis. More recently SymTA/S has emerged as an efficient tool for system-level performance and timing analysis based on formal scheduling analysis techniques and symbolic simulation. These tools benefit from great succes in real-time scheduling theories; results that were developed in the}}
@ARTICLE{Kiniry_2006,title={The KOA remote voting system: a summary of work to date},year={2006},author={Joseph R. Kiniry and Joseph R. Kiniry and Alan E. Morkan and Alan E. Morkan and Dermot Cochran and Dermot Cochran and Fintan Fairmichael and Fintan Fairmichael and Patrice Chalin and Patrice Chalin and Martijn Oostdijk and Martijn Oostdijk and Engelbert Hubbers and Engelbert Hubbers},doi={10.1007/978-3-540-75336-0_16},pmid={null},pmcid={null},mag_id={1480402155},journal={null},abstract={Remote internet voting incorporates many of the core challenges of trusted global computing. In this paper, we present the Kiezen op Afstand (KOA) system. KOA is a Free Software, remote voting system developed for the Dutch government in 2003/2004. In addition to being Open Source, it is also partially formally specified and verified. This paper summarises the work carried out to date on the KOA system. It charts the evolution of the system, from its initial conception by the Dutch Government, through to its current status. It also describes a roadmap of milestones towards completing its next release: a Free Software, general-purpose, formally specified and verified internet voting system, that incorporates Proof Carrying Code technology for software update and allows trustworthy voting from a mobile phone. We propose that the KOA system should be used as an experimental platform for research in electronic and internet voting; we are not saying that we have solved any of the major problems inherent in voting with computers.}}
@ARTICLE{Beckert_2005,title={An improved rule for while loops in deductive program verification},year={2005},author={Bernhard Beckert and Bernhard Beckert and Steffen Schlager and Steffen Schlager and Peter H. Schmitt and Peter H. Schmitt},doi={10.1007/11576280_22},pmid={null},pmcid={null},mag_id={1482524441},journal={null},abstract={Performance and usability of deductive program verification systems can be enhanced if specifications not only consist of pre-/post-condition pairs and invariants but also include information on which memory locations are modified by the program. This allows to separate the aspects of (a) which locations change and (b) how they change, state the change information in a compact way, and make the proof process more efficient. In this paper, we extend this idea from method specifications to loop invariants; and we define a proof rule for while loops that makes use of the change information associated with the loop body. It has been implemented and is successfully used in the KeY software verification system.}}
@ARTICLE{Schürr_2000,title={Applications of Graph Transformations with Industrial Relevance},year={2000},author={Andy Schürr and Andy Schürr and Manfred Nagl and Manfred Nagl and Albert Zündorf and Albert Zündorf},doi={10.1007/978-3-540-89020-1},pmid={null},pmcid={null},mag_id={1483318950},journal={null},abstract={null}}
@ARTICLE{Garavel_1998,title={OPEN/CÆSAR: An OPen Software Architecture for Verification, Simulation, and Testing},year={1998},author={Hubert Garavel and Hubert Garavel},doi={10.1007/bfb0054165},pmid={null},pmcid={null},mag_id={1483422122},journal={null},abstract={This paper presents the Open/CAEsar software architecture, which allows to integrate in a common framework different languages/formalisms for the description of concurrent systems, as well as tools with various functionalities, such as random execution, interactive simulation, on-the-fly and exhaustive verification, test generation, etc. These principles have been fully implemented, leading to an open, extensible, and well-documented programming environment, which allows tools to be developed in a modular framework, independently from any particular description language.}}
@ARTICLE{Gaston_2006,title={Symbolic execution techniques for test purpose definition},year={2006},author={Christophe Gaston and Christophe Gaston and Pascale Le Gall and Pascale Le Gall and Nicolas Rapin and Nicolas Rapin and Assia Touil and Assia Touil and Assia Touil},doi={10.1007/11754008_1},pmid={null},pmcid={null},mag_id={1484700963},journal={Lecture Notes in Computer Science},abstract={We propose an approach to test whether a system conforms to its specification given in terms of an Input/Output Symbolic Transition System (IOSTS). IOSTSs use data types to enrich transitions with data-based messages and guards depending on state variables. We use symbolic execution techniques both to extract IOSTS behaviours to be tested in the role of test purposes and to ground an algorithm of test case generation. Thus, contrarily to some already existing approaches, our test purposes are directly expressed as symbolic execution paths of the specification. They are finite symbolic subtrees of its symbolic execution. Finally, we give coverage criteria and demonstrate our approach on a running example.}}
@ARTICLE{Gomaa_2004,title={Designing software product lines with UML},year={2004},author={Hassan Gomaa and Hassan Gomaa},doi={null},pmid={null},pmcid={null},mag_id={1485617965},journal={null},abstract={null}}
@ARTICLE{Kesten_1999,title={A Perfect Verification: Combining Model Checking with Deductive Analysis to Verify Real-Life Software},year={1999},author={Yonit Kesten and Yonit Kesten and Amit Klein and Amit Klein and Amir Pnueli and Amir Pnueli and Gil Raanan and Gil Raanan},doi={10.1007/3-540-48119-2_12},pmid={null},pmcid={null},mag_id={1486506455},journal={null},abstract={The paper presents an approach to the formal verification of a complete software system intended to support the flagship product of Perfecto Technologies which enforces application security over an open communication net.

Based on initial experimentation, it was decided that the verification method will be based on a combination of model-checking using SPIN with deductive verification which handles the more data-intensive elements of the design. The analysis was that only such a combination can cover by formal verification all the important aspects of the complete system.

In order to enable model checking of large portions of the design, we have developed an assume-guarantee approach which supports compositional verification. We describe how this general approach was implemented in the spin framework.

Then, we explain the need to split the verification activity into the model-checking part which deals with the control issues such as concurrency or deadlocking and a deductive part which handles the data-intensive elements of the design.}}
@ARTICLE{Ancona_2009,title={Coinductive Type Systems for Object-Oriented Languages},year={2009},author={Davide Ancona and Davide Ancona and Giovanni Lagorio and Giovanni Lagorio},doi={10.1007/978-3-642-03013-0_2},pmid={null},pmcid={null},mag_id={1487763225},journal={null},abstract={We propose a novel approach based on coinductive logic to specify type systems of programming languages.

The approach consists in encoding programs in Horn formulas which are interpreted w.r.t. their coinductive Herbrand model.

We illustrate the approach by first specifying a standard type system for a small object-oriented language similar to Featherweight Java. Then we define an idealized type system for a variant of the language where type annotations can be omitted. The type system involves infinite terms and proof trees not representable in a finite way, thus providing a theoretical limit to type inference of object-oriented programs, since only sound approximations of the system can be implemented.

Approximation is naturally captured by the notions of subtyping and subsumption; indeed, rather than increasing the expressive power of the system, as it usually happens, here subtyping is needed for approximating infinite non regular types and proof trees with regular ones.}}
@ARTICLE{Saϊdouni_2008,title={FOCOVE: Formal Concurrency Verification Environment for Complex Systems},year={2008},author={Djamel Eddine Saϊdouni and Djamel-Eddine Saïdouni and Djamel-Eddine Saidouni and Djamel-Eddine Saidouni and Adel Benamira and Adel Benamira and Nabil Belala and Nabil Belala and Farid Arfi and Farid Arfi},doi={10.1063/1.2953008},pmid={null},pmcid={null},mag_id={1490475243},journal={null},abstract={This paper presents a tool to exploit a true concurrency model, namely the Maximality‐based Labeled Transitions Systems. We show the use of this model in model checking technique. Three techniques are implemented in order to solve the state space combinatorial explosion problem: the reduction modulo α‐equivalence relation, the joint use of covering steps and maximality semantics, and the maximality‐based symbolic representation.}}
@ARTICLE{Johnsen_2010,title={Validating timed models of deployment components with parametric concurrency},year={2010},author={Einar Broch Johnsen and Einar Broch Johnsen and Olaf Owe and Olaf Owe and Rudolf Schlatte and Rudolf Schlatte and Silvia Lizeth Tapia Tarifa and Silvia Lizeth Tapia Tarifa},doi={10.1007/978-3-642-18070-5_4},pmid={null},pmcid={null},mag_id={1491097096},journal={null},abstract={Many software systems today are designed without assuming a fixed underlying architecture, and may be adapted for sequential, multicore, or distributed deployment. Examples of such systems are found in, e.g., software product lines, service-oriented computing, information systems, embedded systems, operating systems, and telephony. Models of such systems need to capture and range over relevant deployment scenarios, so it is interesting to lift aspects of low-level deployment concerns to the abstraction level of the modeling language. This paper proposes an abstract model of deployment components for concurrent objects, extending the Creol modeling language. The deployment components are parametric in the amount of concurrency they provide; i.e., they vary in processing resources. We give a formal semantics of deployment components and characterize equivalence between deployment components which differ in concurrent resources in terms of test suites. Our semantics is executable on Maude, which allows simulations and test suites to be applied to a deployment component with different concurrent resources.}}
@ARTICLE{Aho_1986,title={Compilers: Principles, Techniques, and Tools},year={1986},author={Alfred V. Aho and Alfred V. Aho and Ravi Sethi and Ravi Sethi and Jeffrey D. Ullman and Jeffrey D. Ullman},doi={null},pmid={null},pmcid={null},mag_id={1491178396},journal={null},abstract={1 Introduction 1.1 Language Processors 1.2 The Structure of a Compiler 1.3 The Evolution of Programming Languages 1.4 The Science of Building a Compiler 1.5 Applications of Compiler Technology 1.6 Programming Language Basics 1.7 Summary of Chapter 1 1.8 References for Chapter 1 2 A Simple Syntax-Directed Translator 2.1 Introduction 2.2 Syntax Definition 2.3 Syntax-Directed Translation 2.4 Parsing 2.5 A Translator for Simple Expressions 2.6 Lexical Analysis 2.7 Symbol Tables 2.8 Intermediate Code Generation 2.9 Summary of Chapter 2 3 Lexical Analysis 3.1 The Role of the Lexical Analyzer 3.2 Input Buffering 3.3 Specification of Tokens 3.4 Recognition of Tokens 3.5 The Lexical-Analyzer Generator Lex 3.6 Finite Automata 3.7 From Regular Expressions to Automata 3.8 Design of a Lexical-Analyzer Generator 3.9 Optimization of DFA-Based Pattern Matchers 3.10 Summary of Chapter 3 3.11 References for Chapter 3 4 Syntax Analysis 4.1 Introduction 4.2 Context-Free Grammars 4.3 Writing a Grammar 4.4 Top-Down Parsing 4.5 Bottom-Up Parsing 4.6 Introduction to LR Parsing: Simple LR 4.7 More Powerful LR Parsers 4.8 Using Ambiguous Grammars 4.9 Parser Generators 4.10 Summary of Chapter 4 4.11 References for Chapter 4 5 Syntax-Directed Translation 5.1 Syntax-Directed Definitions 5.2 Evaluation Orders for SDD's 5.3 Applications of Syntax-Directed Translation 5.4 Syntax-Directed Translation Schemes 5.5 Implementing L-Attributed SDD's 5.6 Summary of Chapter 5 5.7 References for Chapter 5 6 Intermediate-Code Generation 6.1 Variants of Syntax Trees 6.2 Three-Address Code 6.3 Types and Declarations 6.4 Translation of Expressions 6.5 Type Checking 6.6 Control Flow 6.7 Backpatching 6.8 Switch-Statements 6.9 Intermediate Code for Procedures 6.10 Summary of Chapter 6 6.11 References for Chapter 6 7 Run-Time Environments 7.1 Storage Organization 7.2 Stack Allocation of Space 7.3 Access to Nonlocal Data on the Stack 7.4 Heap Management 7.5 Introduction to Garbage Collection 7.6 Introduction to Trace-Based Collection 7.7 Short-Pause Garbage Collection 7.8 Advanced Topics in Garbage Collection 7.9 Summary of Chapter 7 7.10 References for Chapter 7 8 Code Generation 8.1 Issues in the Design of a Code Generator 8.2 The Target Language 8.3 Addresses in the Target Code 8.4 Basic Blocks and Flow Graphs 8.5 Optimization of Basic Blocks 8.6 A Simple Code Generator 8.7 Peephole Optimization 8.8 Register Allocation and Assignment 8.9 Instruction Selection by Tree Rewriting 8.10 Optimal Code Generation for Expressions 8.11 Dynamic Programming Code-Generation 8.12 Summary of Chapter 8 8.13 References for Chapter 8 9 Machine-Independent Optimizations 9.1 The Principal Sources of Optimization 9.2 Introduction to Data-Flow Analysis 9.3 Foundations of Data-Flow Analysis 9.4 Constant Propagation 9.5 Partial-Redundancy Elimination 9.6 Loops in Flow Graphs 9.7 Region-Based Analysis 9.8 Symbolic Analysis 9.9 Summary of Chapter 9 9.10 References for Chapter 9 10 Instruction-Level Parallelism 10.1 Processor Architectures 10.2 Code-Scheduling Constraints 10.3 Basic-Block Scheduling 10.4 Global Code Scheduling 10.5 Software Pipelining 10.6 Summary of Chapter 10 10.7 References for Chapter 10 11 Optimizing for Parallelism and Locality 11.1 Basic Concepts 11.2 Matrix Multiply: An In-Depth Example 11.3 Iteration Spaces 11.4 Affine Array Indexes 11.5 Data Reuse 11.6 Array Data-Dependence Analysis 11.7 Finding Synchronization-Free Parallelism 11.8 Synchronization Between Parallel Loops 11.9 Pipelining 11.10 Locality Optimizations 11.11 Other Uses of Affine Transforms 11.12 Summary of Chapter 11 11.13 References for Chapter 11 12 Interprocedural Analysis 12.1 Basic Concepts 12.2 Why Interprocedural Analysis? 12.3 A Logical Representation of Data Flow 12.4 A Simple Pointer-Analysis Algorithm 12.5 Context-Insensitive Interprocedural Analysis 12.6 Context-Sensitive Pointer Analysis 12.7 Datalog Implementation by BDD's 12.8 Summary of Chapter 12 12.9 References for Chapter 12 A A Complete Front End A.1 The Source Language A.2 Main A.3 Lexical Analyzer A.4 Symbol Tables and Types A.5 Intermediate Code for Expressions A.6 Jumping Code for Boolean Expressions A.7 Intermediate Code for Statements A.8 Parser A.9 Creating the Front End B Finding Linearly Independent Solutions Index}}
@ARTICLE{Bouquet_2003,title={Reification of Executable Test Scripts in Formal Specification-Based Test Generation: The Java Card Transaction Mechanism Case Study},year={2003},author={Fabrice Bouquet and Fabrice Bouquet and Bruno Legeard and Bruno Legeard},doi={10.1007/978-3-540-45236-2_42},pmid={null},pmcid={null},mag_id={1491663515},journal={null},abstract={Automatic generation of test cases from formal specification is a very promising way both to give a rationale for deciding the scope of testing and to reduce the time for test design and coding. In order to achieve this purpose, formal specification-based methods must solve the problem of executable test script generation from abstract test cases and automatic verdict assignment. This question requires calculating oracles, mapping between the abstract and concrete representations and monitoring test execution. In this paper, we present an effective use in the testing process of automatically generated test suites on an industrial application of Java Card Transaction Mechanism. Abstract test cases are synthesized from a B formal specification using a boundary value approach. From the abstract test cases, executable scripts are generated using execution context pattern and representation mappings. This is fully supported by a tool-set, called BZ-Testing-Tools. On the basis of this Java Card case study, we describe the difficulties that arose and present some generic solutions embedded in the BZ-Testing-Tools environment.}}
@ARTICLE{Weißleder_2008,title={Deriving Input Partitions from UML Models for Automatic Test Generation},year={2008},author={Stephan Weißleder and Stephan Weißleder and Bernd-Holger Schlingloff and Bernd-Holger Schlingloff},doi={10.1007/978-3-540-69073-3_17},pmid={null},pmcid={null},mag_id={1492469996},journal={null},abstract={In this paper, we deal with model-based automatic test generation. We show how to use UML state machines, UML class diagrams, and OCL expressions to automatically derive partitions of input parameter value ranges for boundary testing. We present a test generation algorithm and describe an implementation of this algorithm. Finally, we discuss our approach and compare it to commercial tools.}}
@ARTICLE{Dastani_2009,title={Normative Multi-agent Programs and Their Logics},year={2009},author={Mehdi Dastani and Mehdi Dastani and Davide Grossi and Davide Grossi and John‐Jules Ch. Meyer and John-Jules Ch. Meyer and Nick Tinnemeier and Nick Tinnemeier},doi={10.1007/978-3-642-05301-6_2},pmid={null},pmcid={null},mag_id={1492713212},journal={null},abstract={Multi-agent systems are viewed as consisting of individual agents whose behaviors are regulated by an organization artefact. This paper presents a simplified version of a programming language that is designed to implement norm-based artefacts. Such artefacts are specified in terms of norms being enforced by monitoring, regimenting and sanctioning mechanisms. The syntax and operational semantics of the programming language are introduced and discussed. A logic is presented that can be used to specify and verify properties of programs developed in this language.}}
@ARTICLE{Seghir_2011,title={A lightweight approach for loop summarization},year={2011},author={Mohamed Nassim Seghir and Mohamed Nassim Seghir},doi={10.1007/978-3-642-24372-1_25},pmid={null},pmcid={null},mag_id={1493580732},journal={null},abstract={A problem common to most of the tools based on the abstraction refinement paradigm is the divergence of the CEGAR process. In particular, infinitely many (spurious) counterexamples may arise from unfolding the same (while- or for-) loop in the given program again and again; this leads to an infinite or at least too large sequence of refinement steps. Loop summarization is an approach that permits to overcome this problem. It consists of abstracting not just states but also the state changes (transition relation) induced by structured program statements. The effectiveness of this approach depends on two factors: (a) the computation of loop summaries must not be the bottleneck of the verification algorithm (b) loop summaries must be precise enough to prove the property of interest. We present a technique that permits to achieve both goals. It uses inference rules to compute summaries. A lightweight test is performed to check whether a given loop matches the premise of a given rule. If so, a summary is automatically inferred by instantiating the rule. Despite its simplicity, our technique performs well in practice. We were able to verify safety properties for many examples which are out of the scope of several existing tools.}}
@ARTICLE{Ravn_1998,title={Formal Techniques in Real-Time and Fault-Tolerant Systems: 5th International Symposium, FTRTFT '98, Lyngby, Denmark, September 14-18, 1998},year={1998},author={Anders P. Ravn and Anders P. Ravn and Hans Rischel and Hans Rischel},doi={null},pmid={null},pmcid={null},mag_id={1494712994},journal={null},abstract={Challenges in the utilization of formal methods.- On the need for practical formal methods.- A general framework for the composition of timed systems.- Operational and logical semantics for polling real-time systems.- A finite-domain semantics for testing temporal logic specifications.- Duration Calculus of Weakly Monotonic Time.- Reuse in requirements engineering: Discovery and application of a real-time requirement pattern.- A modular visual model for hybrid systems.- Integrating real-time structured design and formal techniques.- Duration Calculus in the specification of safety requirements.- Automated stream-based analysis of fault-tolerance.- Designing a provably correct robt control system using a 'lean' formal method.- Static analysis to identify invariants in RSML specifications.- Partition refinement in real-time model checking.- Formal verification of stabilizing systems.- Synchronizing clocked transition systems.- Some decidability results for duration calculus under synchronous interpretation.- Fair synchronous transition systems and their liveness proofs.- Dynamical properties of timed automata.- An algorithm for the approximative analysis of rectangular automata.- On checking parallel real-time systems for linear duration properties.- A practical and complete algorithm for testing real-time systems.- Mechanical verification of clock synchronization algorithms.- Compiling graphical real-time specifications into silicon.- Towards a formal semantics of verilog using duration calculus.- The ICOS synthesis environment.- Kronos: A model-checking tool for real-time systems.- SGLOT: A visual tool for structural LOTOS specifications.- Discrete-time Promela and Spin.- Moby/PLC - Graphical development of PLC-automata.- Predictability in critical systems.}}
@ARTICLE{Saaty_1982,title={Decision Making for Leaders: The Analytical Hierarchy Process for Decisions in a Complex World},year={1982},author={Thomas L. Saaty and Thomas L. Saaty},doi={null},pmid={null},pmcid={null},mag_id={1496767193},journal={null},abstract={null}}
@ARTICLE{Shu_2006,title={Message confidentiality testing of security protocols: passive monitoring and active checking},year={2006},author={Guoqiang Shu and Guoqiang Shu and David Lee and David Lee},doi={10.1007/11754008_23},pmid={null},pmcid={null},mag_id={1496920626},journal={Lecture Notes in Computer Science},abstract={Security protocols provide critical services for distributed communication infrastructures. However, it is a challenge to ensure the correct functioning of their implementations, particularly, in the presence of malicious parties. We study testing of message confidentiality – an essential security property. We formally model protocol systems with an intruder using Dolev-Yao model. We discuss both passive monitoring and active testing of message confidentiality. For adaptive testing, we apply a guided random walk that selects next input on-line based on transition coverage and intruder's knowledge acquisition. For mutation testing, we investigate a class of monotonic security flaws, for which only a small number of mutants need to be tested for a complete checking. The well-known Needham-Schroeder-Lowe protocol is used to illustrate our approaches.}}
@ARTICLE{Schröter_2003,title={The model-checking Kit},year={2003},author={Claus Schröter and Claus Schröter and Stefan Schwoon and Stefan Schwoon and Javier Esparza and Javier Esparza},doi={10.1007/3-540-44919-1_29},pmid={null},pmcid={null},mag_id={1497489765},journal={null},abstract={The Model-Checking Kit [8] is a collection of programs which allow to model finite state systems using a variety of modelling languages, and verify them using a variety of checkers, including deadlock-checkers, reachability-checkers, and model-checkers for the temporal logics CTL and LTL [7].}}
@ARTICLE{Aziz_1996,title={Verifying Continuous Time Markov Chains},year={1996},author={Adnan Aziz and Adnan Aziz and Adnan Aziz and Kumud Sanwal and Kumud K. Sanwal and Vigyan Singhal and Vigyan Singhal and Robert K. Brayton and Robert K. Brayton},doi={10.1007/3-540-61474-5_75},pmid={null},pmcid={null},mag_id={1498125516},journal={null},abstract={We present a logical formalism for expressing properties of continuous time Markov chains. The semantics for such properties arise as a natural extension of previous work on discrete time Markov chains to continuous time. The major result is that the verification problem is decidable; this is shown using results in algebraic and transcendental number theory.}}
@ARTICLE{Schäfer_2010,title={JCoBox: generalizing active objects to concurrent components},year={2010},author={Jan Schäfer and Jan Schäfer and Arnd Poetzsch-Heffter and Arnd Poetzsch-Heffter},doi={10.1007/978-3-642-14107-2_13},pmid={null},pmcid={null},mag_id={1498296889},journal={null},abstract={Concurrency in object-oriented languages is still waiting for a satisfactory solution. Formany application areas, standardmechanisms like threads and locks are too low level and have shown to be error-prone and notmodular enough. Lately the actor paradigm has regained attention as a possible solution to concurrency in OOLs.

We propose JCoBox: a Java extension with an actor-like concurrencymodel based on the notion of concurrently running object groups, so-called coboxes. Communication is based on asynchronous method calls with standard objects as targets. Cooperative multi-tasking within coboxes allows for combining active and reactive behavior in a simple and safe way. Futures and promises lead to a data-driven synchronization of tasks.

This paper describes the concurrency model, the formal semantics, and the implementation of JCoBox, and shows that the performance of the implementation is comparable to state-of-the-art actor-based language implementations for the JVM.}}
@ARTICLE{Cohen_2010,title={Local verification of global invariants in concurrent programs},year={2010},author={Ernie Cohen and Ernie Cohen and Michał Moskal and Michał Moskal and Wolfram Schulte and Wolfram Schulte and Stephan Tobies and Stephan Tobies},doi={10.1007/978-3-642-14295-6_42},pmid={null},pmcid={null},mag_id={1498831343},journal={null},abstract={We describe a practical method for reasoning about realistic concurrent programs Our method allows global two-state invariants that restrict update of shared state We provide simple, sufficient conditions for checking those global invariants modularly The method has been implemented in VCC, an automatic, sound, modular verifier for concurrent C programs VCC has been used to verify functional correctness of tens of thousands of lines of Microsoft's Hyper-V virtualization platform and of SYSGO's embedded real-time operating system PikeOS.}}
@ARTICLE{El-Far_2002,title={Model‐Based Software Testing},year={2002},author={Ibrahim K. El-Far and Ibrahim K. El-Far and James A. Whittaker and James A. Whittaker},doi={10.1002/0471028959.sof207},pmid={null},pmcid={null},mag_id={1501347099},journal={null},abstract={There is an abundance of testing styles in the discipline of software engineering today. Over the last few decades, many of these have come to be used and adopted by the industry as solutions to address the increasing demand for assuring software quality. Since 1990 or so, perhaps as an outcome of the popularization of object orientation and models in software engineering, there has been a growth in black-box testing techniques that are collectively dubbed model-based testing. Model-based testing (MBT) is a general term that signifies an approach that bases common testing tasks such as test case generation and test result evaluation [see Jorgensen (1995) for some of these basic terms and concepts] on a model of the application under test.



This generic definition is probably the least general statement that can be made about MBT. One of the striking issues about MBT is the nonstandard lingo that reflects a diversity of philosophies and models. Another issue that one will eventually realize in studying this set of techniques is that the models and techniques have yet to travel the long course to maturity. Disregarding these concerns, model-based techniques have gained the attention of practitioners and theoreticians alike. The wealth of published work portraying case studies in both academic and industrial settings is a sign of the newfound interest in this youthful branch of testing.



Model-based techniques have substantial appeal. The first sign of potential are studies showing that testing a variety of applications has been met with success when MBT was employed. This article discussed what a model is and gives some models that have been useful for testing. Information given or choosing a module, building it, and testing it, The present and future status of model-based software testing is detailed.


Keywords:

model-based software testing (MBT);
definition;
background;
drawbacks;
language;
fundamental tasks;
model choice;
model building;
generating tests;
key concerns}}
@ARTICLE{Heckel_2008,title={Tutorial Introduction to Graph Transformation},year={2008},author={Reiko Heckel and Reiko Heckel},doi={10.1007/978-3-540-87405-8_31},pmid={null},pmcid={null},mag_id={1502272632},journal={null},abstract={This tutorial is intended as a general introduction to graph transformation for participants to the conference or its satellite events who are not familiar with the mainstream approaches and concepts of the area. The tutorial will start with an informal introduction to the basic concepts of graph transformation, such as graphs, rules, transformations, discussing semantic choices such as the handling of dangling edges during rewriting, and extensions such as attributes, types, or inheritance.

In the second part, the tutorial will give a survey of typical applications of graph transformation, for example as a specification language and semantic model for concurrent and distributed systems, as a model transformation language for defining syntax, semantics, and manipulation of visual models, etc.

Finally, the tutorial will go into some details about the theory of (in particular) the algebraic approach to graph transformation, its formal foundations and relevant theory and tools. This shall enable the participants to better appreciate the conference and its satellite events.}}
@ARTICLE{Dietl_2007,title={Generic universe types},year={2007},author={Werner Dietl and Werner Dietl and Sophia Drossopoulou and Sophia Drossopoulou and Péter Müller and Peter Müller},doi={10.1007/978-3-540-73589-2_3},pmid={null},pmcid={null},mag_id={1502550949},journal={null},abstract={Ownership is a powerful concept to structure the object store and to control aliasing and modifications of objects. This paper presents an ownership type system for a Java-like programming language with generic types. Like our earlier Universe type system, Generic Universe Types enforce the owner-as-modifier discipline. This discipline does not restrict aliasing, but requires modifications of an object to be initiated by its owner. This allows owner objects to control state changes of owned objects, for instance, to maintain invariants. Generic Universe Types require a small annotation overhead and provide strong static guarantees. They are the first type system that combines the owner-as-modifier discipline with type genericity.}}
@ARTICLE{Clarke_1993,title={Symbolic Model Checking},year={1993},author={Edmund M. Clarke and Edmund M. Clarke and Edmund M. Clarke and Kenneth L. McMillan and Kenneth L. McMillan and Sérgio Vale Aguiar Campos and Sérgio Campos and Vasiliki Hartonas-Garmhausen and Vassili Hartonas-Garmhausen},doi={null},pmid={null},pmcid={null},mag_id={1503170978},journal={null},abstract={Symbolic model checking is a powerful formal specification and verification method that has been applied successfully in several industrial designs. Using symbolic model checking techniques it is possible to verify industrial-size finite state systems. State spaces with up to 1030 states can be exhaustively searched in minutes. Models with more than 10120 states have been verified using special techniques.}}
@ARTICLE{Velroyen_2008,title={Non-termination checking for imperative programs},year={2008},author={Helga Velroyen and Helga Velroyen and Philipp Rümmer and Philipp Rümmer},doi={10.1007/978-3-540-79124-9_11},pmid={null},pmcid={null},mag_id={1503406871},journal={null},abstract={While termination checking tailored to real-world library code or frameworks has received ever-increasing attention during the last years, the complementary question of disproving termination properties as a means of debugging has largely been ignored so far. We present an approach to automatic non-termination checking that relates to termination checking in the same way as symbolic testing does to program verification. Our method is based on the automated generation of invariants that show that terminating states of a program are unreachable from certain initial states. Such initial states are identified using constraint-solving techniques. The method is fully implemented on top of a program verification system and available for download. We give an empirical evaluation of the approach using a collection of non-terminating example programs.}}
@ARTICLE{Grogono_1994,title={Copying, Sharing, and Aliasing.},year={1994},author={Peter Grogono and Peter Grogono and Patrice Chalin and Patrice Chalin},doi={null},pmid={null},pmcid={null},mag_id={1503818804},journal={null},abstract={null}}
@ARTICLE{Clavel_2007,title={All About Maude - A High-Performance Logical Framework: How to Specify, Program, and Verify Systems in Rewriting Logic},year={2007},author={Manuel Clavel and Manuel Clavel and Francisco Durán and Francisco Durán and Steven Eker and Steven Eker and Patrick Lincoln and Patrick Lincoln and Patrick Lincoln and Patrick Lincoln and Patrick Lincoln and Narciso Martí-Oliet and Narciso Martí-Oliet and José Meseguer and José Meseguer and Carolyn Talcott and Carolyn L. Talcott},doi={null},pmid={null},pmcid={null},mag_id={1504007491},journal={null},abstract={I: Core Maude.- Using Maude.- Syntax and Basic Parsing.- Functional Modules.- A Hierarchy of Data Types: From Trees to Sets.- System Modules.- Playing with Maude.- Module Operations.- Predefined Data Modules.- Specifying Parameterized Data Structures in Maude.- Object-Based Programming.- Model Checking Invariants Through Search.- LTL Model Checking.- Reflection, Metalevel Computation, and Strategies.- Metaprogramming Applications.- Mobile Maude.- User Interfaces and Metalanguage Applications.- II: Full Maude.- Full Maude: Extending Core Maude.- Object-Oriented Modules.- III: Applications and Tools.- A Sampler of Application Areas.- Some Tools.- IV: Reference.- Debugging and Troubleshooting.- Complete List of Maude Commands.- Core Maude Grammar.}}
@ARTICLE{Hubert_2008,title={Semantic Foundations and Inference of Non-null Annotations},year={2008},author={Laurent Hubert and Laurent Hubert and Thomas Jensen and Thomas Jensen and David Pichardie and David Pichardie},doi={10.1007/978-3-540-68863-1_9},pmid={null},pmcid={null},mag_id={1504196925},journal={null},abstract={This paper proposes a semantics-based automatic null pointer analysis for inferring non-null annotations of fields in object-oriented programs. The analysis is formulated for a minimalistic OO language and is expressed as a constraint-based abstract interpretation of the program which for each field of a class infers whether the field is definitely non-null or possibly null after object initialization. The analysis is proved correct with respect to an operational semantics of the minimalistic OO language. This correctness proof has been machine checked using the Coq proof assistant. We also prove the analysis complete with respect to the non-null type system proposed by Fahndrich and Leino, in the sense that for every typable program the analysis is able to prove the absence of null dereferences without any hand-written annotations. Experiments with a prototype implementation of the analysis show that the inference is feasible for large programs.}}
@ARTICLE{Jéron_1999,title={Test Generation Derived from Model-Checking},year={1999},author={Thierry Jéron and Thierry Jéron and Pierre Morel and Pierre Morel},doi={10.1007/3-540-48683-6_12},pmid={null},pmcid={null},mag_id={1506375872},journal={null},abstract={Model-checking and testing are different activities, at least conceptually. While model-checking consists in comparing two specifications at different abstraction levels, testing consists in trying to find errors or gain some confidence in the correctness of an implementation with respect to a specification by the execution of test cases. Nevertheless, there are also similarities in models and algorithms. We argue for this by giving a new on-the-fly test generation algorithm which is an adaptation of a classical graph algorithm which also serves as a basis of some model-checking algorithms. This algorithm is the Tarjan's algorithm which computes the strongly connected components of a digraph.}}
@ARTICLE{Simon_2006,title={Coinductive logic programming},year={2006},author={Luke Simon and Luke Simon and Ajay Mallya and Ajay Mallya and Ajay Bansal and Ajay Bansal and Gopal Gupta and Gopal Gupta},doi={10.1007/11799573_25},pmid={null},pmcid={null},mag_id={1506396519},journal={null},abstract={We extend logic programming’s semantics with the semantic dual of traditional Herbrand semantics by using greatest fixed-points in place of least fixed-points. Executing a logic program then involves using coinduction to check inclusion in the greatest fixed-point. The resulting coinductive logic programming language is syntactically identical to, yet semantically subsumes logic programming with rational terms and lazy evaluation. We present a novel formal operational semantics that is based on synthesizing a coinductive hypothesis for this coinductive logic programming language. We prove that this new operational semantics is equivalent to the declarative semantics. Our operational semantics lends itself to an elegant and efficient goal directed proof search in the presence of rational terms and proofs. We describe a prototype implementation of this operational semantics along with applications of coinductive logic programming.}}
@ARTICLE{Wegener_2002,title={Automatic test data generation for structural testing of embedded software systems by evolutionary testing},year={2002},author={Joachim Wegener and Joachim Wegener and Kerstin Buhr and Kerstin Buhr and Hartmut Pohlheim and Hartmut Pohlheim},doi={null},pmid={null},pmcid={null},mag_id={1508225079},journal={null},abstract={Testing is the most important analytic quality assurance measure for software. The systematic design of test cases is crucial for test quality. Structure-oriented test methods, which define test cases on the basis of the internal program structures, are widely used.

Evolutionary testing is a promising approach for the automation of structural test case design which searches test data that fulfil given structural test criteria by means of evolutionary computation. In this paper we present our evolutionary test environment, which performs fully automatic test data generation for most structural test methods. We shall report on the results gained from the testing of real-world software modules. For most modules we reached full coverage for the structural test criteria.}}
@ARTICLE{Giesl_2005,title={Proving and disproving termination of higher-order functions},year={2005},author={Jürgen Giesl and Jürgen Giesl and René Thiemann and René Thiemann and Peter Schneider–Kamp and Peter Schneider-Kamp},doi={10.1007/11559306_12},pmid={null},pmcid={null},mag_id={1508261706},journal={null},abstract={The dependency pair technique is a powerful modular method for automated termination proofs of term rewrite systems (TRSs). We present two important extensions of this technique: First, we show how to prove termination of higher-order functions using dependency pairs. To this end, the dependency pair technique is extended to handle (untyped) applicative TRSs. Second, we introduce a method to prove non-termination with dependency pairs, while up to now dependency pairs were only used to verify termination. Our results lead to a framework for combining termination and non-termination techniques for first- and higher-order functions in a very flexible way. We implemented and evaluated our results in the automated termination prover AProVE.}}
@ARTICLE{Große_2008,title={Measuring the Quality of a SystemC Testbench by Using Code Coverage Techniques},year={2008},author={Daniel Große and Daniel Große and Hernan Peraza and Hernan Peraza and Wolfgang Klingauf and Wolfgang Klingauf and Rolf Drechsler and Rolf Drechsler},doi={10.1007/978-1-4020-8297-9_6},pmid={null},pmcid={null},mag_id={1510454827},journal={null},abstract={The system description language SystemC enables to quickly create executable specifications at adequate levels of abstraction for both hardware/software integration and fast design space exploration. Besides the modelling of a system, verification has become a dominant factor in circuit and system design. Since SystemC is a versatile language based on C++, testbenches at different abstraction levels can easily be built. But the fault coverage of a manually developed testbench is hard to quantify. In this paper, an approach for measuring the quality of SystemC testbenches is presented. The approach is based on dedicated code coverage techniques and identifies all the parts of a SystemC model that have not been tested. Experimental results show the applicability of our methodology.}}
@ARTICLE{Fernandez_1996,title={Using On-The-Fly Verification Techniques for the Generation of test Suites},year={1996},author={Jean-Claude Fernandez and Jean-Claude Fernandez and Claude Jard and Claude Jard and Thierry Jéron and Thierry Jéron and César Viho and César Viho},doi={10.1007/3-540-61474-5_82},pmid={null},pmcid={null},mag_id={1510782929},journal={null},abstract={In this paper we attempt to demonstrate that on-the-fly techniques, developed in the context of verification, can help in deriving test suites. Test purposes are used in practice to select test cases according to some properties of the specification. We define a consistency pre-order linking test purposes and specifications. We give a set of rules to check this consistency and to derive a complete test case with preamble, postamble, verdicts and timers. The algorithm, which implements the construction rules, is based on a depth first traversal of a synchronous product between the test purpose and the specification. We shortly relate our experience on an industrial protocol with TGV, a first prototype of the algorithm implemented as a component of the C ADP toolbox.}}
@ARTICLE{Konstantas_1995,title={Interoperation of object-oriented applications},year={1995},author={Dimitri Konstantas and Dimitri Konstantas},doi={null},pmid={null},pmcid={null},mag_id={1513055721},journal={null},abstract={One of the important advantages of the object-oriented design and development methodology is the ability to reuse existing software modules. However the introduction of many programming languages with different syntax, semantics and/or paradigms has created the need for a consistent inter-language interoperability support framework. We present a brief overview of the most characteristic interoperability support methods and frameworks allowing the access and reuse of objects from different programming environments and focus on the interface bridging object-oriented interoperability support approach.}}
@ARTICLE{Ulrich_1999,title={Architectures for Testing Distributed Systems},year={1999},author={Andreas Ulrich and Andreas Ulrich and Hartmut König and Hartmut König},doi={10.1007/978-0-387-35567-2_7},pmid={null},pmcid={null},mag_id={1513384635},journal={null},abstract={Stabilizing network infrastructures has moved the focus of software system engineering to the development of distributed applications running on top of the network. The complexity of distributed systems and their inherent concurrency pose high requirements on their design and implementation. This is also true for the validation of the systems, in particular the test. Compared to protocol testing the test of distributed systems and applications requires different methods for deriving test cases and for running the test. In this paper, we discuss architectures for testing distributed, concurrent systems. We suggest three different models: a global tester that has total control over the distributed system under test (SUT) and, more interestingly, two types of distributed testers comprising several concurrent tester components. The test architectures rely on a grey-box testing approach that allows to observe internal interactions of the SUT by the tester. In order to assure a correct assessment of the test data collected by the distributed tester components, the tester has to maintain a correct global view on the SUT. This can be achieved either by the use of redundant points of control and observation or by test coordination procedures. We outline the features of each approach and discuss their benefits and shortages. Finally, we describe possible simplifications for the architectures.}}
@ARTICLE{Esparza_2001,title={A BDD-Based Model Checker for Recursive Programs},year={2001},author={Javier Esparza and Javier Esparza and Stefan Schwoon and Stefan Schwoon},doi={10.1007/3-540-44585-4_30},pmid={null},pmcid={null},mag_id={1514446453},journal={null},abstract={We present a model-checker for boolean programs with (possibly recursive) procedures and the temporal logic LTL. The checker is guaranteed to terminate even for (usually faulty) programs in which the depth of the recursion is not bounded. The algorithm uses automata to finitely represent possibly infinite sets of stack contents and BDDs to compactly represent finite sets of values of boolean variables. We illustrate the checker on some examples and compare it with the Bebop tool of Ball and Rajamani.}}
@ARTICLE{Burdonov_2002,title={UniTesK Test Suite Architecture},year={2002},author={Igor Burdonov and I. B. Bourdonov and A. S. Kossatchev and A. S. Kossatchev and Victor V. Kuliamin and Victor V. Kuliamin and Alexander K. Petrenko and Alexandre Petrenko},doi={10.1007/3-540-45614-7_5},pmid={null},pmcid={null},mag_id={1514939953},journal={null},abstract={The article presents the main components of the test suite architecture underlying UniTesK test development technology, an automated specification based test development technology for use in industrial testing of general-purpose software. The architecture presented contains such elements as automatically generated oracles, components to monitor formally defined test coverage criteria, and test scenario specifications for test sequence generation with the help of an automata based testing mechanism. This work stems from the ISP RAS results of academic research and 7-years experience in industrial application of formal testing techniques [1].}}
@ARTICLE{Böhme_2008,title={HOL-Boogie -- An Interactive Prover for the Boogie Program-Verifier},year={2008},author={Sascha Böhme and Sascha Böhme and K. Rustan M. Leino and K. Rustan M. Leino and Burkhart Wolff and Burkhart Wolff},doi={10.1007/978-3-540-71067-7_15},pmid={null},pmcid={null},mag_id={1515169568},journal={null},abstract={Boogieis a program verification condition generator for an imperative core language. It has front-ends for the programming languages C# and C enriched by annotations in first-order logic.

Its verification conditions -- constructed via a wpcalculus from these annotations -- are usually transferred to automated theorem provers such as Simplifyor Z3. In this paper, however, we present a proof-environment, HOL-BoogieP, that combines Boogie with the interactive theorem prover Isabelle/HOL. In particular, we present specific techniques combining automated and interactive proof methods for code-verification.

We will exploit our proof-environment in two ways: First, we present scenarios to "debug" annotations (in particular: invariants) by interactive proofs. Second, we use our environment also to verify "background theories", i.e. theories for data-types used in annotations as well as memory and machine models underlying the verification method for C.}}
@ARTICLE{Geist_1996,title={Coverage-Directed Test Generation Using Symbolic Techniques},year={1996},author={Daniel Geist and Daniel Geist and Monica Farkas and Monica Farkas and Avner Landver and Avner Landver and Yossi Lichtenstein and Yossi Lichtenstein and Shmuel Ur and Shmuel Ur and Yaron Wolfsthal and Yaron Wolfsthal},doi={10.1007/bfb0031805},pmid={null},pmcid={null},mag_id={1516339813},journal={null},abstract={In this paper, we present a verification methodology that integrates formal verification techniques with verification by simulation, thereby providing means for generating simulation test suites that ensure coverage. We derive the test suites by means of BDD-based symbolic techniques for describing and traversing the implementation state space. In our approach, we provide a high-level of control over the generated test suites; a powerful abstraction mechanism directs the generation procedure to specific areas, that are the focus for verification, thereby withstanding the state explosion problem. The abstraction is achieved by partitioning the implementation state variables into categories of interest. We also depart from the traditional graph-algorithmic model for conformance testing; instead, using temporal logic assertions, we can generate a test suite where the set of state sequences (paths) satisfies some temporal properties as well as guaranteeing transition coverage. Our methodology has been successfully applied to the generation of test suites for IBM PowerPC and AS/400 systems.}}
@ARTICLE{Fabre_2006,title={On the construction of pullbacks for safe petri nets},year={2006},author={Éric Fabre and Eric Fabre},doi={10.1007/11767589_10},pmid={null},pmcid={null},mag_id={1517294405},journal={null},abstract={The product of safe Petri nets is a well known operation : it generalizes to concurrent systems the usual synchronous product of automata. In this paper, we consider a more general way of combining nets, called a pullback. The pullback operation generalizes the product to nets which interact both by synchronized transitions and/or by a shared sub-net (i.e. shared places and transitions). To obtain all pullbacks, we actually show that all equalizers can be defined in the category of safe nets. Combined to the known existence of products in this category, this gives more than what we need : we actually obtain that all small limits exist, i.e. that safe nets form a complete category.}}
@ARTICLE{Berthomieu_2007,title={Model checking bounded prioritized time Petri nets},year={2007},author={Bernard Berthomieu and Bernard Berthomieu and Florent Peres and Florent Peres and François Vernadat and François Vernadat},doi={10.1007/978-3-540-75596-8_37},pmid={null},pmcid={null},mag_id={1517759046},journal={null},abstract={In a companion paper [BPV06], we investigated the expressiveness of Time Petri Nets extended with Priorities and showed that it is very close to that Timed Automata, in terms of weak timed bisimilarity. As a continuation of this work we investigate here the applicability of the available state space abstractions for Bounded Time Petri Nets to Bounded Prioritized Time Petri Nets. We show in particular that a slight extension of the "strong state classes" construction of [BV03] provides a convenient state space abstraction for these nets, preserving markings, states, and LTL formulas. Interestingly, and conversely to Timed Automata, the construction proposed does not require to compute polyhedra differences.}}
@ARTICLE{Canal_2006,title={Synchronizing behavioural mismatch in software composition},year={2006},author={Carlos Canal and Carlos Canal and Pascal Poizat and Pascal Poizat and Gwen Salaün and Gwen Salaün},doi={10.1007/11768869_7},pmid={null},pmcid={null},mag_id={1518900212},journal={null},abstract={Software Adaptation is a crucial issue for the development of a real market of components promoting software reuse. Recent work in this field has addressed several problems related to interface and behavioural mismatch. In this paper, we present our proposal for software adaptation, which builds on previous work overcoming some of its limitations, and makes a significant advance to solve pending issues. Our approach is based on the use of synchronous vectors and regular expressions for governing adaptation rules, and is supported by dedicated algorithms and tools.}}
@ARTICLE{Soundarajan_2004,title={Incremental Reasoning for Object Oriented Systems},year={2004},author={Neelam Soundarajan and Neelam Soundarajan and Stephen Fridella and Stephen Fridella},doi={10.1007/978-3-540-39993-3_15},pmid={null},pmcid={null},mag_id={1519921373},journal={Lecture Notes in Computer Science},abstract={Inheritance and polymorphism are key mechanisms of the object-oriented approach that enable designers to develop systems in an incremental manner. In this paper, we develop techniques for reasoning incrementally about the behavior of such systems. A derived class designer will be able, using the proposed approach, to arrive at the richer behavior that polymorphic methods inherited from the base class will exhibit in the derived class, without reanalyzing the code bodies of these methods. The approach is illustrated by applying it to a simple case study.}}
@ARTICLE{Stokkink_2013,title={Divergent quiescent transition systems},year={2013},author={W.G.J. Stokkink and W.G.J. Stokkink and Mark Timmer and Mark Timmer and Mariëlle Stoelinga and Mariëlle Stoelinga},doi={10.1007/978-3-642-38916-0_13},pmid={null},pmcid={null},mag_id={1519975715},journal={null},abstract={Quiescence is a fundamental concept in modelling system behaviour, as it explicitly represents the fact that no output is produced in certain states. The notion of quiescence is also essential to model-based testing: if a particular implementation under test does not provide any output, then the test evaluation algorithm must decide whether or not to allow this behaviour. To explicitly model quiescence in all its glory, we introduce Divergent Quiescent Transition Systems (DQTSs).}}
@ARTICLE{Bulwahn_2008,title={Imperative Functional Programming with Isabelle/HOL},year={2008},author={Lukas Bulwahn and Lukas Bulwahn and Alexander Krauß and Alexander Krauss and Florian Haftmann and Florian Haftmann and Levent Erkök and Levent Erkök and John Matthews and John Matthews},doi={10.1007/978-3-540-71067-7_14},pmid={null},pmcid={null},mag_id={1522519483},journal={null},abstract={We introduce a lightweight approach for reasoning about programs involving imperative data structures using the proof assistant Isabelle/HOL. It is based on shallow embedding of programs, a polymorphic heap model using enumeration encodings and type classes, and a state-exception monad similar to known counterparts from Haskell. Existing proof automation tools are easily adapted to provide a verification environment. The framework immediately allows for correct code generation to ML and Haskell. Two case studies demonstrate our approach: An array-based checker for resolution proofs, and a more efficient bytecode verifier.}}
@ARTICLE{Rümmer_2007,title={Proving programs incorrect using a sequent calculus for Java dynamic logic},year={2007},author={Philipp Rümmer and Philipp Rümmer and Muhammad Ali Shah and Muhammad Ali Shah},doi={10.1007/978-3-540-73770-4_3},pmid={null},pmcid={null},mag_id={1522637266},journal={null},abstract={Program verification is concerned with proving that a program is correct and adheres to a given specification. Testing a program, in contrast, means to search for a witness that the program is incorrect. In the present paper, we use a program logic for Java to prove the incorrectness of programs. We show that this approach, carried out in a sequent calculus for dynamic logic, creates a connection between calculi and proof procedures for program verification and test data generation procedures. Starting with a program logic enables to find more general and more complicated counterexamples for the correctness of programs.}}
@ARTICLE{Engel_2007,title={Generating unit tests from formal proofs},year={2007},author={Christian Engel and Christian Engel and Reiner Hähnle and Reiner Hähnle},doi={10.1007/978-3-540-73770-4_10},pmid={null},pmcid={null},mag_id={1523035663},journal={null},abstract={We present a new automatic test generation method for JAVA CARD based on attempts at formal verification of the implementation under test (IUT). Self-contained unit tests in JUnit format are generated automatically. The advantages of the approach are: (i) it exploits the full information available in the IUT and in its formal model giving very good hybrid coverage; (ii) a non-trivial formal model of the IUT is unnecessary; (iii) it is adaptable to the skills that users may possess in formal methods.}}
@ARTICLE{Laviron_2008,title={SubPolyhedra: A (More) Scalable Approach to Infer Linear Inequalities},year={2008},author={Vincent Laviron and Vincent Laviron and Francesco Logozzo and Francesco Logozzo},doi={10.1007/978-3-540-93900-9_20},pmid={null},pmcid={null},mag_id={1525502778},journal={null},abstract={We introduce Subpolyhedra ( SubPoly  ) a new numerical abstract domain to infer and propagate linear inequalities.  Subpoly  is as expressive as Polyhedra, but it drops some of the deductive power to achieve scalability.  Subpoly  is based on the insight that the reduced product of linear equalities and intervals produces powerful yet scalable analyses. Precision can be recovered using hints. Hints can be automatically generated or provided by the user in the form of annotations.

We implemented  Subpoly  on the top of  Clousot  , a generic abstract interpreter for  .Net  .  Clousot  with  Subpoly  analyzes very large and complex code bases in few minutes.  Subpoly  can efficiently capture linear inequalities among hundreds of variables, a result well-beyond state-of-the-art implementations of Polyhedra.}}
@ARTICLE{Milner_1993,title={An Action Structure for Synchronous pi-Calculus},year={1993},author={Robin Milner and Robin Milner},doi={10.1007/3-540-57163-9_6},pmid={null},pmcid={null},mag_id={1526533984},journal={null},abstract={After a review of relevant notions, an action structure is presented for the π-calculus. This yields a version of π-calculus which is synchronous in the sense of MEIJE or SCCS, i.e. an arbitrary amount of computation may take place in a single transition. The main new technical result is the construction of an incident set for the action structure, which guarantees a congruential strong bisimilarity for the calculus. The incident set is characterized using a new form of graphical representation for actions.}}
@ARTICLE{Yin_1984,title={Case Study Research: Design and Methods},year={1984},author={Robert K. Yin and Robert K. Yin},doi={null},pmid={null},pmcid={null},mag_id={1527311855},journal={null},abstract={Foreword, by Donald T. Campbell Preface 1. INTRODUCTION: How to Know Whether and When to Use Case Studies as a Research Method The Case Study as a Research Method Comparing Case Studies With Other Research Methods in the Social Sciences Different Kinds of Case Studies, But a Common Definition Summary 2. DESIGNING CASE STUDIES: Identifying Your Case(s) and Establishing the Logic of Your Case Study General Approach to Designing Case Studies Criteria for Judging the Quality of Research Designs Case Study Designs Modest Advice in Selecting Case Study Designs 3. PREPARING TO COLLECT CASE STUDY EVIDENCE: What You Need to Do Before Starting to Collect Case Study Data The Case Study Investigator: Desired Skills Preparation and Training for a Specific Case Study The Case Study Protocol Screening the Candidate "Cases" for Your Case Study The Pilot Case Study Summary 4. COLLECTING CASE STUDY EVIDENCE: The Principles You Should Follow in Working With Six Sources of Evidence Six Sources of Evidence Three Principles of Data Collection Summary 5. ANALYZING CASE STUDY EVIDENCE: How to Start Your Analysis, Your Analytic Choices, and How They Work An Analytic Strategy: More Than Familiarity With Analytic Tools Five Analytic Techniques Pressing for a High-Quality Analysis Summary 6. REPORTING CASE STUDIES: How and What to Compose Targeting Case Study Reports Case Study Reprots as Part of Larger, Mixed Methods Studies Illustrative Structures for Case Study Compositions Procedures in Doing a Case Study Report What Makes an Examplary Case Study? References Author Index Subject Index About the Author}}
@ARTICLE{Ábrahám-Mumm_2002,title={Verification for Java's Reentrant Multithreading Concept},year={2002},author={Erika Ábrahám-Mumm and Erika Ábrahám-Mumm and Frank S. de Boer and Frank S. de Boer and Willem P. de Roever and Willem P. de Roever and Willem P. de Roever and Martín Steffen and Martin Steffen},doi={10.1007/3-540-45931-6_2},pmid={null},pmcid={null},mag_id={1528309244},journal={null},abstract={Besides the features of a class-based object-oriented language, Java integrates concurrency via its thread-classes, allowing for a multithreaded flow of control. the concurrency model offers coordination via lock-synchronization, and communication by synchronous message passing, including re-entrant method calls, and by instance variables shared among threads.To reason about multithreaded programs, we introduce in this paper an assertional proof method for JavaMT ("Multi-Threaded Java"), a small concurrent sublanguage of Java, covering the mentioned concurrency issues as well as the object-based core of Java, i.e., object creation, side effects, and aliasing, but leaving aside inheritance and subtyping.}}
@ARTICLE{Smans_2009,title={Implicit Dynamic Frames: Combining Dynamic Frames and Separation Logic},year={2009},author={Jan Smans and Jan Smans and Bart Jacobs and Bart Jacobs and Frank Piessens and Frank Piessens},doi={10.1007/978-3-642-03013-0_8},pmid={null},pmcid={null},mag_id={1528322206},journal={null},abstract={The dynamic frames approach has proven to be a powerful formalism for specifying and verifying object-oriented programs. However, it requires writing and checking many frame annotations. In this paper, we propose a variant of the dynamic frames approach that eliminates the need to explicitly write and check frame annotations. Reminiscent of separation logic's frame rule, programmers write access assertions inside pre- and postconditions instead of writing frame annotations. From the precondition, one can then infer an upper bound on the set of locations writable or readable by the corresponding method. We implemented our approach in a tool, and used it to automatically verify several challenging programs, including subject-observer, iterator and linked list.}}
@ARTICLE{Bousquet_2000,title={Formal Test Automation: The Conference Protocol with TGV/TORX},year={2000},author={Lydie Du Bousquet and Lydie du Bousquet and Solofo Ramangalahy and Solofo Ramangalahy and Séverine Simon and Severine Simon and César Viho and César Viho and Axel Belinfante and Axel Belinfante and R.G. de Vries and René G. de Vries},doi={10.1007/978-0-387-35516-0_14},pmid={null},pmcid={null},mag_id={1529660919},journal={null},abstract={We present an experiment of automated formal conformance testing of the Conference Protocol Entity as reported in [2]. Our approach differs from other experiments, since it investigates the combination of the tools TGV for abstract test generation and TorX for test execution.}}
@ARTICLE{Cavalcanti_2008,title={A note on traces refinement and the conf relation in the unifying theories of programming},year={2008},author={Ana Cavalcanti and Ana Cavalcanti and Marie-Claude Gaudel and Marie-Claude Gaudel},doi={10.1007/978-3-642-14521-6_4},pmid={null},pmcid={null},mag_id={1530449155},journal={null},abstract={There is a close relation between the failures-divergences and the UTP models of CSP, but they are not equivalent. For example, miracles are not available in the failures-divergences model; the UTP theory is richer and can be used to give semantics to data-rich process algebras like Circus. Previously, we have defined functions that calculate the failures-divergences model of a CSP process characterised by a UTP relation. In this note, we use these functions to calculate the UTP characterisations of traces refinement and of the conf relation that is widely used in testing. In addition, we prove that the combination of traces refinement and conf corresponds to refinement of processes in Circus. This result is the basis for a formal testing technique based on Circus; as usual in testing, we restrict ourselves to divergence-free processes.}}
@ARTICLE{Langerak_1989,title={A Testing Theory for LOTOS using Deadlock Detection},year={1989},author={Rom Langerak and Rom Langerak},doi={null},pmid={null},pmcid={null},mag_id={1530935046},journal={null},abstract={null}}
@ARTICLE{Filliâtre_2007,title={The Why/Krakatoa/Caduceus platform for deductive program verification},year={2007},author={Jean Christophe Filliâtre and Jean-Christophe Filliâtre and Claude Marché and Claude Marché},doi={10.1007/978-3-540-73368-3_21},pmid={null},pmcid={null},mag_id={1532097571},journal={null},abstract={We present the Why/Krakatoa/Caduceus set of tools for deductive verification of Java and C source code.}}
@ARTICLE{Kovács_2003,title={Applying mutation analysis to SDL specifications},year={2003},author={Géza Kovács and Gábor Kovács and Gábor Kovács and Gábor Kovács and Zoltán Pap and Zoltán Pap and Dung Le Viet and Dung Le Viet and Antal Wu-Hen-Chang and Antal Wu-Hen-Chang and Gyula Csopaki and Gyula Csopaki},doi={10.1007/3-540-45075-0_16},pmid={null},pmcid={null},mag_id={1532542677},journal={Lecture Notes in Computer Science},abstract={Mutation analysis is a fault based testing method used initially for code based software testing, and lately for specification based testing and validation as well. In this paper, the method is applied to SDL (Specification and Description Language) specifications. It is used to automate the process of conformance test generation and selection for telecommunications protocols. We present two algorithms for automatic test generation and selection. These provide the basis of the Test Selector tool developed at the Budapest University of Technology and Economics. We present the results of an empirical study using the tool.}}
@ARTICLE{Arnold_1996,title={The Java Programming Language},year={1996},author={Ken Arnold and Ken Arnold and Ken Arnold and James Gosling and James Gosling and David Holmes and David Holmes},doi={null},pmid={null},pmcid={null},mag_id={1533109738},journal={null},abstract={From the Publisher:
Co-authored by the creator of the Java technology and an experienced object-oriented developer, The Java (TM)Programming Language, Second Edition, is the definitive resource for all serious Java programmers. This book will give you a solid foundation in Java programming language strategies and techniques. It features a concise introduction to the language; detailed descriptions of Java's commands, constructs, and libraries; and numerous real-world examples that show you how to exploit the language's power, portability, and flexibility. You will find in-depth and progressively advanced coverage of classes and objects, interfaces, exception-handling, threads and multitasking, and packages. In addition, the book describes the Java core library packages, including I/O, standard utilities, language types, and system classes. Thoroughly revised from start to finish, this second edition fully integrate, is the definitive resource for all serious Java programmers. This book will give you a solid foundation in Java programming language strategies and techniques. It features a concise introduction to the language; detailed descriptions of Java's commands, constructs, and libraries; and numerous real-world examples that show you how to exploit the language's power, portability, and flexibility. You will find in-depth and progressively advanced coverage of classes and objects, interfaces, exception-handling, threads and multitasking, and packages. In addition, the book describes the Java core library packages, including I/O, standard utilities, language types, and system classes. Thoroughly revised from start to finish, this second edition fully integrates Java 1.1 into both text and examples. This edition includes the changes introduced in Java 1.1, such as nested classes (including anonymous classes), threading issues, character-based streams, object-serialization, documentation comments, new utility classes, plus internationalization and localization. The book lets you in on the rationale behind Java's design, direct from the language's creator, as well as the tradeoffs involved in using specific features. With these insights, you will have the understanding you need to begin developing Java applications and applets.}}
@ARTICLE{Ciobanu_2008,title={Modelling and verification of timed interaction and migration},year={2008},author={Gabriel Ciobanu and Gabriel Ciobanu and Maciej Koutny and Maciej Koutny},doi={10.1007/978-3-540-78743-3_16},pmid={null},pmcid={null},mag_id={1534426552},journal={null},abstract={We present a process algebra where timeouts of interactions and adaptable migrations in a distributed environment with explicit locations can be defined. Timing constraints allow to control the interaction (communication) between co-located mobile processes, and a migration action with variable destination supports flexible movement from one location to another. We define an operational semantics, and outline a structural translation of the proposed process algebra into operationally equivalent finite high level timed Petri nets. The purpose of such a translation is two fold. First, it yields a formal semantics for timed interaction and migration which is both compositional and allows to deal directly with concurrency and causality. Second, it should facilitate the use of simulation and verification tools developed within the area of Petri nets.}}
@ARTICLE{Pnueli_1996,title={A Platform for Combining Deductive with Algorithmic Verification},year={1996},author={Amir Pnueli and Amir Pnueli and Elad Shahar and Elad Shahar},doi={10.1007/3-540-61474-5_68},pmid={null},pmcid={null},mag_id={1535168632},journal={null},abstract={We describe a computer-aided verification system which combines deductive with algorithmic (model-checking) verification methods. The system, called tlv (for temporal verification system), is constructed as an additional layer superimposed on top of the cmu smv system, and can verify finite-state systems relative to linear temporal logic (ltl) as well as ctl specifications. The systems to be verified can be either hardware circuits written in the smv design language or finite-state reactive programs written in a simple programming language (spl).}}
@ARTICLE{Beckert_2007,title={Verification of Object-Oriented Software. the Key Approach},year={2007},author={Bernhard Beckert and Bernhard Beckert and Reiner Hähnle and Reiner Hähnle and Peter H. Schmitt and Peter H. Schmitt},doi={10.1007/978-3-540-69061-0},pmid={null},pmcid={null},mag_id={1537084112},journal={null},abstract={The ultimate goal of program verification is not the theory behind the tools or the tools themselves, but the application of the theory and tools in the software engineering process. Our society relies on the correctness of a vast and growing amount of software. Improving the software engineering process is an important, long-term goal with many steps. Two of those steps are the KeY tool and this KeY book.

The material is presented on an advanced level suitable for graduate courses and, of course, active researchers with an interest in verification. The underlying verification paradigm is deductive verification in an expressive program logic. The logic used for reasoning about programs is not a minimalist version suitable for theoretical investigations, but an industrial-strength version. The first-order part is equipped with a type system for modelling of object hierarchies, with underspecification, and with various built-in theories. The program logic covers full Java Card (plus a bit more such as multi-dimensional arrays, characters, and long integers). A lot of emphasis is thereby put on specification, including two widely-used object-oriented specification languages (OCL and JML) and even an interface to natural language generation. The generation of proof obligations from specified code is discussed at length. The book is rounded off by two substantial case studies that are included and presented in detail.}}
@ARTICLE{Springintveld_1996,title={Minimizable Timed Automata},year={1996},author={Jan Springintveld and Jan Springintveld and Frits Vaandrager and Frits W. Vaandrager},doi={10.1007/3-540-61648-9_38},pmid={null},pmcid={null},mag_id={1537628828},journal={null},abstract={State minimization plays a fundamental role in both classical automata theory and in the theory of reactive systems. Many algorithms and results are based on the fact that for each finite automaton there exists an equivalent minimum state automaton that can be effectively computed and that is unique up to isomorphism.}}
@ARTICLE{Fabbri_1995,title={Mutation Testing Applied to Validate Specifications Based on Petri Nets},year={1995},author={Sandra Fabbri and Sandra Fabbri and José Carlos Maldonado and José Carlos Maldonado and Paulo César Masiero and Paulo Cesar Masiero and Márcio Eduardo Delamaro and Márcio Eduardo Delamaro and E. Wong and E. Wong},doi={10.1007/978-0-387-34945-9_24},pmid={null},pmcid={null},mag_id={1540134266},journal={null},abstract={Testing is one of the fundamental software development life cycle activities. Considering Reactive Systems such as: metro control, patient hospital monitoring and communication protocols, the testing activity becomes more relevant as errors in these systems can promote severe economical and social losses. The objective of this work is to evaluate the adequacy of applying the Mutation Analysis criterion to validate Petri Net based specifications. A set of mutation operators for Petri Nets, a key point for using Mutation Analysis, as well as the results of applying manually these operators to a Petri Net modeling a level 3 protocol are presented. We also examine the ideas of constrained and randomly selected mutation in this context.}}
@ARTICLE{Kohavi_2010,title={Switching and Finite Automata Theory},year={2010},author={Zvi Kohavi and Zvi Kohavi and Niraj K. Jha and Niraj K. Jha},doi={null},pmid={null},pmcid={null},mag_id={1543281322},journal={null},abstract={Understand the structure, behavior, and limitations of logic machines with this thoroughly updated third edition. Many new topics are included, such as CMOS gates, logic synthesis, logic design for emerging nanotechnologies, digital system testing, and asynchronous circuit design, to bring students up-to-speed with modern developments. The intuitive examples and minimal formalism of the previous edition are retained, giving students a text that is logical and easy to follow, yet rigorous. Kohavi and Jha begin with the basics, and then cover combinational logic design and testing, before moving on to more advanced topics in finite-state machine design and testing. Theory is made easier to understand with 200 illustrative examples, and students can test their understanding with over 350 end-of-chapter review questions.}}
@ARTICLE{Baldwin_1979,title={Heuristics for Determining Equivalence of Program Mutations.},year={1979},author={Dan Baldwin and Douglas Baldwin and Frederick G. Sayward and Frederick Sayward},doi={null},pmid={null},pmcid={null},mag_id={1543852556},journal={null},abstract={Abstract : A mutant M of a program P is a program derived from P by making some well defined simple change in P. Some initial investigations on automatically detecting equivalent mutants of a program are presented. The idea is based on the observation that compiler optimization can be considered a process of altering a program to an equivalent but more efficient mutant of the program. Thus the inverse of compiler optimization techniques can be seen as, in essence, equivalent mutant detection. (Author)}}
@ARTICLE{Cohen_2009,title={VCC: A Practical System for Verifying Concurrent C},year={2009},author={Ernie Cohen and Ernie Cohen and Markus Dahlweid and Markus Dahlweid and Mark Hillebrand and Mark Hillebrand and Dirk Leinenbach and Dirk Leinenbach and Michał Moskal and Michał Moskal and Thomas Santen and Thomas Santen and Wolfram Schulte and Wolfram Schulte and Stephan Tobies and Stephan Tobies},doi={10.1007/978-3-642-03359-9_2},pmid={null},pmcid={null},mag_id={1545681762},journal={null},abstract={VCC is an industrial-strength verification environment for low-level concurrent system code written in C. VCC takes a program (annotated with function contracts, state assertions, and type invariants) and attempts to prove the correctness of these annotations. It includes tools for monitoring proof attempts and constructing partial counterexample executions for failed proofs. This paper motivates VCC, describes our verification methodology, describes the architecture of VCC, and reports on our experience using VCC to verify the Microsoft Hyper-V hypervisor.}}
@ARTICLE{Herber_2009,title={Combining Model Checking and Testing in a Continuous HW/SW Co-verification Process},year={2009},author={Paula Herber and Paula Herber and Florian Friedemann and Florian Friedemann and Sabine Glesner and Sabine Glesner},doi={10.1007/978-3-642-02949-3_10},pmid={null},pmcid={null},mag_id={1545855762},journal={null},abstract={SystemC is widely used for modeling and simulation in hardware/software co-design. However, the co-verification techniques used for SystemC designs are mostly ad-hoc and non-systematic. In this paper, we present an approach to overcome this problem by a systematic, formally founded quality assurance process. Based on a combination of model checking and conformance testing, we obtain a HW/SW co-verification flow that supports HW/SW co-development throughout the whole design process. In addition, we present a novel test algorithm that generates conformance tests for SystemC designs offline and that can cope with non-deterministic systems. To this end, we use a timed automata model of the SystemC design to compute expected simulation or test results. We have implemented the model checking and conformance testing framework and give experimental results to show the applicability of our approach.}}
@ARTICLE{DeMillo_1980,title={Mutation Analysis as a Tool for Software Quality Assurance.},year={1980},author={Richard A DeMillo and Richard A DeMillo and Richard A. DeMillo},doi={null},pmid={null},pmcid={null},mag_id={1546197612},journal={null},abstract={Abstract : A protocol for using mutation analysis as a tool for software quality assurance is described. The results of experiments on the reliability of this method are also described. (Author)}}
@ARTICLE{Harel_1989,title={On the development of reactive systems},year={1989},author={David Harel and David Harel and Amir Pnueli and Amir Pnueli},doi={10.1007/978-3-642-82453-1_17},pmid={null},pmcid={null},mag_id={1546220007},journal={null},abstract={Some observations are made concerning the process of developing complex systems. A broad class of systems, termed reactive, is singled out as being particularly problematic when it comes to finding satisfactory methods for behavioral description. In this paper we recommend the recently proposed statechart method for this purpose. Moreover, it is observed that most reactive systems cannot be developed in a linear stepwise fashion, but, rather, give rise to a two-dimensional development process, featuring behavioral aspects in the one dimension and implementational ones in the other. Concurrency may occur in both dimensions, as orthogonality of states in the one and as parallelism of subsystems in the other. A preliminary approach to working one’s way through this “magic square” of system development is then presented. The ideas described herein seem to be relevant to a wide variety of application areas.}}
@ARTICLE{Huisman_2004,title={Checking Absence of Illicit Applet Interactions: A Case Study},year={2004},author={Marieke Huisman and Marieke Huisman and Dilian Gurov and Dilian Gurov and Christoph Sprenger and Christoph Sprenger and Gennady Chugunov and Gennady Chugunov},doi={10.1007/978-3-540-24721-0_6},pmid={null},pmcid={null},mag_id={1546306366},journal={null},abstract={This paper presents the use of a method - and its corre- sponding tool set - for compositional verification of applet interactions on a realistic industrial smart card case study. The case study, an elec- tronic purse, is provided by smart card producer Gemplus as a test case for formal methods for smart cards. The verification method focuses on the possible interactions between different applets, co-existing on the same card, and provides a technique to specify and detect illicit interac- tions between these applets. The method is compositional, thus support- ing post-issuance loading of applets. The correctness of a global system property can algorithmically be inferred from local applet properties. Later, when loading applets on a card, the implementations are matched against these local properties, in order to guarantee the global property. The theoretical framework underlying our method has been presented elsewhere; the present paper evaluates its practical usability by means of an industrial case study. In particular, we outline the tool set that we have assembled to support the verification process, combining existing model checkers with newly developed tools, tailored to our method.}}
@ARTICLE{Robinson_1997,title={The B Method and the B Toolkit},year={1997},author={Ken Robinson and Ken Robinson},doi={10.1007/bfb0000503},pmid={null},pmcid={null},mag_id={1546336821},journal={null},abstract={The B Method is a full spectrum formal software development method that covers the software process from specification to implementation. The method uses state machines, defined using logic and set theory with a notation similar to that of Z, that export operations. The method supports a notion of refinement and implementation, which is based on the notion of refinement in the refinement calculus with the exception that there is no distinction between procedural and data refinement. The B Toolkit is a configuration tool that manages developments under the B Method, generating proof obligations and supporting tools for the discharge of those proof obligations. There is also support for the generation of documentation, and for the browsing of developments.}}
@ARTICLE{Damasceno_2015,title={Symbolic Test Case Generation of Compositional Real-Time Systems Driven by Interruptions},year={2015},author={Adriana C. Damasceno and Adriana Carla Damasceno and Patrícia D. L. Machado and Patrícia D. L. Machado and Wilkerson L. Andrade and Wilkerson L. Andrade and W. Torres and Wesley Nunes Marques Torres},doi={10.1109/isorc.2015.38},pmid={null},pmcid={null},mag_id={1546416838},journal={null},abstract={Real-time systems are composed of subsystems that may communicate by means of interruptions. An interruption is an event that requires preemption of a resource held by an executing subsystem. This subsystem may resume its execution from the point where it stopped when interruption handling finishes its execution. Testing systems composed of interruptions is hard since interruptions may happen at different points of execution. This fact demands the tester to apply a systematic procedure to specify and perform testing of the interruption behavior at specific points of interest. In this paper, we propose a symbolic approach to generating interruption test cases from real-time system models. We define an interruption operator for a timed model along with its properties. An empirical study illustrates its applicability to test application level interruptions on an Android system.}}
@ARTICLE{Yi_1994,title={Automatic verification of real-time communicating systems by constraint-solving.},year={1994},author={Wang Yi and Wang Yi and Paul Pettersson and Paul Pettersson and Mats Daniels and Mats Daniels},doi={null},pmid={null},pmcid={null},mag_id={1546631539},journal={null},abstract={In this paper, an algebra of timed processes with real-valued clocks is presented, which serves as a formal description language for real-time communicating systems. We show that requirements such as “a process will never reach an undesired state” can be verified by solving a simple class of constraint systems on the clock-variables. A complete method for reachability analysis associated with the language is developed, and implemented as an automatic verification tool based on constraint-solving techniques. Finally as examples, we study and verify the safety-properties of Fischer’s mutual exclusion protocol and a railway crossing controller.}}
@ARTICLE{Park_1981,title={Concurrency and Automata on Infinite Sequences},year={1981},author={David S. Park and David Park and David Park},doi={10.1007/bfb0017309},pmid={null},pmcid={null},mag_id={1547562281},journal={Theoretical Computer Science},abstract={The paper is concerned with ways in which fair concurrency can be modeled using notations for omega-regular languages - languages containing infinite sequences, whose recognizers a.re modified forms of Buchi or Muller-McNaughton automata. There are characterization of these languages in terms of recursion equation sets which involve both minimal and maximal fix point operators. The class of ω-regular languages is closed under a fair concurrency operator. A general  method for proving/deciding equivalences between such languages is obtained, derived from Milner's notion of "simulation".}}
@ARTICLE{Banerjee_2008,title={Regional Logic for Local Reasoning about Global Invariants},year={2008},author={Anindya Banerjee and Anindya Banerjee and David A. Naumann and David A. Naumann and Stan Rosenberg and Stan Rosenberg},doi={10.1007/978-3-540-70592-5_17},pmid={null},pmcid={null},mag_id={1548974835},journal={null},abstract={Shared mutable objects pose grave challenges in reasoning, especially for data abstraction and modularity. This paper presents a novel logic for error-avoiding partial correctness of programs featuring shared mutable objects. Using a first order assertion language, the logic provides heap-local reasoning about mutation and separation, via ghost fields and variables of type `region' (finite sets of object references). A new form of modifies clause specifies write, read, and allocation effects using region expressions; this supports effect masking and a frame rule that allows a command to read state on which the framed predicate depends. Soundness is proved using a standard program semantics. The logic facilitates heap-local reasoning about object invariants: disciplines such as ownership are expressible but not hard-wired in the logic.}}
@ARTICLE{Hennessy_2007,title={A Distributed Pi-Calculus},year={2007},author={Matthew Hennessy and Matthew Hennessy},doi={null},pmid={null},pmcid={null},mag_id={1549245411},journal={null},abstract={Distributed systems are fast becoming the norm in computer science. Formal mathematical models and theories of distributed behaviour are needed in order to understand them. This book proposes a distributed pi-calculus called Dpi, for describing the behaviour of mobile agents in a distributed world. It is based on an existing formal language, the pi-calculus, to which it adds a network layer and a primitive migration construct. A mathematical theory of the behaviour of these distributed systems is developed, in which the presence of types plays a major role. It is also shown how in principle this theory can be used to develop verification techniques for guaranteeing the behavior of distributed agents. The text is accessible to computer scientists with a minimal background in discrete mathematics. It contains an elementary account of the pi-calculus, and the associated theory of bisimulations. It also develops the type theory required by Dpi from first principles. • First book on formal foundations of distributed computation • Accessible introduction to the theory of the pi-calculus, with many exercises • Contains many worked examples and over 70 exercises}}
@ARTICLE{Grogono_2000,title={Copying and Comparing: Problems and Solutions},year={2000},author={Peter Grogono and Peter Grogono and Markku Sakkinen and Markku Sakkinen},doi={10.1007/3-540-45102-1_11},pmid={null},pmcid={null},mag_id={1551733566},journal={null},abstract={In object oriented programming, it is sometimes necessary to copy objects and to compare them for equality or inequality. We discuss some of the issues involved in copying and comparing objects and we address the problem of generating appropriate copying and comparing operations automatically, a service that is not provided by most object oriented languages and environments. Automatic generation appears to be not only desirable, because hand-coding these methods is mechanical and yet error-prone, but also feasible, because the form of the code is simple and largely predictable.

Some languages and some object models presented in the literature do support generic copying and comparing, typically defining separate "shallow" and "deep" versions of both operations. A close examination of these definitions reveals inadequacies. If the objects involved are simple, copying and comparing them is straightforward. However, there are at least three areas in which insufficient attention has been given to copying and comparing complex objects: (1) values are not distinguished from objects; (2) aggregation is not distinguished from association; and (3) the correct handling of linked structures other than trees is neglected.

Solving the third problem requires a mechanism built into the language, such as exists in Eiffel. Building such a mechanism without modifying the language requires a language with sufficient reflexive facilities, such as Smalltalk. Even then, the task is difficult and the result is likely to be insecure.

We show that fully automatic generation of copying and comparing operations is not feasible because compilers and other software tools have access only to the structure of the objects and not to their semantics. Nevertheless, it is possible to provide default methods that do most of the work correctly and can be fine-tuned with a small additional amount of hand-coding.

We include an example that illustrates the application of our proposals to C++. It is based on additional declarations handled by a preprocessor.}}
@ARTICLE{Moura_2007,title={Efficient E-Matching for SMT Solvers},year={2007},author={Leonardo de Moura and Leonardo de Moura and Nikolaj Bjørner and Nikolaj Bjørner},doi={10.1007/978-3-540-73595-3_13},pmid={null},pmcid={null},mag_id={1552077729},journal={null},abstract={Satisfiability Modulo Theories (SMT) solvers have proven highly scalable, efficient and suitable for integrating theory reasoning. However, for numerous applications from program analysis and verification, the ground fragment is insufficient, as proof obligations often include quantifiers. A well known approach for quantifier reasoning uses a matching algorithm that works against an E-graph to instantiate quantified variables. This paper introduces algorithms that identify matches on E-graphs incrementally and efficiently. In particular, we introduce an index that works on E-graphs, called E-matching code treesthat combine features of substitution and code trees, used in saturation based theorem provers. E-matching code trees allow performing matching against several patterns simultaneously. The code trees are combined with an additional index, called the inverted path index, which filters E-graph terms that may potentially match patterns when the E-graph is updated. Experimental results show substantial performance improvements over existing state-of-the-art SMT solvers.}}
@ARTICLE{Larsen_1993,title={Time Abstracted Bisimiulation: Implicit Specifications and Decidability},year={1993},author={Kim Guldstrand Larsen and Kim Guldstrand Larsen and Wang Yi and Wang Yi},doi={10.1007/3-540-58027-1_8},pmid={null},pmcid={null},mag_id={1552249698},journal={null},abstract={In the last few years a number of real-time process calculi have emerged with the purpose of capturing important quantitative aspects of real-time systems. In addition, a number of process equivalences sensitive to time-quantities have been proposed, among these the notion of timed (bisimulation) equivalence in [RR86, DS89, HR91, BB89, NRSV90, MT90, Wan91b].}}
@ARTICLE{Uribe_2000,title={Combinations of Model Checking and Theorem Proving},year={2000},author={Tomás E. Uribe and Tomás E. Uribe},doi={10.1007/10720084_11},pmid={null},pmcid={null},mag_id={1552618645},journal={null},abstract={The two main approaches to the formal verification of reactive systems are based, respectively, on model checking (algorithmic verification) and theorem proving (deductive verification). These two approaches have complementary strengths and weaknesses, and their combination promises to enhance the capabilities of each. This paper surveys a number of methods for doing so. As is often the case, the combinations can be classified according to how tightly the different components are integrated, their range of application, and their degree of automation.}}
@ARTICLE{Kassios_2006,title={Dynamic frames: support for framing, dependencies and sharing without restrictions},year={2006},author={Ioannis T. Kassios and Ioannis T. Kassios},doi={10.1007/11813040_19},pmid={null},pmcid={null},mag_id={1553421328},journal={null},abstract={This paper addresses the frame problem for programming theories that support both sharing and encapsulation through specification variables. The concept of dynamic frames is introduced. It is shown how a programming theory with dynamic frames supports both features, without the use of alias control or any other kind of restriction. In contrast, other approaches introduce a number of restrictions to the programs to ensure soundness.}}
@ARTICLE{Balakrishnan_2008,title={SLR: Path-Sensitive Analysis through Infeasible-Path Detection and Syntactic Language Refinement},year={2008},author={Gogul Balakrishnan and Gogul Balakrishnan and Sriram Sankaranarayanan and Sriram Sankaranarayanan and Franjo Ivančić and Franjo Ivancic and Wei Ou and Ou Wei and Aarti Gupta and Aarti Gupta},doi={10.1007/978-3-540-69166-2_16},pmid={null},pmcid={null},mag_id={1553630057},journal={null},abstract={We present a technique for detecting semantically infeasible paths in programs using abstract interpretation. Our technique uses a sequence of path-insensitive forward and backward runs of an abstract interpreter to infer paths in the control flow graph that cannot be exercised in concrete executions of the program.

We then present a syntactic language refinement (SLR) technique that automatically excludes semantically infeasible paths from a program during static analysis. SLR allows us to iteratively prove more properties. Specifically, our technique simulates the effect of a path-sensitive analysis by performing syntactic language refinement over an underlying path-insensitive static analyzer. Finally, we present experimental results to quantify the impact of our technique on an abstract interpreter for C programs.}}
@ARTICLE{Bosch_2000,title={Design and Use of Software Architectures: Adopting and Evolving a Product-Line Approach},year={2000},author={Jan Bosch and Jan Bosch},doi={null},pmid={null},pmcid={null},mag_id={1554977156},journal={null},abstract={1. Software Architecture and Product Lines. I. THE DESIGNING OF SOFTWARE ARCHITECTURES. 2. Design of Software Architectures. 3. Software Architectural Design: Case Studies. 4. Functionality-Based Architectural Design. 5. Assessing Software Architectures. 6. Transformation of Software Architectures. II. SOFTWARE PRODUCT LINES. 7. Software Product Lines: An Introduction. 8. Software Product Lines: Case Studies. 9. Designing a Product-Line Architecture. 10. Developing Components: Traditional. 11. Developing Components: Object-Oriented Frameworks. 12. Family-Based System Development. 13. Evolving Product-Line Assets. 14. Organizing for Software Product Lines. 15. Industrial Experiences. References. Index.}}
@ARTICLE{Blanc_2008,title={Scoot: a tool for the analysis of SystemC models},year={2008},author={Nicolas Blanc and Nicolas Blanc and Daniel Kroening and Daniel Kroening and Natasha Sharygina and Natasha Sharygina},doi={10.1007/978-3-540-78800-3_36},pmid={null},pmcid={null},mag_id={1556078275},journal={null},abstract={SystemC is a system-level modeling language and offers support for concurrency and arbitrary-width bit-vector arithmetic. The existing static analyzers for SystemC consider only small fragments of the language. We present Scoot, a model extractor for SystemC based on a C++ frontend. The models generated by Scoot can serve multiple purposes, ranging from verification and simulation to synthesis. Exemplarily, we report results indicating that our tool can be used to improve the performance of dynamic execution up to a factor of five.}}
@ARTICLE{Beyer_2004,title={The Blast Query Language for Software Verification},year={2004},author={Dirk Beyer and Dirk Beyer and Adam Chlipala and Adam Chlipala and Thomas A. Henzinger and Thomas A. Henzinger and Ranjit Jhala and Ranjit Jhala and Rupak Majumdar and Rupak Majumdar},doi={10.1007/978-3-540-27864-1_2},pmid={null},pmcid={null},mag_id={1556262481},journal={null},abstract={Blast is an automatic verification tool for checking temporal safety properties of C programs. Blast is based on lazy predicate abstraction driven by interpolation-based predicate discovery. In this paper, we present the Blast specification language. The language specifies program properties at two levels of precision. At the lower level, monitor automata are used to specify temporal safety properties of program executions (traces). At the higher level, relational reachability queries over program locations are used to combine lower-level trace properties. The two-level specification language can be used to break down a verification task into several independent calls of the model-checking engine. In this way, each call to the model checker may have to analyze only part of the program, or part of the specification, and may thus succeed in a reduction of the number of predicates needed for the analysis. In addition, the two-level specification language provides a means for structuring and maintaining specifications.}}
@ARTICLE{Matthews_2010,title={And then again},year={2010},author={Bob Matthews and Bob Matthews and Bob Matthews},doi={null},pmid={null},pmcid={null},mag_id={1556434397},journal={null},abstract={This exhibition was a key survey of contemporary printmaking identifying current themes and advances in new and traditional techniques and processes.  The exhibition brought together invited artists from Portugal and the UK.}}
@ARTICLE{Krstić_2002,title={Verifying BDD Algorithms through Monadic Interpretation},year={2002},author={Sava Krstić and Sava Krstić and John Matthews and John Matthews},doi={10.1007/3-540-47813-2_13},pmid={null},pmcid={null},mag_id={1556606117},journal={null},abstract={Many symbolic model checkers use Binary Decision Diagrams (BDDs) to efficiently determine whether two Boolean formulas are semantically equivalent. For realistic problems, the size of the generated BDDs can be enormous, and constructing them can easily become a performance bottleneck. As a result, most state-of-the-art BDD programs are written as highly optimized imperative C programs, increasing the risk of soundness defects in their implementation. This paper describes the use of monadic interpreters to formally verify BDD algorithms at a higher level of abstraction than the original C program, but still at a concrete enough level to retain their essential imperative features. Our hope is then that verification of the original C program can be achieved by strictly localized refinement reasoning.During this work we encountered the surprising fact that modeling imperative recursive algorithms monadically often results in logical functions that are both partial and nestedly-recursive in their (hidden) state parameters, making termination proofs difficult.}}
@ARTICLE{Laplante_1992,title={Real-Time Systems Design and Analysis},year={1992},author={Phillip A. Laplante and Phillip A. Laplante},doi={null},pmid={null},pmcid={null},mag_id={1557079818},journal={null},abstract={Preface. Chapter 1. Basic Real-Time Concepts. Chapter 2. Hardware Considerations. Chapter 3. Real-Time Operating Systems. Chapter 4. Software Requirements. Chapter 5. Software Systems Design. Chapter 6. The Software Production Process. Chapter 7. Performance Estimation and Optimization. Chapter 8. Engineering Considerations. References. Glossary. Index.}}
@ARTICLE{Pierce_2002,title={Types and Programming Languages},year={2002},author={Benjamin C. Pierce and Benjamin C. Pierce},doi={null},pmid={null},pmcid={null},mag_id={1557561422},journal={null},abstract={A type system is a syntactic method for automatically checking the absence of certain erroneous behaviors by classifying program phrases according to the kinds of values they compute. The study of type systems -- and of programming languages from a type-theoretic perspective -- has important applications in software engineering, language design, high-performance compilers, and security.This text provides a comprehensive introduction both to type systems in computer science and to the basic theory of programming languages. The approach is pragmatic and operational; each new concept is motivated by programming examples and the more theoretical sections are driven by the needs of implementations. Each chapter is accompanied by numerous exercises and solutions, as well as a running implementation, available via the Web. Dependencies between chapters are explicitly identified, allowing readers to choose a variety of paths through the material.The core topics include the untyped lambda-calculus, simple type systems, type reconstruction, universal and existential polymorphism, subtyping, bounded quantification, recursive types, kinds, and type operators. Extended case studies develop a variety of approaches to modeling the features of object-oriented languages.}}
@ARTICLE{Moller_1990,title={A temporal calculus of communicating systems},year={1990},author={Faron Moller and Faron Moller and C. Tofts and Chris Tofts},doi={10.1007/bfb0039073},pmid={null},pmcid={null},mag_id={1559986400},journal={null},abstract={In this paper, we introduce a calculus of communicating systems which allows for the expression and analysis of timing constraints, for example as is important for real-time processes. We present the language, along with its formal semantics, and derive algebraic laws for reasoning about processes in the language. Though the core language is simple, we show that the language has several powerful derived operators which we demonstrate to be useful in several examples.}}
@ARTICLE{Lopez-Herrejon_2001,title={A Standard Problem for Evaluating Product-Line Methodologies},year={2001},author={Roberto E. Lopez-Herrejon and Roberto E. Lopez-Herrejon and Don Batory and Don Batory},doi={10.1007/3-540-44800-4_2},pmid={null},pmcid={null},mag_id={1560222722},journal={null},abstract={We propose a standard problem to evaluate product-line methodologies. It relies on common knowledge from Computer Science, so that domain-knowledge can be easily acquired, and it is complex enough to expose the fundamental concepts of product-line methodologies. As a reference point, we present a solution to this problem using the GenVoca design methodology. We explain a series of modeling, implementation, and benchmarking issues that we encountered, so that others can understand and compare our solution with theirs.}}
@ARTICLE{Petri_1962,title={Kommunikation mit Automaten},year={1962},author={C. A. Petri and Carl Adam Petri and C. A. Petri},doi={null},pmid={null},pmcid={null},mag_id={1561267729},journal={null},abstract={Diese Arbeit befasst sich mit den begrifflichen Grundlagen einer Theorie der Kommunikation. Die Aufgabe dieser Theorie soll es sein, moglichst viele Erscheinungen bei der Informationsubertragung und Informationswandlung in einheitlicher und exakter Weise zu beschreiben.
The theory of automata is shown not capable of representing the actual physical flow of information in the solution of a recursive problem. The argument proceeds as follows:
1. We assume the following postulates:
a) there exists an upper bound on the speed of signals;
b) there exists an upper bound on the density with which information can be stored.

2. Automata of fixed, finite size can recognize, at best, only iteratively defined classes of input sequences. (See Kleene (11) and Copi, Elgot, and Wright (8).)

3. Recursively defined classes of input sequences that cannot be defined iteratively can be recognized only by automata of unbounded size.

4. In order for an automaton to solve a (soluble) recursive problem, the possibility must be granted that it can be extended unboundedly in whatever way might be required.

5. Automata (as actual hardware) formulated in accordance with automata theory will, after a finite number of extensions, conflict with at least one of the postulates named above.
Suitable conceptual structures for an exact theory of communication are then discussed, and a theory of communication proposed.
All of the really useful results of automata theory may be expressed by means of these new concepts. Moreover, the results retain their usefulness and the new nrocedure has definite advantages over the older ones.
The proposed representation differs from each of the presently known theories concerning information on at least one of the following essential points:
1. The existence of a metric is assumed for either space nor time nor for other physical magnitudes.
2. Time is introduced as a strictly local relation between states.
3. The objects of the theory are discrete, and they are combined and produced only by means of strictly finite techniques.

The following conclusions drawn from the results of this work may be cited as of some practical interest:
1. The tolerance requirements for the response characteristics of computer components can be substantially weakened if the computer is suitably structured.
2. It is possible to design computers structurally in such a way that they are asynchronous, all parts operating in parallel, and can be extended arbitrarily without interrupting their computation.
3. For complicated organizational processes of any given sort the theory yields a means of representation that with equal rigor and simplicity accomplishes more than the theory of synchronous automata.}}
@ARTICLE{Jéron_2006,title={Symbolic Determinisation of Extended Automata},year={2006},author={Thierry Jéron and Thierry Jéron and Hervé Marchand and Hervé Marchand and Hervé Marchand and Hervé Marchand and Vlad Rusu and Vlad Rusu},doi={10.1007/978-0-387-34735-6_18},pmid={null},pmcid={null},mag_id={1562837134},journal={null},abstract={We define a symbolic determinisation procedure for a class of infinite-state systems, which consists of automata extended with symbolic variables that may be infinite-state. The subclass of extended automata for which the procedure terminates is characterised as bounded lookahead extended automata. It corresponds to automata for which, in any location, the observation of a bounded-length trace is enough to infer the first transition actually taken. We discuss applications of the algorithm to the verification, testing, and diagnosis of infinite-state systems.}}
@ARTICLE{Quigley_2003,title={A Programming Logic for Java Bytecode Programs},year={2003},author={Claire Quigley and Claire Louise Quigley},doi={10.1007/10930755_3},pmid={null},pmcid={null},mag_id={1564435947},journal={null},abstract={Using the Isabelle theorem prover [10] we have developed a programming logic for Java bytecode, and demonstrated that it can be used to prove properties of simple bytecode programs involving loops. Our motivation for this was to produce a method by which Java Just-In-Time (JIT) compilers could be assisted to produce more efficient code. This paper discusses the issues involved in the development of the programming logic as it stands, and suggests possible extensions to it. We also describe our experiences of the difficulties inherent in carrying out proof at the level of bytecode instructions, along with the benefits and disadvantages of using a mechanized proof tool.}}
@ARTICLE{Rodrı́guez_2009,title={A General Testability Theory},year={2009},author={Ismael Rodrı́guez and Ismael Rodríguez},doi={10.1007/978-3-642-04081-8_38},pmid={null},pmcid={null},mag_id={1565992638},journal={null},abstract={We present a general framework allowing to classify testing problems into five testability classes. Classes differ in the number of tests we must apply to precisely determine whether the system is correct or not. The conditions that enable/disable finite testability are analyzed. A general method to reduce a testing problem into another is presented. The complexity of finding complete test suites and measuring the suitability of incomplete suites is analyzed.}}
@ARTICLE{Artho_2003,title={Experiments with test case generation and runtime analysis},year={2003},author={Cyrille Artho and Cyrille Artho and D. Drusinksy and Doron Drusinksy and Allen Goldberg and Allen Goldberg and Klaus Havelund and Klaus Havelund and Mike Lowry and Michael Lowry and Corina S. Pǎsǎreanu and Corina S. Pasareanu and Corina S. Pasareanu and Grigore Roşu and Grigore Rosu and Willem Visser and Willem Visser},doi={10.1007/3-540-36498-6_5},pmid={null},pmcid={null},mag_id={1566440953},journal={Lecture Notes in Computer Science},abstract={Software testing is typically an ad hoc process where human testers manually write many test inputs and expected test results, perhaps automating their execution in a regression suite. This process is cumbersome and costly. This paper reports preliminary results on an approach to further automate this process. The approach consists of combining automated test case generation based on systematically exploring the program's input domain, with runtime analysis, where execution traces are monitored and verified against temporal logic specifications, or analyzed using advanced algorithms for detecting concurrency errors such as data races and deadlocks. The approach suggests to generate specifications dynamically per input instance rather than statically once-and-for-all. The paper describes experiments with variants of this approach in the context of two examples, a planetary rover controller and a space craft fault protection system.}}
@ARTICLE{Alcalde_2004,title={Network Protocol System Passive Testing for Fault Management: A Backward Checking Approach},year={2004},author={Baptiste Alcalde and Baptiste Alcalde and Ana Cavalli and Ana Cavalli and Dongluo Chen and Dongluo Chen and Davy Khuu and Davy Khuu and David Lee and David Lee},doi={10.1007/978-3-540-30232-2_10},pmid={null},pmcid={null},mag_id={1566793636},journal={null},abstract={Passive testing has proved to be a powerful technique for protocol system fault detection by observing its input/output behaviors yet without interrupting its normal operations. To improve the fault detection capabilities we propose a backward checking method that analyzes in a backward fashion the input/output trace from passive testing and its past. It effectively checks both the control and data portion of a protocol system, compliments the forward checking approaches, and detects more errors. We present our algorithm, study its termination and complexity, and report experiment results on the protocol SCP.}}
@ARTICLE{Sato_2003,title={Cache control program},year={2003},author={Tomoaki Sato and Kenji Tonami and Yuji Kato},doi={null},pmid={null},pmcid={null},mag_id={1568256251},journal={null},abstract={A cache control program that reduces cache control load. The cache control programs functions as a multi-bind cache (MBC) manager of a file server. The MBC manager manages a cache memory as a plurality of extents. The MBC manager generates cache IDs including object identifications for identifying the objects of each hierarchical level. Further, the MBC manager generates a cache header table indicating the relation of the cache IDs and the cache extents for each hierarchical level.}}
@ARTICLE{Staworko_2009,title={Equivalence of deterministic nested word to word transducers},year={2009},author={Sławek Staworko and Slawomir Staworko and G Laurence and Grégoire Laurence and Aurélien Lemay and Aurélien Lemay and Joachim Niehren and Joachim Niehren},doi={10.1007/978-3-642-03409-1_28},pmid={null},pmcid={null},mag_id={1570372713},journal={null},abstract={We study the equivalence problem of deterministic nested word to word transducers and show it to be surprisingly robust. Modulo polynomial time reductions, it can be identified with 4 equivalence problems for diverse classes of deterministic non-copying order-preserving transducers. In particular, we present polynomial time back and fourth reductions to the morphism equivalence problem on context free languages, which is known to be solvable in polynomial time.}}
@ARTICLE{Drusinsky_2006,title={Modeling and Verification Using UML Statecharts: A Working Guide to Reactive System Design, Runtime Monitoring and Execution-based Model Checking},year={2006},author={Doron Drusinsky and Doron Drusinsky},doi={null},pmid={null},pmcid={null},mag_id={1570738019},journal={null},abstract={OUTLINE 1. Motivation 1.1 The Need for Computer Based Verification 1.2 The Desired Process Triangle 1.3 The Need for Integration: UML Statecharts and Formal Specifications 2. Assertion Languages, Applications, and Tools 2.1 Primary Assertion/Specification Languages: Temporal Logic and Statecharts 2.2 Applications: formal methods, run-time monitoring 2.2.1 Classical Formal Methods. 2.2.2 Runtime Monitoring and Verification. 2.2.3 Automatic Test Generation and Execution-based Model Checking. 2.2.4 Run-time Control Flow using Assertions (Exception Handling) 2.3 Writing Temporal Logic Assertions: the Process 2.4 Tools 3. Integration: UML-Statecharts Integrated with Temporal Logic Assertions 3.1 Statechart Specifications 3.2 TLCharts 3.3. Tools 4. Advanced Topics 4.1 Petri Nets for Distributed Systems. 4.2 Other Specification Languages: CTL, CTL+, Z. 4.3 Automata over Infinite Sequences. 4.4. Automata and Semigroups. 4.5 LTL vs. First Order Logic and SQL, LTL vs. Regular Expressions. 4.6 LTL and Statechart Semantics. 4.6 Knowledge Monitoring.}}
@ARTICLE{Brucker_2005,title={Interactive testing with HOL-TestGen},year={2005},author={Achim D. Brucker and Achim D. Brucker and Burkhart Wolff and Burkhart Wolff},doi={10.1007/11759744_7},pmid={null},pmcid={null},mag_id={1571510408},journal={null},abstract={HOL-TestGen is a test environment for specification-based unit testing build upon the proof assistant Isabelle/HOL . While there is considerable skepticism with regard to interactive theorem provers in testing communities, we argue that they are a natural choice for (automated) symbolic computations underlying systematic tests. This holds in particular for the development on non-trivial formal test plans of complex software, where some parts of the overall activity require inherently guidance by a test engineer. In this paper, we present the underlying methods for both black box and white box testing in interactive unit test scenarios. HOL-TestGen can also be understood as a unifying technical and conceptual framework for presenting and investigating the variety of unit test techniques in a logically consistent way.}}
@ARTICLE{Alur_2010,title={Temporal reasoning for procedural programs},year={2010},author={Rajeev Alur and Rajeev Alur and Swarat Chaudhuri and Swarat Chaudhuri},doi={10.1007/978-3-642-11319-2_7},pmid={null},pmcid={null},mag_id={1572346004},journal={null},abstract={While temporal verification of programs is a topic with a long history, its traditional basis—semantics based on word languages—is ill-suited for modular reasoning about procedural programs. We address this issue by defining the semantics of procedural (potentially recursive) programs using languages of nested words and developing a framework for temporal reasoning around it. This generalization has two benefits. First, this style of reasoning naturally unifies Manna-Pnueli-style temporal reasoning with Hoare-style reasoning about structured programs. Second, it allows verification of “non-regular” properties of specific procedural contexts—e.g., “If a lock is acquired in a context, then it is released in the same context.” We present proof rules for a variety of properties such as local safety, local response, and staircase reactivity; our rules are sufficient to prove all temporal properties over nested words. We show that our rules are sound and relatively complete.}}
@ARTICLE{Souza_2000,title={Mutation testing applied to Estelle specifications},year={2000},author={Simone do Rocio Senger de Souza and S. do Rocio Senger de Souza and José Carlos Maldonado and José Carlos Maldonado and Sandra Fabbri and Sandra Fabbri and Wanderley Lopes de Souza and W. Lopes de Souza},doi={10.1023/a:1008978021407},pmid={null},pmcid={null},mag_id={1574590844},journal={null},abstract={Many researchers have pursued the establishment of a low-cost, effective testing and validation strategy at the program level as well as at the specification level. Mutation Testing is an error-based approach, originally introduced for program testing, that provides testers a systematic way to evaluate how good a given tester is. Some studies have also investigated its use to generate testers. In this article the application of Mutation Testing for validating Estelle specifications is proposed. A mutation operator set for Estelle-one of the crucial points for effectively applying Mutation Testing-is defined, addressing: the validation of the behavior of the modules, the communication among modules and the architecture of the specification. In this scope, these operators can be taken as a fault model. Considering this context, a strategy for validating Estelle-based specification is proposed and exemplified using the alternating-bit protocol.}}
@ARTICLE{Groote_1990,title={Transition system specifications with negative premises},year={1990},author={Jan Friso Groote and Jan Friso Groote},doi={10.1007/bfb0039069},pmid={null},pmcid={null},mag_id={1575387215},journal={null},abstract={Abstract   In this article the general approach to Plotkin-style operational semantics of Groote and Vaandrager (1989) is extended to transition system specifications (TSSs) with rules that may contain negative premises. Two problems arise: firstly the rules may be inconsistent, and secondly it is not obvious how a TSS determines a transition relation. We present a general method, based on the stratification technique in logic programming, to prove consistency of a set of rules and we show how a specific transition relation can be associated with a TSS in a natural way. Then a special format for the rules, the  ntyft/ntyxt  format, is defined. It is shown that for this format three important theorems hold. The first theorem says that bisimulation is a congruence if all operators are defined using this format. The second theorem states that, under certain restrictions, a TSS in  ntyft  format can be added conservatively to a TSS in pure  ntyft/ntyxt  format. Finally, it is shown that the trace congruence for image-finite processes induced by the pure  ntyft/ntyxt  format is precisely bisimulation equivalence.}}
@ARTICLE{Arnedo_2003,title={Fast testing of critical properties through passive testing},year={2003},author={José Antonio Arnedo and José Antonio Arnedo and Ana Cavalli and Ana Cavalli and Manuel Núñez and Manuel Núñez},doi={10.1007/3-540-44830-6_22},pmid={null},pmcid={null},mag_id={1575611811},journal={Lecture Notes in Computer Science},abstract={We present a novel methodology to perform passive testing. The usual approach consists in recording the trace produced by the implementation under test and trying to find a fault by comparing this trace with the specification. We propose a more active approach to passive testing where the minimum set of (critical) properties required to a correct implementation may be explicitly indicated. In short, an invariant expresses that each time that the implementation under test performs a given sequence of input/output actions, then it must show a behavior reflected in the invariant. By using an adaptation of the classical pattern matching algorithms on strings, we obtain that the complexity of checking whether an invariant is fulfilled by the observed trace is in O(n ċ m), where n and m are the lengths of the trace and the invariant, respectively. If the length of the invariant is much smaller than the length of the trace then this complexity is almost linear with respect to the length of the trace. Actually, this is usually the case for most practical examples. In addition to our methodology, we present the case study that was the driving force for the development of our theory: The Wireless Application Protocol (WAP). We present a test architecture for WAP as well as the experimental results obtained from the application of our passive testing with invariants approach.}}
@ARTICLE{Harman_2001,title={The relationship between program dependence and mutation analysis},year={2001},author={Mark Harman and Mark Harman and Rob Hierons and Robert M. Hierons and Sebastian Danicic and Sebastian Danicic},doi={10.1007/978-1-4757-5939-6_4},pmid={null},pmcid={null},mag_id={1575737046},journal={null},abstract={This paper presents some connections between dependence analysis and mutation testing. Specifically, dependence analysis can be applied to two problems in mutation testing, captured by the questions: 1. How do we avoid the creation of equivalent mutants? 2. How do we generate test data that kills non-equivalent mutants? The theoretical connections described here suggest ways in which a dependence analysis tool might be used, in combination with existing tools for mutation testing, for test-data generation and equivalent-mutant decision. In this paper the variable orientated, fine grained dependence framework of Jackson and Rollins is used to achieve these two goals. This framework of dependence analysis appears to be better suited to mutation testing than the more traditional, Program Dependence Graph (PDG) approach, used in slicing and other forms of program analysis. The relationship between dependence analysis and mutation testing is used to define an augmented mutation testing process, with starts and ends with dependence analysis phases. The pre-analysis removes a class of equivalent mutants from further analysis, while the post-analysis phase is used to simplify the human effort required to study the few mutants that evade the automated phases of the process.}}
@ARTICLE{Aştefǎnoaei_2008,title={A Verification Framework for Normative Multi-Agent Systems},year={2008},author={Lǎcrǎmioara Aştefǎnoaei and Lacramioara Astefanoaei and Lăcrămioara Aştefănoaei and Mehdi Dastani and Mehdi Dastani and John-Jules Meyer and John-Jules Ch. Meyer and Frank S. de Boer and Frank S. Boer and Frank S. de Boer},doi={10.1007/978-3-540-89674-6_9},pmid={null},pmcid={null},mag_id={1575830530},journal={null},abstract={This paper presents a programming language that facilitates the implementation of coordination artifacts which in turn can be used to regulate the behaviour of individual agents. The programming language provides constructs inspired by social and organisational concepts. The operational semantics of the language is prototyped in Maude, a rewrite logic software. Properties of the coordination artifacts are model-checked with the Maude LTL model-checker.}}
@ARTICLE{Jackson_2003,title={Alloy: a logical modelling language},year={2003},author={Daniel Jackson and Daniel Jackson},doi={10.1007/3-540-44880-2_1},pmid={null},pmcid={null},mag_id={1576452827},journal={null},abstract={Alloy, like Z, is a language for modelling software systems. Indeed, it draws many of its good ideas from Z: in particular, representing all data structures with sets and relations, and representing behaviour and properties with simple formulas. Unlike Z, however, Alloy was designed with automatic analysis in mind. A constraint solver based on reduction to SAT can check properties of Alloy models, and simulate execution (even of implicit operations). The key idea is to consider all possible bindings of a formula that assign no more than some small number of atoms to each given type. The result is a flexible mechanism that provides rapid and concrete feedback during evolution of a model. It cannot prove properties, but by exhausting all small test cases, it usually succeeds in finding bugs rapidly.

In my talk, I'll explain the fundamental ideas underlying Alloy and its analysis: its basis in relation rather than sets, and the compromises (notably a restriction to first order structures and formulas) that make analysis possible. I'll compare Alloy's specification-structuring mechanism, the signature, to Z's schema. I'll illustrate some modelling idioms that we have developed in using Alloy, focusing on how mutation is represented. I'll also show some examples of typical analyses, including a trace-based analysis that employs the idea of 'machine diameter' from bounded model checking to ensure that all reachable states are considered.}}
@ARTICLE{Cok_2004,title={ESC/Java2: uniting ESC/Java and JML},year={2004},author={David R. Cok and David R. Cok and Joseph R. Kiniry and Joseph R. Kiniry},doi={10.1007/978-3-540-30569-9_6},pmid={null},pmcid={null},mag_id={1580328473},journal={null},abstract={The ESC/Java tool was a lauded advance in effective static checking of realistic Java programs, but has become out-of-date with respect to Java and the Java Modeling Language (JML). The ESC/Java2 project, whose progress is described in this paper, builds on the final release of ESC/Java from DEC/SRC in several ways. It parses all of JML, thus can be used with the growing body of JML-annotated Java code; it has additional static checking capabilities; and it has been designed, constructed, and documented in such a way as to improve the tool's usability to both users and researchers. It is intended that ESC/Java2 be used for further research in, and larger-scale case studies of, annotation and verification, and for studies in programmer productivity that may result from its integration with other tools that work with JML and Java. The initial results of the first major use of ESC/Java2, that of the verification of parts of the tally subsystem of the Dutch Internet voting system are presented as well.}}
@ARTICLE{Griesemer_2000,title={A Compiler for the Java HotSpotTM Virtual Machine},year={2000},author={Robert Griesemer and Robert Griesemer and Srdjan Mitrovic and Srdjan Mitrovic},doi={null},pmid={null},pmcid={null},mag_id={1580598666},journal={null},abstract={null}}
@ARTICLE{En‐Nouaary_2003,title={A guided method for testing timed input output automata},year={2003},author={Abdeslam En‐Nouaary and Abdeslam En-Nouaary and Rachida Dssouli and Rachida Dssouli},doi={10.1007/3-540-44830-6_16},pmid={null},pmcid={null},mag_id={1581209726},journal={Lecture Notes in Computer Science},abstract={Real-time systems are those systems whose behaviors are time dependent. Reliability is one of the characteristics of such systems and testing is one of the techniques that can be used to ensure reliable real-time systems. This paper presents a method for testing real-time systems specified by Timed Input Output Automata (TIOA). Our method is based on the concept of test purposes. The use of test purposes helps reduce the number of test cases generated since an exhaustive testing of a TIOA causes the well-known state explosion problem. The approach we present in this paper consists of three main steps. First, a synchronous product of the specification and test purpose is computed. Then, a subautomaton (called Grid Automata) representing a subset of the state space of this product is derived. Finally, test cases are generated from the resulting grid automata. The test cases generated by our method are executable and can easily be represented in TTCN (Tabular Tree Combined Notation).}}
@ARTICLE{Lammich_2010,title={The isabelle collections framework},year={2010},author={Peter Lammich and Peter Lammich and Andreas Lochbihler and Andreas Lochbihler},doi={10.1007/978-3-642-14052-5_24},pmid={null},pmcid={null},mag_id={1581325065},journal={null},abstract={The Isabelle Collections Framework (ICF) provides a unified framework for using verified collection data structures in Isabelle/HOL formalizations and generating efficient functional code in ML, Haskell, and OCaml. Thanks to its modularity, it is easily extensible and supports switching to different data structures any time. For good integration with applications, a data refinement approach separates the correctness proofs from implementation details. The generated code based on the ICF lies in better complexity classes than the one that uses Isabelle's default setup (logarithmic vs. linear time). In a case study with tree automata, we demonstrate that the ICF is easy to use and efficient: An ICF based, verified tree automata library outperforms the unverified Timbuk/Taml library by a factor of 14.}}
@ARTICLE{Li_2004,title={Property-oriented testing of real-time systems},year={2004},author={Shuhao Li and Shuhao Li and Ji Wang and Ji Wang and Ji Wang and Ji Wang and Wei Dong and Wei Dong and Zhichang Qi and Zhichang Qi},doi={10.1109/apsec.2004.78},pmid={null},pmcid={null},mag_id={1582007022},journal={null},abstract={Although statecharts has gained widespread use as a formalism for modeling reactive real-time systems, testing these systems still confronts some difficulties, of which a major one is the existence of numerous and complex system behaviors. It is extremely difficult to conduct comprehensive and in-depth testing of such real-time systems. This paper presents an approach to property-oriented real-time testing. Necessary real-time extensions are proposed such that the time-enriched statecharts can describe nontrivial timing constraints. The properties to be tested are characterized by a restricted real-time logic. Then the targeted test sequences are derived from the real-time models according to the user-specified properties. Using this approach, testing efforts can be focused on particular properties of the real-time systems and usually only a small portion of the total behaviors needs to be tested.}}
@ARTICLE{Milner_1991,title={Operational and algebraic semantics of concurrent processes},year={1991},author={Robin Milner and Robin Milner},doi={10.1016/b978-0-444-88074-1.50024-x},pmid={null},pmcid={null},mag_id={1582665553},journal={null},abstract={null}}
@ARTICLE{Denzin_2012,title={Collecting and Interpreting Qualitative Materials},year={2012},author={Norman K. Denzin and Norman K. Denzin and Yvonna S. Lincoln and Yvonna S. Lincoln},doi={null},pmid={null},pmcid={null},mag_id={1582770597},journal={null},abstract={Preface - Norman K. Denzin and Yvonna S. Lincoln About the Editors About the Contributors 1. Introduction: The Discipline and Practice of Qualitative Research - Norman K.Denzin and Yvonna S. Lincoln I. Methods of Collecting and Analyzing Empirical Materials 2. Narrative Inquiry: Still a Field in the Making - Susan E. Chase 3. Critical Arts-based Inquiry: The Pedagogy and Performance of a Radical Ethical Aesthetic - Susan Finley 4. Oral History - Linda Shopes 5. Observations on Observation: Continuities and Challenges - Michael Angrosino and Judith Rosenberg 6. Visual Methodology: Toward a More Seeing Research - Jon D. Prosser 7. Performative Autoethnography: Critical Embodiments and Possibilities - Tami Spry 8. The Methods, Politics, and Ethics of Representation in Online Ethnography - Sarah Gaston 9. Analyzing Talk and Text - Anssi Parakyla and Johanna Ruusuvuori 10. Focus Groups: Contingent Articulations of Pedagogy, Politics, and Inquiry - George Kamberelis and Greg Dimitriadis II. The Art and Practices of Interpretation, Evaluation, and Presentation 11. Qualitative Research, Science, and Government: Evidence, Criteria, Policy, and Politics - Harry Torrance 12. Reflections on Interpretive Adequacy in Qualitative Research - David L. Altheide and John M. Johnson 13. Analysis and Representation Across the Continuum - Laura L. Ellingson 14. Post Qualitative Research: The Critique and the Coming After - Elisabeth Adams St. Pierre 15. Qualitative Research and Technology: In the Midst of a Revolution - Judith Davidson and Silvana diGregorio 16. The Elephant in the Living Room, or Extending the Conversation About the Politics of Evidence - Norman K. Denzin 17. Writing into Position: Strategies for Composition and Evaluation - Ronald J. Pelias 18. Evaluation as a Relationally Responsible Practice - Tineke Abma and Guy A.M. Widdershoven, Norman K. Denzin and Yvonna S. Lincoln Author Index Subject Index}}
@ARTICLE{Müller_2003,title={SystemC: methodologies and applications},year={2003},author={Wolfgang Müller and Wolfgang H. Müller and Wolfgang Rosenstiel and Wolfgang Rosenstiel and Jürgen Ruf and Jürgen Ruf},doi={null},pmid={null},pmcid={null},mag_id={1583548393},journal={null},abstract={Foreword. Preface. 1: A SystemC Based System On Chip Modelling and Design Methodology Y. Vanderperren, M. Pauwels, W. Dehaene, A.Berna, F. Ozdemir. 2: Using Transactional Level Models in a SoC Design Flow A. Clouard, K. Jain, F. Ghenassia, L. Maillet-Contoz, J.-P. Strassen. 3: Refining a High Level SystemC Model B. Niemann, F. Mayer, F.J. Rabano Rubio, M. Speitel. 4: An ASM Based SystemC Simulation Semantics W. Muller, J. Ruf, W. Rosenstiel. 5: SystemC as a Complete Design and Validation Environment A. Fin, F. Fummi, G. Pravadelli. 6: System Level Performance Estimation N. Pazos, W. Brunnbauer, J. Foag, T. Wild. 7: Design of Protocol Dominated Digital Systems R. Siegmund, U. Pross, D. Muller. 8: Object Oriented Hardware Design and Synthesis Based on SystemC 2.0 E. Grimpe, W. Nebel, F. Oppenheimer, T. Schubert. 9: Embedded Software Generation from SystemC for Platform Based Design F. Herrera, V. Fernandez, P. Sanchez, E. Villar. 10: SystemC-AMS: Rationales, State of the Art, and Examples K. Einwich, P. Schwarz, C. Grimm, C. Meise. 11: Modeling and Refinement of Mixed-Signal Systems with SystemC C. Grimm. References. Index.}}
@ARTICLE{Leavens_1999,title={JML: A Notation for Detailed Design},year={1999},author={Gary T. Leavens and Gary T. Leavens and Albert L. Baker and Albert L. Baker and Clyde Ruby and Clyde Ruby},doi={10.1007/978-1-4615-5229-1_12},pmid={null},pmcid={null},mag_id={1583826417},journal={null},abstract={JML is a behavioral interface specification language tailored to Java. It is designed to be written and read by working software engineers, and should require only modest mathematical training. It uses Eiffel-style syntax combined with model-based semantics, as in VDM and Larch. JML supports quantifiers, specification-only variables, and other enhancements that make it more expressive for specification than Eiffel and easier to use than VDM and Larch.}}
@ARTICLE{Cardell-Oliver_1998,title={A Practical and Complete Algorithm for Testing Real-Time Systems},year={1998},author={Rachel Cardell-Oliver and Rachel Cardell-Oliver and T. D. Glover and Tim Glover},doi={10.1007/bfb0055352},pmid={null},pmcid={null},mag_id={1584511540},journal={Lecture Notes in Computer Science},abstract={This paper presents a formal method for generating conformance tests for real-time systems. Our algorithm is complete in that, under a test hypothesis, if the system being tested passes every test generated then the tested system is bisimilar to its specification. Because the test algorithm has exponential worst case complexity and finite state automata models of real-time systems are typically very large, a judicious choice of model is critical for the successful testing of real-time systems. Developing such a model and demonstrating its effectiveness are the main contributions of this paper.}}
@ARTICLE{Schäfer_2007,title={Linking Programs to Architectures: An Object-Oriented Hierarchical Software Model Based on Boxes},year={2007},author={Jan Schäfer and Jan Schäfer and Markus Reitz and Markus Reitz and Jean-Marie Gaillourdet and Jean-Marie Gaillourdet and Arnd Poetzsch-Heffter and Arnd Poetzsch-Heffter},doi={10.1007/978-3-540-85289-6_10},pmid={null},pmcid={null},mag_id={1585327258},journal={null},abstract={Modeling software systems has several purposes. The model provides a communication means between developers, a backbone to specify and check properties of the system, and a structure to organize, explain, and develop the implementation of the system. The focus of our approach is to address these purposes for hierarchically structured, object-oriented software systems. The hierarchical structure refers to the component instances at runtime: a runtime componentmay consist of a dynamically changing number of objects and other runtime components. Our modeling technique builds on and extends the concepts of class-based object-oriented languages. Runtime components are created by instantiating box classes. The modeling technique provides ports to tame object references and aliasing and to decouple components from their environment. It supports dynamic linkage, i.e. ports can be connected and disconnected at runtime. The used concurrency model is based on the join calculus.}}
@ARTICLE{Ahrendt_2005,title={Automatic validation of transformation rules for java verification against a rewriting semantics},year={2005},author={Wolfgang Ahrendt and Wolfgang Ahrendt and Andreas Röth and Andreas Roth and Ralf Sasse and Ralf Sasse},doi={10.1007/11591191_29},pmid={null},pmcid={null},mag_id={1586257880},journal={null},abstract={This paper presents a methodology for automatically validating program transformation rules that are part of a calculus for Java source code verification. We target the Java Dynamic Logic calculus which is implemented in the interactive prover of the KeY system. As a basis for validation, we take an existing SOS style rewriting logic semantics for Java, formalized in the input language of the Maude system. That semantics is ‘lifted’ to cope with schematic programs like the ones appearing in program transformation rules. The rewriting theory is further extended to generate valid initial states for involved program fragments, and to check the final states for equivalence. The result is used in frequent validation runs over the relevant fragment of the calculus in the KeY system.}}
@ARTICLE{Glabbeek_1989,title={Equivalence Notions for Concurrent Systems and Refinement of Actions (Extended Abstract)},year={1989},author={Rob J. van Glabbeek and Rob van Glabbeek and Ursula Goltz and Ursula Goltz},doi={null},pmid={null},pmcid={null},mag_id={1586537374},journal={null},abstract={null}}
@ARTICLE{Nicola_1983,title={Testing Equivalence for Processes},year={1983},author={Rocco De Nicola and Rocco De Nicola and Matthew Hennessy and Matthew Hennessy},doi={null},pmid={null},pmcid={null},mag_id={1586966546},journal={null},abstract={Given a set of processes and a set of tests on these processes we show how to define in a natural way three different equivalences on processes. These equivalences are applied to a particular language CCS. We give associated complete proof systems and fully abstract models. These models have a simple representation in terms of trees.}}
@ARTICLE{Nicollin_1991,title={An Overview and Synthesis on Timed Process Algebras},year={1991},author={Xavier Nicollin and Xavier Nicollin and Joseph Sifakis and Joseph Sifakis},doi={10.1007/3-540-55179-4_36},pmid={null},pmcid={null},mag_id={1587275578},journal={null},abstract={We present an overview and synthesis of existing results about process algebras for the specification and analysis of timed systems. The motivation is double: present an overview of some relevant and representative approaches and suggest a unifying framework for them.}}
@ARTICLE{Vogler_1997,title={Partial order semantics and read arcs},year={1997},author={Walter Vogler and Walter Vogler},doi={10.1016/s0304-3975(01)00234-1},pmid={null},pmcid={null},mag_id={1587747784},journal={null},abstract={We study a new partial order semantics of Petri nets with read arcs, where read arcs model reading without consuming, which is often more adequate than the destructive-read-and-rewrite modelled in ordinary nets without read arcs. As basic observations we take ST-traces, which are sequences of transition starts and ends. We define processes of our nets and derive two partial orders modelling causality and start precedence. These partial orders are related to basic observations and their system states just as in the ordinary approach the single partial order of a process is related to firing sequences and reachable markings. Our approach also supports a new view of concurrency as captured by steps.}}
@ARTICLE{Harel_2003,title={Message sequence charts},year={2003},author={David Harel and David Harel and David Harel and P. S. Thiagarajan and P. S. Thiagarajan},doi={10.1007/0-306-48738-1_4},pmid={null},pmcid={null},mag_id={1587849506},journal={null},abstract={Message sequence charts (MSCs) constitute an attractive visual formalism that is widely used to capture system requirements during the early design stages in domains such as telecommunication software. A version of MSCs called sequence diagrams is one of the behavioral diagram types adopted in the UML. In this chapter we survey MSCs and their extensions. In particular, we discuss high level MSCs, which allow MSCs to be combined in various regular ways, and the more recent mechanism of communicating transaction processes, which can be used to structure sequence charts to capture system behaviors more directly. We also discuss in some detail live sequence charts (LSCs), a multi-modal extension of MSCs with considerably richer expressive power, and the play-in/out method that makes it possible to use LSCs directly as an executable specification.}}
@ARTICLE{Andreae_2006,title={Scoped types and aspects for real-time java},year={2006},author={Chris Andreae and Chris Andreae and Yvonne Coady and Yvonne Coady and Celina Gibbs and Celina Gibbs and James Noble and James Noble and Jan Vítek and Jan Vitek and Tian Zhao and Tian Zhao},doi={10.1007/11785477_7},pmid={null},pmcid={null},mag_id={1589143006},journal={null},abstract={Real-time systems are notoriously difficult to design and implement, and, as many real-time problems are safety-critical, their solutions must be reliable as well as efficient and correct. While higher-level programming models (such as the Real-Time Specification for Java) permit real-time programmers to use language features that most programmers take for granted (objects, type checking, dynamic dispatch, and memory safety) the compromises required for real-time execution, especially concerning memory allocation, can create as many problems as they solve. This paper presents Scoped Types and Aspects for Real-Time Systems (STARS) a novel programming model for real-time systems. Scoped Types give programmers a clear model of their programs' memory use, and, being statically checkable, prevent the run-time memory errors that bedevil models such as RTSJ. Our Aspects build on Scoped Types guarantees so that Real-Time concerns can be completely separated from applications' base code. Adopting the integrated Scoped Types and Aspects approach can significantly improve both the quality and performance of a real-time Java systems, resulting in simpler systems that are reliable, efficient, and correct.}}
@ARTICLE{Sinnott_1995,title={Irish Voters Decide: Voting Behaviour in Elections and Referendums Since 1918},year={1995},author={Richard Sinnott and Richard O. Sinnott},doi={null},pmid={null},pmcid={null},mag_id={1589396075},journal={null},abstract={Analyzing Irish electoral behaviour party portraits parties and voters - some quantitative evidence general election outcomes - periods, trends and change regions, cartography and correlations - evidence from aggregate data questions and answers - evidence from survey data transfer patterns and voting behaviour referendums - constitutional rules and particular issues second-order elections - local, European and presidential context, comparability and change.}}
@ARTICLE{Aichernig_2009,title={Qualitative Action Systems},year={2009},author={Bernhard K. Aichernig and Bernhard K. Aichernig and Harald Brandl and Harald Brandl and Willibald Krenn and Willibald Krenn},doi={10.1007/978-3-642-10373-5_11},pmid={null},pmcid={null},mag_id={1591139372},journal={null},abstract={An extension to action systems is presented facilitating the modeling of continuous behavior in the discrete domain. The original action system formalism has been developed by Back et al. in order to describe parallel and distributed computations of discrete systems, i.e. systems with discrete state space and discrete control. In order to cope with hybrid systems, i.e. systems with continuous evolution and discrete control, two extensions have been proposed: hybrid action systems and continuous action systems . Both use differential equations (relations) to describe continuous evolution. Our version of action systems takes an alternative approach by adding a level of abstraction: continuous behavior is modeled by Qualitative Differential Equations that are the preferred choice when it comes to specifying abstract and possibly non-deterministic requirements of continuous behavior. Because their solutions are transition systems, all evolutions in our qualitative action systems are discrete.

Based on hybrid action systems, we develop a new theory of qualitative action systems and discuss how we have applied such models in the context of automated test-case generation for hybrid systems.}}
@ARTICLE{Garavel_2001,title={System Design of a CC-NUMA Multiprocessor Architecture Using Formal Specification, Model-Checking, Co-Simulation, and Test Generation},year={2001},author={Hubert Garavel and Hubert Garavel and César Viho and César Viho and Massimo Zendri and Massimo Zendri},doi={10.1007/s100090100044},pmid={null},pmcid={null},mag_id={1591361873},journal={International Journal on Software Tools for Technology Transfer},abstract={The application of formal methods to system-level design of hardware components is still an open issue for which concrete case-studies are needed. We present here an industrial experiment concerning the application of the process algebraic language Lotos (ISO standard 8807) to the design of Polykid, a CC-NUMA (Cache Coherent -- Non Uniform Memory Access) multiprocessor architecture developed by Bull. The formal descriptions developed for Polykid have served as a basis not only for model-checking verification using CADP (Caesar/Aldebaran Development Package), but also for hardware-software co-simulation using the Exec/Caesar tool, and for automatic generation of executable tests using the TGV tool.}}
@ARTICLE{Prehofer_1997,title={Feature-oriented programming: A fresh look at objects},year={1997},author={Christian Prehofer and Christian Prehofer},doi={10.1007/bfb0053389},pmid={null},pmcid={null},mag_id={1591471358},journal={null},abstract={We propose a new model for flexible composition of objects from a set of features. Features are similar to (abstract) subclasses, but only provide the core functionality of a (sub)class. Overwriting other methods is viewed as resolving feature interactions and is specified separately for two features at a time. This programming model allows to compose features (almost) freely in a way which generalizes inheritance and aggregation. For a set of n features, an exponential number of different feature combinations is possible, assuming a quadratic number of interaction resolutions. We present the feature model as an extension of Java and give two translations to Java, one via inheritance and the other via aggregation. We further discuss parameterized features, which work nicely with our feature model and can be translated into Pizza, an extension of Java.}}
@ARTICLE{Queille_1982,title={Specification and verification of concurrent systems in CESAR},year={1982},author={Jean-Pierre Queille and Jean-Pierre Queille and Joseph Sifakis and Joseph Sifakis},doi={10.1007/3-540-11494-7_22},pmid={null},pmcid={null},mag_id={1593428110},journal={null},abstract={The aim of this paper is to illustrate by an example, the alternating bit protocol, the use of CESAR, an interactive system for aiding the design of distributed applications.}}
@ARTICLE{Meyer_1997,title={Object-Oriented Software Construction, 2nd Edition},year={1997},author={Bertrand Meyer and Bertrand Meyer},doi={null},pmid={null},pmcid={null},mag_id={1593874741},journal={null},abstract={null}}
@ARTICLE{Adjir_2009,title={Testing Real-Time Systems Using TINA},year={2009},author={Noureddine Adjir and Noureddine Adjir and Pierre de Saqui-Sannes and Pierre de Saqui-Sannes and Kamel Mustapha Rahmouni and Kamel Mustapha Rahmouni},doi={10.1007/978-3-642-05031-2_1},pmid={null},pmcid={null},mag_id={1594588771},journal={null},abstract={The paper presents a technique for model-based black-box conformance testing of real-time systems using the Time Petri Net Analyzer TINA. Such test suites are derived from a prioritized time Petri net composed of two concurrent sub-nets specifying respectively the expected behaviour of the system under test and its environment.We describe how the toolbox TINA has been extended to support automatic generation of time-optimal test suites. The result is optimal in the sense that the set of test cases in the test suite have the shortest possible accumulated time to be executed. Input/output conformance serves as the notion of implementation correctness, essentially timed trace inclusion taking environment assumptions into account. Test cases selection is based either on using manually formulated test purposes or automatically from various coverage criteria specifying structural criteria of the model to be fulfilled by the test suite. We discuss how test purposes and coverage criterion are specified in the linear temporal logic SE-LTL, derive test sequences, and assign verdicts.}}
@ARTICLE{Boulmé_2007,title={Interpreting invariant composition in the b method using the spec# ownership relation: a way to explain and relax b restrictions},year={2007},author={Sylvain Boulmé and Sylvain Boulmé and Marie-Laure Potet and Marie-Laure Potet},doi={10.1007/11955757_4},pmid={null},pmcid={null},mag_id={1595316703},journal={Lecture Notes in Computer Science},abstract={In the B method, the invariant of a component cannot be violated outside its own operations. This approach has a great advantage: the users of a component can assume its invariant without having to prove it. But, B users must deal with important architecture restrictions that ensure the soundness of reasonings involving invariants. Moreover, understanding how these restrictions ensure soundness is not trivial. This paper studies a meta-model of invariant composition, inspired from the Spec# approach. Basically, in this model, invariant violations are monitored using ghost variables. The consistency of assumptions about invariants is controlled by very simple proof obligations. Hence, this model provides a simple framework to understand B composition rules and to study some conservative extensions of B authorizing more architectures and providing more control on components initialization.}}
@ARTICLE{Hessel_2007,title={Cover - A Real-Time Test Case Generation Tool},year={2007},author={Anders Hessel and Anders Hessel and Paul Pettersson and Paul Pettersson},doi={null},pmid={null},pmcid={null},mag_id={1595529556},journal={null},abstract={Testing is the dominant verification technique used in the software industry today. The use of automatic test case execution increases, but the creation of test cases remains manual and thus error prone and expensive. To automate generation and selection of test cases, model-based testing techniques have been suggested. In this thesis two central problems in model-based testing are addressed: the problem of how to formally specify coverage criteria, and the problem of how to generate a test suite from a formal timed system model, such that the test suite satisfies a given coverage criterion. We use model checking techniques to explore the state-space of a model until a set of traces is found that together satisfy the coverage criterion. A key observation is that a coverage criterion can be viewed as consisting of a set of items, which we call coverage items. Each coverage item can be treated as a separate reachability problem. Based on our view of coverage items we define a language, in the form of parameterized observer automata, to formally describe coverage criteria. We show that the language is expressive enough to describe a variety of common coverage criteria described in the literature. Two algorithms for test case generation with observer automata are presented. The first algorithm returns a trace that satisfies all coverage items with a minimum cost. We use this algorithm to generate a test suite with minimal execution time. The second algorithm explores only states that may increase the already found set of coverage items. This algorithm works well together with observer automata. The developed techniques have been implemented in the tool CoVer. The tool has been used in a case study together with Ericsson where a WAP gateway has been tested. The case study shows that the techniques have industrial strength.}}
@ARTICLE{Saaty_1991,title={Prediction, projection and forecasting : applications of the analytic hierarchy process in economics, finance, politics, games and sports},year={1991},author={Thomas L. Saaty and Thomas L. Saaty and Luis G. Vargas and Luis G. Vargas and Luis González and Luis Javier González},doi={null},pmid={null},pmcid={null},mag_id={1595758044},journal={null},abstract={We predict when we say in advance, foretell, or prophesy what is likely to happen in the future. We project when we calculate the numerical value associated with a future event. We forecast, a special kind of prediction, on data of past happenings to generate or cast data for future by relying happenings. Generally, one predicts (yes, no) a war, an earthquake or the outcome of a chess match, projects the value of the GNP or of unemployment, and forecasts the weather and, more scientifically, the economic trends. Prediction, projection, and forecasting must be constrained in time and space: when and where. Often the accuracy of a forecast is of interest along with how sensitive the outcome is to changes in the factors involved. Is there a basis for improving the wisdom we need to make correct and useful predictions? We believe there is, and that it can be cultivated by studying the approach given here along with the various examples. To the best of our knowledge, no other work has approached prediction in the scientific framework of hierarchies. Prediction is the synthesis of past and present in an attempt to foretell the future. In our view, creation is not the ultimate phenomenon of the world. Nature creates forms and so do we. The problem is to surmise the eventual purpose, impact, and use of creation. It is the synthesis or outcome of bringing together the results of creation that we need to predict.}}
@ARTICLE{Binder_1999,title={Testing Object-Oriented Systems: Models, Patterns, and Tools},year={1999},author={Robert V. Binder and Robert V. Binder},doi={null},pmid={null},pmcid={null},mag_id={1596127723},journal={null},abstract={List of Figures. List of Tables. List of Procedures. Foreword. Preface. Acknowledgments. I. PRELIMINARIES. 1. A Small Challenge. 2. How to Use This Book. Reader Guidance. Conventions. FAQs for Object-oriented Testing. Test Process. 3. Testing: A Brief Introduction. What Is Software Testing? Definitions. The Limits of Testing. What Can Testing Accomplish? Bibliographic Notes. 4. With the Necessary Changes: Testing and Object-oriented Software. The Dismal Science of Software Testing. Side Effects of the Paradigm. Language-specific Hazards. Coverage Models for Object-oriented Testing. An OO Testing Manifesto. Bibliographic Notes. II. MODELS. 5. Test Models. Test Design and Test Models. Bibliographic Notes. 6. Combinational Models. How Combinational Models Support Testing. How to Develop a Decision Table. Deriving the Logic Function. Decision Table Validation. Test Generation. Choosing a Combinational Test Strategy. Bibliographic Notes. 7. State Machines. Motivation. The Basic Model. The FREE State Model. State-based Test Design. Bibliographic Notes. 8. A Tester's Guide to the UML. Introduction. General-purpose Elements. Use Case Diagram. Class Diagram. Sequence Diagram. Activity Diagram. Statechart Diagram. Collaboration Diagram. Component Diagram. Deployment Diagram. Graphs, Relations, and Testing. Bibliographic Notes. III. PATTERNS. 9. Results-oriented Test Strategy. Results-oriented Testing. Test Design Patterns. Test Design Template. Documenting Test Cases, Suites, and Plans. Bibliographic Notes. 10. Classes. Class Test and Integration. Preliminaries. Method Scope Test Design Patterns. Category-Partition. Combinational Function Test. Recursive Function Test. Polymorphic Message Test. Class Scope Test Design Patterns. Invariant Boundaries. Nonmodal Class Test. Quasi-modal Class Test. Modal Class Test. Flattened Class Scope Test Design Patterns. Polymorphic Server Test. Modal Hierarchy Test. Bibliographic Notes. 11. Reusable Components. Testing and Reuse. Test Design Patterns. Abstract Class Test. Generic Class Test. New Framework Test. Popular Framework Test. Bibliographic Notes. 12. Subsystems. Subsystems. Subsystem Test Design Patterns. Class Association Test. Round-trip Scenario Test. Controlled Exception Test. Mode Machine Test. Bibliographic Notes. 13. Integration. Integration in Object-oriented Development. Integration Patterns. Subsystem/System Scope. Big Bang Integration. Bottom-up Integration. Top-down Integration. Collaboration Integration. Backbone Integration. Layer Integration. Client/Server Integration. Distributed Services Integration. High-frequency Integration. Bibliographic Notes. 14. Application Systems. Testing Application Systems. Test Design Patterns. Extended Use Case Test. Covered in CRUD. Allocate Tests by Profile. Implementation-specific Capabilities. Post-development Testing. Note on Testing Performance Objectives. Bibliographic Notes. 15. Regression Testing. Preliminaries. Test Patterns. Retest All. Retest Risky Use Cases. Retest by Profile. Retest Changed Code. Retest Within Firewall. Bibliographic Notes. IV. TOOLS. 16. Test Automation. Why Testing Must Be Automated. Limitations and Caveats. 17. Assertions. Introduction. Implementation-based Assertions. Responsibility-based Assertions. Implementation. The Percolation Pattern. Deployment. Limitations and Caveats. Some Assertion Tools. Bibliographic Notes. 18. Oracles. Introduction. Oracle Patterns. Comparators. Bibliographic Notes. 19. Test Harness Design. How to Develop a Test Harness. Test Case Patterns. Test Case/Test Suite Method. Test Case/Test Suite Class. Catch All Exceptions. Test Control Patterns. Server Stub. Server Proxy. Driver Patterns. TestDriver Superclass. Percolate the Object Under Test. Symmetric Driver. Subclass Driver. Private Access Driver. Test Control Interface. Drone. Built-in Test Driver. Test Execution Patterns. Command Line Test Bundle. Incremental Testing Framework. Fresh Objects. A Test Implementation Syntax. Bibliographic Notes. Appendix. BigFoot's Tootsie: A Case Study. Requirements. OOA/D for Capability-driven Testing. Implementation. Glossary. References. Index. 0201809389T04062001}}
@ARTICLE{Henzinger_2003,title={Software verification with BLAST},year={2003},author={Thomas A. Henzinger and Thomas A. Henzinger and Ranjit Jhala and Ranjit Jhala and Rupak Majumdar and Rupak Majumdar and Grégoire Sutre and Grégoire Sutre},doi={10.1007/3-540-44829-2_17},pmid={null},pmcid={null},mag_id={1596552075},journal={null},abstract={Blast (the Berkeley Lazy Abstraction Software verification Tool) is a verification system for checking safety properties of C programs using automatic property-driven construction and model checking of software abstractions. Blast implements an abstract-model check-refine loop to check for reachability of a specified label in the program. The abstract model is built on the fly using predicate abstraction. This model is then checked for reachability. If there is no (abstract) path to the specified error label, Blast reports that the system is safe and produces a succinct proof. Otherwise, it checks if the path is feasible using symbolic execution of the program. If the path is feasible, Blast outputs the path as an error trace, otherwise, it uses the infeasibility of the path to refine the abstract model. Blast short-circuits the loop from abstraction to verification to refinement, integrating the three steps tightly through “lazy abstraction” [5]. This integration can offer significant advantages in performance by avoiding the repetition of work from one iteration of the loop to the next.}}
@ARTICLE{Berthomieu_2006,title={Bridging the gap between timed automata and bounded time petri nets},year={2006},author={Bernard Berthomieu and Bernard Berthomieu and Florent Peres and Florent Peres and François Vernadat and François Vernadat},doi={10.1007/11867340_7},pmid={null},pmcid={null},mag_id={1596935793},journal={null},abstract={Several recent papers investigate the relative expressiveness of Timed Automata and Time Petri Nets, two widespread models for realtime systems. It has been shown notably that Timed Automata and Bounded Time Petri Nets are equally expressive in terms of timed language acceptance, but that Timed Automata are strictly more expressive in terms of weak timed bisimilarity. This paper compares Timed Automata with Bounded Time Petri Nets extended with static Priorities, and shows that two large subsets of these models are equally expressive in terms of weak timed bisimilarity.}}
@ARTICLE{Ural_2007,title={An EFSM-based passive fault detection approach},year={2007},author={Hasan Ural and Hasan Ural and Zhi Xu and Zhi Xu},doi={10.1007/978-3-540-73066-8_23},pmid={null},pmcid={null},mag_id={1598641709},journal={null},abstract={Extended Finite State Machine (EFSM)-based passive fault detection involves modeling the system under test (SUT) as an EFSM M, monitoring the input/output behaviors of the SUT, and determining whether these behaviors relate to faults within the SUT. We propose a new approach for EFSM-based passive fault detection which randomly selects a state in M and checks whether there is a trace in M starting from this state which is compatible with the observed behaviors. If a compatible trace is found, we determine that observed behaviors are not sufficient to declare the SUT to be faulty; otherwise, we check another unchecked state. If all the states have been checked and no compatible trace is found, we declare that the SUT is faulty. We use a Hybrid method in our approach which combines the use of both Interval Refinement and Simplex methods to improve the performance of passive fault detection.}}
@ARTICLE{Briones_2005,title={Test Derivation from Timed Automata},year={2005},author={Laura Brandán Briones and Laura Brandan Briones and Mathias Röhl and Mathias Röhl},doi={10.1007/11498490_10},pmid={null},pmcid={null},mag_id={1601908032},journal={null},abstract={A real-time system is a discrete system whose state changes occur in real-numbered time [AH97]. For testing real-time systems, specification languages must be extended with constructs for expressing real-time constraints, the implementation relation must be generalized to consider the temporal dimension, and the data structures and algorithms used to generate tests must be revised to operate on a potentially infinite set of states.}}
@ARTICLE{Burns_2004,title={Concurrent and Real-Time Programming in Java},year={2004},author={Alan Burns and Alan Burns and Andy Wellings and Andy Wellings},doi={null},pmid={null},pmcid={null},mag_id={1603602584},journal={null},abstract={Ada is the only ISO-standard, object-oriented, concurrent, real-time programming language. It is intended for use in large, long-lived applications where reliability and efficiency are essential, particularly real-time and embedded systems. In this book, Alan Burns and Andy Wellings give a thorough, self-contained account of how the Ada tasking model can be used to construct a wide range of concurrent and real-time systems. This is the only book that focuses on an in-depth discussion of the Ada tasking model. Following on from the authors' earlier title Concurrency in Ada, this book brings the discussion up to date to include the new Ada 2005 language and the recent advances in real-time programming techniques. It will be of value to software professionals and advanced students of programming alike: indeed every Ada programmer will find it essential reading and a primary reference work that will sit alongside the language reference manual.}}
@ARTICLE{Milner_1999,title={Communicating and Mobile Systems: the Pi-Calculus},year={1999},author={Robin Milner and Robin Milner},doi={null},pmid={null},pmcid={null},mag_id={1603799276},journal={null},abstract={Glossary Part I. Communicating Systems: 1. Introduction 2. Behaviour of automata 3. Sequential processes and bisimulation 4. Concurrent processes and reaction 5. Transitions and strong equivalence 6. Observation equivalence: theory 7. Observation equivalence: examples Part II. The pi-Calculus: 8. What is mobility? 9. The pi-calculus and reaction 10. Applications of the pi-calculus 11. Sorts, objects and functions 12. Commitments and strong bisimulation 13. Observation equivalence and examples 14. Discussion and related work Bibliography Index.}}
@ARTICLE{Aichernig_2009,title={Model-based mutation testing of hybrid systems},year={2009},author={Bernhard K. Aichernig and Bernhard K. Aichernig and Harald Brandl and Harald Brandl and Elisabeth Jöbstl and Elisabeth Jöbstl and Willibald Krenn and Willibald Krenn},doi={10.1007/978-3-642-17071-3_12},pmid={null},pmcid={null},mag_id={1604401435},journal={null},abstract={This paper presents a novel model-based testing approach developed in the MOGENTES project. The aim is to test embedded systems controlling a continuous environment, i.e., hybrid systems. We present our two key abstractions against which we systematically test for conformance. (1) Classical action systems are used to model the discrete controller behavior. (2) Qualitative differential equations are used to model the evolutions of the environment. The latter is based on a technique from the domain of Artificial Intelligence called qualitative reasoning. Mutation testing on these models is used to generate effective test cases. A test case generator has been developed that searches for all test cases that would kill a mutant. The mutant models represent our fault models. The generated test cases are then executed on the implementation in order to systematically exclude the possibility that a mutant has been implemented.}}
@ARTICLE{Barnett_2005,title={Boogie: a modular reusable verifier for object-oriented programs},year={2005},author={Mike Barnett and Mike Barnett and Bor-Yuh Evan Chang and Bor-Yuh Evan Chang and Robert DeLine and Robert DeLine and Bart Jacobs and Bart Jacobs and K. Rustan M. Leino and K. Rustan M. Leino},doi={10.1007/11804192_17},pmid={null},pmcid={null},mag_id={1606177908},journal={null},abstract={A program verifier is a complex system that uses compiler technology, program semantics, property inference, verification-condition generation, automatic decision procedures, and a user interface. This paper describes the architecture of a state-of-the-art program verifier for object-oriented programs.}}
@ARTICLE{Jacky_2007,title={Model-Based Software Testing and Analysis with C#},year={2007},author={Jonathan Jacky and Jonathan Jacky and Margus Veanes and Margus Veanes and Colin Campbell and Colin Campbell and Colin Campbell and Wolfram Schulte and Wolfram Schulte},doi={null},pmid={null},pmcid={null},mag_id={1606436461},journal={null},abstract={This book teaches new methods for specifying, analyzing, and testing software; essentials for creating high-quality software. These methods increase the automation in each of these steps, making them more timely, more thorough, and more effective. The authors work through several realistic case studies in-depth and detail, using a toolkit built on the C# language and the .NET framework. Readers can also apply the methods in analyzing and testing systems in many other languages and frameworks. Intended for professional software developers including testers, and for university students, this book is suitable for courses on software engineering, testing, specification, or applications of formal methods.}}
@ARTICLE{Emerson_1991,title={Temporal and modal logic},year={1991},author={E. Allen Emerson and E. Allen Emerson},doi={10.1016/b978-0-444-88074-1.50021-4},pmid={null},pmcid={null},mag_id={1612453857},journal={null},abstract={null}}
@ARTICLE{D’Hondt_2010,title={ECOOP 2010 – Object-Oriented Programming},year={2010},author={Theo D’Hondt and Theo D'Hondt},doi={10.1007/978-3-642-14107-2},pmid={null},pmcid={null},mag_id={1641171388},journal={Lecture Notes in Computer Science},abstract={null}}
@ARTICLE{Stevens_2014,title={Bidirectionally Tolerating Inconsistency: Partial Transformations},year={2014},author={Perdita Stevens and Perdita Stevens},doi={10.1007/978-3-642-54804-8_3},pmid={null},pmcid={null},mag_id={1650869244},journal={null},abstract={A foundational property of bidirectional transformations is that they should be correct: that is, the transformation should succeed in restoring consistency between any models it is given. In practice, however, transformation engines sometimes fail to restore consistency, e.g. because there is no consistent model to return, or because the tool is unable to select a best model to return from among equally good candidates. In this paper, we formalise properties that may nevertheless hold in such circumstances and discuss relationships and implications.}}
@ARTICLE{Ayache_1995,title={OBSERVER A CONCEPT FOR ON-LINE DETECTION OF CONTROL ERRORS IN CONCURRENT SYSTEMS},year={1995},author={Jean-Michel Ayache and Jean-Michel Ayache and Pierre Azéma and Pierre Azéma and Michel Diaz and Michel Diaz},doi={10.1109/ftcsh.1995.532616},pmid={null},pmcid={null},mag_id={1657671450},journal={null},abstract={null}}
@ARTICLE{Bader_1998,title={Testing concurrency and communication in distributed objects},year={1998},author={Adnan Bader and A. Bader and A. S. M. Sajeev and A.S.M. Sajeev and Sita Ramakrishnan and S. Ramakrishnan},doi={10.1109/hipc.1998.738017},pmid={null},pmcid={null},mag_id={1669744221},journal={null},abstract={Concurrency and communication are two of the key features of distributed systems. These features can make systematic testing of distributed systems a complex task. A major problem is the explosion of the test space because of the potential for arbitrary interference of concurrent threads. This paper describes an approach for systematic testing of such systems in an object-oriented context. We use statecharts for system specification, and model the system behaviour as event-sequences. A test case, therefore, is primarily an event-sequence with concurrent threads represented as interleaving events. Communication-states with associated events represent communication between objects. The test-space explosion is controlled by an extension to Chow's (1978) algorithm for generating test sequences for finite state machines. The number of test sequences we require is O(n/sup 2/), where n is the sum of all events in all concurrent statecharts.}}
@ARTICLE{Hemer_2005,title={A formal approach to component adaptation and composition},year={2005},author={David Hemer and David Hemer},doi={null},pmid={null},pmcid={null},mag_id={1672500832},journal={null},abstract={Component based software engineering (CBSE), can in principle lead to savings in the time and cost of software development, by encouraging software reuse. However the reality is that CBSE has not been widely adopted. From a technical perspective, the reason is largely due to the difficulty of locating suitable components in the library and adapting these components to meet the specific needs of the user.Formal approaches to retrieval - using formal notations for interface specification, and semantic based matching techniques - have been proposed as a solution to the retrieval problem. These approaches are aimed at overcoming the lack of precision and ambiguity associated with text-based component interfaces, requirements and retrieval techniques. However these approaches fail to adequately address the problem of component adaptation and composition.In this paper we describe how component adaptation and composition strategies can be defined using parameterised library templates. We define a variety of templates, including wrapper templates that adapt a single program component, and architecture templates that combine program components. We include definitions for sequential architectures, independent architectures and alternative architectures. These library templates are formally specified, so we are able to employ existing formal-based retrieval strategies to match problem specifications against library templates. We discuss how adaptation and composition can be semi-automated by the library templates defined in this paper in combination with existing retrieval strategies.}}
@ARTICLE{Lomuscio_2011,title={Runtime Monitoring of Contract Regulated Web Services},year={2011},author={Alessio Lomuscio and Alessio Lomuscio and Wojciech Penczek and Wojciech Penczek and Monika Solanki and Monika Solanki and Maciej Szreter and Maciej Szreter},doi={10.3233/fi-2011-566},pmid={null},pmcid={null},mag_id={1676919590},journal={Fundamenta Informaticae},abstract={We investigate the problem of locally monitoring contract regulated behaviours in agent-based web services. We encode contract clauses in service specifications by using extended timed automata. We propose a non intrusive local monitoring framework along with an API to monitor the fulfillment (or violation) of contractual obligations. A key feature of the framework is that it is fully symbolic thereby providing a scalable solution to monitoring. At runtime execution steps generated by the service are passed as input to the runtime monitor. Conformance of the execution against the service specification is checked using a symbolically represented extended timed automaton. This allows us to monitor service behaviours over large state spaces generated by multiple, long running contracts. We illustrate our methodology by monitoring a service composition scenario from the vehicle repair domain, and report on the experimental results.}}
@ARTICLE{Barthe_2006,title={mobius mobility ubiquity security},year={2006},author={Gilles Barthe and Lennart Beringer and Pierre Crégut and Benjamin Grégoire and Martin Hofmann and Peter Müller and Erik Poll and Germán Puebla and Ian Stark and Eric Vétillard},doi={10.1007/978-3-540-75336-0_2},pmid={null},pmcid={null},mag_id={1681943465},journal={null},abstract={Through their global, uniform provision of services and their distributed nature, global computers have the potential to profoundly enhance our daily life. However, they will not realize their full potential, unless the necessary levels of trust and security can be guaranteed.}}
@ARTICLE{Sipma_1999,title={Deductive Model Checking},year={1999},author={Henny B. Sipma and Henny B. Sipma and Tomás E. Uribe and Tomás E. Uribe and Zohar Manna and Zohar Manna},doi={10.1023/a:1008791913551},pmid={null},pmcid={null},mag_id={1695437353},journal={null},abstract={We present an extension of classical tableau-based model checking procedures to the case of infinite-state systems, using deductive methods in an incremental construction of the behavior graph. Logical formulas are used to represent infinite sets of states in an abstraction of this graph, which is repeatedly refined in the search for a counterexample computation, ruling out large portions of the graph before they are expanded to the state-level. This can lead to large savings, even in the case of finite-state systems. Only local conditions need to be checked at each step, and previously proven properties can be used to further constrain the search. Although the resulting method is not always automatic, it provides a flexible, general and complete framework that can integrate a diverse number of other verification tools.}}
@ARTICLE{Sipma_1996,title={Deductive Model Checking},year={1996},author={Henny B. Sipma and Henny B. Sipma and Tomás E. Uribe and Tomás E. Uribe and Zohar Manna and Zohar Manna},doi={10.1007/3-540-61474-5_70},pmid={null},pmcid={null},mag_id={1699259381},journal={null},abstract={We present an extension of classical tableau-based model checking procedures to the case of infinite-state systems, using deductive methods in an incremental construction of the behavior graph. Logical formulas are used to represent infinite sets of states in an abstraction of this graph, which is repeatedly refined in the search for a counterexample computation, ruling out large portions of the graph before they are expanded to the state-level. This can lead to large savings, even in the case of finite-state systems. Only local conditions need to be checked at each step, and previously proven properties can be used to further constrain the search. Although the resulting method is not always automatic, it provides a flexible and general framework that can be used to integrate a diverse number of other verification tools.}}
@ARTICLE{Ciobanu_2012,title={Flexible software architecture and language for mobile agents},year={2012},author={Gabriel Ciobanu and Gabriel Ciobanu and Călin Juravle and Călin Juravle},doi={10.1002/cpe.1854},pmid={null},pmcid={null},mag_id={1700678307},journal={Concurrency and Computation: Practice and Experience},abstract={In this paper, we present a flexible software architecture and a language for systems of mobile agents starting from a formalism with timed interactions and explicit locations. The language supports the specification of a distributed system, that is, agents and their physical distribution, and allows a timed migration in a distributed environment. Advanced software technologies are used to define the software architecture and the agents language, facilitating also the agents development. We illustrate the system by a dynamic network discovery in which the agents take into account the latency and the CPU load when choosing where to migrate. Copyright © 2011 John Wiley & Sons, Ltd.}}
@ARTICLE{Mazurkiewicz_1988,title={Basic notions of trace theory},year={1988},author={Antoni Mazurkiewicz and Antoni Mazurkiewicz},doi={10.1007/bfb0013025},pmid={null},pmcid={null},mag_id={1705394006},journal={null},abstract={The concept of traces has been introduced for describing non-sequential behaviour of concurrent systems via its sequential observations. Traces represent concurrent processes in the same way as strings represent sequential ones. The theory of traces can be used as a tool for reasoning about nets and it is hoped that applying this theory one can get a calculus of the concurrent processes analogous to that available for sequential systems. The following topics will be discussed: algebraic properties of traces, trace models of some concurrency phenomena, fixed-point calculus for finding the behaviour of nets, modularity, and some applications of the presented theory.}}
@ARTICLE{Cadar_2008,title={KLEE: unassisted and automatic generation of high-coverage tests for complex systems programs},year={2008},author={Cristian Cadar and Cristian Cadar and Daniel Dunbar and Daniel Dunbar and Daniel Dunbar and Dawson Engler and Dawson Engler},doi={null},pmid={null},pmcid={null},mag_id={1710734607},journal={null},abstract={We present a new symbolic execution tool, KLEE, capable of automatically generating tests that achieve high coverage on a diverse set of complex and environmentally-intensive programs. We used KLEE to thoroughly check all 89 stand-alone programs in the GNU COREUTILS utility suite, which form the core user-level environment installed on millions of Unix systems, and arguably are the single most heavily tested set of open-source programs in existence. KLEE-generated tests achieve high line coverage -- on average over 90% per tool (median: over 94%) -- and significantly beat the coverage of the developers' own hand-written test suite. When we did the same for 75 equivalent tools in the BUSYBOX embedded system suite, results were even better, including 100% coverage on 31 of them.

We also used KLEE as a bug finding tool, applying it to 452 applications (over 430K total lines of code), where it found 56 serious bugs, including three in COREUTILS that had been missed for over 15 years. Finally, we used KLEE to crosscheck purportedly identical BUSYBOX and COREUTILS utilities, finding functional correctness errors and a myriad of inconsistencies.}}
@ARTICLE{Hallet_2005,title={A Formal Semantics for Weak References},year={2005},author={J. J. Hallet and J. J. Hallet and A. J. Kfoury and A. J. Kfoury},doi={null},pmid={null},pmcid={null},mag_id={1717653128},journal={null},abstract={A weak reference is a reference to an object that is not followed by the pointer tracer when garbage collection is called. That is, a weak reference cannot prevent the object it references from being garbage collected. Weak references remain a troublesome programming feature largely because there is not an accepted, precise semantics that describes their behavior (in fact, we are not aware of any formalization of their semantics). The trouble is that weak references allow reachable objects to be garbage collected, therefore allowing garbage collection to influence the result of a program. Despite this difficulty, weak references continue to be used in practice for reasons related to efficient storage management, and are included in many popular programming languages (Standard ML, Haskell, OCaml, and Java). We give a formal semantics for a calculus called λweak that includes weak references and is derived from Morrisett, Felleisen, and Harper’s λgc. λgc formalizes the notion of garbage collection by means of a rewrite rule. Such a formalization is required to precisely characterize the semantics of weak references. However, the inclusion of a garbage-collection rewrite-rule in a language with weak references introduces non-deterministic evaluation, even if the parameter-passing mechanism is deterministic (call-by-value in our case). This raises the question of confluence for our rewrite system. We discuss natural restrictions under which our rewrite system is confluent, thus guaranteeing uniqueness of program result. We define conditions that allow other garbage collection algorithms to co-exist with our semantics of weak references. We also introduce a polymorphic type system to prove the absence of erroneous program behavior (i.e., the absence of “stuck evaluation”) and a corresponding type inference algorithm. We prove the type system sound and the inference algorithm sound and complete.}}
@ARTICLE{Vytopil_1993,title={Formal Techniques in Real-Time and Fault-Tolerant Systems},year={1993},author={Jan Vytopil and Jan Vytopil},doi={10.1007/978-1-4615-3220-0},pmid={null},pmcid={null},mag_id={1720140439},journal={null},abstract={I Concepts and Foundations.- 1 Terminology and Paradigms for Fault Tolerance.- 2 Fault Tolerance as Self-Similarity.- 3 Parameterized Semantics for Fault Tolerant Real-Time Systems.- 4 Modeling Real-Time and Reliability.- II Applications.- 5 A Fault-Masking and Transient-Recovery Model for Digital Flight-Control Systems.- 6 Specification and Verification of Recovery in Asynchronous Communicating Systems.- 7 CSP, Formal Software Engineering and the Development of Fault-Tolerant Systems.}}
@ARTICLE{Fabbri_1999,title={Proteum/FSM: a tool to support finite state machine validation based on mutation testing},year={1999},author={Sandra Fabbri and Sandra Fabbri and José Carlos Maldonado and José Carlos Maldonado and Márcio Eduardo Delamaro and Márcio Eduardo Delamaro},doi={10.1109/sccc.1999.810159},pmid={null},pmcid={null},mag_id={1727134308},journal={null},abstract={The quality of the VV&T-Verification, Validation and Testing-activity is extremely relevant to the software development process. Testing techniques and criteria have been investigated in the context of VV&T of reactive systems specifications, providing mechanisms to the VV&T activity quality assessment. The establishment of a low-cost, effective testing and validation strategy and the development of supporting tools have been pursued by many researchers for coding and specification as well. This paper discusses the main architectural and operational aspects of a tool, named Proteum/FSM, that supports the application of mutation testing for validating reactive systems specifications based on finite state machines (FSM). Further improvements and research issues are briefly discussed.}}
@ARTICLE{Chaki_2004,title={State/Event-Based Software Model Checking},year={2004},author={Sagar Chaki and Sagar Chaki and Edmund M. Clarke and Edmund M. Clarke and Joël Ouaknine and Joël Ouaknine and Natasha Sharygina and Natasha Sharygina and Nishant Sinha and Nishant Sinha},doi={10.1007/978-3-540-24756-2_8},pmid={null},pmcid={null},mag_id={1756958128},journal={null},abstract={We present a framework for model checking concurrent software systems which incorporates both states and events. Contrary to other state/event approaches, our work also integrates two powerful verification techniques, counterexample-guided abstraction refinement and compositional reasoning. Our specification language is a state/event extension of linear temporal logic, and allows us to express many properties of software in a concise and intuitive manner. We show how standard automata-theoretic LTL model checking algorithms can be ported to our framework at no extra cost, enabling us to directly benefit from the large body of research on efficient LTL verification.}}
@ARTICLE{Khoumsi_2002,title={A Method for Testing the Conformance of Real Time Systems},year={2002},author={Ahmed Khoumsi and Ahmed Khoumsi},doi={10.1007/3-540-45739-9_20},pmid={null},pmcid={null},mag_id={1779584415},journal={Lecture Notes in Computer Science},abstract={The aim of conformance testing is to check whether an implementation conforms to a specification. We consider the case where the specification contains timing constraints and is described by a model called Timed Automata (TA). The state space of a TA can be infinite due to the infinite number of time values. In a recent work, we proposed a method to finitely represent the state space. The proposed method transforms a TA into an equivalent finite state automaton using two special types of events, Set and Exp, and denoted se-FSA.In the present article, we propose a conformance testing method which is applicable when the specification is described by a TA. First, we use the above-mentioned transformation procedure for representing the specification by a se-FSA. Second, we propose a procedure for generating test sequences from the se-FSA describing the specification. And third, we propose a simple architecture for executing the generated test sequences.}}
@ARTICLE{Bozga_2002,title={IF-2.0: A Validation Environment for Component-Based Real-Time Systems},year={2002},author={Marius Bozga and Marius Bozga and Susanne Graf and Susanne Graf and Laurent Mounier and Laurent Mounier},doi={10.1007/3-540-45657-0_26},pmid={null},pmcid={null},mag_id={1782531476},journal={null},abstract={It is widely recognised that the automated validation of complex systems can hardly be achieved without tool integration. The development of the IF-1.0 toolbox [3] was initiated several years ago, in order to provide an open validation platform for timed asynchronous systems (such as telecommunication protocols or distributed applications, in general). The toolbox was built upon an intermediate representation language based on extended timed automata. In particular, this representation allowed us to study the semantics of real-time primitives for asynchronous systems. Currently, the toolbox contains dedicated tools on the intermediate language (such as compilers, static analysers and model-checkers) as well as front-ends to various specification languages and validation tools (academic and commercial ones). Among the dedicated tools, we focused on static analysis (such as slicing and abstraction) which are mandatory for an automated validation of complex systems. Finally, the toolbox was successfully used on several case studies, the most relevant ones being presented in [4].}}
@ARTICLE{Leino_2007,title={Automatic verification of textbook programs that usecomprehensions},year={2007},author={K. Rustan M. Leino and K. Rustan M. Leino and Rosemary Monahan and Rosemary Monahan},doi={null},pmid={null},pmcid={null},mag_id={1793366261},journal={null},abstract={Textbooks on program verification make use of simple programs in
mathematical domains as illustrative examples. Mechanical verification tools can
give students a quicker way to learn, because the feedback cycle can be reduced
from days (waiting for hand-proofs to be graded by the teaching assistant) to
seconds or minutes (waiting for the tool’s output). However, the mathematical
domains that are so familiar to students (for example, sum-comprehensions) are
not directly supported by first-order SMT solvers.
This paper presents a technique for translating common comprehension expressions
(sum, count , product , min, and max) into verification conditions
that can be tackled by two first-order SMT solvers. The technique has been implemented
in the Spec# program verifier. The paper also reports on the experience
of using Spec# to verify several challenging programming examples drawn from
a textbook by Dijkstra and Feijen.}}
@ARTICLE{Uyar_2005,title={Timing fault models for systems with multiple timers},year={2005},author={M. Ümit Uyar and M. Umit Uyar and Yu Wang and Yu Wang and Samrat S. Batth and S.S. Batth and Adriana Wise and Adriana Wise and Mariusz A. Fecko and Mariusz A. Fecko},doi={10.1007/11430230_14},pmid={null},pmcid={null},mag_id={1796323021},journal={Lecture Notes in Computer Science},abstract={Multiple timing faults, although detectable individually, can hide each other’s faulty behavior making the faulty system indistinguishable from a non-faulty one. A set of graph augmentations are introduced for single timing faults. The fault detection capability of the augmentations is analyzed in the presence of multiple timing faults and shown that multiple occurrences of a class of timing faults can be detected.}}
@ARTICLE{Apel_2008,title={Superimposition: a language-independent approach to software composition},year={2008},author={Sven Apel and Sven Apel and Christian Lengauer and Christian Lengauer},doi={10.1007/978-3-540-78789-1_2},pmid={null},pmcid={null},mag_id={1798085331},journal={null},abstract={Superimposition is a composition technique that has been applied successfully in several areas of software development. In order to unify several languages and tools that rely on superimposition, we present an underlying language-independent model that is based on feature structure trees (FSTs). Furthermore, we offer a tool, called FSTComposer, that composes software components represented by FSTs. Currently, the tool supports the composition of components written in Java, Jak, XML, and plain text. Three nontrivial case studies demonstrate the practicality of our approach.}}
@ARTICLE{Brockschmidt_2010,title={Termination graphs for Java bytecode},year={2010},author={Marc Brockschmidt and Marc Brockschmidt and Carsten Otto and Carsten Otto and Christian von Essen and Christian von Essen and Jürgen Giesl and Jürgen Giesl},doi={10.1007/978-3-642-17172-7_2},pmid={null},pmcid={null},mag_id={1808131711},journal={null},abstract={To prove termination of Java Bytecode (JBC) automatically, we transform JBC to finite termination graphs which represent all possible runs of the program. Afterwards, the graph can be translated into "simple" formalisms like term rewriting and existing tools can be used to prove termination of the resulting term rewrite system (TRS). In this paper we show that termination graphs indeed capture the semantics of JBC correctly. Hence, termination of the TRS resulting from the termination graph implies termination of the original JBC program.}}
@ARTICLE{Delzanno_2001,title={Constraint-based Deductive Model Checking},year={2001},author={Giorgio Delzanno and Giorgio Delzanno and Andreas Podelski and Andreas Podelski},doi={10.1007/s100090100049},pmid={null},pmcid={null},mag_id={1813383841},journal={International Journal on Software Tools for Technology Transfer},abstract={We show that constraint logic programming (CLP) can serve as a conceptual basis and as a practical implementation platform for the model checking of infinite-state systems. CLP programs are logical formulas (built up from constraints) that have both a logical interpretation and an operational semantics. Our contributions are: (1) a translation of concurrent systems (imperative programs) into CLP programs with the same operational semantics; and (2) a deductive method for verifying safety and liveness properties of the systems which is based on the logical interpretation of the CLP programs produced by the translation. We have implemented the method in a CLP system and verified well-known examples of infinite-state programs over integers, using linear constraints here as opposed to Presburger arithmetic as in previous solutions.}}
@ARTICLE{Saaty_1980,title={The analytic hierarchy process : planning, priority setting, resource allocation},year={1980},author={Thomas L. Saaty and Thomas L. Saaty},doi={null},pmid={null},pmcid={null},mag_id={1822004825},journal={null},abstract={null}}
@ARTICLE{Barringer_2004,title={Rule-Based Runtime Verification},year={2004},author={Howard Barringer and Howard Barringer and Allen Goldberg and Allen Goldberg and Klaus Havelund and Klaus Havelund and Koushik Sen and Koushik Sen},doi={10.1007/978-3-540-24622-0_5},pmid={null},pmcid={null},mag_id={1825341937},journal={null},abstract={We present a rule-based framework for defining and implementing finite trace monitoring logics, including future and past time temporal logic, extended regular expressions, real-time logics, interval logics, forms of quantified temporal logics, and so on. Our logic, Eagle, is implemented as a Java library and involves novel techniques for rule definition, manipulation and execution. Monitoring is done on a state-by-state basis, without storing the execution trace.}}
@ARTICLE{Baumann_1997,title={Communication Concepts for Mobile Agent Systems},year={1997},author={J. Baumann and Joachim Baumann and Fritz Hohl and Fritz Hohl and Fritz Hohl and Nikolaos Radouniklis and Nikolaos Radouniklis and Kurt Rothermel and Kurt Rothermel and Markus Straßer and Markus Straßer},doi={10.1007/3-540-62803-7_29},pmid={null},pmcid={null},mag_id={1833356332},journal={null},abstract={Driven by the question how to identify potential communication partners and the need for well-suited communication schemes in agent-based systems, we discuss two communication concepts: sessions and global event management.}}
@ARTICLE{Giese_2001,title={Incremental Closure of Free Variable Tableaux},year={2001},author={Martin A. Giese and Martin Giese and Martin Giese},doi={10.1007/3-540-45744-5_46},pmid={null},pmcid={null},mag_id={1841089713},journal={null},abstract={This paper presents a technique for automated theorem proving with free variable tableaux that does not require backtracking. Most existing automated proof procedures using free variable tableaux require iterative deepening and backtracking over applied instantiations to guarantee completeness. If the correct instantiation is hard to find, this can lead to a significant amount of duplicated work. Incremental Closure is a way of organizing the search for closing instantiations that avoids this inefficiency.}}
@ARTICLE{Rusu_2000,title={An Approach to Symbolic Test Generation},year={2000},author={Vlad Rusu and Vlad Rusu and Lydie Du Bousquet and Lydie du Bousquet and Thierry Jéron and Thierry Jéron},doi={10.1007/3-540-40911-4_20},pmid={null},pmcid={null},mag_id={1847824087},journal={null},abstract={Test generation is a program-synthesis problem: starting from the formal specification of a system under test, and from a test purpose describing a set of behaviours to be tested, compute a reactive program that observes an implementation of the system to detect nonconformant behaviour, while trying to control it towards satisfying the test purpose. In this paper we describe an approach for generating symbolic test cases, in the form of input-output automata with variables and parameters.}}
@ARTICLE{Aştefǎnoaei_2009,title={The refinement of choreographed multi-agent systems},year={2009},author={Lǎcrǎmioara Aştefǎnoaei and Lacramioara Astefanoaei and Lăcrămioara Aştefănoaei and Frank S. de Boer and Frank S. de Boer and Mehdi Dastani and Mehdi Dastani},doi={10.1007/978-3-642-11355-0_2},pmid={null},pmcid={null},mag_id={1852995892},journal={null},abstract={This paper generalises the theory of agent refinement from [1] to multi-agent systems in the presence of new coordination mechanisms extended with real time. The generalisation is such that refinement is compositional. This means that refinement at the individual level implies refinement at the multi-agent system level. Compositionality is an important property since it reduces heavily the verification process. Thus having a theory of refinement is a crucial step towards the verification of multi-agent systems' correctness.}}
@ARTICLE{Albert_2011,title={Simulating concurrent behaviors with worst-case cost bounds},year={2011},author={Elvira Albert and Elvira Albert and Samir Genaim and Samir Genaim and Miguel Gómez-Zamalloa and Miguel Gómez-Zamalloa and Einar Broch Johnsen and Einar Broch Johnsen and Rudolf Schlatte and Rudolf Schlatte and Silvia Lizeth Tapia Tarifa and Silvia Lizeth Tapia Tarifa and S. Lizeth Tapia Tarifa},doi={10.1007/978-3-642-21437-0_27},pmid={null},pmcid={null},mag_id={1874151619},journal={null},abstract={Modern software systems are increasingly being developed for deployment on a range of architectures. For this purpose, it is interesting to capture aspects of low-level deployment concerns in high-level modeling languages. In this paper, an executable object-oriented modeling language is extended with resource-restricted deployment components. To analyze model behavior a formal methodology is proposed to assess resource consumption, which balances the scalability of the method and the reliability of the obtained results. The approach applies to a general notion of resource, including traditional cost measures (e.g., time, memory) as well as concurrency-related measures (e.g., requests to a server, spawned tasks). The main idea of our approach is to combine reliable (but expensive) worst-case cost analysis of statically predictable parts of the model with fast (but inherently incomplete) simulations of the concurrent aspects in order to avoid the state-space explosion. The approach is illustrated by the analysis of memory consumption.}}
@ARTICLE{Gibson−Robinson_2014,title={FDR3 — A Modern Refinement Checker for CSP},year={2014},author={Thomas Gibson−Robinson and Thomas Gibson-Robinson and Philip Armstrong and Philip Armstrong and Alexandre Boulgakov and Alexandre Boulgakov and A. W. Roscoe and A. W. Roscoe},doi={10.1007/978-3-642-54862-8_13},pmid={null},pmcid={null},mag_id={1882160752},journal={null},abstract={FDR3 is a complete rewrite of the CSP refinement checker FDR2, incorporating a significant number of enhancements. In this paper we describe the operation of FDR3 at a high level and then give a detailed description of several of its more important innovations. This includes the new multi-core refinement-checking algorithm that is able to achieve a near linear speed up as the number of cores increase. Further, we describe the new algorithm that FDR3 uses to construct its internal representation of CSP processes—this algorithm is more efficient than FDR2’s, and is able to compile a large class of CSP processes to more efficient internal representations. We also present experimental results that compare FDR3 to related tools, which show it is unique (as far as we know) in being able to scale beyond the bounds of main memory.}}
@ARTICLE{Bernot_1991,title={Testing against formal specifications: a theoretical view},year={1991},author={Gilles Bernot and Gilles Bernot},doi={10.1007/3540539816_63},pmid={null},pmcid={null},mag_id={1885303164},journal={null},abstract={Resume : Cet article developpe une theorie pour le test de logiciel. Si l’on dispose d’une specification formelle des proprietes attendues du logiciel, on peut etudier formellement la verification de logiciel par rapport a sa specification. L’avantage principal de la theorie developpee ici est de preciser explicitement ce que l’on fait lorsque le processus de verification melange des methodes de preuve (pour les proprietes cruciales) et des techniques de test (pour les autres proprietes). Deux etapes sont essentielles pour le test: le choix d’un jeu de test, puis la decision du succes ou de l’echec de ce jeu de test lorsqu’il est soumis au programme. Nous montrons qu’il est possible de determiner la qualite d’un jeu de test, ainsi que la fiabilite de la decision de succes ou echec, par le biais d’hypotheses sur le programme teste. Nous decrivons alors des schemas d’hypothese qui permettent de selectionner automatiquement des jeux de test a partir d’une specification algebrique.}}
@ARTICLE{Jackson_2006,title={Software Abstractions: Logic, Language, and Analysis},year={2006},author={Daniel Jackson},doi={null},pmid={null},pmcid={null},mag_id={1895387792},journal={null},abstract={In Software Abstractions Daniel Jackson introduces an approach to software design that draws on traditional formal methods but exploits automated tools to find flaws as early as possible. This approach--which Jackson calls "lightweight formal methods" or "agile modeling"--takes from formal specification the idea of a precise and expressive notation based on a tiny core of simple and robust concepts but replaces conventional analysis based on theorem proving with a fully automated analysis that gives designers immediate feedback. Jackson has developed Alloy, a language that captures the essence of software abstractions simply and succinctly, using a minimal toolkit of mathematical notions. This revised edition updates the text, examples, and appendixes to be fully compatible with the latest version of Alloy (Alloy 4). The designer can use automated analysis not only to correct errors but also to make models that are more precise and elegant. This approach, Jackson says, can rescue designers from "the tarpit of implementation technologies" and return them to thinking deeply about underlying concepts. Software Abstractions introduces the key elements: a logic, which provides the building blocks of the language; a language, which adds a small amount of syntax to the logic for structuring descriptions; and an analysis, a form of constraint solving that offers both simulation (generating sample states and executions) and checking (finding counterexamples to claimed properties).}}
@ARTICLE{Campbell_2014,title={Randomised Testing of a Microprocessor Model Using SMT-Solver State Generation},year={2014},author={B. K. Campbell and Brian Campbell and Ian Stark and Ian Stark},doi={10.1007/978-3-319-10702-8_13},pmid={null},pmcid={null},mag_id={1896082546},journal={null},abstract={We validate a HOL4 model of the ARM Cortex-M0 microcontroller core by testing the model’s behaviour on randomly chosen instructions against a real chip.}}
@ARTICLE{Ammann_1998,title={Using model checking to generate tests from specifications},year={1998},author={Paul Ammann and Paul Ammann and Paul E. Black and Paul E. Black and William J. Majurski and W. Majurski},doi={10.1109/icfem.1998.730569},pmid={null},pmcid={null},mag_id={1896160926},journal={null},abstract={We apply a model checker to the problem of test generation using a new application of mutation analysis. We define syntactic operators, each of which produces a slight variation on a given model. The operators define a form of mutation analysis at the level of the model checker specification. A model checker generates countersamples which distinguish the variations from the original specification. The countersamples can easily be turned into complete test cases, that is, with inputs and expected results. We define two classes of operators: those that produce test cases from which a correct implementation must differ, and those that produce test cases with which it must agree. There are substantial advantages to combining a model checker with mutation analysis. First, test case generation is automatic; each countersample is a complete test case. Second, in sharp contrast to program-based mutation analysis, equivalent mutant identification is also automatic. We apply our method to an example specification and evaluate the resulting test sets with coverage metrics on a Java implementation.}}
@ARTICLE{Sampaio_2009,title={Compositional Verification of Input-Output Conformance via CSP Refinement Checking},year={2009},author={Augusto Sampaio and Augusto Sampaio and Sidney C. Nogueira and Sidney Nogueira and Alexandre Mota and Alexandre Mota and Alexandre Mota},doi={10.1007/978-3-642-10373-5_2},pmid={null},pmcid={null},mag_id={1896301006},journal={null},abstract={This paper contributes to a testing theory, based on the CSP process algebra, whose conformance relation (cspio) distinguishes input and output events. Although cspio has been defined in terms of the standard CSP traces model, we show that our theory can be immediately extended to address deadlock, outputlock and livelock situations if a special output event is used to represent quiescence. This is formally established by showing that this broader view of cspio is equivalent to Tretmans' ioco relation. Furthermore, we address compositional conformance verification, establishing compositionality properties for cspio with respect to process composition operators. Our testing theory has been adopted in an industrial context involving a collaboration with Motorola, whose focus is on the testing of mobile applications. Some examples are presented to illustrate the overall approach.}}
@ARTICLE{Matteis_2005,title={A 600mV 1.32mW 75dB-DR 4/sup th/-order baseband analog filter for UMTS receivers},year={2005},author={M. De Matteis and M. De Matteis and S. D'Amico and S. D 'Amico and A. Basçhirotto and A. Baschirotto and Andrea Baschirotto},doi={10.1109/rme.2005.1543025},pmid={null},pmcid={null},mag_id={1906669652},journal={null},abstract={In this paper a 600mV supply voltage 4/sup th/ order low-pass analog filter for UMTS receivers is presented. The filter is designed using two 2/sup nd/ order filtering stages based on biquadratic cells, where a suitable level shifter allows to achieve rail-to-rail input and output swing, while operating at a supply voltage of V/sub th/+3V/sub OV/. From a single 600mV supply voltage in a CMOS 0.13/spl mu/m technology the single-ended analog filter features a THD=-49dB for a 480mVpp output signal amplitude.}}
@ARTICLE{Fabbri_1994,title={Mutation analysis testing for finite state machines},year={1994},author={Sandra Fabbri and S.C. Pinto Ferraz Fabbri and Márcio Eduardo Delamaro and Márcio Eduardo Delamaro and José Carlos Maldonado and José Carlos Maldonado and Paulo César Masiero and Paulo Cesar Masiero},doi={10.1109/issre.1994.341378},pmid={null},pmcid={null},mag_id={1908148254},journal={null},abstract={Proposes the application of the mutation analysis criterion in the context of specifications based on finite state machines. The main concepts of finite state machines and mutation analysis are briefly introduced. An experiment is reported which manually applies mutation analysis to a finite state machine modeling a Class 0 ISO transport protocol specification, using two test-sequence generator criteria-the W method and the TT (transition tours) method. The results obtained are presented, and evidences are given that the use of mutation analysis is effective in this context. Finally, the lines of evolution of the work presented in this paper are briefly discussed. >}}
@ARTICLE{Beizer_1983,title={Software Testing Techniques},year={1983},author={Boris Beizer and Boris Beizer and Boris Beizer},doi={null},pmid={null},pmcid={null},mag_id={1910771831},journal={null},abstract={null}}
@ARTICLE{Offutt_1996,title={Detecting equivalent mutants and the feasible path problem},year={1996},author={A. Jefferson Offutt and A.J. Offutt and Jie Pan and Jie Pan},doi={10.1109/cmpass.1996.507890},pmid={null},pmcid={null},mag_id={1924720657},journal={null},abstract={Mutation testing is a technique for testing software units that has great potential for improving the quality of testing, and thereby increasing our ability to assure the high reliability of critical software. The paper presents a technique that uses mathematical constraints to automatically detect equivalent mutant programs. The paper also describes how the approach is used for the feasible path problem. The paper describes how test criteria are formalized as mathematical constraint systems, how equivalent mutants are represented as infeasible constraints, and how infeasible constraints are detected. A proof of concept implementation has been developed to demonstrate this technique, and experimental results from using this tool are presented. Limitations of the system and the method are described, and proposals for improvements are made.}}
@ARTICLE{Pressman_1982,title={Software Engineering: A Practitioner's Approach},year={1982},author={Roger S. Pressman},doi={null},pmid={null},pmcid={null},mag_id={1936022305},journal={null},abstract={null}}
@ARTICLE{Shiflett_2005,title={Essential PHP Security},year={2005},author={Chris Shiflett and Chris Shiflett},doi={null},pmid={null},pmcid={null},mag_id={1939551207},journal={null},abstract={Being highly flexible in building dynamic, database-driven web applications makes the PHP programming language one of the most popular web development tools in use today. It also works beautifully with other open source tools, such as the MySQL database and the Apache web server. However, as more web sites are developed in PHP, they become targets for malicious attackers, and developers need to prepare for the attacks. Security is an issue that demands attention, given the growing frequency of attacks on web sites. Essential PHP Security explains the most common types of attacks and how to write code that isn't susceptible to them. By examining specific attacks and the techniques used to protect against them, you will have a deeper understanding and appreciation of the safeguards you are about to learn in this book. In the much-needed (and highly-requested) Essential PHP Security, each chapter covers an aspect of a web application (such as form processing, database programming, session management, and authentication). Chapters describe potential attacks with examples and then explain techniques to help you prevent those attacks. Topics covered include: Preventing cross-site scripting (XSS) vulnerabilities Protecting against SQL injection attacks Complicating session hijacking attempts You are in good hands with author Chris Shiflett, an internationally-recognized expert in the field of PHP security. Shiflett is also the founder and President of Brain Bulb, a PHP consultancy that offers a variety of services to clients around the world.}}
@ARTICLE{Kiniry_2008,title={Secret Ninja Formal Methods},year={2008},author={Joseph R. Kiniry and Joseph R. Kiniry and Daniel M. Zimmerman and Daniel M. Zimmerman},doi={10.1007/978-3-540-68237-0_16},pmid={null},pmcid={null},mag_id={1940325564},journal={null},abstract={The use of formal methods can significantly improve software quality. However, many instructors and students consider formal methods to be too difficult, impractical, and esoteric for use in undergraduate classes. This paper describes a method, used successfully at several universities, that combines ninja stealth with the latest advances in formal methods tools and technologies to integrate applied formal methods into software engineering courses.}}
@ARTICLE{Leino_2010,title={A polymorphic intermediate verification language: design and logical encoding},year={2010},author={K. Rustan M. Leino and K. Rustan M. Leino and Philipp Rümmer and Philipp Rümmer},doi={10.1007/978-3-642-12002-2_26},pmid={null},pmcid={null},mag_id={1946187443},journal={null},abstract={Intermediate languages are a paradigm to separate concerns in software verification systems when bridging the gap between programming languages and the logics understood by theorem provers. While such intermediate languages traditionally only offer rather simple type systems, this paper argues that it is both advantageous and feasible to integrate richer type systems with features like (higher-ranked) polymorphism and quantification over types. As a concrete solution, the paper presents the type system of Boogie 2, an intermediate verification language that is used in several program verifiers. The paper gives two encodings of types and formulae in simply typed logic such that SMT solvers and other theorem provers can be used to discharge verification conditions.}}
@ARTICLE{Teague_2008,title={Coercion-resistant tallying for STV voting},year={2008},author={Vanessa Teague and Vanessa Teague and Kim Ramchen and Kim Ramchen and Lee Naish and Lee Naish},doi={null},pmid={null},pmcid={null},mag_id={1946916043},journal={null},abstract={There are many advantages to voting schemes in which voters rank all candidates in order, rather than just choosing their favourite. However, these schemes inherently suffer from a coercion problem when there are many candidates, because a coercer can demand a certain permutation from a voter and then check whether that permutation appears during tallying. In this paper, we solve this problem for the popular STV system, by constructing an algorithm for the verifiable tallying of encrypted votes. Our construction improves upon existing work because it extends to multiple-seat STV and reveals less information than other schemes.}}
@ARTICLE{Schaefer_2010,title={Delta-oriented programming of software product lines},year={2010},author={Ina Schaefer and Ina Schaefer and Lorenzo Bettini and Lorenzo Bettini and Ferruccio Damiani and Ferruccio Damiani and Nico Tanzarella and Nico Tanzarella},doi={10.1007/978-3-642-15579-6_6},pmid={null},pmcid={null},mag_id={1949851419},journal={null},abstract={Feature-oriented programming (FOP) implements software product lines by composition of feature modules. It relies on the principles of stepwise development. Feature modules are intended to refer to exactly one product feature and can only extend existing implementations. To provide more flexibility for implementing software product lines, we propose delta-oriented programming (DOP) as a novel programming language approach. A product line is represented by a core module and a set of delta modules. The core module provides an implementation of a valid product that can be developed with well-established single application engineering techniques. Delta modules specify changes to be applied to the core module to implement further products by adding, modifying and removing code. Application conditions attached to delta modules allow handling combinations of features explicitly. A product implementation for a particular feature configuration is generated by applying incrementally all delta modules with valid application condition to the core module. In order to evaluate the potential of DOP, we compare it to FOP, both conceptually and empirically.}}
@ARTICLE{Yu_2010,title={STRANGER: an automata-based string analysis tool for PHP},year={2010},author={Fang Yu and Fang Yu and Muath Alkhalaf and Muath Alkhalaf and Tevfik Bultan and Tevfik Bultan},doi={10.1007/978-3-642-12002-2_13},pmid={null},pmcid={null},mag_id={1952344271},journal={null},abstract={Stranger is an automata-based string analysis tool for finding and eliminating string-related security vulnerabilities in PHP applications. Stranger uses symbolic forward and backward reachability analyses to compute the possible values that the string expressions can take during program execution. Stranger can automatically (1) prove that an application is free from specified attacks or (2) generate vulnerability signatures that characterize all malicious inputs that can be used to generate attacks.}}
@ARTICLE{Bause_2012,title={Stochastic Petri Nets: An Introduction to the Theory},year={2012},author={Falko Bause and Falko Bause and Pieter S. Kritzinger and Pieter S. Kritzinger},doi={null},pmid={null},pmcid={null},mag_id={1952732242},journal={null},abstract={I Stochastic Theory.- 1 Random Variables.- 1.1 Probability Theory Refresher.- 1.2 Discrete Random Variables.- 1.3 Continuous Random Variables.- 1.4 Moments of a Random Variable.- 1.5 Joint Distributions of Random Variables.- 1.6 Stochastic Processes.- 2 Markov Processes.- 2.1 Discrete Time Markov Chains.- 2.1.1 Steady State Distribution.- 2.1.2 Absorbing Chains and Transient Behaviour.- 2.2 Semi-Markov Processes.- 2.2.1 Formal Model of a Semi-Markov Process.- 2.2.2 Interval Transition Probabilities.- 2.2.3 Steady State Behaviour.- 2.3 Continuous Time Markov Chains.- 2.3.1 Steady State Distribution.- 2.4 Embedded Markov Chains.- 3 General Queueing Systems.- 3.1 Little's Law.- 3.2 Birth-Death Processes.- 3.3 Poisson Process.- 3.4 M/M/1 Queue.- 3.5 M/M/m Queue.- 3.6 Queues with Processor Sharing Scheduling Strategy.- 3.7 Queues with Infinite Servers.- 3.8 Queues with Priority Service.- 4 Further Reading.- II Petri Nets.- 5 Place-Transit ion Nets.- 5.1 Structure of Place-Transition Nets.- 5.2 Dynamic Behaviour of Place-Transition Nets.- 5.3 Properties of Place-Transition Nets.- 5.4 Analysis of Place-Transit ion Nets.- 5.4.1 Analysis of the Reachability Set.- 5.4.2 Invariant Analysis.- 5.4.3 Analysis of Net Classes.- Analysis of State Machines.- Analysis of Marked Graphs.- Analysis of EFC-nets.- 5.4.4 Reduction and Synthesis Analysis.- 5.5 Further Remarks on Petri Nets.- 6 Coloured Petri Nets.- 7 Further Reading.- III Time-Augmented Petri Nets.- 8 Stochastic Petri Nets.- 9 Generalized Stochastic Petri Nets.- 9.1 Quantitative Analysis of GSPNs.- 9.2 Qualitative Analysis of GSPNs.- 9.2.1 Qualitative Analysis of EFC-GSPNs.- 9.3 Further Remarks on GSPNs.- 10 Queueing Petri Nets.- 10.1 Quantitative Analysis of QPNs.- 10.2 Qualitative Analysis of QPNs.- 10.2.1 Qualitative Analysis of EFC-QPNs.- 10.3 Some Remarks on Quantitative Analysis.- 11 Further Reading.- 12 Application Examples.- 12.1 Resource Sharing.- 12.2 Node of a DQDB network.- 13 Solutions to Selected Exercises.}}
@ARTICLE{Henke_1998,title={Case Studies in Meta-Level Theorem Proving},year={1998},author={Friedrich W. von Henke and Friedrich W. Henke and Friedrich W. von Henke and Stephan Pfab and Stephan Pfab and Holger Pfeifer and Holger Pfeifer and Harald Rueß and Harald Rueß},doi={10.1007/bfb0055152},pmid={null},pmcid={null},mag_id={1956146622},journal={null},abstract={We describe an extension of the PVS system that provides a reasonably efficient and practical notion of reflection and thus allows for soundly adding formalized and verified new proof procedures. These proof procedures work on representations of a part of the underlying logic and their correctness is expressed at the object level using a computational reflection function. The implementation of the PVS system has been extended with an efficient evaluation mechanism, since the practicality of the approach heavily depends on careful engineering of the core system, including efficient normalization of functional expressions. We exemplify the process of applying meta-level proof procedures with a detailed description of the encoding of cancellation in commutative monoids and of the kernel of a BDD package.}}
@ARTICLE{Soundarajan_1998,title={Inheritance: from code reuse to reasoning reuse},year={1998},author={Neelam Soundarajan and Neelam Soundarajan and Stephen Fridella and Stephen Fridella},doi={10.1109/icsr.1998.685745},pmid={null},pmcid={null},mag_id={1956544187},journal={null},abstract={In the object-oriented approach, a designer can, given an existing base class, use inheritance to build a derived class that extends, or that slightly differs from the base class, but in order to exploit the full potential of inheritance to build systems incrementally, the designer must also be able to reason about the derived class incrementally. This paper presents a specification notation and verification procedure that allows such incremental reasoning out; the approach makes important use of the concrete specification of a class, in addition to the usual abstract specification. The reasoning reuse that the approach enables is illustrated by applying it to a simple example.}}
@ARTICLE{Barnett_2004,title={The spec# programming system: an overview},year={2004},author={Mike Barnett and Mike Barnett and K. Rustan M. Leino and K. Rustan M. Leino and Wolfram Schulte and Wolfram Schulte},doi={10.1007/978-3-540-30569-9_3},pmid={null},pmcid={null},mag_id={1959256509},journal={null},abstract={The Spec# programming system is a new attempt at a more cost effective way to develop and maintain high-quality software. This paper describes the goals and architecture of the Spec# programming system, consisting of the object-oriented Spec# programming language, the Spec# compiler, and the Boogie static program verifier. The language includes constructs for writing specifications that capture programmer intentions about how methods and data are to be used, the compiler emits run-time checks to enforce these specifications, and the verifier can check the consistency between a program and its specifications.}}
@ARTICLE{Cimatti_2011,title={KRATOS: a software model checker for SystemC},year={2011},author={Alessandro Cimatti and Alessandro Cimatti and Alberto Griggio and Alberto Griggio and Andrea Micheli and Andrea Micheli and Iman Narasamdya and Iman Narasamdya and Marco Roveri and Marco Roveri},doi={10.1007/978-3-642-22110-1_24},pmid={null},pmcid={null},mag_id={1959882242},journal={null},abstract={The growing popularity of SystemC has attracted research aimed at the formal verification of SystemC designs. In this paper we present KRATOS, a software model checker for SystemC. KRATOS verifies safety properties, in the form of program assertions, by allowing users to explore two directions in the verification. First, by relying on the translation from SystemC designs to sequential C programs, KRATOS is capable of model checking the resulting C programs using the symbolic lazy predicate abstraction technique. Second, KRATOS implements a novel algorithm, called ESST, that combines Explicit state techniques to deal with the SystemC Scheduler, with Symbolic techniques to deal with the Threads. KRATOS is built on top of NUSMV and MATHSAT, and uses state-ofthe-art SMT-based techniques for program abstractions and refinements.}}
@ARTICLE{Woodward_1992,title={OBJTEST: an experimental testing tool for algebraic specifications},year={1992},author={Martin R. Woodward and M.R. Woodward},doi={null},pmid={null},pmcid={null},mag_id={1961028816},journal={null},abstract={Algebraic specifications involve the development of 'axioms' or equations to model the behaviour of systems. The technique is one example of a formal method of specification. By using the equations to drive a process of term-rewriting, test expressions can be evaluated, thus providing an execution facility. Such animation certainly helps in checking typographical and notational errors. However, there is still a need for thorough testing of algebraic specifications to uncover more subtle errors. The author describes in outline, a prototype testing tool for algebraic specifications, OBJTEST, built around the ObjEx system. The two principal facets of the tool are the automatic generation of 'exhaustive' sets of test expressions from a specification, followed by the use of these test expressions in mutation testing of the given specification.}}
@ARTICLE{Asaadi_2011,title={Towards model-based testing of electronic funds transfer systems},year={2011},author={Hamid Reza Asaadi and Hamid Reza Asaadi and Ramtin Khosravi and Ramtin Khosravi and Mohammad Reza Mousavi and Mohammad Reza Mousavi and Neda Noroozi and N Neda Noroozi},doi={10.1007/978-3-642-29320-7_17},pmid={null},pmcid={null},mag_id={1962872449},journal={null},abstract={We report on our first experience with applying model-based testing techniques to an operational Electronic Funds Transfer (EFT) switch. The goal is to test the conformance of the EFT switch to the standard flows described by the ISO 8583 standard. To this end, we first make a formalization of the transaction flows specified in the ISO 8583 standard in terms of a Labeled Transition System (LTS). This formalization paves the way for model-based testing based on the formal notion of Input-Output Conformance (IOCO) testing. We adopt and augment IOCO testing for our particular application domain. We develop a prototype implementation and apply our proposed techniques in practice. We discuss the encouraging obtained results and the observed shortcomings of the present approach. We outline a roadmap to remedy the shortcomings and enhance the test results.}}
@ARTICLE{Salmeron_2005,title={An AHP-based methodology to rank critical success factors of executive information systems},year={2005},author={Jose L. Salmeron and Jose L. Salmeron and Inés Herrero and Ines Herrero},doi={10.1016/j.csi.2004.09.002},pmid={null},pmcid={null},mag_id={1963772036},journal={Computer Standards & Interfaces},abstract={For academics and practitioners concerned with computer-based information systems, one central issue is the study of critical success factors (CSF) of information systems development and implementation. Whereas several critical success factors analyses appear in the literature, most of them do not have any technical background. In this paper, we propose the use of the analytic hierarchy process (AHP) to set critical success factors priorities. Results suggest that technical elements are less critical than information and human factors and that an adequate knowledge of the information requirements of users is the most important critical success factors related with executive information systems (EIS).}}
@ARTICLE{Schaefer_2010,title={Pure delta-oriented programming},year={2010},author={Ina Schaefer and Ina Schaefer and Ferruccio Damiani and Ferruccio Damiani},doi={10.1145/1868688.1868696},pmid={null},pmcid={null},mag_id={1963857392},journal={null},abstract={Delta-oriented programming (DOP) is a modular approach for implementing software product lines. Delta modules generalize feature modules by allowing removal of functionality. However, DOP requires to select one particular product as core product from which all products are generated. In this paper, we propose pure delta-oriented programming (Pure DOP) that is a conceptual simplification of traditional DOP. In Pure DOP, the requirement of one designated core product is dropped. Instead, program generation only relies on delta modules comprising program modifications such that Pure DOP is more flexible than traditional DOP. Furthermore, we show that Pure DOP is a true generalization of FOP and supports proactive, reactive and extractive product line engineering.}}
@ARTICLE{Harman_2001,title={An overview of program slicing},year={2001},author={Mark Harman and Mark Harman and Robert M. Hierons and Robert M. Hierons},doi={10.1002/swf.41},pmid={null},pmcid={null},mag_id={1964060567},journal={Software Focus},abstract={MARK HARMAN and ROBERT HIERONS review three semantic paradigms for slicing — static, dynamic and conditioned; and two syntactic paradigms — syntax-preserving and amorphous. Slicing has been applied to many software development problems including testing, reuse, maintenance and evolution. This paper describes the main forms of program slice and some of the applications to which slicing has been put. Copyright © 2001 John Wiley & Sons, Ltd.}}
@ARTICLE{Burdy_2005,title={An overview of JML tools and applications},year={2005},author={Lilian Burdy and Lilian Burdy and Yoonsik Cheon and Yoonsik Cheon and David R. Cok and David R. Cok and M. Ernst and Michael D. Ernst and Joseph R. Kiniry and Joseph R. Kiniry and Gary T. Leavens and Gary T. Leavens and K. Rustan M. Leino and K. Rustan M. Leino and Erik Poll and Erik Poll},doi={10.1007/s10009-004-0167-4},pmid={null},pmcid={null},mag_id={1964830323},journal={null},abstract={The Java Modeling Language (JML) can be used to specify the detailed design of Java classes and interfaces by adding annotations to Java source files. The aim of JML is to provide a specification language that is easy to use for Java programmers and that is supported by a wide range of tools for specification typechecking, runtime debugging, static analysis, and verification.This paper gives an overview of the main ideas behind JML, details about JML’s wide range of tools, and a glimpse into existing applications of JML.}}
@ARTICLE{Artho_2003,title={High-Level Data Races},year={2003},author={Cyrille Artho and Cyrille Artho and Klaus Havelund and Klaus Havelund and Armin Biere and Armin Biere},doi={10.1002/stvr.281},pmid={null},pmcid={null},mag_id={1965462925},journal={Software Testing, Verification & Reliability},abstract={Data races are a common problem in concurrent and multi-threaded programming. Experience shows that the classical notion of a data race is not powerful enough to capture certain types of inconsistencies occurring in practice. This paper investigates data races on a higher abstraction layer. This enables detection of inconsistent uses of shared variables, even if no classical race condition occurs. For example, a data structure representing a coordinate pair may have to be treated atomically. By lifting the meaning of a data race to a higher level, such problems can now be covered. The paper defines the concepts ‘view’ and ‘view consistency’ to give a notation for this novel kind of property. It describes what kinds of errors can be detected with this new definition, and where its limitations are. It also gives a formal guideline for using data structures in a multi-threaded environment. © US Government copyright}}
@ARTICLE{Nilsen_2006,title={A type system to assure scope safety within safety-critical Java modules},year={2006},author={Kelvin Nilsen and Kelvin Nilsen and Kelvin Nilsen},doi={10.1145/1167999.1168017},pmid={null},pmcid={null},mag_id={1965561833},journal={null},abstract={To address the needs of safety-critical system developers, a type system based on Java 5.0 meta-data annotations and special bytecode verification techniques is described. This type system enables programmers to develop code for which the byte code verifier is able to prove the absence of scoped memory protocol errors, thereby eliminating the need for run-time assignment checks. Benefits of the type system include improved software reliability, easier maintenance and integration of independently developed realtime software modules, and higher performance.}}
@ARTICLE{Larsen_1997,title={Time-abstracted bisimulation: implicit specifications and decidability},year={1997},author={Kim Guldstrand Larsen and Kim Guldstrand Larsen and Wang Yi and Yi Wang},doi={10.1006/inco.1997.2623},pmid={null},pmcid={null},mag_id={1967761514},journal={Information & Computation},abstract={In the last few years a number of real-time process calculi have emerged with the purpose of capturing important quantitative aspects of real-time systems. In addition, a number of process equivalences sensitive to time-quantities have been proposed, among these the notion of timed (bisimulation) equivalence in [RR86, DS89, HR91, BB89, NRSV90, MT90, Wan91b].}}
@ARTICLE{Li_2005,title={Modular Verification of Open Features Using Three-Valued Model Checking},year={2005},author={Harry C. Li and Harry C. Li and Harry C. Li and Shriram Krishnamurthi and Shriram Krishnamurthi and Kathi Fisler and Kathi Fisler},doi={10.1007/s10515-005-2643-9},pmid={null},pmcid={null},mag_id={1968718409},journal={null},abstract={Feature-oriented programming organizes programs around features rather than objects, thus better supporting extensible, product-line architectures. Programming languages increasingly support this style of programming, but programmers get little support from verification tools. Ideally, programmers should be able to verify features independently of each other and use automated compositional reasoning techniques to infer properties of a system from properties of its features. Achieving this requires carefully designed interfaces: they must hold sufficient information to enable compositional verification, yet tools should be able to generate this information automatically because experience indicates programmers cannot or will not provide it manually. We present a model of interfaces that supports automated, compositional, feature-oriented model checking. To demonstrate their utility, we automatically detect the feature-interaction problems originally found manually by Robert Hall in an email suite case study.}}
@ARTICLE{Campbell_1996,title={Sheep cloned by nuclear transfer from a cultured cell line},year={1996},author={Keith Campbell and Keith H.S. Campbell and Jim McWhir and Jim McWhir and William A. Ritchie and W. A. Ritchie and Ian Wilmut and Ian Wilmut},doi={10.1038/380064a0},pmid={8598906},pmcid={null},mag_id={1970894097},journal={Nature},abstract={NUCLEAR transfer has been used in mammals as both a valuable tool in embryological studies1 and as a method for the multiplication of 'elite' embryos2–4. Offspring have only been reported when early embryos, or embryo-derived cells during primary culture, were used as nuclear donors5,6. Here we provide the first report, to our knowledge, of live mammalian offspring following nuclear transfer from an established cell line. Lambs were born after cells derived from sheep embryos, which had been cultured for 6 to 13 passages, were induced to quiesce by serum starvation before transfer of their nuclei into enucleated oocytes. Induction of quiescence in the donor cells may modify the donor chromatin structure to help nuclear reprogramming and allow development. This approach will provide the same powerful opportunities for analysis and modification of gene function in livestock species that are available in the mouse through the use of embryonic stem cells7.}}
@ARTICLE{Bjørk_2010,title={Lightweight Time Modeling in Timed Creol},year={2010},author={Joakim Bjørk and Joakim Bjørk and Einar Broch Johnsen and Einar Broch Johnsen and Olaf Owe and Olaf Owe and Rudolf Schlatte and Rudolf Schlatte},doi={10.4204/eptcs.36.4},pmid={null},pmcid={null},mag_id={1971333828},journal={null},abstract={Creol is an object-oriented modeling language in which inherently concurrent objects exchange asynchronous method calls. The operational semantics of Creol is written in an actor-based style, formulated in rewriting logic. The operational semantics yields a language interpreter in the Maude system, which can be used to analyze models. Recently, Creol has been applied to the modeling of systems with radio communication, such as sensor systems. With radio communication, messages expire and, if sent simultaneously, they may collide in the air. In order to capture these and other properties of distributed systems, we extended Creol’s operational semantics with a notion of time. We exploit the framework of a language interpreter to use a lightweight notion of time, in contrast to that needed for a general purpose specification language. This paper presents a timed extension of Creol, including the semantics and the implementation strategy, and discusses its properties using an extended example. The approach can be generalized to other concurrent object or actor-based systems.}}
@ARTICLE{Pockrandt_2011,title={Model checking a SystemC/TLM design of the AMBA AHB protocol},year={2011},author={Marcel Pockrandt and Marcel Pockrandt and Paula Herber and Paula Herber and Sabine Glesner and Sabine Glesner},doi={10.1109/estimedia.2011.6088527},pmid={null},pmcid={null},mag_id={1971552050},journal={null},abstract={Transaction Level Modeling (TLM) is gaining more and more importance to quickly evaluate design alternatives in multimedia systems and other mixed HW/SW systems. However, the comprehensive and automated verification of TLM models is still a difficult challenge. In previous work, we presented an approach for model checking of SystemC/TLM designs based on a transformation into Uppaal timed automata. In this paper, we present an optimized version of our previously proposed transformation, and show its effectiveness with experimental results from an industrial case study. The key idea is to generate a Uppaal model that is especially tailored for being model checked. This significantly reduces the semantic state space and makes model checking considerably faster and less memory-consuming. We demonstrate this by comparing the verification times of both versions for our previously used case study, and by presenting results from a new and larger case study, namely a TLM implementation of the AMBA Advanced High-performance Bus (AHB). The AMBA bus is one of the most popular on-chip bus architectures in IP-based embedded SoCs, and it is used in many multimedia applications. The case study shows that with the proposed optimizations, our approach is applicable for industrial real world examples. The detection of a serious bug, namely a deadlock situation in a certain scenario, and also the verification of some important safety, liveness, and timing properties provide evidence for the usefulness of our approach.}}
@ARTICLE{Payet_2008,title={Loop detection in term rewriting using the eliminating unfoldings},year={2008},author={Étienne Payet and Étienne Payet},doi={10.1016/j.tcs.2008.05.013},pmid={null},pmcid={null},mag_id={1972319677},journal={Theoretical Computer Science},abstract={In this paper, we present a fully automatizable approach to detecting loops in standard term rewriting. Our method is based on semi-unification and an unfolding operation which processes both forwards and backwards and considers variable subterms. We also describe a technique to reduce the explosion of rules caused by the unfolding process. The idea is to eliminate from the set of unfoldings some rules that are estimated as useless for detecting loops. This is done by an approximation which consists in pruning the left-hand or right-hand side of the rules used to unfold. The analyser that we have implemented is able to solve most of the examples from the Termination Competition'07 that do not terminate due to a loop.}}
@ARTICLE{Savage_1997,title={Eraser: a dynamic data race detector for multithreaded programs},year={1997},author={Stefan Savage and Stefan Savage and Michael T. Burrows and Michael Burrows and Greg Nelson and Greg Nelson and Patrick Sobalvarro and Patrick G. Sobalvarro and Thomas E. Anderson and Thomas Anderson},doi={10.1145/265924.265927},pmid={null},pmcid={null},mag_id={1972544179},journal={ACM Transactions on Computer Systems},abstract={Multithreaded programming is difficult and error prone. It is easy to make a mistake in synchronization that produces a data race, yet it can be extremely hard to locate this mistake during debugging. This article describes a new tool, called Eraser, for dynamically detecting data races in lock-based multithreaded programs. Eraser uses binary rewriting techniques to monitor every shared-monory reference and verify that consistent locking behavior is observed. We present several case studies, including undergraduate coursework and a multithreaded Web search engine, that demonstrate the effectiveness of this approach.}}
@ARTICLE{Ülengin_1994,title={Forecasting foreign exchange rates: A comparative evaluation of AHP},year={1994},author={Füsun Ülengin and Füsun Ülengin and Burç Ülengin and Burç Ülengin},doi={10.1016/0305-0483(94)90031-0},pmid={null},pmcid={null},mag_id={1974014617},journal={Omega-international Journal of Management Science},abstract={null}}
@ARTICLE{Schaefer_2011,title={Formal Methods in Software Product Line Engineering},year={2011},author={Ina Schaefer and Ina Schaefer and Reiner Hähnle and Reiner Hähnle},doi={10.1109/mc.2011.47},pmid={null},pmcid={null},mag_id={1974033943},journal={IEEE Computer},abstract={Formal methods could overcome the limitations of current SPLE practice, ensuring high product quality while decreasing time to market.}}
@ARTICLE{Cheng_2002,title={Construction Partnering Process and Associated Critical Success Factors: Quantitative Investigation},year={2002},author={Eddie W.L. Cheng and Eddie W.L. Cheng and Heng Li and Heng Li},doi={10.1061/(asce)0742-597x(2002)18:4(194)},pmid={null},pmcid={null},mag_id={1974047734},journal={Journal of Management in Engineering},abstract={This paper aims at examining a customized model of construction partnering in order to highlight the relationships between the critical success factors and individual partnering process stages. It used two surveys (a simple rating method and the analytic hierarchy process) to produce empirical evidence for identifying the critical success factors and the partnering process stages. Results suggest that this study is consistent with previous findings that a general partnering process consists of three stages (formation, application, and completion/reactivation). In addition, it reaffirms that there are critical common factors affecting the whole partnering process and critical functional factors influencing individual process stages. Specifically, the four critical common factors are top management support, open communication, effective coordination, and mutual trust. Also, the critical functional factors at the stage of partnering formation are team building, facilitator, and partnering agreement; those of...}}
@ARTICLE{Daca_2014,title={Compositional Specifications for ioco Testing},year={2014},author={Przemysław Daca and Przemysław Daca and Thomas A. Henzinger and Thomas A. Henzinger and Thomas A. Henzinger and Willibald Krenn and Willibald Krenn and Dejan Ničković and Dejan Nickovic},doi={10.1109/icst.2014.50},pmid={null},pmcid={null},mag_id={1974673257},journal={null},abstract={Model-based testing is a promising technology for black-box software and hardware testing, in which test cases are generated automatically from high-level specifications. Nowadays, systems typically consist of multiple interacting components and, due to their complexity, testing presents a considerable portion of the effort and cost in the design process. Exploiting the compositional structure of system specifications can considerably reduce the effort in model-based testing. Moreover, inferring properties about the system from testing its individual components allows the designer to reduce the amount of integration testing. In this paper, we study compositional properties of the ioco-testing theory. We propose a new approach to composition and hiding operations, inspired by contract-based design and interface theories. These operations preserve behaviors that are compatible under composition and hiding, and prune away incompatible ones. The resulting specification characterizes the input sequences for which the unit testing of components is sufficient to infer the correctness of component integration without the need for further tests. We provide a methodology that uses these results to minimize integration testing effort, but also to detect potential weaknesses in specifications. While we focus on asynchronous models and the ioco conformance relation, the resulting methodology can be applied to a broader class of systems.}}
@ARTICLE{Hierons_2001,title={Testing a system specified using Statecharts and Z},year={2001},author={Robert M. Hierons and Robert M. Hierons and Sadegh Sadeghipour and Sadegh Sadeghipour and Harbhajan Singh and Harbhajan Singh},doi={10.1016/s0950-5849(00)00145-2},pmid={null},pmcid={null},mag_id={1974845322},journal={Information & Software Technology},abstract={Abstract   A hybrid specification language μSZ, in which the dynamic behaviour of a system is described using Statecharts and the data and the data transformations are described using Z, has been developed for the specification of embedded systems. This paper describes an approach to testing from a deterministic sequential specification written in μSZ. By considering the Z specifications of the operations, the extended finite state machine (EFSM) defined by the Statechart can be rewritten to produce an EFSM that has a number of properties that simplify test generation. Test generation algorithms are introduced and applied to an example. While this paper considers μSZ specifications, the approaches described might be applied whenever the specification is an EFSM whose states and transitions are specified using a language similar to Z.}}
@ARTICLE{Cavalli_1996,title={Standardization of formal methods in conformance testing of communication protocols},year={1996},author={Ana Cavalli and Ana R. Cavalli and Ana R. Cavalli and Ana Cavalli and Jean-Philippe Favreau and Jean Philippe Favreau and Marc Phalippou and Marc Phalippou},doi={10.1016/s0169-7552(96)00015-3},pmid={null},pmcid={null},mag_id={1975640419},journal={Computer Networks and Isdn Systems},abstract={null}}
@ARTICLE{Meseguer_1992,title={Conditional rewriting logic as a unified model of concurrency},year={1992},author={José Meseguer and José Meseguer},doi={10.1016/0304-3975(92)90182-f},pmid={null},pmcid={null},mag_id={1976195354},journal={Theoretical Computer Science},abstract={null}}
@ARTICLE{Dietl_2005,title={Universes: Lightweight Ownership for JML.},year={2005},author={Werner Dietl and Werner Dietl and Péter Müller and Peter Müller},doi={10.5381/jot.2005.4.8.a1},pmid={null},pmcid={null},mag_id={1978303100},journal={The Journal of Object Technology},abstract={Object-oriented programs with arbitrary object structures are difficult to understand, to maintain, and to reason about. Ownership has been applied successfully to structure the object store and to restrict how references can be passed and used. We describe how ownership relations can be expressed in the Java Modeling Language, JML. These ownership specifications can be checked by standard verification techniques, runtime assertion checking, ownership type systems, or combinations of these techniques. We show that the combination of the lightweight Universe type system and JML specifications is flexible enough to handle interesting implementations while keeping the annotation and checking overhead small. The Universe type system has been implemented in the JML compiler. This integration enables the application of ownership-based verification techniques to programs specified in JML.}}
@ARTICLE{Tanasă_2012,title={Schedulability Analysis for the Dynamic Segment of FlexRay: A Generalization to Slot Multiplexing},year={2012},author={Bogdan Tanasă and Bogdan Tanasa and Unmesh D. Bordoloi and Unmesh D. Bordoloi and Stefanie Kosuch and Stefanie Kosuch and Petru Eles and Petru Eles and Zebo Peng and Zebo Peng},doi={10.1109/rtas.2012.10},pmid={null},pmcid={null},mag_id={1979959894},journal={null},abstract={FlexRay, developed by a consortium of over hundred automotive companies, is a real-time communication protocol for automotive networks. In this paper, we propose a new approach for timing analysis of the event-triggered component of FlexRay, known as the dynamic segment. Our technique accounts for the fact that the FlexRay standard allows slot multiplexing, i.e., the same priority can be assigned to more than one message. Existing techniques have either ignored slot multiplexing in their analysis or made simplifying assumptions that severely limit achieving high bandwidth utilization. Moreover, we show that our technique returns less pessimistic results compared to previously known techniques even in the case where slot multiplexing is ignored.}}
@ARTICLE{Damiani_2011,title={Verifying traits: a proof system for fine-grained reuse},year={2011},author={Ferruccio Damiani and Ferruccio Damiani and Johan Dovland and Johan Dovland and Einar Broch Johnsen and Einar Broch Johnsen and Ina Schaefer and Ina Schaefer},doi={10.1145/2076674.2076682},pmid={null},pmcid={null},mag_id={1981619145},journal={null},abstract={Traits have been proposed as a more flexible mechanism for code structuring in object-oriented programming than class inheritance, for achieving fine-grained code reuse. A trait originally developed for one purpose can be modified and reused in a completely different context. Formalizations of traits have been extensively studied, and implementations of traits have started to appear in programming languages. However, work on formally establishing properties of trait-based programs has so far mostly concentrated on type systems. This paper proposes the first deductive proof system for a trait-based object-oriented language. If a specification for a trait can be given a priori, covering all actual usage of that trait, our proof system is modular as each trait is analyzed only once. In order to reflect the flexible reuse potential of traits, our proof system additionally allows new specifications to be added to a trait in an incremental way which does not violate established proofs. We formalize and show the soundness of the proof system.}}
@ARTICLE{Cytron_1991,title={Efficiently computing static single assignment form and the control dependence graph},year={1991},author={Ron K. Cytron and Ron Cytron and Jeanne Ferrante and Jeanne Ferrante and Barry K. Rosen and Barry K. Rosen and Mark N. Wegman and Mark N. Wegman and F. Kenneth Zadeck and F. Kenneth Zadeck},doi={10.1145/115372.115320},pmid={null},pmcid={null},mag_id={1982205631},journal={ACM Transactions on Programming Languages and Systems},abstract={In optimizing compilers, data structure choices directly influence the power and efficiency of practical program optimization. A poor choice of data structure can inhibit optimization or slow compilation to the point that advanced optimization features become undesirable. Recently, static single assignment form and the control dependence graph have been proposed to represent data flow and control flow properties of programs. Each of these previously unrelated techniques lends efficiency and power to a useful class of program optimizations. Although both of these structures are attractive, the difficulty of their construction and their potential size have discouraged their use. We present new algorithms that efficiently compute these data structures for arbitrary control flow graphs. The algorithms use {\em dominance frontiers}, a new concept that may have other applications. We also give analytical and experimental evidence that all of these data structures are usually linear in the size of the original program. This paper thus presents strong evidence that these structures can be of practical use in optimization.}}
@ARTICLE{Huang_2004,title={Securing web application code by static analysis and runtime protection},year={2004},author={Yao-Wen Huang and Yao-Wen Huang and Fang Yu and Fang Yu and Ching-Nam Hang and Christian Hang and Chung-Hung Tsai and Chung-Hung Tsai and Der-Tsai Lee and Der-Tsai Lee and Sy‐Yen Kuo and Sy-Yen Kuo},doi={10.1145/988672.988679},pmid={null},pmcid={null},mag_id={1983142587},journal={null},abstract={Security remains a major roadblock to universal acceptance of the Web for many kinds of transactions, especially since the recent sharp increase in remotely exploitable vulnerabilities have been attributed to Web application bugs. Many verification tools are discovering previously unknown vulnerabilities in legacy C programs, raising hopes that the same success can be achieved with Web applications. In this paper, we describe a sound and holistic approach to ensuring Web application security. Viewing Web application vulnerabilities as a secure information flow problem, we created a lattice-based static analysis algorithm derived from type systems and typestate, and addressed its soundness. During the analysis, sections of code considered vulnerable are instrumented with runtime guards, thus securing Web applications in the absence of user intervention. With sufficient annotations, runtime overhead can be reduced to zero. We also created a tool named.WebSSARI (Web application Security by Static Analysis and Runtime Inspection) to test our algorithm, and used it to verify 230 open-source Web application projects on SourceForge.net, which were selected to represent projects of different maturity, popularity, and scale. 69 contained vulnerabilities. After notifying the developers, 38 acknowledged our findings and stated their plans to provide patches. Our statistics also show that static analysis reduced potential runtime overhead by 98.4%.}}
@ARTICLE{Jackson_2001,title={Prioritising customers and other stakeholders using the AHP},year={2001},author={John Jackson and John Jackson},doi={10.1108/eum0000000005728},pmid={null},pmcid={null},mag_id={1983176496},journal={European Journal of Marketing},abstract={Marketers often presume that the statement “the customer always comes first” is an axiom. Other specialists usually do the same with their own stakeholder priorities. Other than for internal political and prestige purposes, this is helpful neither to marketing nor to stakeholder strategic management. This research did show that customers are regarded as key stakeholders generally, and also for the achieving of most corporate objectives. They were in effect regarded as a type of default priority stakeholder. However, two other stakeholder groups – managers and employees – were ranked as just as important or more important to the achieving of organisational success generally and several of the corporate objectives. Slogans about customers always being No. 1 need to be dismissed by professional marketers. Equally important, the research indicates that marketing might well need to give as much attention to the strategic management of managers and employees as it does to customers.}}
@ARTICLE{Clarke_2001,title={STG: a tool for generating symbolic test programs and oracles from operational specifications},year={2001},author={David M. Clarke and Duncan Clarke and Thierry Jéron and Thierry Jéron and Vlad Rusu and Vlad Rusu and Elena Zinovieva and Elena Zinovieva},doi={10.1145/503209.503252},pmid={null},pmcid={null},mag_id={1983843822},journal={null},abstract={We report on a tool we have developed that automates the derivation of tests from specifications. The tool implements conformance testing techniques to derive symbolic tests that incorporate their own oracles from formal operational specifications. It was applied for testing a simple version of the CEPS (Common Electronic Purse Specification).}}
@ARTICLE{Chouali_2010,title={Adapting Component Behaviours Using Interface Automata},year={2010},author={Samir Chouali and Samir Chouali and Sebti Mouelhi and Sebti Mouelhi and Hassan Mountassir and Hassan Mountassir},doi={10.1109/seaa.2010.34},pmid={null},pmcid={null},mag_id={1984973570},journal={null},abstract={One of the principal goal of Component-Based Software Engineering (CBSE) is to allow the reuse of components in diverse situations without affecting their codes. To reach this goal, it is necessary to propose approaches to adapt a component with its environment when behavioural mismatches occur during their interactions. In this paper, we present a formal approach based on interface automata to adapt components in order to eliminate possible behavioural mismatches, and then insure more flexible interoperability between components.}}
@ARTICLE{Knuth_1977,title={Fast Pattern Matching in Strings},year={1977},author={Donald E. Knuth and Donald E. Knuth and James H. Morris and James Morris and James H. Morris and James H. Morris and Vaughan R. Pratt and Vaughan R. Pratt},doi={10.1137/0206024},pmid={null},pmcid={null},mag_id={1985108724},journal={SIAM Journal on Computing},abstract={An algorithm is presented which finds all occurrences of one given string within another, in running time proportional to the sum of the lengths of the strings. The constant of proportionality is low enough to make this algorithm of practical use, and the procedure can also be extended to deal with some more general pattern-matching problems. A theoretical application of the algorithm shows that the set of concatenations of even palindromes, i.e., the language $\{\alpha \alpha ^R\}^*$, can be recognized in linear time. Other algorithms which run even faster on the average are also considered.}}
@ARTICLE{Herber_2010,title={Automated conformance evaluation of SystemC designs using timed automata},year={2010},author={Paula Herber and Paula Herber and Marcel Pockrandt and Marcel Pockrandt and Sabine Glesner and Sabine Glesner},doi={10.1109/etsym.2010.5512761},pmid={null},pmcid={null},mag_id={1985217339},journal={null},abstract={SystemC is widely used for modeling and simulation in hardware/software co-design. However, the co-verification techniques used for SystemC designs are mostly ad-hoc and non-systematic. A particularly severe drawback is that simulation results have to be evaluated manually. In previous work, we proposed to overcome this problem by conformance testing. We presented an algorithm that uses an abstract SystemC design to compute expected output traces, which are then compared with those of a refined design to evaluate its correctness. The main disadvantage of the algorithm is that it is very expensive because it computes the output traces offline and has to cope with non-deterministic systems. Furthermore, the designer has to compare the results manually with the outputs of a design under test. In this paper, we present an approach for efficient and fully-automatic conformance evaluation of SystemC designs. To achieve this, we first present optimizations of our previously proposed algorithm for the generation of conformance tests that drastically reduce computation time and memory consumption. The main idea is to exploit the specifics of the SystemC semantics to reduce the number of semantic states that have to be kept in memory during state-space exploration. Second, we present an approach to generate SystemC test benches from a set of expected output traces. These test benches allow fully-automatic test execution and conformance evaluation. Together with our previously presented model checking framework for abstract Sys-temC designs, we yield a fully-automatic HW/SW co-verification framework for SystemC that supports the whole design process. We demonstrate the performance and error detecting capability of our approach with experimental results.}}
@ARTICLE{Rapps_1985,title={Selecting Software Test Data Using Data Flow Information},year={1985},author={S. Rapps and S. Rapps and Elaine J. Weyuker and Elaine J. Weyuker},doi={10.1109/tse.1985.232226},pmid={null},pmcid={null},mag_id={1985551847},journal={IEEE Transactions on Software Engineering},abstract={This paper defines a family of program test data selection criteria derived from data flow analysis techniques similar to those used in compiler optimization. It is argued that currently used path selection criteria, which examine only the control flow of a program, are inadequate quate. Our procedure associates with each point in a program at which a variable is defined, those points at which the value is used. Several test data selection criteria, differing in the type and number of these associations, are defined and compared.}}
@ARTICLE{Sangiovanni-Vincentelli_2003,title={Electronic-system design in the automobile industry},year={2003},author={A. Sangiovanni-Vincentelli and Alberto Sangiovanni-Vincentelli},doi={10.1109/mm.2003.1209462},pmid={null},pmcid={null},mag_id={1986178703},journal={IEEE Micro},abstract={Electronic components are now essential to control a car's movements and chemical, mechanical, and electrical processes; to provide entertainment and communication; and to ensure safety. A new platform-based methodology can revolutionize the way a car is designed.}}
@ARTICLE{Hierons_2012,title={Implementation relations and test generation for systems with distributed interfaces.},year={2012},author={Robert M. Hierons and Robert M. Hierons and Mercedes G. Merayo and Mercedes G. Merayo and Manuel Núñez and Manuel Núñez},doi={10.1007/s00446-011-0149-1},pmid={null},pmcid={null},mag_id={1987604851},journal={Distributed Computing},abstract={Some systems interact with their environment at physically distributed interfaces called ports and we separately observe sequences of inputs and outputs at each port. As a result we cannot reconstruct the global sequence that occurred and this reduces our ability to distinguish different systems in testing or in use. In this paper we explore notions of conformance for an input output transition system that has multiple ports, adapting the widely used ioco implementation relation to this situation. We consider two different scenarios. In the first scenario the agents at the different ports are entirely independent. Alternatively, it may be feasible for some external agent to receive information from more than one of the agents at the ports of the system, these local behaviours potentially being brought together and here we require a stronger implementation relation. We define implementation relations for these scenarios and prove that in the case of a single-port system the new implementation relations are equivalent to ioco. In addition, we define what it means for a test case to be controllable and give an algorithm that decides whether this condition holds. We give a test generation algorithm to produce sound and complete test suites. Finally, we study two implementation relations to deal with partially specified systems.}}
@ARTICLE{Oheimb_2001,title={Hoare Logic for Java in Isabelle/HOL},year={2001},author={David von Oheimb and David von Oheimb},doi={10.1002/cpe.598},pmid={null},pmcid={null},mag_id={1987693120},journal={Concurrency and Computation: Practice and Experience},abstract={SUMMARY This article presents a Hoare-style calculus for a substantial subset of Java Card, which we call Java . In particular, the language includes side-effecting expressions, mutual recursion, dynamic method binding, full exception handling, and static class initialization. The Hoare logic of partial correctness is proved not only sound (w.r.t. our operational semantics of Java , described in detail elsewhere) but even complete. It is the first logic for an object-oriented language that is provably complete. The completeness proof uses a refinement of the Most General Formula approach. The proof of soundness gives new insights into the role of type safety. Further by-products of this work are a new general methodology for handling side-effecting expressions and their results, the discovery of the strongest possible rule of consequence, and a flexible Call rule for mutual recursion. We also give a small but non-trivial application example. All definitions and proofs have been done formally with the interactive theorem prover Isabelle/HOL. This guarantees not only rigorous definitions, but also gives maximal confidence in the results obtained.}}
@ARTICLE{Lu_1994,title={Integrating QFD, AHP and Benchmarking in Strategic Marketing},year={1994},author={Min Lu and Min Hua Lu and Christian N. Madu and Christian N. Madu and Chu‐Hua Kuei and Chu-Hua Kuei and Dena Winokur and Dena Winokur},doi={10.1108/08858629410053470},pmid={null},pmcid={null},mag_id={1988037460},journal={Journal of Business & Industrial Marketing},abstract={Develops a strategic planning framework for long‐range marketing policy making. Uses this framework to explore the relationship between marketing orientation and total quality management and extends Kotler′s model of three types of marketing strategic orientation on the perspective of total quality management. Uses two group consensus management techniques (quality function deployment and analytic hierarchy process) and competitive benchmarking to demonstrate how a particular company can make a decision on which strategic marketing orientation to adopt. Furthermore, shows how a company can evaluate itself relative to its competitors on the basis of strategic marketing orientation.}}
@ARTICLE{Gupta_2008,title={Proving non-termination},year={2008},author={Ashutosh Gupta and Ashutosh Gupta and Thomas A. Henzinger and Thomas A. Henzinger and Rupak Majumdar and Rupak Majumdar and Andrey Rybalchenko and Andrey Rybalchenko and Ru-Gang Xu and Ru-Gang Xu},doi={10.1145/1328438.1328459},pmid={null},pmcid={null},mag_id={1989813138},journal={null},abstract={The search for proof and the search for counterexamples (bugs) are complementary activities that need to be pursued concurrently in order to maximize the practical success rate of verification tools.While this is well-understood in safety verification, the current focus of liveness verification has been almost exclusively on the search for termination proofs. A counterexample to termination is an infinite programexecution. In this paper, we propose a method to search for such counterexamples. The search proceeds in two phases. We first dynamically enumerate lasso-shaped candidate paths for counterexamples, and then statically prove their feasibility. We illustrate the utility of our nontermination prover, called TNT, on several nontrivial examples, some of which require bit-level reasoning about integer representations.}}
@ARTICLE{Mari_2010,title={Formal verification of a modern SAT solver by shallow embedding into Isabelle/HOL},year={2010},author={Filip Mari and Filip Mari},doi={10.1016/j.tcs.2010.09.014},pmid={null},pmcid={null},mag_id={1991066194},journal={Theoretical Computer Science},abstract={We present a formalization and a formal total correctness proof of a MiniSAT-like SAT solver within the system Isabelle/HOL. The solver is based on the DPLL procedure and employs most state-of-the-art SAT solving techniques, including the conflict-guided backjumping, clause learning, and the two-watched unit propagation scheme. A shallow embedding into Isabelle/HOL is used and the solver is expressed as a set of recursive HOL functions. Based on this specification, the Isabelle's built-in code generator can be used to generate executable code in several supported functional languages (Haskell, SML, and OCaml). The SAT solver implemented in this way is, to our knowledge, the first fully formally and mechanically verified modern SAT solver.}}
@ARTICLE{Badri_2001,title={A comprehensive 0-1 goal programming model for project selection},year={2001},author={Masood Badri and Masood A. Badri and Daniel Cochece Davis and Donald Davis and Donna F. Davis and Donna F. Davis},doi={10.1016/s0263-7863(99)00078-2},pmid={null},pmcid={null},mag_id={1991188116},journal={International Journal of Project Management},abstract={null}}
@ARTICLE{Goswami_2013,title={Model-based development and verification of control software for electric vehicles},year={2013},author={Dip Goswami and Dip Goswami and Martin Lukasiewycz and Martin Lukasiewycz and Matthias Kauer and Matthias Kauer and Sebastian Steinhorst and Sebastian Steinhorst and Alejandro Masrur and Alejandro Masrur and Samarjit Chakraborty and Samarjit Chakraborty and S. Ramesh and S. Ramesh and S. Ramesh},doi={10.1145/2463209.2488853},pmid={null},pmcid={null},mag_id={1993867150},journal={null},abstract={Most innovations in the automotive domain are realized by electronics and software. Modern cars have up to 100 Electronic Control Units (ECUs) that implement a variety of control applications in a distributed fashion. The tasks are mapped onto different ECUs, communicating via a heterogeneous network, comprising communication buses like CAN, FlexRay, and Ethernet. For electric vehicles, software functions play an essential role, replacing hydraulic and mechanic control systems. While model-based software development and verification are already used extensively in the automotive domain, their importance significantly increases in electric vehicles as safety-critical functions might no longer rely on mechanical (fall-back) solutions. The need for reducing costs, size, and weight in electric vehicles has also resulted in a considerable interest in topics such as the consolidation of ECUs as well as efficient implementation of control software. In this paper we discuss two broad issues related to model-based software development and verification in electric vehicles. The first is concerned with how to ensure that model-level semantics are preserved in an implementation, which has important implications on the verification and certification of control software. The second issue is related to techniques for reducing the computational and communication demands of distributed automotive control algorithms. For both these topics we provide a broad introduction to the problem followed by a discussion on state-of-the-art techniques.}}
@ARTICLE{Ashayeri_1998,title={Global business process re-engineering: A system dynamics-based approach},year={1998},author={J. Ashayeri and Jalal Ashayeri and R. Keij and R. Keij and A. Bröker and A. Broker},doi={10.1108/01443579810225478},pmid={null},pmcid={null},mag_id={1994377245},journal={International Journal of Operations & Production Management},abstract={This paper presents a structured approach for business process re‐engineering (BPR) in a large (global) corporation. The paper defines BPR, explores the principles and assumptions behind re‐engineering, and looks for common factors behind its successes or failures. In order to overcome the potential failures, we propose to view a global business as a system and use the system dynamics methodology in conjunction with analytic network process. This approach will enable corporations to understand the relationships among the components of a global business and link different sub‐systems properly when a re‐engineering process is taking place. The purpose of this approach is to increase the effectiveness of the re‐engineering process.}}
@ARTICLE{Liskov_1994,title={A behavioral notion of subtyping},year={1994},author={Barbara Liskov and Barbara Liskov and Jeannette M. Wing and Jeannette M. Wing},doi={10.1145/197320.197383},pmid={null},pmcid={null},mag_id={1995008247},journal={ACM Transactions on Programming Languages and Systems},abstract={The use of hierarchy is an important component of object-oriented design. Hierarchy allows the use of type families, in which higher level supertypes capture the behavior that all of their subtypes have in common. For this methodology to be effective, it is necessary to have a clear understanding of how subtypes and supertypes are related. This paper takes the position that the relationship should ensure that any property proved about supertype objects also holds for its subtype objects. It presents two ways of defining the subtype relation, each of which meets this criterion, and each of which is easy for programmers to use. The subtype relation is based on the specifications of the sub- and supertypes; the paper presents a way of specifying types that makes it convenient to define  the subtype relation. The paper also discusses the ramifications of this notion of subtyping on the design of type families.}}
@ARTICLE{Brandl_2010,title={Automated Conformance Verification of Hybrid Systems},year={2010},author={Harald Brandl and Harald Brandl and Martin Weiglhofer and Martin Weiglhofer and Bernhard K. Aichernig and Bernhard K. Aichernig},doi={10.1109/qsic.2010.53},pmid={null},pmcid={null},mag_id={1996435633},journal={null},abstract={Due to the combination of discrete events and continuous behavior the validation of hybrid systems is a challenging task. Nevertheless, as for other systems the correctness of such hybrid systems is a major concern. In this paper we present a new approach for verifying the input-output conformance of two hybrid systems. This approach can be used to generate mutation-based test cases. We specify a hybrid system within the framework of Qualitative Action Systems. Here, besides conventional discrete actions, the continuous dynamics of hybrid systems is described with so called qualitative actions. This paper then shows how labeled transition systems can be used to describe the trace semantics of Qualitative Action Systems. The labeled transition systems are used to verify the conformance between two Qualitative Action Systems. Finally, we present first experimental results on a water tank system.}}
@ARTICLE{Wei_2007,title={Passive online rogue access point detection using sequential hypothesis testing with TCP ACK-pairs},year={2007},author={Wei Wei and Wei Wei and Wei Wei and Kyoungwon Suh and Kyoungwon Suh and Bing Wang and Bing Wang and Yu Gu and Yu Gu and Jim Kurose and Jim Kurose and Don Towsley and Don Towsley},doi={10.1145/1298306.1298357},pmid={null},pmcid={null},mag_id={1996789571},journal={null},abstract={Rogue (unauthorized) wireless access points pose serious security threats to local networks. In this paper, we propose two online algorithms to detect rogue access points using sequential hypothesis tests applied to packet-header data collected passively at a monitoring point. One algorithm requires training sets, while the other does not. Both algorithms extend our earlier TCP ACK-pair technique to differentiate wired and wireless LAN TCP traffic, and exploit the fundamental properties of the 802.11 CSMA/CA MAC protocol and the half duplex nature of wireless channels. Our algorithms make prompt decisions as TCP ACK-pairs are observed, and only incur minimum computation and storage overhead. We have built a system for online rogue-access-point detection using these algorithms and deployed it at a university gateway router. Extensive experiments in various scenarios have demonstrated the excellent performance of our approach: the algorithm that requires training provides rapid detection and is extremely accurate (the detection is mostly within 10 seconds, with very low false positive and false negative ratios); the algorithm that does not require training detects 60%-76% of the wireless hosts without any false positives; both algorithms are light-weight (with computation and storage overhead well within the capability of commodity equipment).}}
@ARTICLE{Edwards_1994,title={SMARTS and SMARTER: Improved Simple Methods for Multiattribute Utility Measurement},year={1994},author={Ward Edwards and Ward Edwards and F. Hutton Barron and F. Hutton Barron},doi={10.1006/obhd.1994.1087},pmid={null},pmcid={null},mag_id={1998111652},journal={Organizational Behavior and Human Decision Processes},abstract={Abstract   This paper presents two approximate methods for multiattribute utility measurement, SMARTS and SMARTER, each based on an elicitation procedure for weights. Both correct an error in SMART, originally proposed by Edwards in 1977, and in addition SMARTER is simpler to use. SMARTS uses linear approximations to single-dimension utility functions, an additive aggregation model, and swing weights. The paper proposes tests for the usability of these approximations. SMARTER, based on a formally justifiable weighting procedure developed by Barron and Barrett, uses the same procedures as SMARTS except that it omits the second of two elicitation steps in swing weights, substituting calculations based on ranks. It can be shown to perform about 98% as well as SMARTS does, without requiring any difficult judgments from elicitees.}}
@ARTICLE{Lee_1994,title={Probabilistic diagnosis of multiprocessor systems},year={1994},author={Sunggu Lee and Sunggu Lee and Kang G. Shin and Kang G. Shin},doi={10.1145/174666.174669},pmid={null},pmcid={null},mag_id={1998719936},journal={ACM Computing Surveys},abstract={This paper critically surveys methods for the automated probabilistic diagnosis of large multiprocessor systems. In recent years, much of the work on system-level diagnosis has focused on probabilistic methods, which can diagnose intermittently faulty processing nodes and can be applied in  general  situations on  general  interconnection networks. The theory behind the probabilistic diagnosis methods is explained, and the various diagnosis algorithms are described in simple terms with the aid of a running example. The diagnosis methods are compared and analyzed, and a chart is produced, showing the comparative advantages of the various diagnosis algorithms on the basis of several factors important to the probabilistic diagnosis.}}
@ARTICLE{Tan_2004,title={Specification-based testing with linear temporal logic},year={2004},author={Li Tan and L. Tan and Oleg Sokolsky and Oleg Sokolsky and I. Lee and Insup Lee},doi={10.1109/iri.2004.1431509},pmid={null},pmcid={null},mag_id={1998872168},journal={null},abstract={This paper considers the specification-based testing in which the requirement is given in the linear temporal logic (LTL). The required LTL property must hold on all the executions of the system, which are often infinite in size and/or in length. The central piece of our framework is a property-coverage metric. Based on requirement mutation, the metric measures how well a property has been tested by a test suite. We define a coverage criterion based on the metric that selects a finite set of tests from all the possible executions of the system. We also discuss the technique of generating a test suite for specification testing by using the counterexample mechanism of a model checker. By exploiting the special structure of a generated test, we are able to reduce a test with infinite length to an equivalent one of finite length. Our framework provides a model-checking-assisted approach that generates a test suite that is finite in size and in length for testing linear temporal properties on an implementation.}}
@ARTICLE{Apel_2010,title={Type safety for feature-oriented product lines},year={2010},author={Sven Apel and Sven Apel and Christian Kästner and Christian Kästner and Armin Gröβlinger and Armin Gröβlinger and Christian Lengauer and Christian Lengauer},doi={10.1007/s10515-010-0066-8},pmid={null},pmcid={null},mag_id={1999214372},journal={null},abstract={A feature-oriented product line is a family of programs that share a common set of features. A feature implements a stakeholder's requirement and represents a design decision or configuration option. When added to a program, a feature involves the introduction of new structures, such as classes and methods, and the refinement of existing ones, such as extending methods. A feature-oriented decomposition enables a generator to create an executable program by composing feature code solely on the basis of the feature selection of a user--no other information needed. A key challenge of product line engineering is to guarantee that only well-typed programs are generated. As the number of valid feature combinations grows combinatorially with the number of features, it is not feasible to type check all programs individually. The only feasible approach is to have a type system check the entire code base of the feature-oriented product line. We have developed such a type system on the basis of a formal model of a feature-oriented Java-like language. The type system guaranties type safety for feature-oriented product lines. That is, it ensures that every valid program of a well-typed product line is well-typed. Our formal model including type system is sound and complete.}}
@ARTICLE{Behjati_2008,title={An effective approach for model checking SystemC designs},year={2008},author={Razieh Behjati and Razieh Behjati and Razieh Behjati and Hamideh Sabouri and Hamideh Sabouri and Niloofar Razavi and Niloofar Razavi and Marjan Sirjani and Marjan Sirjani},doi={10.1109/acsd.2008.4574596},pmid={null},pmcid={null},mag_id={1999623278},journal={null},abstract={SystemC is a system level modeling language with the goal of enabling verification at higher levels of abstraction. In this paper, we propose a mapping from SystemC designs to Rebeca models supported by an automatic tool, Sytra. Rebeca verification tool set is then available for verifying LTL and CTL properties. The mapping is aimed to preserve the concurrent and event-driven nature of SystemC. This work is part of a project (Sysfier) to formally verify SystemC designs. The applicability of our approach is shown by a set of small and medium sized case studies, and the scalability of the approach is shown by the verification of a single-cycle MIPS design.}}
@ARTICLE{Ciobanu_2006,title={Timers for Distributed Systems},year={2006},author={Gabriel Ciobanu and Gabriel Ciobanu and Cristian Prisacariu and Cristian Prisacariu},doi={10.1016/j.entcs.2006.07.013},pmid={null},pmcid={null},mag_id={2000280189},journal={Electronic Notes in Theoretical Computer Science},abstract={null}}
@ARTICLE{Cassez_2006,title={Structural Translation from Time Petri Nets to Timed Automata},year={2006},author={Franck Cassez and Franck Cassez and Olivier H. Roux and Olivier Roux},doi={10.1016/j.jss.2005.12.021},pmid={null},pmcid={null},mag_id={2000518871},journal={Journal of Systems and Software},abstract={Abstract   In this paper, we consider Time Petri Nets (TPN) where time is associated with transitions. We give a formal semantics for TPNs in terms of Timed Transition Systems. Then, we propose a translation from TPNs to Timed Automata (TA) that preserves the behavioral semantics (timed bisimilarity) of the TPNs. For the theory of TPNs this result is twofold: (i) reachability problems and more generally TCTL model-checking are decidable for bounded TPNs; (ii) allowing strict time constraints on transitions for TPNs preserves the results described in (i). The practical applications of the translation are: (i) one can specify a system using both TPNs and Timed Automata and a precise semantics is given to the composition; (ii) one can use existing tools for analyzing timed automata (like K ronos , U ppaal  or C mc ) to analyze TPNs. In this paper we describe the new feature of the tool R omeo  that implements our translation of TPNs in the U ppaal  input format. We also report on experiments carried out on various examples and compare the result of our method to state-of-the-art tool for analyzing TPNs.}}
@ARTICLE{Jaghoori_2006,title={Modere: the model-checking engine of Rebeca},year={2006},author={Mohammad Mahdi Jaghoori and Mohammad Mahdi Jaghoori and Ali Movaghar and Ali Movaghar and Marjan Sirjani and Marjan Sirjani},doi={10.1145/1141277.1141704},pmid={null},pmcid={null},mag_id={2001987516},journal={null},abstract={Rebeca is an actor-based language with formal semantics that can be used in modeling concurrent and distributed software and protocols. Automatic verification of these systems in the design stage helps develop error free systems. In this paper, we describe the model checking tool developed for verification of Rebeca models. This tool uses partial order reduction technique for reducing the size of the state space generated for a given model. Using this tool for model checking Rebeca yields much better results than the previous attempts for model checking Rebeca.}}
@ARTICLE{Hopcroft_1979,title={Introduction to Automata Theory, Languages, and Computation},year={1979},author={John E. Hopcroft and Rajeev Motwani and Rotwani and Jeffrey D. Ullman},doi={null},pmid={null},pmcid={null},mag_id={2002089154},journal={null},abstract={null}}
@ARTICLE{Le_2013,title={Verifying SystemC using an intermediate verification language and symbolic simulation},year={2013},author={Hoang M. Le and Hoang M. Le and Daniel Große and Daniel Große and Daniel Grosse and Vladimir Herdt and Vladimir Herdt and Rolf Drechsler and Rolf Drechsler},doi={10.1145/2463209.2488877},pmid={null},pmcid={null},mag_id={2002446960},journal={null},abstract={Formal verification of SystemC is challenging. Before dealing with symbolic inputs and the concurrency semantics, a front-end is required to translate the design to a formal model. The lack of such front-ends has hampered the development of efficient back-ends so far.   In this paper, we propose an isolated approach by using an Intermediate Verification Language (IVL). This enables a SystemC-to-IVL translator (frond-end) and an IVL verifier (back-end) to be developed independently. We present a compact but general IVL that together with an extensive benchmark set will facilitate future research.   Furthermore, we propose an efficient symbolic simulator integrating Partial Order Reduction. Experimental comparison with existing approaches has shown its potential.}}
@ARTICLE{Shafik_2012,title={RAEF: A Power Normalized System-Level Reliability Analysis and Estimation Framework},year={2012},author={Rishad Shafik and Rishad A. Shafik and Bashir M. Al-Hashimi and Bashir M. Al-Hashimi and Jimson Mathew and Jimson Mathew and Dhiraj K. Pradhan and Dhiraj K. Pradhan and Saraju P. Mohanty and Saraju P. Mohanty},doi={10.1109/isvlsi.2012.42},pmid={null},pmcid={null},mag_id={2003533904},journal={null},abstract={System-level reliability estimation is a crucial aspect in reliable design of embedded systems. Recently reported estimation techniques use separate measurements of power consumption and reliability to demonstrate the trade-offs between them. However, we will argue in this paper that such measurements cannot determine comparative reliability of system components with different power consumptions and hence a composite measurement of reliability and power consumption is required. Underpinning this argument, we propose a SystemC based system-level reliability analysis and estimation framework, RAEF, using a novel composite metric, power normalized reliability (PNR), defined as the ratio of reliability and power consumption. We show that PNR based estimation enables insightful reliability analysis of different system components. We evaluate the effectiveness of such estimation in RAEF using a case study of MPEG-2 decoder with four processing cores considering single-event upset (SEU) based soft error model. Using this setup, we analyze and compare PNR based estimation with existing reliability evaluations at different system hierarchies. Furthermore, we demonstrate the advantages of RAEF in assessing design choices highlighting the impact of voltage scaling and architecture allocation.}}
@ARTICLE{Zheng_2008,title={Automated generation of test suites from formal specifications of real-time reactive systems},year={2008},author={Mao Zheng and M. Zheng and Vangalur S. Alagar and Vasu Alagar and Olga Ormandjieva and Olga Ormandjieva},doi={10.1016/j.jss.2007.05.009},pmid={null},pmcid={null},mag_id={2004090505},journal={Journal of Systems and Software},abstract={null}}
@ARTICLE{Millo_1979,title={Social processes and proofs of theorems and programs},year={1979},author={Richard A. De Millo and Richard A. De Millo and Richard J. Lipton and Richard J. Lipton and Richard J. Lipton and Alan J. Perlis and Alan J. Perlis and Alan J. Perlis},doi={10.1145/359104.359106},pmid={null},pmcid={null},mag_id={2005973420},journal={Communications of The ACM},abstract={It is argued that formal verifications of programs, no matter how obtained, will not play the same key role in the development of computer science and software engineering as proofs do in mathematics. Furthermore the absence of continuity, the inevitability of change, and the complexity of specification of significantly many real programs make the formal verification process difficult to justify and manage. It is felt that ease of formal verification should not dominate program language design.}}
@ARTICLE{Dovland_2010,title={Lazy behavioral subtyping},year={2010},author={Johan Dovland and Johan Dovland and Einar Broch Johnsen and Einar Broch Johnsen and Olaf Owe and Olaf Owe and Martín Steffen and Martin Steffen},doi={10.1016/j.jlap.2010.07.008},pmid={null},pmcid={null},mag_id={2006935274},journal={The Journal of Logic and Algebraic Programming},abstract={Abstract   Inheritance combined with late binding allows flexible code reuse but complicates formal reasoning significantly, as a method call’s receiver class is not statically known. This is especially true when programs are incrementally developed by extending class hierarchies. This paper develops a novel method to reason about late bound method calls. In contrast to traditional behavioral subtyping, reverification of method specifications is avoided without restricting method overriding to fully behavior-preserving redefinition. The approach ensures that when analyzing the methods of a class, it suffices to consider that class and its superclasses. Thus, the full class hierarchy is not needed, and  incremental  reasoning is supported. We formalize this approach as a calculus which lazily imposes context-dependent subtyping constraints on method definitions. The calculus ensures that all method specifications required by late bound calls remain satisfied when new classes extend a class hierarchy. The calculus does not depend on a specific program logic, but the examples in the paper use a Hoare style proof system. We show soundness of the analysis method. The paper finally demonstrates how lazy behavioral subtyping can be combined with interface specifications to produce an incremental and modular reasoning system for object-oriented class hierarchies.}}
@ARTICLE{Fee_1994,title={Estimating the time-zero spectrum in time-resolved emmsion measurements of solvation dynamics},year={1994},author={Richard S. Fee and R.S. Fee and R. S. Fee and Mark Maroncelli and Mark Maroncelli},doi={10.1016/0301-0104(94)00019-0},pmid={null},pmcid={null},mag_id={2006978357},journal={Chemical Physics},abstract={null}}
@ARTICLE{Ouédraogo_2010,title={SetExp: a method of transformation of timed automata into finite state automata},year={2010},author={Lucien Ouédraogo and Lucien Ouedraogo and Ahmed Khoumsi and Ahmed Khoumsi and Mustapha Nourelfath and Mustapha Nourelfath},doi={10.1007/s11241-010-9103-8},pmid={null},pmcid={null},mag_id={2006984763},journal={Real-time Systems},abstract={Real-time discrete event systems are discrete event systems with timing constraints, and can be modeled by timed automata. The latter are convenient for modeling real-time discrete event systems. However, due to their infinite state space, timed automata are not suitable for studying real-time discrete event systems. On the other hand, finite state automata, as the name suggests, are convenient for modeling and studying non-real time discrete event systems. To take into account the advantages of finite state automata, an approach for studying real-time discrete event systems is to transform, by abstraction, the timed automata modeling them into finite state automata which describe the same behaviors. Then, studies are performed on the finite state automata model by adapting methods designed for non real-time discrete event systems. In this paper, we present a method for transforming timed automata into special finite state automata called Set-Exp automata. The method, called SetExp, models the passing of time as real events in two types: Set events which correspond to resets with programming of clocks, and Exp events which correspond to the expiration of clocks. These events allow to express the timing constraints as events order constraints. SetExp limits the state space explosion problem in comparison to other transformation methods of timed automata, notably when the magnitude of the constants used to express the timing constraints are high. Moreover, SetExp is suitable, for example, in supervisory control and conformance testing of real-time discrete event systems.}}
@ARTICLE{Wassermann_2007,title={Sound and precise analysis of web applications for injection vulnerabilities},year={2007},author={Gary Wassermann and Gary Wassermann and Zhendong Su and Zhendong Su},doi={10.1145/1250734.1250739},pmid={null},pmcid={null},mag_id={2008158744},journal={null},abstract={Web applications are popular targets of security attacks. One common type of such attacks is SQL injection, where an attacker exploits faulty application code to execute maliciously crafted database queries. Bothstatic and dynamic approaches have been proposed to detect or prevent SQL injections; while dynamic approaches provide protection for deployed software, static approaches can detect potential vulnerabilities before software deployment. Previous static approaches are mostly based on tainted information flow tracking and have at least some of the following limitations: (1) they do not model the precise semantics of input sanitization routines; (2) they require manually written specifications, either for each query or for bug patterns; or (3) they are not fully automated and may require user intervention at various points in the analysis. In this paper, we address these limitations by proposing a precise, sound, and fully automated analysis technique for SQL injection. Our technique avoids the need for specifications by consideringas attacks those queries for which user input changes the intended syntactic structure of the generated query. It checks conformance to this policy byconservatively characterizing the values a string variable may assume with a context free grammar, tracking the nonterminals that represent user-modifiable data, and modeling string operations precisely as language transducers. We have implemented the proposed technique for PHP, the most widely-used web scripting language. Our tool successfully discovered previously unknown and sometimes subtle vulnerabilities in real-world programs, has a low false positive rate, and scales to large programs (with approx. 100K loc).}}
@ARTICLE{Bird_1983,title={Automatic generation of random self-checking test cases},year={1983},author={Drew Bird and D. L. Bird and Cuauhtemoc Munoz and C. U. Munoz},doi={10.1147/sj.223.0229},pmid={null},pmcid={null},mag_id={2009007001},journal={Ibm Systems Journal},abstract={A technique of automatically generating random software test cases is described. The nature of such test cases ensures that they will execute to completion, and their execution is predicted at the time of generation. Wherever possible the test cases are self-checking. At run-time their execution is compared witht he predicted execution. Also described are implementations of the technique that have been used to test various IBM programs-- PL/I language processors, sort/merge programs, and Graphical Data Display Manager alphanumeric and graphics support.}}
@ARTICLE{Ölveczky_2002,title={Specification of real-time and hybrid systems in rewriting logic},year={2002},author={Peter Csaba Ölveczky and Peter Csaba Ölveczky and José Meseguer and José Meseguer},doi={10.1016/s0304-3975(01)00363-2},pmid={null},pmcid={null},mag_id={2011886298},journal={Theoretical Computer Science},abstract={This paper explores the application of rewriting logic to the executable formal modeling of real-time and hybrid systems. We give general techniques by which such systems can be specified as ordinary rewrite theories, and show that a wide range of real-time and hybrid system models, including object-oriented systems, timed automata, hybrid automata, timed and phase transition systems, and timed extensions of Petri nets, can indeed be expressed in rewriting logic quite naturally and directly. Since rewriting logic is executable and is supported by several language implementations, our approach complements property-oriented methods and tools less well suited for execution purposes, and can be used as the basis for symbolic simulation and formal analysis of real-time and hybrid systems. The relationships with the timed rewriting logic approach of Kosiuczenko and Wirsing are also studied.}}
@ARTICLE{Simão_2011,title={Generating asynchronous test cases from test purposes},year={2011},author={Adenilso da Silva Simão and Adenilso Simao and Alexandre Petrenko and Alexandre Petrenko},doi={10.1016/j.infsof.2011.06.006},pmid={null},pmcid={null},mag_id={2013254626},journal={Information & Software Technology},abstract={null}}
@ARTICLE{Zeng_2007,title={Optimization of wastewater treatment alternative selection by hierarchy grey relational analysis.},year={2007},author={Guangming Zeng and Guangming Zeng and Ru Jiang and Ru Jiang and Guohe Huang and Guohe Huang and Min Xu and Min Xu and Jianbing Li and Jianbing Li},doi={10.1016/j.jenvman.2005.12.024},pmid={16635543},pmcid={null},mag_id={2013462463},journal={Journal of Environmental Management},abstract={null}}
@ARTICLE{Andrews_2005,title={Is mutation an appropriate tool for testing experiments},year={2005},author={James H. Andrews and James H. Andrews and Lionel C. Briand and Lionel C. Briand and Yvan Labiche and Yvan Labiche},doi={10.1145/1062455.1062530},pmid={null},pmcid={null},mag_id={2013711971},journal={null},abstract={The empirical assessment of test techniques plays an important role in software testing research. One common practice is to instrument faults, either manually or by using mutation operators. The latter allows the systematic, repeatable seeding of large numbers of faults; however, we do not know whether empirical results obtained this way lead to valid, representative conclusions. This paper investigates this important question based on a number of programs with comprehensive pools of test cases and known faults. It is concluded that, based on the data available thus far, the use of mutation operators is yielding trustworthy results (generated mutants are similar to real faults). Mutants appear however to be different from hand-seeded faults that seem to be harder to detect than real faults.}}
@ARTICLE{Payet_2006,title={Nontermination inference of logic programs},year={2006},author={Étienne Payet and Étienne Payet and Frédéric Mesnard and Fred Mesnard},doi={10.1145/1119479.1119481},pmid={null},pmcid={null},mag_id={2014410720},journal={ACM Transactions on Programming Languages and Systems},abstract={We present a static analysis technique for nontermination inference of logic programs. Our framework relies on an extension of the subsumption test, where some specific argument positions can be instantiated while others are generalized. We give syntactic criteria to statically identify such argument positions from the text of a program. Atomic left looping queries are generated bottom-up from selected subsets of the binary unfoldings of the program of interest. We propose a set of correct algorithms for automating the approach. Then, nontermination inference is tailored to attempt proofs of optimality of left termination conditions computed by a termination inference tool. An experimental evaluation is reported and the analyzers can be tried online at http://www.univ-reunion.fr/~gcc. When termination and nontermination analysis produce complementary results for a logic procedure, then with respect to the leftmost selection rule and the language used to describe sets of atomic queries, each analysis is optimal and together, they induce a characterization of the operational behavior of the logic procedure.}}
@ARTICLE{Brat_2013,title={Experimental Evaluation of Verification and Validation Tools on Martian Rover Software},year={2013},author={Guillaume Brat and Guillaume Brat and Doron Drusinsky and Doron Drusinsky and Dimitra Giannakopoulou and Dimitra Giannakopoulou and Allen Goldberg and Allen Goldberg and Klaus Havelund and Klaus Havelund and Mike Lowry and Michael Lowry and Corina S. Pǎsǎreanu and Corina S. Pasareanu and Corina S. Pasareanu and Arnaud Venet and Arnaud Venet and Willem Visser and Willem Visser and Richard Washington and Rich Washington},doi={null},pmid={null},pmcid={null},mag_id={2016208194},journal={null},abstract={We report on a study to determine the maturity of different verification and validation technologies (V&V) applied to a representative example of NASA flight software. The study consisted of a controlled experiment where three technologies (static analysis, runtime analysis and model checking) were compared to traditional testing with respect to their ability to find seeded errors in a prototype Mars Rover controller. What makes this study unique is that it is the first (to the best of our knowledge) controlled experiment to compare formal methods based tools to testing on a realistic industrial-size example, where the emphasis was on collecting as much data on the performance of the tools and the participants as possible. The paper includes a description of the Rover code that was analyzed, the tools used, as well as a detailed description of the experimental setup and the results. Due to the complexity of setting up the experiment, our results cannot be generalized, but we believe it can still serve as a valuable point of reference for future studies of this kind. It confirmed our belief that advanced tools can outperform testing when trying to locate concurrency errors. Furthermore, the results of the experiment inspired a novel framework for testing the next generation of the Rover.}}
@ARTICLE{Glover_1999,title={A modular tool for test generation for real-time systems},year={1999},author={T. D. Glover and T. Glover and Rachel Cardell-Oliver and R. Cardell-Oliver},doi={10.1049/ic:19990009},pmid={null},pmcid={null},mag_id={2017931509},journal={null},abstract={A tool is presented which aims to meet the need for the automatic generation of test cases for real time control systems. The modular nature of the tool allows different specification languages to be used, and permits the user to experiment with a variety of simplifying transformations in order to produce a tractable model. Finally, a suitable test strategy can be selected. This flexibility allows the user to make the most appropriate selection of test cases, and at each stage the underlying assumptions are made explicit. Confidence in the implementation is expressed not as a percentage cover but as an explicit set of assumptions that are satisfied. It remains to investigate larger examples in order to establish the most effective techniques for reducing the number of test cases to a manageable size. (4 pages)}}
@ARTICLE{AbouTrab_2013,title={Testing Real-Time Embedded Systems using Timed Automata based approaches},year={2013},author={Mohammad Saeed AbouTrab and M. Saeed AbouTrab and Michael Brockway and Michael Brockway and Steve Counsell and Steve Counsell and Robert M. Hierons and Robert M. Hierons},doi={10.1016/j.jss.2012.12.030},pmid={null},pmcid={null},mag_id={2018525611},journal={Journal of Systems and Software},abstract={null}}
@ARTICLE{Chen_2009,title={UML Activity Diagram-Based Automatic Test Case Generation For Java Programs},year={2009},author={Mingsong Chen and Mingsong Chen and Xiaokang Qiu and Xiaokang Qiu and Wei Xu and Wei Xu and Linzhang Wang and Linzhang Wang and Jiwen Zhao and Jianhua Zhao and Xuandong Li and Xuandong Li},doi={10.1093/comjnl/bxm057},pmid={null},pmcid={null},mag_id={2018704560},journal={The Computer Journal},abstract={Test case generation based on design specifications is an important part of testing processes. In this paper, Unified Modeling Language activity diagrams are used as design specifications. By setting up several test adequacy criteria with respect to activity diagrams, an automatic approach is presented to generate test cases for Java programs. Instead of directly deriving test cases from activity diagrams, this approach selects test cases from a set of randomly generated ones according to a given test adequacy criterion. In the approach, we first instrument a Java program under testing according to its activity diagram model, and randomly generate abundant test cases for the program. Then, by running the instrumented program we obtain the corresponding program execution traces. Finally, by matching these traces with the behavior of the activity diagram, a reduced set of test cases are selected according to the given test adequacy criterion. This approach can also be used to check the consistency between the program execution traces and the behavior of activity diagrams.}}
@ARTICLE{Saaty_1987,title={A new macroeconomic forecasting and policy evaluation method using the analytic hierarchy process},year={1987},author={Thomas L. Saaty and Thomas L. Saaty},doi={10.1016/0270-0255(87)90479-9},pmid={null},pmcid={null},mag_id={2019768330},journal={Mathematical Modelling},abstract={null}}
@ARTICLE{Hessel_2007,title={A Global Algorithm for Model-Based Test Suite Generation},year={2007},author={Anders Hessel and Anders Hessel and Paul Pettersson and Paul Pettersson},doi={10.1016/j.entcs.2007.08.005},pmid={null},pmcid={null},mag_id={2019836161},journal={Electronic Notes in Theoretical Computer Science},abstract={Abstract   Model-based testing has been proposed as a technique to automatically verify that a system conforms to its specification. A popular approach is to use a model-checker to produce a set of test cases by formulating the test generation problem as a reachability problem. To guide the selection of test cases, a coverage criterion is often used. A coverage criterion can be seen as a set of items to be covered, called coverage items. We propose an on-the-fly algorithm that generates a test suite that covers all feasible coverage items. The algorithm returns a set of traces that includes a path fulfilling each item, without including redundant paths. The reachability algorithm explores a state only if it might increase the total coverage. The decision is global in the sense that it does not only regard each individual local search branch in isolation, but the  total coverage in all branches  together. For simpler coverage criteria as location of edge coverage, this implies that each model state is never explored twice.  The algorithm presented in this paper has been implemented in the test generation tool  Uppaal co  ✓  er . We present encouraging results from applying the tool to a set of experiments and in an industrial sized case study.}}
@ARTICLE{Saaty_1977,title={A Scaling Method for Priorities in Hierarchical Structures},year={1977},author={Thomas L. Saaty and Thomas L. Saaty},doi={10.1016/0022-2496(77)90033-5},pmid={null},pmcid={null},mag_id={2020407132},journal={Journal of Mathematical Psychology},abstract={null}}
@ARTICLE{Chen_2007,title={Towards RTL test generation from SystemC TLM specifications},year={2007},author={Mingsong Chen and Mingsong Chen and Prabhat Mishra and Prabhat Mishra and D. Kalita and Dhrubajyoti Kalita},doi={10.1109/hldvt.2007.4392793},pmid={null},pmcid={null},mag_id={2022788782},journal={null},abstract={SystemC transaction level modeling (TLM) is widely used to reduce the overall design and validation effort of complex system-on-chip (SOC) architectures. Due to lack of efficient techniques, the amount of reuse between abstraction levels is limited in many scenarios such as reuse of TLM level tests for RTL validation. This paper presents a top-down methodology for generation of RTL tests from SystemC TLM specifications. This paper makes two important contributions: automatic test generation from TLM specification using a transition-based coverage metric and automatic translation of TLM tests into RTL tests using a set of transformation rules. Our initial results using a router design demonstrate the usefulness of our approach by capturing various functional errors as well as inconsistencies in the implementation.}}
@ARTICLE{Rodrı́guez_2007,title={HOTL: Hypotheses and observations testing logic},year={2007},author={Ismael Rodrı́guez and Ismael Rodríguez and Mercedes G. Merayo and Mercedes G. Merayo and Manuel Núñez and Manuel Núñez},doi={10.1016/j.jlap.2007.03.002},pmid={null},pmcid={null},mag_id={2024026870},journal={The Journal of Logic and Algebraic Programming},abstract={null}}
@ARTICLE{Jayaraman_2007,title={Model composition in product lines and feature interaction detection using critical pair analysis},year={2007},author={Praveen Jayaraman and Praveen Jayaraman and Jon Whittle and Jon Whittle and Ahmed Elkhodary and Ahmed Elkhodary and Hassan Gomaa and Hassan Gomaa},doi={10.1007/978-3-540-75209-7_11},pmid={null},pmcid={null},mag_id={2025383609},journal={null},abstract={Software product lines (SPL) are an established technology for developing families of systems. In particular, they focus on modeling commonality and variability, that is, they are based on identifying features common to all members of the family and variable features that appear only in some members. Model-based development methods for product lines advocate the construction of SPL requirements, analysis and design models for features. This paper describes an approach for maintaining feature separation during modeling using a UML composition language based on graph transformations. This allows models of features to be reused more easily. The language can be used to compose the SPL models for a given set of features. Furthermore, critical pair analysis is used to detect dependencies and conflicts between features during analysis and design modeling. The approach is supported by a tool that allows automated composition of UML models of features and detection of some kinds of feature interactions.}}
@ARTICLE{Hierons_2008,title={The Effect of the Distributed Test Architecture on the Power of Testing},year={2008},author={Robert M. Hierons and Robert M. Hierons and Hasan Ural and Hasan Ural},doi={10.1093/comjnl/bxm096},pmid={null},pmcid={null},mag_id={2026184507},journal={The Computer Journal},abstract={There has been much interest in testing from finite-state machines (FSMs). If the system under test can be modelled by the (minimal) FSM N then testing from an (minimal) FSM M is testing to check that N is isomorphic to M. In the distributed test architecture, there are multiple interfaces/ports and there is a tester at each port. This can introduce controllability/synchronization and observability problems. This paper shows that the restriction to test sequences that do not cause controllability problems and the inability to observe the global behaviour in the distributed test architecture, and thus relying only on the local behaviour at remote testers, introduces fundamental limitations into testing. There exist minimal FSMs that are not equivalent, and so are not isomorphic, and yet cannot be distinguished by testing in this architecture without introducing controllability problems. Similarly, an FSM may have non-equivalent states that cannot be distinguished in the distributed test architecture without causing controllability problems: these are said to be locally s-equivalent and otherwise they are locally s-distinguishable. This paper introduces the notion of two states or FSMs being locally s-equivalent and formalizes the power of testing in the distributed test architecture in terms of local s-equivalence. It introduces a polynomial time algorithm that, given an FSM M, determines which states of M are locally s-equivalent and produces minimal length input sequences that locally s-distinguish states that are not locally s-equivalent. An FSM is locally s-minimal if it has no pair of locally s-equivalent states. This paper gives an algorithm that takes an FSM M and returns a locally s-minimal FSM M′ that is locally s-equivalent to M.}}
@ARTICLE{Alberts_1976,title={The economics of software quality assurance},year={1976},author={David S. Alberts and David S. Alberts and David S. Alberts},doi={10.1145/1499799.1499863},pmid={null},pmcid={null},mag_id={2026431993},journal={null},abstract={This paper presents an examination into the economics of software quality assurance. An analysis of the software life-cycle is performed to determine where in the cycle the application of quality assurance techniques would be most beneficial. The number and types of errors occurring at various phases of the software life-cycle are estimated. A variety of approaches in increasing software quality (including Structured Programming, Top Down Design, Programmer Management Techniques and Automated Tools) are reviewed and their potential impact on quality and costs are examined.}}
@ARTICLE{Koymans_1990,title={Specifying real-time properties with metric temporal logic},year={1990},author={Ron Koymans and Ron Koymans},doi={10.1007/bf01995674},pmid={null},pmcid={null},mag_id={2026629052},journal={Real-time Systems},abstract={This paper is motivated by the need for a formal specification method for real-time systems. In these systemsquantitative temporal properties play a dominant role. We first characterize real-time systems by giving a classification of such quantitative temporal properties. Next, we extend the usual models for temporal logic by including a distance function to measure time and analyze what restrictions should be imposed on such a function. Then we introduce appropriate temporal operators to reason about such models by turning qualitative temporal operators into (quantitative) metric temporal operators and show how the usual quantitative temporal properties of real-time systems can be expressed in this metric temporal logic. After we illustrate the application of metric temporal logic to real-time systems by several examples, we end this paper with some conclusions.}}
@ARTICLE{Bayse_2005,title={A passive testing approach based on invariants: application to the WAP},year={2005},author={Emmanuel Bayse and Emmanuel Bayse and Ana Cavalli and Ana Cavalli and Manuel Núñez and Manuel Núñez and Fatiha Zaïdi and Fatiha Zaïdi},doi={10.1016/j.comnet.2004.09.009},pmid={null},pmcid={null},mag_id={2026774266},journal={Computer Networks},abstract={null}}
@ARTICLE{Goodwin_1998,title={Decision Analysis for Management Judgment},year={1998},author={Paul Goodwin and Paul Goodwin and George W. Wright and George Wright},doi={null},pmid={null},pmcid={null},mag_id={2026909540},journal={null},abstract={In an increasingly complex world, decision analysis has a major role to play in helping decision-makers to gain insights into the problems they face. Decision Analysis for Management Judgment is unique in its breadth of coverage of decision analysis methods. It covers both the psychological problems that are associated with unaided managerial decision making and the decision analysis methods designed to overcome them. It is presented and explained in a clear, straightforward manner without using mathematical notation. The fourth edition has been fully revised and updated and includes a number of changes to reflect the latest developments in the field.}}
@ARTICLE{Khomenko_2006,title={Merged processes: a new condensed representation of Petri net behaviour},year={2006},author={Victor Khomenko and Victor Khomenko and A. Kondratyev and Alex Kondratyev and Maciej Koutny and Maciej Koutny and Walter Vogler and Walter Vogler},doi={10.1007/s00236-006-0023-y},pmid={null},pmcid={null},mag_id={2027131958},journal={Acta Informatica},abstract={Model checking based on Petri net unfoldings is an approach widely applied to cope with the state space explosion problem. In this paper, we propose a new condensed representation of a Petri net’s behaviour called merged processes, which copes well not only with concurrency, but also with other sources of state space explosion, viz sequences of choices and non-safeness. Moreover, this representation is sufficiently similar to the traditional unfoldings, so that a large body of results developed for the latter can be re-used. Experimental results indicate that the proposed representation of a Petri net’s behaviour alleviates the state space explosion problem to a significant degree and is suitable for model checking.}}
@ARTICLE{Budd_1985,title={Program testing by specification mutation},year={1985},author={Timothy A. Budd and Timothy A. Budd and Ajei S. Gopal and Ajei S. Gopal},doi={10.1016/0096-0551(85)90011-6},pmid={null},pmcid={null},mag_id={2027711654},journal={Computer Languages},abstract={null}}
@ARTICLE{Bravetti_2002,title={The theory of interactive generalized semi-Markov processes},year={2002},author={Mario Bravetti and Mario Bravetti and Roberto Gorrieri and Roberto Gorrieri},doi={10.1016/s0304-3975(01)00043-3},pmid={null},pmcid={null},mag_id={2028079360},journal={Theoretical Computer Science},abstract={null}}
@ARTICLE{Jensen_1982,title={REPORTING OF MANAGEMENT FORECASTS: AN EIGENVECTOR MODEL FOR ELICITATION AND REVIEW OF FORECASTS},year={1982},author={Robert E. Jensen and Robert E. Jensen},doi={10.1111/j.1540-5915.1982.tb00127.x},pmid={null},pmcid={null},mag_id={2031830662},journal={Decision Sciences},abstract={Increasingly, business firms will be disclosing management forecasts in financial reports to investors. Single estimates based on a consensus view (e.g., Delphi estimates) may be used. It seems logical, however, to consider methods that seek formally to derive internally consistent subjective views regarding the sensitivity of assumptions and interacting events on forecasts.



The major purpose of this paper is to extend micro-level cross-impact analysis to a macro-level eigenvalue analysis approach that elicits views of the effects of assumptions and interaction events on entire scenarios. Both methods are illustrated and contrasted. They are complementary rather than alternative methods. In particular, each method offers some potential in the development of operational guidelines for elicitation and reporting of management forecasts. In addition, the eigenvalue analysis offers several key advantages that make it potentially useful in preparing entire scenario forecasts.}}
@ARTICLE{Figueiredo_2006,title={Generating interaction test cases for mobile phone systems from use case specifications},year={2006},author={André Figueiredo and André L. L. de Figueiredo and Wilkerson L. Andrade and Wilkerson L. Andrade and Patrícia D. L. Machado and Patrícia D. L. Machado},doi={10.1145/1218776.1218788},pmid={null},pmcid={null},mag_id={2033463528},journal={ACM Sigsoft Software Engineering Notes},abstract={The mobile phone market has become even more competitive, demanding high quality standards. In this context, applications are built as sets of functionalities, called features. Such features are combined in use scenarios of the application. Due to the fact that the features are usually developed in isolation, the tests of their interactions in such scenarios are compromised. In this paper, we present a proposal of specifying feature interaction requirements with use cases; generating a behavioral model from such specification; and a strategy for generating test cases from the behavioral model that aims to extract feature interaction scenarios in such a way that interactions can be tested.}}
@ARTICLE{Manet_2008,title={An evaluation of dynamic partial reconfiguration for signal and image processing in professional electronics applications},year={2008},author={Philippe Manet and Philippe Manet and Daniel Maufroid and Daniel Maufroid and Leonardo Tosi and Leonardo Tosi and Grégory Gailliard and Grégory Gailliard and Olivier Mulertt and Olivier Mulertt and Marco Di Ciano and Marco Di Ciano and Jean-Didier Legat and Jean-Didier Legat and Denis Aulagnier and Denis Aulagnier and Christian Gamrat and Christian Gamrat and Christian Gamrat and Raffaele Liberati and Raffaele Liberati and Vincenzo Barba and Vincenzo La Barba and Pol Cuvelier and Pol Cuvelier and Bruno Rousseau and Bertrand Rousseau and Paul Gelineau and Paul Gelineau},doi={10.1155/2008/367860},pmid={null},pmcid={null},mag_id={2033524082},journal={Eurasip Journal on Embedded Systems},abstract={Signal and image processing applications require a lot of computing resources. For low-volume applications like in professional electronics applications, FPGA are used in combination with DSP and GPP in order to reach the performances required by the product roadmaps. Nevertheless, FPGA designs are static, which raises a flexibility issue with new complex or software defined applications like software-defined radio (SDR). In this scope, dynamic partial reconfiguration (DPR) is used to bring a virtualization layer upon the static hardware of FPGA. During the last decade, DPR has been widely studied in academia. Nevertheless, there are very few real applications using it, and therefore, there is a lack of feedback providing relevant issues to address in order to improve its applicability. This paper evaluates the interest and limitations when using DPR in professional electronics applications and provides guidelines to improve its applicability. It makes a fair evaluation based on experiments made on a set of signal and image processing applications. It identifies the missing elements of the design flow to use DPR in professional electronics applications. Finally, it introduces a fast reconfiguration manager providing an 84-time improvement compared to the vendor solution.}}
@ARTICLE{Jiang_2007,title={An analysis approach for testing exception handling programs},year={2007},author={Shujuan Jiang and Shujuan Jiang and Yuanpeng Jiang and Yuanpeng Jiang},doi={10.1145/1288258.1288259},pmid={null},pmcid={null},mag_id={2033946387},journal={Sigplan Notices},abstract={Exception handling is a powerful mechanism that separates the error handling code from normal code. However, incorrect usage of exception will bring about more potential faults in the code. Based on the study of exception model of C++, the paper proposes a precise and efficient representation of programs with exception handling constructs---Exception Control Flow Graph, which can represent explicitly the implicit control flow of exception and exception propagation path. Then it presents the structure testing criteria of programs with exception handling constructs based on the representation method, and gives the computing method. The approach overcomes the limitations of previous incorrect analysis because of failing to account for the effects of exception handling constructs.}}
@ARTICLE{Hierons_1997,title={Testing from a finite state machine: Extending invertibility to sequences},year={1997},author={Robert M. Hierons and Robert M. Hierons},doi={10.1093/comjnl/40.4.220},pmid={null},pmcid={null},mag_id={2033996441},journal={The Computer Journal},abstract={When testing a system modelled as a finite state machine it is desirable to minimize the effort required. It has been demonstrated that it is possible to utilize test sequence overlap in order to reduce the test effort and this overlap has been represented by using invertible transitions. In this paper invertibility will be extended to sequences in order to reduce the test effort further and encapsulate a more general type of test sequence overlap. It will also be shown that certain properties of invertible sequences can be used in the generation of state identification sequences.}}
@ARTICLE{Gurov_2008,title={Compositional verification of sequential programs with procedures},year={2008},author={Dilian Gurov and Dilian Gurov and Marieke Huisman and Marieke Huisman and Christoph Sprenger and Christoph Sprenger},doi={10.1016/j.ic.2008.03.003},pmid={null},pmcid={null},mag_id={2034063260},journal={Information & Computation},abstract={We present a method for algorithmic, compositional verification of control-flow-based safety properties of sequential programs with procedures. The application of the method involves three steps: (1) decomposing the desired global property into local properties of the components, (2) proving the correctness of the property decomposition by using a maximal model construction, and (3) verifying that the component implementations obey their local specifications. We consider safety properties of both the structure and the behaviour of program control flow. Our compositional verification method builds on a technique proposed by Grumberg and Long that uses maximal models to reduce compositional verification of finite-state parallel processes to standard model checking. We present a novel maximal model construction for the fragment of the modal @m-calculus with boxes and greatest fixed points only, and adapt it to control-flow graphs modelling components described in a sequential procedural language. We extend our verification method to programs with private procedures by defining an abstraction, presented as an inlining transformation. All algorithms have been implemented in a tool set automating all required verification steps. We validate our approach on an electronic purse case study.}}
@ARTICLE{Cazorla_2003,title={Algebraic theory of probabilistic and nondeterministic processes},year={2003},author={Diego Cazorla and Diego Cazorla and Fernando Cuartero and Fernando Cuartero and Valentín Valero and Valentín Valero and Fernando L. Pelayo and Fernando L. Pelayo and José E. Pardo and J. Jose Pardo},doi={10.1016/s1567-8326(02)00040-1},pmid={null},pmcid={null},mag_id={2034188142},journal={The Journal of Logic and Algebraic Programming},abstract={null}}
@ARTICLE{Bettini_2010,title={Implementing software product lines using traits},year={2010},author={Lorenzo Bettini and Lorenzo Bettini and Ferruccio Damiani and Ferruccio Damiani and Ina Schaefer and Ina Schaefer},doi={10.1145/1774088.1774530},pmid={null},pmcid={null},mag_id={2035145332},journal={null},abstract={A software product line (SPL) is a set of software systems with well-defined commonalities and variabilities that are developed by managed reuse of common artifacts. In this paper, we present a novel approach to implement SPL by fine-grained reuse mechanisms which are orthogonal to class-based inheritance. We introduce the Featherweight Record-Trait Java (FRTJ) calculus where units of product functionality are modeled by traits, a construct that was already shown useful with respect to code reuse, and by records, a construct that complements traits to model the variability of the state part of products explicitly. Records and traits are assembled in classes that are used to build products. This composition of product functionalities is realized by explicit operators of the calculus, allowing code manipulations for modeling product variability. The FRTJ type system ensures that the products in the SPL are type-safe by type-checking only once the records, traits and classes shared by different products. Moreover, type-safety of an extension of a (type-safe) SPL can be guaranteed by checking only the newly added parts.}}
@ARTICLE{Leavens_2005,title={How the design of JML accommodates both runtime assertion checking and formal verification},year={2005},author={Gary T. Leavens and Gary T. Leavens and Yoonsik Cheon and Yoonsik Cheon and Curtis Clifton and Curtis Clifton and Clyde Ruby and Clyde Ruby and David R. Cok and David R. Cok},doi={10.1016/j.scico.2004.05.015},pmid={null},pmcid={null},mag_id={2035529004},journal={Science of Computer Programming},abstract={Specifications that are used in detailed design and in the documentation of existing code are primarily written and read by programmers. However, most formal specification languages either make heavy use of symbolic mathematical operators, which discourages use by programmers, or limit assertions to expressions of the underlying programming language, which makes it difficult to write exact specifications. Moreover, using assertions that are expressions in the underlying programming language can cause problems both in runtime assertion checking and in formal verification, because such expressions can potentially contain side effects. The Java Modeling Language, JML, avoids these problems. It uses a side-effect free subset of Java's expressions to which are added a few mathematical operators (such as the quantifiers \forall and \exists). JML also hides mathematical abstractions, such as sets and sequences, within a library of Java classes. The goal is to allow JML to serve as a common notation for both formal verification and runtime assertion checking; this gives users the benefit of several tools without the cost of changing notations.}}
@ARTICLE{Ciobanu_2011,title={Timed Mobility in process algebra and Petri nets},year={2011},author={Gabriel Ciobanu and Gabriel Ciobanu and Maciej Koutny and Maciej Koutny},doi={10.1016/j.jlap.2011.05.002},pmid={null},pmcid={null},mag_id={2036541902},journal={The Journal of Logic and Algebraic Programming},abstract={Abstract  We present a process algebra called TiMo in which timeouts of interactions and adaptable migrations in a distributed environment with explicit locations can be specified. Timing constraints allow to control the communication between co-located mobile processes, and a migration action with variable destination supports flexible movement from one location to another. The model of time is based on local clocks rather than a global clock.  We provide a structural translation of TiMo into behaviourally equivalent high level timed Petri nets. As a result, we obtain a formal net semantics for timed interaction and migration which is both structural and allows one to deal directly with concurrency and causality.}}
@ARTICLE{Schneider_2000,title={Enforceable security policies},year={2000},author={Fred B. Schneider and Fred B. Schneider},doi={10.1145/353323.353382},pmid={null},pmcid={null},mag_id={2036910349},journal={ACM Transactions on Information and System Security},abstract={A precise characterization is given for the class of security policies enforceable with mechanisms that work by monitoring system execution, and automata are introduced for specifying exactly that class of security policies. Techniques to enforce security policies specified by such automata are also discussed.}}
@ARTICLE{Ravi_2005,title={Analyzing alternatives in reverse logistics for end-of-life computers: ANP and balanced scorecard approach},year={2005},author={Vadlamani Ravi and V. Ravi and Ravi Shankar and Ravi Shankar and Manoj Kumar Tiwari and Manoj Kumar Tiwari},doi={10.1016/j.cie.2005.01.017},pmid={null},pmcid={null},mag_id={2038680959},journal={Computers & Industrial Engineering},abstract={null}}
@ARTICLE{Gaubert_1999,title={Petri Net Languages and Infinite Subsets of Nm},year={1999},author={Stéphane Gaubert and Stéphane Gaubert and Stéphane Gaubert and Stéphane Gaubert and Alessandro Giua and Alessandro Giua},doi={10.1006/jcss.1999.1634},pmid={null},pmcid={null},mag_id={2039309162},journal={Journal of Computer and System Sciences},abstract={Families of Petri net languages are usually defined by varying the type of transition labeling and the class of subsets of Nm to be used as sets of final markings (m is the number of places). So far three main classes of subsets have been studied: the trivial class containing as single element Nm, the class of finite subsets of Nm, and the class of ideals (or covering subsets) of Nm. In this paper we extend the known hierarchy of Petri net languages by considering the classes of semi-cylindrical, star-free, recognizable, rational (or semilinear) subsets of Nm. We compare the related Petri net languages. For arbitrarily labeled and for ?-free labeled Petri net languages, the above hierarchy collapses: one does not increase the generality by considering semilinear accepting sets instead of the usual finite ones. However, for free-labeled and for deterministic Petri net languages, we show that one gets new distinct subclasses of languages, for which several decidability problems become solvable. We establish as intermediate results some properties of star-free subsets of general monoids.}}
@ARTICLE{Havelund_2000,title={Model Checking JAVA Programs Using Java Pathfinder},year={2000},author={Klaus Havelund and Klaus Havelund and Thomas Pressburger and Thomas Pressburger},doi={10.1007/s100090050043},pmid={null},pmcid={null},mag_id={2040060046},journal={International Journal on Software Tools for Technology Transfer},abstract={This paper describes a translator called Java PathFinder (Jpf), which translates from Java to Promela, the modeling language of the Spin model checker. Jpf translates a given Java program into a Promela model, which then can be model checked using Spin. The Java program may contain assertions, which are translated into similar assertions in the Promela model. The Spin model checker will then look for deadlocks and violations of any stated assertions. Jpf generates a Promela model with the same state space characteristics as the Java program. Hence, the Java program must have a finite and tractable state space. This work should be seen in a broader attempt to make formal methods applicable within NASA’s areas such as space, aviation, and robotics. The work is a continuation of an effort to formally analyze, using Spin, a multi-threaded operating system for the Deep-Space 1 space craft, and of previous work in applying existing model checkers and theorem provers to real applications.}}
@ARTICLE{D’Argenio_2005,title={A theory of Stochastic systems. Part II: Process algebra},year={2005},author={Pedro R. D’Argenio and Pedro R. D'Argenio and Pedro R. D'Argenio and Joost-Pieter Katoen and Joost-Pieter Katoen},doi={10.1016/j.ic.2005.07.002},pmid={null},pmcid={null},mag_id={2040082021},journal={Information & Computation},abstract={This paper introduces ♠ (pronounce spades), a stochastic process algebra for discrete-event systems, that extends traditional process algebra with timed actions whose delay is governed by general (a.o. continuous) probability distributions. The operational semantics is defined in terms of stochastic automata, a model that uses clocks--like in timed automata--to symbolically represent randomly timed systems, cf. the accompanying paper [P.R. D'Argenio, J.-P. Katoen, A theory of stochastic systems. Part I: Stochastic automata. Inf. Comput. (2005), to appear]. We show that stochastic automata and ♠ are equally expressive, and prove that the operational semantics of a term up to α-conversion of clocks, is unique (modulo symbolic bisimulation). (Open) probabilistic and structural bisimulation are proven to be congruences for ♠, and are equipped with an equational theory. The equational theory is shown to be complete for structural bisimulation and allows to derive an expansion law.}}
@ARTICLE{Aiken_1999,title={Introduction to set constraint-based program analysis},year={1999},author={Alexander Aiken and Alex Aiken},doi={10.1016/s0167-6423(99)00007-6},pmid={null},pmcid={null},mag_id={2040152339},journal={Science of Computer Programming},abstract={Abstract   This paper given an introduction to using set constraints to specify program analyses. Several standard analysis problems are formulated using set constraints, which serves both to illustrate the style of using constraints to specify program analysis problems and the range of application of set constraints.}}
@ARTICLE{Woodward_1993,title={Errors in algebraic specifications and an experimental mutation testing tool},year={1993},author={Martin R. Woodward and Martin R. Woodward and M.R. Woodward},doi={10.1049/sej.1993.0027},pmid={null},pmcid={null},mag_id={2040913826},journal={Software Engineering Journal},abstract={There is an increasing trend towards more formality in the development of specifications of software systems in order to reduce the likelihood of errors as early as possible in the development process. The algebraic approach to specification, with its equational form, leads to the added advantage of executability via the process of term rewriting. Nevertheless, erroneous algebraic specifications can still be developed. As evidence of possible errors, the algebraic specifications produced by students have been analysed and the results are presented. The paper describes OBJTEST, a prototype testing system for algebraic specifications. The two principal facets of the system are the user-guided automatic generation of sets of test expressions from a specification, and then the use of these test expressions in mutation testing of the given specification. Some preliminary experience with the system is reported. >}}
@ARTICLE{Kwiatkowska_2009,title={PRISM: probabilistic model checking for performance and reliability analysis},year={2009},author={Marta Kwiatkowska and Marta Kwiatkowska and Gethin Norman and Gethin Norman and David Parker and David Parker},doi={10.1145/1530873.1530882},pmid={null},pmcid={null},mag_id={2041509965},journal={null},abstract={Probabilistic model checking is a formal verification technique for the modelling and analysis of stochastic systems. It has proved to be useful for studying a wide range of quantitative properties of models taken from many diffierent application domains. This includes, for example, performance and reliability properties of computer and communication systems. In this paper, we give an overview of the probabilistic model checking tool PRISM, focusing in particular on its support for continuous-time Markov chains and Markov reward models, and how these can be used to analyse performability properties.}}
@ARTICLE{Dutta_2003,title={Prioritising information systems projects in higher education},year={2003},author={Ritaban Dutta and R. Dutta and Thomas F. Burgess and T.F. Burgess},doi={10.1108/10650740310491324},pmid={null},pmcid={null},mag_id={2041707385},journal={Campus-wide Information Systems},abstract={This paper focuses on solving the problem faced by higher education institutions (HEIs) when prioritising information systems (IS) projects. In many instances HEIs face many more potential IS projects than they can cope with. Making good decisions as to which projects should be supported becomes a complex but crucial issue. In practice, managers may resort to simple heuristics or rules of thumb to decide on projects that should be evaluated against competing objectives. In such complex situations structured approaches exist such as multi‐criteria decision making. The case study reported here explores the application of such a technique, simple multi‐attribute rating technique (SMART), to support the IS Department of Leeds University when prioritising IS projects.}}
@ARTICLE{Merlin_1976,title={Recoverability of Communication Protocols--Implications of a Theoretical Study},year={1976},author={Philip M. Merlin and P. Merlin and David Farber and D. Farber},doi={10.1109/tcom.1976.1093424},pmid={null},pmcid={null},mag_id={2042801853},journal={IEEE Transactions on Communications},abstract={A study is presented which permits the formal analysis and synthesis of recoverable computer communication protocols. This study is based on a formal representation of processes by a model of computation, the Petri nets (PN's). The PN model is generalized to include a representation of the possible failures, and then, the concept of "recoverability" is formally defined. A set of necessary and sufficient conditions which a process must satisfy in order to be recoverable is derived. In the PN model, the processes that satisfy these conditions are shown to have some practical limitations. A new model, the time-Petri net (TPN), is introduced to remove these limitations. This new model allows the introduction of constraints in the execution times of its part. As shown in this paper, the TPN appears to be a suitable model for the study of practical recoverable processes. Several practical communication protocols are formally designed and analyzed using this new model, and some interesting properties of these protocols are formally derived.}}
@ARTICLE{Cousot_1977,title={Abstract interpretation: a unified lattice model for static analysis of programs by construction or approximation of fixpoints},year={1977},author={Patrick Cousot and Patrick Cousot and Radhia Cousot and Radhia Cousot},doi={10.1145/512950.512973},pmid={null},pmcid={null},mag_id={2043100293},journal={null},abstract={A program denotes computations in some universe of objects. Abstract interpretation of programs consists in using that denotation to describe computations in another universe of abstract objects, so that the results of abstract execution give some information on the actual computations. An intuitive example (which we borrow from Sintzoff [72]) is the rule of signs. The text -1515 * 17 may be understood to denote computations on the abstract universe {(+), (-), (±)} where the semantics of arithmetic operators is defined by the rule of signs. The abstract execution -1515 * 17 → -(+) * (+) → (-) * (+) → (-), proves that -1515 * 17 is a negative number. Abstract interpretation is concerned by a particular underlying structure of the usual universe of computations (the sign, in our example). It gives a summary of some facets of the actual executions of a program. In general this summary is simple to obtain but inaccurate (e.g. -1515 + 17 → -(+) + (+) → (-) + (+) → (±)). Despite its fundamentally incomplete results abstract interpretation allows the programmer or the compiler to answer questions which do not need full knowledge of program executions or which tolerate an imprecise answer, (e.g. partial correctness proofs of programs ignoring the termination problems, type checking, program optimizations which are not carried in the absence of certainty about their feasibility, …).}}
@ARTICLE{Montanari_1998,title={An Introduction to History Dependent Automata},year={1998},author={Ugo Montanari and Ugo Montanari and Marco Pistore and Marco Pistore},doi={10.1016/s1571-0661(05)80696-6},pmid={null},pmcid={null},mag_id={2043190445},journal={Electronic Notes in Theoretical Computer Science},abstract={Automata (or labeled transition systems) are widely used as operational models in the field of process description languages like CCS 13]. There are however classes of formalisms that are not modelled adequately by the automata. This is the case, for instance, of the ?-calculus 15,14], an extension of CCS where channels can be used as values in the communications and new channels can be created dynamically. Due to the necessity to represent the creation of new channels infinite automata are obtained in this case also for very simple agents and a non-standard definition of bisimulation is required.In this paper we present an enhanced version of automata, called history dependent automata, that are adequate to represent the operational semantics of ?-calculus and of other history dependent formalisms. We also define a bisimulation equivalence on history dependent automata, that captures ?-calculus bisimulation. The results presented here are discussed in more detail in 21].}}
@ARTICLE{Lynch_1992,title={Using mappings to prove timing properties},year={1992},author={Nancy Lynch and Nancy Lynch and Hagit Attiya and Hagit Attiya},doi={10.1007/bf02252683},pmid={null},pmcid={null},mag_id={2043695863},journal={Distributed Computing},abstract={A new technique for proving timing properties for timing-based algorithms is described; it is an extension of the mapping techniques previously used in proofs of safety properties for asynchronous concurrent systems. The key to the method is a way of representing a system with timing constraints as an automaton whose state includes predictive timing information. Timing assumptions and timing requirements for the system are both represented in this way. A multi-valued mapping from the "assumptions automaton" to the "requirements automaton" is then used to show that the given system satisfies the requirements. One type of mapping is based on a collection of "progress functions" providing measures of progress toward timing goals. The technique is illustrated with two examples, a simple resource manager and a two-process race system.}}
@ARTICLE{Ungar_1987,title={Self: The power of simplicity},year={1987},author={David Ungar and David Ungar and Randall B. Smith and Randall B. Smith and Randall B. Smith},doi={10.1145/38807.38828},pmid={null},pmcid={null},mag_id={2045723688},journal={null},abstract={Self is a new object-oriented language for exploratory programming based on a small number of simple and concrete ideas: prototypes, slots, and behavior. Prototypes combine inheritance and instantiation to provide a framework that is simpler and more flexible than most object-oriented languages. Slots unite variables and procedures into a single construct. This permits the inheritance hierarchy to take over the function of lexical scoping in conventional languages. Finally, because Self does not distinguish state from behavior, it narrows the gaps between ordinary objects, procedures, and closures. Self's simplicity and expressiveness offer new insights into object-oriented computation.}}
@ARTICLE{Kim_2004,title={Java-MaC: A Run-Time Assurance Approach for Java Programs},year={2004},author={Moonzoo Kim and Moonzoo Kim and Mahesh Viswanathan and Mahesh Viswanathan and Sampath Kannan and Sampath Kannan and Insup Lee and Insup Lee and Oleg Sokolsky and Oleg Sokolsky},doi={10.1023/b:form.0000017719.43755.7c},pmid={null},pmcid={null},mag_id={2047276559},journal={null},abstract={We describe Java-MaC, a prototype implementation of the Monitoring and Checking (MaC) architecture for Java programs. The MaC architecture provides assurance that the target program is running correctly with respect to a formal requirements specification by monitoring and checking the execution of the target program at run-time. MaC bridges the gap between formal verification, which ensures the correctness of a design rather than an implementation, and testing, which does not provide formal guarantees about the correctness of the system.

Use of formal requirement specifications in run-time monitoring and checking is the salient aspect of the MaC architecture. MaC is a lightweight formal method solution which works as a viable complement to the current heavyweight formal methods. In addition, analysis processes of the architecture including instrumentation of the target program, monitoring, and checking are performed fully automatically without human direction, which increases the accuracy of the analysis. Another important feature of the architecture is the clear separation between monitoring implementation-dependent low-level behaviors and checking high-level behaviors, which allows the reuse of a high-level requirement specification even when the target program implementation changes. Furthermore, this separation makes the architecture modular and allows the flexibility of incorporating third party tools into the architecture. The paper presents an overview of the MaC architecture and a prototype implementation Java-MaC.}}
@ARTICLE{Havelund_2004,title={An Overview of the Runtime Verification Tool Java PathExplorer},year={2004},author={Klaus Havelund and Klaus Havelund and Grigore Roşu and Grigore Rosu},doi={10.1023/b:form.0000017721.39909.4b},pmid={null},pmcid={null},mag_id={2048327004},journal={null},abstract={We present an overview of the Java PathExplorer runtime verification tool, in short referred to as JPAX. JPAX can monitor the execution of a Java program and check that it conforms with a set of user provided properties formulated in temporal logic. JPAX can in addition analyze the program for concurrency errors such as deadlocks and data races. The concurrency analysis requires no user provided specification. The tool facilitates automated instrumentation of a program's bytecode, which when executed will emit an event stream, the execution trace, to an observer. The observer dispatches the incoming event stream to a set of observer processes, each performing a specialized analysis, such as the temporal logic verification, the deadlock analysis and the data race analysis. Temporal logic specifications can be formulated by the user in the Maude rewriting logic, where Maude is a high-speed rewriting system for equational logic, but here extended with executable temporal logic. The Maude rewriting engine is then activated as an event driven monitoring process. Alternatively, temporal specifications can be translated into automata or algorithms that can efficiently check the event stream. JPAX can be used during program testing to gain increased information about program executions, and can potentially furthermore be applied during operation to survey safety critical systems.}}
@ARTICLE{Emerson_1982,title={Using branching time temporal logic to synthesize synchronization skeletons},year={1982},author={E. Allen Emerson and E. Allen Emerson and Edmund M. Clarke and Edmund M. Clarke},doi={10.1016/0167-6423(83)90017-5},pmid={null},pmcid={null},mag_id={2048355938},journal={Science of Computer Programming},abstract={Abstract   We present a method of constructing concurrent programs in which the synchronization skeleton of the program is automatically synthesized from a (branching time) temporal logic specification. The synthesis method uses a decision procedure based on the finite model property of the logic to determine satisfiability of the specification formula f. If f is satisfiable, then a model for f with a finite number of states is constructed. The synchronization skeleton of a program meeting the specification can be read from this model. If f is unsatisfiable, the specification is inconsistent.}}
@ARTICLE{Milbredt_2010,title={Switched FlexRay: Increasing the effective bandwidth and safety of FlexRay networks},year={2010},author={Paul Milbredt and Paul Milbredt and Bart Vermeulen and Bart Vermeulen and Gökhan Tabanoglu and Gokhan Tabanoglu and Martin Lukasiewycz and Martin Lukasiewycz},doi={10.1109/etfa.2010.5641268},pmid={null},pmcid={null},mag_id={2048604608},journal={null},abstract={With the continued demand for more and innovative functions in series automobiles, significantly higher data-rates and more reliable communication are necessary than traditional automotive bus systems, e.g., the controller area network (CAN), provide. In this paper, we present a novel hardware device for FlexRay networks, which splits the bus into separate branches and operates as a selective central switch.}}
@ARTICLE{Tabourier_1999,title={Passive testing and application to the GSM-MAP protocol},year={1999},author={Marine Tabourier and Marine Tabourier and Ana Cavalli and Ana Cavalli},doi={10.1016/s0950-5849(99)00039-7},pmid={null},pmcid={null},mag_id={2049111714},journal={Information & Software Technology},abstract={null}}
@ARTICLE{DeMillo_1978,title={Hints on Test Data Selection: Help for the Practicing Programmer},year={1978},author={Richard A. DeMillo and Richard A. DeMillo and Richard J. Lipton and Richard J. Lipton and Richard J. Lipton and Frederick G. Sayward and Frederick G. Sayward},doi={10.1109/c-m.1978.218136},pmid={null},pmcid={null},mag_id={2049695835},journal={IEEE Computer},abstract={In many cases tests of a program that uncover simple errors are also effective in uncovering much more complex errors. This so-called coupling effect can be used to save work during the testing process.}}
@ARTICLE{Santhanam_1996,title={A DECISION MODEL FOR INTERDEPENDENT INFORMATION SYSTEM PROJECT SELECTION},year={1996},author={Radhika Santhanam and Radhika Santhanam and George J. Kyparisis and George J. Kyparisis},doi={10.1016/0377-2217(94)00257-6},pmid={null},pmcid={null},mag_id={2050232562},journal={European Journal of Operational Research},abstract={null}}
@ARTICLE{Walsham_1995,title={Interpretive case studies in IS research: nature and method},year={1995},author={Geoff Walsham and Geoff Walsham},doi={10.1057/ejis.1995.9},pmid={null},pmcid={null},mag_id={2050401924},journal={European Journal of Information Systems},abstract={AbstractThere has been an increase in recent years in the number of in-depth case studies which focus on human actions and interpretations surrounding the development and use of computer-based information systems (IS). This paper addresses philosophical and theoretical issues concerning the nature of such interpretive case studies, and methodological issues on the conduct and reporting of this type of research. The paper aims to provide a useful reference point for researchers who wish to work in the interpretive tradition, and more generally to encourage careful work on the conceptualisation and execution of case studies in the information systems field.}}
@ARTICLE{Aichernig_2003,title={Mutation Testing in the Refinement Calculus},year={2003},author={Bernhard K. Aichernig and Bernhard K. Aichernig},doi={10.1007/s00165-003-0011-8},pmid={null},pmcid={null},mag_id={2051085665},journal={Formal Aspects of Computing},abstract={This article discusses mutation testing strategies in the context of refinement. Here, a novel generalisation of mutation testing techniques is presented to be applied to contracts ranging from formal specifications to programs. It is demonstrated that refinement and its dual abstraction are the key notions leading to a precise and yet simple theory of mutation testing. The refinement calculus of Back and von Wright is used to express concepts like contracts, useful mutations, test cases and test coverage.}}
@ARTICLE{Meyer_1992,title={Applying 'design by contract'},year={1992},author={B. Meyer and Bertrand Meyer},doi={10.1109/2.161279},pmid={null},pmcid={null},mag_id={2052363833},journal={IEEE Computer},abstract={Methodological guidelines for object-oriented software construction that improve the reliability of the resulting software systems are presented. It is shown that the object-oriented techniques rely on the theory of design by contract, which underlies the design of the Eiffel analysis, design, and programming language and of the supporting libraries, from which a number of examples are drawn. The theory of contract design and the role of assertions in that theory are discussed. >}}
@ARTICLE{Zhao_2003,title={Problems in the information dissemination of the internet routing},year={2003},author={Youjian Zhao and Yixin Zhao and Xia Yin and Xia Yin and Jianping Wu and Jianping Wu},doi={10.1007/bf02948879},pmid={null},pmcid={null},mag_id={2052388813},journal={Journal of Computer Science and Technology},abstract={Internet routing is achieved by a set of nodes running distributed algorithms-routing protocols. However, many nodes are resistless to wrong messages or improper operations, unable to detect or correct them. Thus a wrong message or an improper operation can easily sweep almost the whole Internet. Such a fragile Internet routing comes from the features of these algorithms and protocols. Besides, the strategies taken by the network equipment manufacturers and administrators also are of important influence. When determining the options or selections in the implementation/operation, they always pay more attention to the expense of a single node or a single area and make some simplifications in implementations and configurations while caring less about the influence on the whole network. This paper tries to illustrate such a scheme is not reasonable at all and suggests the consideration from the view of the overall optimization. From three typical cases involved in the Internet routing, a general model is abstracted, which makes the results significative for more Internet related aspects. This paper evaluates the complexity of the theoretical analysis, then acquires the effect of error information on the whole network through the simulation on the Internet topology. It is shown that even very little error information can incur severe impact on the Internet. And it will take much more efforts of downstream nodes to make remedies. This result is intuitively revealed through the comparisons in the charts and the visual presentations. Then a hierarchical solution to establish the upgrade plan is given, which helps to upgrade the nodes of the network in a most efficient and economical way.}}
@ARTICLE{Snelting_2006,title={Efficient path conditions in dependence graphs for software safety analysis},year={2006},author={Gregor Snelting and Gregor Snelting and Torsten Robschink and Torsten Robschink and Jens Krinke and Jens Krinke},doi={10.1145/1178625.1178628},pmid={null},pmcid={null},mag_id={2054383157},journal={ACM Transactions on Software Engineering and Methodology},abstract={A new method for software safety analysis is presented which uses program slicing and constraint solving to construct and analyze path conditions, conditions defined on a program's input variables which must hold for information flow between two points in a program. Path conditions are constructed from subgraphs of a program's dependence graph, specifically, slices and chops. The article describes how constraint solvers can be used to determine if a path condition is satisfiable and, if so, to construct a witness for a safety violation, such as an information flow from a program point at one security level to another program point at a different security level. Such a witness can prove useful in legal matters.The article reviews previous research on path conditions in program dependence graphs; presents new extensions of path conditions for arrays, pointers, abstract data types, and multithreaded programs; presents new decomposition formulae for path conditions; demonstrates how interval analysis and BDDs (binary decision diagrams) can be used to reduce the scalability problem for path conditions; and presents case studies illustrating the use of path conditions in safety analysis. Applying interval analysis and BDDs is shown to overcome the combinatorial explosion that can occur in constructing path conditions. Case studies and empirical data demonstrate the usefulness of path conditions for analyzing practical programs, in particular, how illegal influences on safety-critical programs can be discovered and analyzed.}}
@ARTICLE{Detlefs_2005,title={Simplify: a theorem prover for program checking},year={2005},author={David Detlefs and David L. Detlefs and Greg Nelson and Greg Nelson and James B. Saxe and James B. Saxe},doi={10.1145/1066100.1066102},pmid={null},pmcid={null},mag_id={2055477538},journal={Journal of the ACM},abstract={This article provides a detailed description of the automatic theorem prover Simplify, which is the proof engine of the Extended Static Checkers ESC/Java and ESC/Modula-3. Simplify uses the Nelson--Oppen method to combine decision procedures for several important theories, and also employs a matcher to reason about quantifiers. Instead of conventional matching in a term DAG, Simplify matches up to equivalence in an E-graph, which detects many relevant pattern instances that would be missed by the conventional approach. The article describes two techniques, error context reporting and error localization, for helping the user to determine the reason that a false conjecture is false. The article includes detailed performance figures on conjectures derived from realistic program-checking problems.}}
@ARTICLE{Johnsen_2009,title={Intra-Object versus Inter-Object: Concurrency and Reasoning in Creol},year={2009},author={Einar Broch Johnsen and Einar Broch Johnsen and Jasmin Christian Blanchette and Jasmin Christian Blanchette and Marcel Kyas and Marcel Kyas and Olaf Owe and Olaf Owe},doi={10.1016/j.entcs.2009.07.007},pmid={null},pmcid={null},mag_id={2056375306},journal={Electronic Notes in Theoretical Computer Science},abstract={In thread-based object-oriented languages, synchronous method calls usually provide the mechanism to transfer control from caller to callee, blocking the caller until the call is completed. This model of control flow is well-suited for sequential and tightly coupled systems but may be criticized in the concurrent and distributed setting, not only for unnecessary delays but also for the reasoning complexity of multithreaded programs. Concurrent objects propose an alternative to multithread concurrency for object-oriented languages, in which each object encapsulates a thread of control and communication between objects is asynchronous. Creol is a formally defined modeling language for concurrent objects which clearly separates intra-object scheduling from inter-object communication by means of interface encapsulation, asynchronous method calls, and internal processor release points. This separation of concerns provides a very clean model of concurrency which significantly simplifies reasoning for highly parallel and distributed object-oriented systems. This paper gives an example-driven introduction to these basic features of Creol and discusses how this separation of concerns influences analysis of Creol models.}}
@ARTICLE{Bruschi_2005,title={A framework for the functional verification of systemC models},year={2005},author={Francesco Bruschi and Francesco Bruschi and Francesco Bruschi and Fabrizio Ferrandi and Fabrizio Ferrandi and D. Sciuto and Donatella Sciuto},doi={10.1007/s10766-005-8908-x},pmid={null},pmcid={null},mag_id={2057212170},journal={International Journal of Parallel Programming},abstract={The problems of error simulation, error model evaluation, and test generation are faced considering the peculiar features of SystemC. In particular, error simulation are considered in the perspective of the transaction level modelling (TLM) capabilities of this emerging system level design language to obtain a coherent, environment for functional verification. The error simulation is accomplished without any modification of the native simulation engine, thus avoiding the problem of upgrading the error simulator together with the language simulation engine. Moreover, error modelling and error simulation tasks are orthogonalized in this approach. With the support of this environment, a test pattern generation algorithm for SystemC descriptions of systems made of interacting Finite State Machines (FSMs) is developed. The approach is based on the definition of the transitions, that represent ordered sets of statements executed within one clock cycle. Through different state sequence paths enumeration strategies, interesting behaviors of the system are obtained.}}
@ARTICLE{Ben-Asher_2006,title={Producing scheduling that causes concurrent programs to fail},year={2006},author={Yosi Ben-Asher and Yosi Ben-Asher and Yaniv Eytani and Yaniv Eytani and Eitan Farchi and Eitan Farchi and Shmuel Ur and Shmuel Ur},doi={10.1145/1147403.1147410},pmid={null},pmcid={null},mag_id={2057572737},journal={null},abstract={A noise maker is a tool that seeds a concurrent program with conditional synchronization primitives (such as yield()) for the purpose of increasing the likelihood that a bug manifest itself. This work explores the theory and practice of choosing where in the program to induce such thread switches at runtime. We introduce a novel fault model that classifies locations as 'good', 'neutral', or 'bad,' based on the effect of a thread switch at the location. We validate our approach by experimenting with a set of programs taken from publicly available multi-threaded benchmark. Our empirical evidence demonstrates that real-life behavior is similar to that derived from the model.}}
@ARTICLE{Demartini_1999,title={A deadlock detection tool for concurrent Java programs},year={1999},author={Claudio Giovanni Demartini and Claudio Giovanni Demartini and Radu Iosif and Radu Iosif and Riccardo Sisto and Riccardo Sisto},doi={10.1002/(sici)1097-024x(199906)29:7<577::aid-spe246>3.0.co;2-v},pmid={null},pmcid={null},mag_id={2059224852},journal={Software - Practice and Experience},abstract={This paper presents some issues related to the design and implementation of a concurrency analysis tool able to detect deadlock situations in Java programs that make use of multithreading mechanisms. An abstract formal model is generated from the Java source using the Java2Spin translator. The model is expressed in the PROMELA language, and the SPIN tool is used to perform its formal analysis. The paper mainly focuses on the design of the Java2Spin translator. A set of experiments, carried out to evaluate the performances of the analysis tool, is also presented. Copyright © 1999 John Wiley & Sons, Ltd.}}
@ARTICLE{Schaefer_2011,title={Compositional type-checking for delta-oriented programming},year={2011},author={Ina Schaefer and Ina Schaefer and Lorenzo Bettini and Lorenzo Bettini and Ferruccio Damiani and Ferruccio Damiani},doi={10.1145/1960275.1960283},pmid={null},pmcid={null},mag_id={2059639972},journal={null},abstract={Delta-oriented programming is a compositional approach to flexibly implementing software product lines. A product line is represented by a code base and a product line declaration. The code base consists of a set of delta modules specifying modifications to object-oriented programs. The product line declaration provides the connection of the delta modules with the product features. This separation increases the reusability of delta modules. In this paper, we provide a foundation for compositional type checking of delta-oriented product lines of Java programs by presenting a minimal core calculus for delta-oriented programming. The calculus is equipped with a constraint-based type system that allows analyzing each delta module in isolation, such that that also the results of the analysis can be reused. By combining the analysis results for the delta modules with the product line declaration it is possible to establish that all the products of the product line are well-typed according to the Java type system.}}
@ARTICLE{Lee_2000,title={Using analytic network process and goal programming for interdependent information system project selection},year={2000},author={Jin Woo Lee and Jin Woo Lee and Jinwoo Lee and Soung Hie Kim and Shinhong Kim and Soung Hie Kim and Soung Hie Kim},doi={10.1016/s0305-0548(99)00057-x},pmid={null},pmcid={null},mag_id={2059999216},journal={Computers & Operations Research},abstract={null}}
@ARTICLE{Burch_1990,title={Symbolic model checking: 10/sup 20/ states and beyond},year={1990},author={Jerry R. Burch and J. R. Burch and Edmund M. Clarke and Edmund M. Clarke and Kenneth L. McMillan and Kenneth L. McMillan and David L. Dill and David L. Dill and L.J. Hwang and L.J. Hwang},doi={10.1016/0890-5401(92)90017-a},pmid={null},pmcid={null},mag_id={2061438988},journal={null},abstract={A general method that represents the state space symbolically instead of explicitly is described. The generality of the method comes from using a dialect of the mu-calculus as the primary specification language. A model-checking algorithm for mu-calculus formulas which uses R.E. Bryant's (1986) binary decision diagrams to represent relations and formulas symbolically is described. It is then shown how the novel mu-calculus model checking algorithm can be used to derive efficient decision procedures for CTL model checking, satisfiability of linear-time temporal logic formulas, strong and weak observational equivalence of finite transition systems, and language containment of finite omega -automata. This eliminates the need to describe complicated graph-traversal or nested fixed-point computations for each decision procedure. The authors illustrate the practicality of their approach to symbolic model checking by discussing how it can be used to verify a simple synchronous pipeline. >}}
@ARTICLE{Apel_2010,title={Detecting Dependences and Interactions in Feature-Oriented Design},year={2010},author={Sven Apel and Sven Apel and Wolfgang Scholz and Wolfgang Scholz and Christian Lengauer and Christian Lengauer and Christian Kästner and Christian Kästner},doi={10.1109/issre.2010.11},pmid={null},pmcid={null},mag_id={2063923508},journal={null},abstract={Feature-oriented software development (FOSD) aims at the construction, customization, and synthesis of large-scale software systems. We propose a novel software design paradigm, called feature-oriented design, that takes the distinguishing characteristics of FOSD into account, especially the clean and consistent mapping between features and their implementations as well as the tendency of features to interact inadvertently. We extend the lightweight modeling language Alloy with support for feature-oriented design and call the extension Feature Alloy. By means of an implementation and four case studies, we demonstrate how feature-oriented design with Feature Alloy facilitates separation of concerns, variability, and reuse of models of individual features and helps defining and detecting semantic dependences and interactions between features.}}
@ARTICLE{Kalibera_2009,title={CDx: a family of real-time Java benchmarks},year={2009},author={Tomáš Kalibera and Tomas Kalibera and Jeff Hagelberg and Jeff Hagelberg and Filip Pizlo and Filip Pizlo and Aleš Plšek and Ales Plsek and Ales Plsek and Ben L. Titzer and Ben Titzer and Jan Vítek and Jan Vitek},doi={10.1145/1620405.1620412},pmid={null},pmcid={null},mag_id={2064849992},journal={null},abstract={Java is becoming a viable platform for hard real-time computing. There are production and research real-time Java VMs, as well as applications in both military and civil sector. Technological advances and increased adoption of Real-time Java contrast significantly with the lack of real-time benchmarks. The few benchmarks that exist are either low-level synthetic micro-benchmarks, or benchmarks used internally by companies, making it difficult to independently verify and repeat reported results.   This paper presents the x (Collision Detector) benchmark suite, an open source application benchmark suite that targets different hard and soft real-time virtual machines. x is, at its core, a real-time benchmark with a single periodic task, which implements aircraft collision detection based on simulated radar frames. The benchmark can be configured to use different sets of real-time features and comes with a number of workloads. We describe the architecture of the benchmark and characterize the workload based on input parameters.}}
@ARTICLE{Reussner_2003,title={Automatic component protocol adaptation with the CoConut /J tool suite},year={2003},author={Ralf Reussner and Ralf Reussner and Ralf Reussner and Ralf Reussner and Ralf Reussner},doi={10.1016/s0167-739x(02)00173-5},pmid={null},pmcid={null},mag_id={2065300036},journal={Future Generation Computer Systems},abstract={null}}
@ARTICLE{Godefroid_1997,title={Model checking for programming languages using VeriSoft},year={1997},author={Patrice Godefroid and Patrice Godefroid},doi={10.1145/263699.263717},pmid={null},pmcid={null},mag_id={2065675749},journal={null},abstract={Verification by state-space exploration, also often referred to as "model checking", is an effective method for analyzing the correctness of concurrent reactive systems (e.g., communication protocols). Unfortunately, existing model-checking techniques are restricted to the verification of properties of models, i.e., abstractions, of concurrent systems.In this paper, we discuss how model checking can be extended to deal directly with "actual" descriptions of concurrent systems, e.g., implementations of communication protocols written in programming languages such as C or C++. We then introduce a new search technique that is suitable for exploring the state spaces of such systems. This algorithm has been implemented in VeriSoft, a tool for systematically exploring the state spaces of systems composed of several concurrent processes executing arbitrary C code. As an example of application, we describe how VeriSoft successfully discovered an error in a 2500-line C program controlling robots operating in an unpredictable environment.}}
@ARTICLE{Karsak_2003,title={Product planning in quality function deployment using a combined analytic network process and goal programming approach},year={2003},author={E. Ertuğrul Karsak and E. Ertugrul Karsak and Sevin Sozer and Sevin Sozer and S. Emre Alptekin and S. Emre Alptekin},doi={10.1016/s0360-8352(02)00191-2},pmid={null},pmcid={null},mag_id={2065752879},journal={Computers & Industrial Engineering},abstract={null}}
@ARTICLE{Dijkstra_1975,title={Guarded commands, nondeterminacy and formal derivation of programs},year={1975},author={Edsger W. Dijkstra and Edsger W. Dijkstra},doi={10.1145/360933.360975},pmid={null},pmcid={null},mag_id={2066210260},journal={Communications of The ACM},abstract={So-called “guarded commands” are introduced as a building block for alternative and repetitive constructs that allow nondeterministic program components for which at least the activity evoked, but possibly even the final state, is not necessarily uniquely determined by the initial state. For the formal derivation of programs expressed in terms of these constructs, a calculus will be be shown.}}
@ARTICLE{Bøgholm_2008,title={Model-based schedulability analysis of safety critical hard real-time Java programs},year={2008},author={Thomas Bøgholm and Thomas Bøgholm and Henrik Kragh-Hansen and Henrik Kragh-Hansen and Petur Olsen and Petur Olsen and Bent Thomsen and Bent Thomsen and Kim G. Larsen and Kim Guldstrand Larsen},doi={10.1145/1434790.1434807},pmid={null},pmcid={null},mag_id={2067306975},journal={null},abstract={In this paper, we present a novel approach to schedulability analysis of Safety Critical Hard Real-Time Java programs. The approach is based on a translation of programs, written in the Safety Critical Java profile introduced in [21] for the Java Optimized Processor [18], to timed automata models verifiable by the Uppaal model checker [23]. Schedulability analysis is reduced to a simple reachability question, checking for deadlock freedom. Model-based schedulability analysis has been developed by Amnell et al. [2], but has so far only been applied to high level specifications, not actual implementations in a programming language. Experiments show that model-based schedulability analysis can result in a more accurate analysis than possible with traditional approaches, thus systems deemed non-schedulable by traditional approaches may in fact be schedulable, as detected by our analysis.   Our approach has been implemented in a tool, named SARTS, successfully used to verify the schedulability of a real-time sorting machine consisting of two periodic and two sporadic tasks. SARTS has also been applied on a number of smaller examples to investigate properties of our approach.}}
@ARTICLE{Núñez_2003,title={Algebraic theory of probabilistic processes},year={2003},author={Manuel Núñez and Manuel Núñez},doi={10.1016/s1567-8326(02)00069-3},pmid={null},pmcid={null},mag_id={2067899880},journal={The Journal of Logic and Algebraic Programming},abstract={Abstract   In this paper we extend de Nicola and Hennessy’s testing theory to deal with probabilities. We say that two processes are testing equivalent if the probabilities with which they pass any test are equal. We present three alternative semantic views of our testing equivalence. First, we introduce adequate extensions of acceptance sets (inducing an operational characterization) and acceptance trees (inducing a denotational semantics). We also present a sound and complete axiomatization of our testing equivalence. So, this paper represents a complete study of the adaptation of the classical testing theory for probabilistic processes.}}
@ARTICLE{Fähndrich_2007,title={Establishing object invariants with delayed types},year={2007},author={Manuel Fähndrich and Manuel Fähndrich and Songtao Xia and Songtao Xia},doi={10.1145/1297027.1297052},pmid={null},pmcid={null},mag_id={2067901725},journal={null},abstract={Mainstream object-oriented languages such as C# and Java provide an initialization model for objects that does not guarantee programmer controlled initialization of fields. Instead, all fields are initialized to default values (0 for scalars and null for non-scalars) on allocation. This is in stark contrast to functional languages, where all parts of an allocation are initialized to programmer-provided values. These choices have a direct impact on two main issues: 1) the prevalence of null in object oriented languages (and its general absence in functional languages), and 2) the ability to initialize circular data structures. This paper explores connections between these differing approaches and proposes a fresh look at initialization. Delayed types are introduced to express and formalize prevalent initialization patterns in object-oriented languages.}}
@ARTICLE{Jensen_1986,title={Matrix scaling of subjective probabilities of economic forecasts},year={1986},author={Robert E. Jensen and Robert E. Jensen and Roger W. Spencer and Roger W. Spencer},doi={10.1016/0165-1765(86)90027-3},pmid={null},pmcid={null},mag_id={2068584570},journal={Economics Letters},abstract={null}}
@ARTICLE{Apel_2005,title={FeatureC++: on the symbiosis of feature-oriented and aspect-oriented programming},year={2005},author={Sven Apel and Sven Apel and Thomas Leich and Thomas Leich and Marko Rosenmüller and Marko Rosenmüller and Gunter Saake and Gunter Saake},doi={10.1007/11561347_10},pmid={null},pmcid={null},mag_id={2071631718},journal={null},abstract={This paper presents FeatureC++, a novel language extension to C++ that supports Feature-Oriented Programming (FOP) and Aspect-Oriented Programming (AOP). Besides well-known concepts of FOP languages, FeatureC++ contributes several novel FOP language features, in particular multiple inheritance and templates for generic programming. Furthermore, FeatureC++ solves several problems regarding incremental software development by adopting AOP concepts. Starting our considerations on solving these problems, we give a summary of drawbacks and weaknesses of current FOP languages in expressing incremental refinements. Specifically, we outline five key problems and present three approaches to solve them: Multi Mixins, Aspectual Mixin Layers, and Aspectual Mixins that adopt AOP concepts in different ways. We use FeatureC++ as a representative FOP language to explain these three approaches. Finally, we present a case study to clarify the benefits of FeatureC++ and its AOP extensions.}}
@ARTICLE{Herber_2013,title={A HW/SW co-verification framework for SystemC},year={2013},author={Paula Herber and Paula Herber and Sabine Glesner and Sabine Glesner},doi={10.1145/2435227.2435257},pmid={null},pmcid={null},mag_id={2072922398},journal={ACM Transactions in Embedded Computing Systems},abstract={SystemC is widely used for modeling and simulation in hardware/software co-design. However, existing verification techniques are mostly ad-hoc and non-systematic. In this article, we present a systematic, comprehensive, and formally founded co-verification framework for digital HW/SW systems that are modeled in SystemC. The framework is based on a formal semantics of SystemC and uses a combination of model checking and testing, whereby testing includes both the automated generation of timed inputs and automated conformance evaluation. We demonstrate its performance and its error detecting capability with two case studies, namely a packet switch and an anti-slip regulation and anti-lock braking system.}}
@ARTICLE{Hennessy_2002,title={Information flow vs. resource access in the asynchronous pi-calculus},year={2002},author={Matthew Hennessy and Matthew Hennessy and James Riely and James Riely},doi={10.1145/570886.570890},pmid={null},pmcid={null},mag_id={2074877666},journal={ACM Transactions on Programming Languages and Systems},abstract={We propose an extension of the asynchronous π-calculus in which a variety of security properties may be captured using types. These are an extension of the input/output types for the π-calculus in which I/O capabilities are assigned specific security levels. The main innovation is a uniform typing system that, by varying slightly the allowed set of types, captures different notions of security.We first define a typing system that ensures that processes running at security level σ cannot access resources with a security level higher than σ. The notion of access control guaranteed by this system is formalized in terms of a Type Safety Theorem.We then show that, by restricting the allowed types, our system prohibits implicit information flow from high-level to low-level processes. We prove that low-level behavior can not be influenced by changes to high-level behavior. This is formalized as a noninterference theorem with respect to may testing.}}
@ARTICLE{Diev_2006,title={Software estimation in the maintenance context},year={2006},author={Sergey Diev and Sergey Diev},doi={10.1145/1118537.1118540},pmid={null},pmcid={null},mag_id={2075284509},journal={ACM Sigsoft Software Engineering Notes},abstract={This article describes an extension of the Use Case Points method of software estimation. The main goal of this extension, called UCPm, is to reflect the specifics of the maintenance phase of software life cycle. UCPm takes into consideration the complexity of the base system. Then, UCPm does not consider the environmental factor as size-contributing entity and defines product size only via unadjusted use case points and technical factor. UCPm also applies four technical factors at the use case level, rather than at the level of the overall product.The method has been applied to more than 30 projects in the course of work on achieving CMM Level 4. It was found that even when requirements are not produced in the use case style, it is relatively easy to build a use case model for the purpose of estimation. It is also believed that the relatively high level of UCP/UCPm reduces the amount of work on estimation. In our preliminary estimates, one use case point maps to approximately four function points.}}
@ARTICLE{Wolper_1981,title={Temporal logic can be more expressive},year={1981},author={Pierre Wolper and Pierre Wolper},doi={10.1109/sfcs.1981.44},pmid={null},pmcid={null},mag_id={2075920544},journal={null},abstract={We start by proving that some properties of sequences are not expressible in Temporal Logic though they are expressible using for instance regular expressions. Then, we show how Temporal Logic can be extended to express any such property definable by a right-linear grammar and hence a regular expression, Finally, we give a decision procedure and complete axiomatization for the extended Temporal Logic.}}
@ARTICLE{Das_2005,title={A practical and fast iterative algorithm for φ-function computation using DJ graphs},year={2005},author={Dibyendu Das and Dibyendu Das and U. Ramakrishna and U. Ramakrishna},doi={10.1145/1065887.1065890},pmid={null},pmcid={null},mag_id={2076015364},journal={ACM Transactions on Programming Languages and Systems},abstract={We present a new and practical method of computing φ-function for all variables in a function for Static Single Assignment (SSA) form. The new algorithm is based on computing the Merge set of each node in the control flow graph of a function (a node here represents a basic block and the terms will be used interchangeably). Merge set of a node n is the set of nodes N, where φ-functions may need to be placed if variables are defined in n. It is not necessary for n to have a definition of a variable in it. Thus, the merge set of n is dictated by the underlying structure of the CFG. The new method presented here precomputes the merge set of every node in the CFG using an iterative approach. Later, these merge sets are used to carry out the actual φ-function placement. The advantages are in examples where dense definitions of variables are present (i.e., original definitions of variables---user defined or otherwise, in a majority of basic blocks). Our experience with SSA in the High Level Optimizer (optimization levels pO3/pO4) shows that most examples from the Spec2000 benchmark suite require a high percentage of basic blocks to have their φ points computed. Previous methods of computing the same relied on the dominance frontier (DF) concept, first introduced by Cytron et al. The method presented in this paper gives a new effective iterative solution to the problem. Also, in cases, where the control flow graph does not change, our method does not require any additional computation for new definitions introduced as part of optimizations. We present implementation details with results from Spec2000 benchmarks. Our algorithm runs faster than the existing methods used.}}
@ARTICLE{Cimatti_2013,title={Software Model Checking SystemC},year={2013},author={Alessandro Cimatti and Alessandro Cimatti and Iman Narasamdya and Iman Narasamdya and Marco Roveri and Marco Roveri},doi={10.1109/tcad.2012.2232351},pmid={null},pmcid={null},mag_id={2076600865},journal={IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},abstract={SystemC is an increasingly used language for writing executable specifications of systems-on-chip. The verification of SystemC, however, is a very difficult challenge. Simulation features great scalability, but can miss important defects. On the other hand, formal verification of SystemC is extremely hard because of the presence of threads, and the intricacies of the communication and scheduling mechanisms. In this paper, we explore formal verification for SystemC by means of software model checking techniques, which have demonstrated substantial progress in recent years. We propose an accurate model of SystemC and three complementary encodings of SystemC to finite-state processes, sequential and threaded programming models. We implement the proposed approaches in a tool chain and carry out a thorough experimental evaluation using several benchmarks taken from the literature on SystemC verification, and experimenting with different state-of-the-art software model checkers. The results clearly show the applicability and efficiency of the proposed approaches. In particular, the results show the effectiveness of the threaded and of the finite-model encodings to prove and disprove properties, respectively.}}
@ARTICLE{Saaty_1984,title={The legitimacy of rank reversal},year={1984},author={Thomas L. Saaty and Thomas L. Saaty and Luis G. Vargas and Luis G. Vargas},doi={10.1016/0305-0483(84)90052-5},pmid={null},pmcid={null},mag_id={2076959426},journal={Omega-international Journal of Management Science},abstract={null}}
@ARTICLE{Ricci_2007,title={Give agents their artifacts: the A&A approach for engineering working environments in MAS},year={2007},author={Alessandro Ricci and Alessandro Ricci and Mirko Viroli and Mirko Viroli and Andrea Omicini and Andrea Omicini},doi={10.1145/1329125.1329308},pmid={null},pmcid={null},mag_id={2077904619},journal={null},abstract={In human society, almost any cooperative working context accounts for different kinds of object, tool, artifacts in general, that humans adopt, share and intelligently exploit so as to support their working activities, in particular social ones. According to theories in human sciences--Activity Theory and Distributed Cognition are two main examples [5, 4]--and to related disciplines in computer science--such as Computer Supported Cooperative Work (CSCW) and Human-Computer Interaction (HCI)--such entities have a key role in determining the success or failure of the activities, playing an essential function in simplifying complex tasks and--more generally--in designing solutions that scale with activity complexity. Such a perspective can be found also in some works in the context of Distributed Artificial Intelligence [1, 2].   Analogously to the human case, we claim that also (cognitive) multiagent systems (MAS) could greatly benefit from the definition and systematic exploitation of a suitable notion of working environment, composed by different sorts of artifacts, dynamically constructed, shared and used by agents to support their working activities.   Along this line, in this paper first we introduce a conceptual framework called AA then, we provide a brief overview of the basic technologies that support such an approach, CARTAGO in particular--a Java-based framework for engineering working environments to be integrated with heterogeneous agent platforms.   Such a perspective is strenghtened by recent efforts in AOSE (Agent-Oriented Software Engineering) that remark the fundamental role of the environment for the engineering of MAS [8]. The AA (ii) cognitive---analogous to designed environment in human society, the properties of such environment abstractions should be conceived to be suitably and effectively exploited by cognitive agents, as intelligent constructors / users / manipulators of the environment.   The work presented in this paper generalises and extends our previous work focussed on coordination artifacts presented at [6], and more recent works about the notion of artifact [7].}}
@ARTICLE{Kristoffersen_2003,title={Runtime Verification of Timed LTL using Disjunctive Normalized Equation Systems},year={2003},author={K. Kristoffersen and Kåre J. Kristoffersen and Christian Roed Pedersen and Christian Pedersen and C. Pedersen and Henrik Reif Andersen and Henrik Reif Andersen},doi={10.1016/s1571-0661(04)81050-8},pmid={null},pmcid={null},mag_id={2078282392},journal={Electronic Notes in Theoretical Computer Science},abstract={Abstract   In this paper we present a new framework for runtime verification of properties of real time systems such as financial systems or backend databases. Such a systems has a semantics which resemples that of timed traces, namely a sequence of states where each state consists of predicates true in this state and then a timestamp explaining when the state is valid. We present a logic, LTLt, which is an extension of LTL with time constraints and a freeze quantifier and show how formulae in this logic are able to express properties of bounded liveness and safety which are ideal for these systems. It is shown how a formula in LTLt may be rewritten to a certain disjunctive normal form suitable for checking a real time system at runtime. The normal form captures the essential part of runtime verification by a set of mutually defined formula identifiers, each expressing two things: What should hold now and which formula identifiers that will need to hold in the next state. As part of the theoretical foundation for this work we propose a characterization of Runtime Verification and address the challenges in developing a method which is both sound and complete while at the same time efficient.}}
@ARTICLE{Bryant_1986,title={Graph-Based Algorithms for Boolean Function Manipulation},year={1986},author={Bryant and Bryant},doi={10.1109/tc.1986.1676819},pmid={null},pmcid={null},mag_id={2080267935},journal={IEEE Transactions on Computers},abstract={In this paper we present a new data structure for representing Boolean functions and an associated set of manipulation algorithms. Functions are represented by directed, acyclic graphs in a manner similar to the representations introduced by Lee [1] and Akers [2], but with further restrictions on the ordering of decision variables in the graph. Although a function requires, in the worst case, a graph of size exponential in the number of arguments, many of the functions encountered in typical applications have a more reasonable representation. Our algorithms have time complexity proportional to the sizes of the graphs being operated on, and hence are quite efficient as long as the graphs do not grow too large. We present experimental results from applying these algorithms to problems in logic design verification that demonstrate the practicality of our approach.}}
@ARTICLE{Chen_2006,title={Automatic test case generation for UML activity diagrams},year={2006},author={Mingsong Chen and Chen Mingsong and Xiaokang Qiu and Qiu Xiaokang and Xuandong Li and Li Xuandong},doi={10.1145/1138929.1138931},pmid={null},pmcid={null},mag_id={2080375059},journal={null},abstract={The test case generation from design specifications is an important work in testing phase. In this paper, we use UML activity diagrams as design specifications, and present an automatic test case generation approach. The approach first randomly generates abundant test cases for a JAVA program under testing. Then, by running the program with the generated test cases, we can get the corresponding program execution traces. Last, by comparing these traces with the given activity diagram according to the specific coverage criteria, we can get a reduced test case set which meets the test adequacy criteria. The approachcan also be used to check the consistency between the program execution traces and the behavior of UML activity diagrams.}}
@ARTICLE{Mokadem_2010,title={Verification of a Timed Multitask System With Uppaal},year={2010},author={Houda Bel Mokadem and H Bel Mokadem and Béatrice Bérard and Béatrice Bérard and Béatrice Bérard and Vincent Gourcuff and V. Gourcuff and Olivier de Smet and O. De Smet and Jean-Marc Roussel and Jean-Marc Roussel},doi={10.1109/tase.2010.2050199},pmid={null},pmcid={null},mag_id={2081306301},journal={IEEE Transactions on Automation Science and Engineering},abstract={System and program verification has been a large area of research since the introduction of computers in industrial systems. It is an especially important issue for critical systems, where errors can cause human and financial damages. Programmable Logic Controllers (PLCs) are now widely used in many industrial systems and verification of the corresponding programs has already been studied in various contexts for a few years, for the benefit of users and system designers. First restricted to an untimed setting, verification was recently extended to systems where quantitative constraints are needed, possibly related to time elapsing. For instance, timed features like TON (Timers ON delay), used in PLC programs, were modeled with timed automata, thus increasing the size of the verification problems addressed. In this framework, we propose the modeling and verification of a particular timed multitask PLC program, which is part of the so-called MSS (Mecatronic Standard System) platform from Bosch Group. In this case study, time aspects are combined with multitask programming, which raises questions related to the reaction time between the detection of a signal and the resulting event. Our model for station 2 of the MSS platform is a network of timed automata, including automata for the operative part and for the control program, which is first described in SFC then translated in Ladder Diagram. This model is constrained with atomicity hypotheses concerning program execution, and model checking of a reaction time property is performed with the tool UPPAAL.}}
@ARTICLE{AbouTrab_2012,title={Specification Mutation Analysis for Validating Timed Testing Approaches Based on Timed Automata},year={2012},author={Mohammad Saeed AbouTrab and M. Saeed AbouTrab and Steve Counsell and Steve Counsell and Robert M. Hierons and Robert M. Hierons},doi={10.1109/compsac.2012.93},pmid={null},pmcid={null},mag_id={2083623672},journal={null},abstract={Testing real-time systems is a non-trivial validation task, especially after adding time as a new dimension to its complexity. In previous research, we introduced a 'priority-based' approach which tested the logical and timing behaviour of real-time systems modelled formally as UPPAAL Timed Automata (UTA). In this paper, we validate the 'priority-based' approach with a comparison to four well-known timed testing approaches based on a Timed Automata (TA) formalism using Specification Mutation Analysis (SMA). We introduce a set of timed and functional mutation operators based on TA. Three case studies are used to run the mutation analysis and mutants are generated according to the proposed mutation operators. The effectiveness of timed testing approaches are determined and contrasted according to the mutation score; we show that our testing approach achieves high mutation adequacy score when compared with others.}}
@ARTICLE{Alur_1999,title={Event-clock automata: a determinizable class of timed automata},year={1999},author={Rajeev Alur and Rajeev Alur and Limor Fix and Limor Fix and Thomas A. Henzinger and Thomas A. Henzinger},doi={10.1016/s0304-3975(97)00173-4},pmid={null},pmcid={null},mag_id={2083964867},journal={Theoretical Computer Science},abstract={null}}
@ARTICLE{Kiniry_2006,title={Soundness and completeness warnings in ESC/Java2},year={2006},author={Joseph R. Kiniry and Joseph R. Kiniry and Alan E. Morkan and Alan E. Morkan and Barry Denby and Barry Denby},doi={10.1145/1181195.1181200},pmid={null},pmcid={null},mag_id={2084963982},journal={null},abstract={Usability is a key concern in the development of verification tools. In this paper, we present an usability extension for the verification tool ESC/Java2. This enhancement is not achieved through extensions to the underlying logic or calculi of ESC/Java2, but instead we focus on its human interface facets. User awareness of the soundness and completeness of the tool is vitally important in the verification process, and lack of information about such is one of the most requested features from ESC/Java2 users, and a primary complaint from ESC/Java2 critics. Areas of unsoundness and incompleteness of ESC/Java2 exist at three levels: the level of the underlying logic; the level of translation of program constructs into verification conditions; and at the level of the theorem prover. The user must be made aware of these issues for each particular part of the source code analysed in order to have confidence in the verification process. Our extension to ESC/Java2 provides clear warnings to the user when unsound or incomplete reasoning may be taking place.}}
@ARTICLE{Jovanović_2006,title={Pixy: a static analysis tool for detecting Web application vulnerabilities},year={2006},author={Nenad Jovanović and Nenad Jovanovic and Christopher Kruegel and Christopher Kruegel and Engin Kirda and Engin Kirda},doi={10.1109/sp.2006.29},pmid={null},pmcid={null},mag_id={2085925880},journal={null},abstract={The number and the importance of Web applications have increased rapidly over the last years. At the same time, the quantity and impact of security vulnerabilities in such applications have grown as well. Since manual code reviews are time-consuming, error-prone and costly, the need for automated solutions has become evident. In this paper, we address the problem of vulnerable Web applications by means of static source code analysis. More precisely, we use flow-sensitive, interprocedural and context-sensitive dataflow analysis to discover vulnerable points in a program. In addition, alias and literal analysis are employed to improve the correctness and precision of the results. The presented concepts are targeted at the general class of taint-style vulnerabilities and can be applied to the detection of vulnerability types such as SQL injection, cross-site scripting, or command injection. Pixy, the open source prototype implementation of our concepts, is targeted at detecting cross-site scripting vulnerabilities in PHP scripts. Using our tool, we discovered and reported 15 previously unknown vulnerabilities in three Web applications, and reconstructed 36 known vulnerabilities in three other Web applications. The observed false positive rate is at around 50% (i.e., one false positive for each vulnerability) and therefore, low enough to permit effective security audits.}}
@ARTICLE{Ramanathan_1995,title={Using AHP for resource allocation problems},year={1995},author={Ramakrishnan Ramanathan and Ramakrishnan Ramanathan and L.S. Ganesh and L.S. Ganesh},doi={10.1016/0377-2217(93)e0240-x},pmid={null},pmcid={null},mag_id={2086167974},journal={European Journal of Operational Research},abstract={null}}
@ARTICLE{Offutt_1987,title={A Fortran 77 interpreter for mutation analysis},year={1987},author={A. Jefferson Offutt and A. J. Offutt and K. N. King and K. N. King},doi={10.1145/960114.29669},pmid={null},pmcid={null},mag_id={2086239197},journal={null},abstract={Mutation analysis is a powerful technique for testing software systems. In the Mothra project, conducted at Georgia Tech's Software Engineering Research Center, mutation analysis is used as a basis for building an integrated software testing environment. Mutation analysis requires the execution of many slightly differing versions of the same program to evaluate the quality of the data used to test the program. In the current version of the Mothra system, a program to be tested is translated to intermediate code, where it and its mutated versions are executed by an interpreter.In this paper, we discuss some of the unique requirements of an interpreter used in a mutation-based testing environment. We then describe how these requirements affected the design and implementation of the Fortran 77 version of the Mothra interpreter. Other topics covered include the architecture of the interpreter and many of the design elements that it incorporates. We also describe the intermediate language used by Mothra and the features of the interpreter that are needed for software testing.}}
@ARTICLE{Tofte_1997,title={Region-based memory management},year={1997},author={Mads Tofte and Mads Tofte and Jean-Pierre Talpin and Jean-Pierre Talpin and Jean-Pierre Talpin},doi={10.1006/inco.1996.2613},pmid={null},pmcid={null},mag_id={2087875803},journal={Information & Computation},abstract={This paper describes a memory management discipline for programs that perform dynamic memory allocation and de-allocation. At runtime, all values are put intoregions. The store consists of a stack of regions. All points of region allocation and de-allocation are inferred automatically, using a type and effect based program analysis. The scheme does not assume the presence of a garbage collector. The scheme was first presented in 1994 (M. Tofte and J.-P. Talpin,in“Proceedings of the 21st ACM SIGPLAN?SIGACT Symposium on Principles of Programming Languages,” pp. 188?201); subsequently, it has been tested in The ML Kit with Regions, a region-based, garbage-collection free implementation of the Standard ML Core language, which includes recursive datatypes, higher-order functions and updatable references L. Birkedal, M. Tofte, and M. Vejlstrup, (1996),in“Proceedings of the 23 rd ACM SIGPLAN?SIGACT Symposium on Principles of Programming Languages,” pp. 171?183. This paper defines a region-based dynamic semantics for a skeletal programming language extracted from Standard ML. We present the inference system which specifies where regions can be allocated and de-allocated and a detailed proof that the system is sound with respect to a standard semantics. We conclude by giving some advice on how to write programs that run well on a stack of regions, based on practical experience with the ML Kit.}}
@ARTICLE{Merayo_2007,title={Formal testing of systems presenting soft and hard deadlines},year={2007},author={Mercedes G. Merayo and Mercedes G. Merayo and Manuel Núñez and Manuel Núñez and Ismael Rodrguez and Ismael Rodrguez},doi={10.1007/978-3-540-75698-9_11},pmid={null},pmcid={null},mag_id={2087901322},journal={null},abstract={We present a formal framework to specify and test systems presenting both soft and hard deadlines. While hard deadlines must be always met on time, soft deadlines can be sometimes met in a different time, usually higher, from the specified one. It is this characteristic (to formally define sometimes) what produces several reasonable alternatives to define appropriate implementation relations, that is, relations to decide wether an implementation is correct with respect to a specification. In addition to introduce these relations, we define a testing framework to test implementations.}}
@ARTICLE{Spoto_2011,title={Precise null-pointer analysis},year={2011},author={Fausto Spoto and Fausto Spoto},doi={10.1007/s10270-009-0132-5},pmid={null},pmcid={null},mag_id={2088238746},journal={Software and Systems Modeling},abstract={In Java, C or C++, attempts to dereference the null value result in an exception or a segmentation fault. Hence, it is important to identify those program points where this undesired behaviour might occur or prove the other program points (and possibly the entire program) safe. To that purpose, null-pointer analysis of computer programs checks or infers non-null annotations for variables and object fields. With few notable exceptions, null-pointer analyses currently use run-time checks or are incorrect or only verify manually provided annotations. In this paper, we use abstract interpretation to build and prove correct a first, flow and context-sensitive static null-pointer analysis for Java bytecode (and hence Java) which infers non-null annotations. It is based on Boolean formulas, implemented with binary decision diagrams. For better precision, it identifies instance or static fields that remain always non-null after being initialised. Our experiments show this analysis faster and more precise than the correct null-pointer analysis by Hubert, Jensen and Pichardie. Moreover, our analysis deals with exceptions, which is not the case of most others; its formulation is theoretically clean and its implementation strong and scalable. We subsequently improve that analysis by using local reasoning about fields that are not always non-null, but happen to hold a non-null value when they are accessed. This is a frequent situation, since programmers typically check a field for non-nullness before its access. We conclude with an example of use of our analyses to infer null-pointer annotations which are more precise than those that other inference tools can achieve.}}
@ARTICLE{Kang_2002,title={Feature-oriented product line engineering},year={2002},author={Kyo C. Kang and Kyo C. Kang and Jaejoon Lee and Jaejoon Lee and Patrick Donohoe and Patrick Donohoe},doi={10.1109/ms.2002.1020288},pmid={null},pmcid={null},mag_id={2090403712},journal={IEEE Software},abstract={The feature-oriented reuse method analyzes and models a product line's commonalities and differences in terms of product features and uses the analysis results to develop architectures and components. The article illustrates, with a home integration system example, how FORM brings efficiency into product line development.}}
@ARTICLE{Prenninger_2005,title={Abstractions for Model-Based Testing},year={2005},author={Wolfgang Prenninger and Wolfgang Prenninger and Alexander Pretschner and Alexander Pretschner},doi={10.1016/j.entcs.2004.02.086},pmid={null},pmcid={null},mag_id={2092422674},journal={Electronic Notes in Theoretical Computer Science},abstract={The idea of model-based testing is to compare the I/O behavior of an explicit behavior model with that of a system under test. This requires the model to be valid. If the model is a simplification of the SUT, then it is easier to check the model and use it for subsequent test case generation than to directly check the SUT. In this case, the different levels of abstraction must be bridged. Not surprisingly, experience shows that choosing the right level of abstraction is crucial to the success of model-based testing. We argue that models for specification purposes, models for test generation, and models for full code generation are likely to be different. The paper classifies and discusses different abstractions. It is intended as a step towards guidelines for those who build behavior models to the end of testing.}}
@ARTICLE{Winther_2011,title={Guarded type promotion: eliminating redundant casts in Java},year={2011},author={Johnni Winther and Johnni Winther},doi={10.1145/2076674.2076680},pmid={null},pmcid={null},mag_id={2092718098},journal={null},abstract={In Java, explicit casts are ubiquitous since they bridge the gap between compile-time and runtime type safety. Since casts potentially throw a ClassCastException, many programmers use a defensive programming style of guarded casts. In this programming style casts are protected by a preceding conditional using the instanceof operator and thus the cast type is redundantly mentioned twice. We propose a new typing rule for Java called Guarded Type Promotion aimed at eliminating the need for the explicit casts when guarded. This new typing rule is backward compatible and has been fully implemented in a Java 6 compiler. Through our extensive testing of real-life code we show that guarded casts account for approximately one fourth of all casts and that Guarded Type Promotion can eliminate the need for 95 percent of these guarded casts.}}
@ARTICLE{Robinson-Mallett_2006,title={Achieving communication coverage in testing},year={2006},author={Christopher Robinson-Mallett and Christopher Robinson-Mallett and Robert M. Hierons and Robert M. Hierons and Peter Liggesmeyer and Peter Liggesmeyer},doi={10.1145/1218776.1218786},pmid={null},pmcid={null},mag_id={2093996328},journal={ACM Sigsoft Software Engineering Notes},abstract={This paper considers the problem of testing the communication between components of a timed distributed software system. We assume that communication is specified using timed interface automata and use computational tree logic (CTL) to define coverage criteria that refer to send- and receive-statements and communication paths. Given such a state-based specification of a distributed system and a concrete coverage goal, a model checker is used in order to determine the coverage provided by a finite set of test-cases, expressed using sequence diagrams. If parts of the specification remain uncovered then a goal is derived so that the model checker can be used to generate test cases that increase the coverage provided by the test suite. A major benefit of the presented approach is the generation of a potentially minimal set of test cases with the confidence that every interaction between components is executed during testing. A potential additional benefit of this approach is that it provides a visual description of the state based testing of distributed systems, which may be beneficial in other contexts such as education and program comprehension. The complexity of our approach strongly depends on the input model, the testing goal, and the model checking algorithm, which is implemented in the used tool. While a particular model checker, UPPAAL, was used, it should be relatively straightforward to adapt the approach for use with other CTL based model checkers.}}
@ARTICLE{Leavens_2006,title={Preliminary design of JML: a behavioral interface specification language for java},year={2006},author={Gary T. Leavens and Gary T. Leavens and Albert L. Baker and Albert L. Baker and Clyde Ruby and Clyde Ruby},doi={10.1145/1127878.1127884},pmid={null},pmcid={null},mag_id={2094160561},journal={ACM Sigsoft Software Engineering Notes},abstract={JML is a behavioral interface specification language tailored to Java(TM). Besides pre- and postconditions, it also allows assertions to be intermixed with Java code; these aid verification and debugging. JML is designed to be used by working software engineers; to do this it follows Eiffel in using Java expressions in assertions. JML combines this idea from Eiffel with the model-based approach to specifications, typified by VDM and Larch, which results in greater expressiveness. Other expressiveness advantages over Eiffel include quantifiers, specification-only variables, and frame conditions.This paper discusses the goals of JML, the overall approach, and describes the basic features of the language through examples. It is intended for readers who have some familiarity with both Java and behavioral specification using pre- and postconditions.}}
@ARTICLE{Chen_2006,title={Verification approach of metropolis design framework for embedded systems},year={2006},author={Xi Chen and Xi Chen and Harry Hsieh and Harry Hsieh and Felice Balarin and Felice Balarin},doi={10.1007/s10766-005-0002-x},pmid={null},pmcid={null},mag_id={2094686007},journal={International Journal of Parallel Programming},abstract={In this paper, we focus on the verification approach of Metropolis, an integrated design framework for heterogeneous embedded systems. The verification approach is based on the formal properties specified in Linear Temporal Logic (LTL) or Logic of Constraints (LOC). Designs may be refined due to synthesis or be abstracted for verification. An automatic abstraction propagation algorithm is used to simplify the design for specific properties. A user-defined starting point may also be used with automatic propagation. Two main verification techniques are implemented in Metropolis: the formal verification utilizing the model checker Spin and the simulation trace checking with automatic generated checkers. Translation algorithms from specification models to verification models, as well as algorithms of generated checkers are discussed. We use several case studies to demonstrate our approach for verification of system level designs at multiple levels of abstraction.}}
@ARTICLE{Saltzer_1975,title={The protection of information in computer systems},year={1975},author={Jerome H. Saltzer and Michael D. Schroeder},doi={10.1109/proc.1975.9939},pmid={null},pmcid={null},mag_id={2095881341},journal={null},abstract={This tutorial paper explores the mechanics of protecting computer-stored information from unauthorized use or modification. It concentrates on those architectural structures-whether hardware or software-that are necessary to support information protection. The paper develops in three main sections. Section I describes desired functions, design principles, and examples of elementary protection and authentication mechanisms. Any reader familiar with computers should find the first section to be reasonably accessible. Section II requires some familiarity with descriptor-based computer architecture. It examines in depth the principles of modern protection architectures and the relation between capability systems and access control list systems, and ends with a brief analysts of protected subsystems and protected objects. The reader who is dismayed by either the prerequisites or the level of detail in the second section may wish to skip to Section III, which reviews the state of the art and current research projects and provides suggestions for further reading.}}
@ARTICLE{Kemmerer_1985,title={Testing Formal Specifications to Detect Design Errors},year={1985},author={Richard A. Kemmerer and R.A. Kemmerer and Richard A. Kemmerer},doi={10.1109/tse.1985.231535},pmid={null},pmcid={null},mag_id={2096536569},journal={IEEE Transactions on Software Engineering},abstract={Formal specification and verification techniques are now apused to increase the reliability of software systems. However, these proaches sometimes result in specifying systems that cannot be realized or that are not usable. This paper demonstrates why it is necessary to test specifications early in the software life cycle to guarantee a system that meets its critical requirements and that also provides the desired functionality. Definitions to provide the framework for classifying the validity of a functional requirement with respect to a formal specification tion are also introduced. Finally, the design of two tools for testing formal specifications is discussed.}}
@ARTICLE{Vieira_2009,title={Using web security scanners to detect vulnerabilities in web services},year={2009},author={Marco Vieira and Marco Vieira and Nuno Antunes and Nuno Antunes and Henrique Madeira and Henrique Madeira},doi={10.1109/dsn.2009.5270294},pmid={null},pmcid={null},mag_id={2096791889},journal={null},abstract={Although web services are becoming business-critical components, they are often deployed with critical software bugs that can be maliciously explored. Web vulnerability scanners allow detecting security vulnerabilities in web services by stressing the service from the point of view of an attacker. However, research and practice show that different scanners have different performance on vulnerabilities detection. In this paper we present an experimental evaluation of security vulnerabilities in 300 publicly available web services. Four well known vulnerability scanners have been used to identify security flaws in web services implementations. A large number of vulnerabilities has been observed, which confirms that many services are deployed without proper security testing. Additionally, the differences in the vulnerabilities detected and the high number of false-positives (35% and 40% in two cases) and low coverage (less than 20% for two of the scanners) observed highlight the limitations of web vulnerability scanners on detecting security vulnerabilities in web services.}}
@ARTICLE{Lee_2002,title={A formal approach for passive testing of protocol data portions},year={2002},author={D. Lee and David Lee and Dongluo Chen and Dongluo Chen and Rujiang Hao and Ruibing Hao and Raymond E. Miller and R.E. Miller and Jianping Wu and Jianping Wu and Jianping Wu and Jianping Wu and Xin Yin and Xia Yin},doi={10.1109/icnp.2002.1181393},pmid={null},pmcid={null},mag_id={2096900538},journal={null},abstract={Passive testing is a process of detecting faults in a system under test by passively observing its input/output behaviors only without interrupting its normal operations, and proves to be a promising technique for network fault management. We study passive testing of data portions of network protocols and present two algorithms, using an event-driven extended finite state machine model. Experimental results on the Internet routing protocol OSPF (open shortest path first) are reported.}}
@ARTICLE{Adjir_2009,title={Time-Optimal Real-Time Test Case Generation Using Prioritized Time Petri Nets},year={2009},author={Noureddine Adjir and Noureddine Adjir and Noureddine Adjir and Pierre de Saqui-Sannes and Pierre de Saqui-Sannes and Mustapha Kamel Rahmouni and Mustapha Kamel Rahmouni and Mustapha Rahmouni and Mustapha Kamel Rahmouni},doi={10.1109/valid.2009.31},pmid={null},pmcid={null},mag_id={2096930234},journal={null},abstract={This paper discusses the application of modelchecking to test generation from specifications written inPrioritized Time Petri Nets. We demonstrate how toautomatically generate conformance test cases, specially timedtest cases with optimal execution time, using the model checkerselt and the path analysis tool plan of the TINA toolbox.Properties are described in the SE-LTL temporal logic. Testcases are generated using manually formulated test purposesor automatically from various coverage criteria of the model.}}
@ARTICLE{Yoon_1998,title={Mutation-based inter-class testing},year={1998},author={Hyung Chul Yoon and Hoijin Yoon and Byoungju Choi and Byoungju Choi and Jin-Ok Jeon and Jin-Ok Jeon},doi={10.1109/apsec.1998.733717},pmid={null},pmcid={null},mag_id={2097124029},journal={null},abstract={Faults and failures due to interaction are the bane of testers. Since their subtlety makes them difficult to recognize and even more difficult to reveal by testing, it is important to specify interaction between classes systematically. In this paper, we propose mutation-based inter-class testing technique. Our inter-class testing technique consists of two procedures: test item identification procedure and test case selection procedure. For the test item identification procedure, we develop the Inheritance-Call graph (ICgraph) to identify the taxonomy of interaction of public methods between classes. For the test case selection procedure, we design a new criterion, state-based mutation testing criterion (SMTC), by applying mutation analysis to the state diagram representing class behavior. Mutation analysis is a well-known method for measuring test case adequacy which involves the mutation of a program by the introduction of small syntactic change in the program. The implementation of our proposed technique on a sample program shows that our technique leads to a set of test cases which detect errors in inter-class relation.}}
@ARTICLE{Schneider_2003,title={Enforceable security policies},year={2003},author={Fred B. Schneider and Fred B. Schneider},doi={10.1109/fits.2003.1264930},pmid={null},pmcid={null},mag_id={2097368879},journal={null},abstract={A precise characterization is given for the class of security policies that can be enforced using mechanisms that work by monitoring system execution, and a class of automata is introduced for specifying those security policies. Techniques to enforce security policies specified by such automata are also discussed. READERS NOTE: A substantially revised version of this document is available at http://cs-tr.cs.cornell.edu:80/Dienst/UI/1.0/Display/ncstrl.cornell/TR99-1759}}
@ARTICLE{Eker_2003,title={The maude LTL model checker and its implementation},year={2003},author={Steven Eker and Steven Eker and José Meseguer and José Meseguer and Ambarish Sridharanarayanan and Ambarish Sridharanarayanan},doi={10.1007/3-540-44829-2_16},pmid={null},pmcid={null},mag_id={2097974511},journal={null},abstract={A model checker typically supports two different levels of specification: (1) a system specification level, in which the concurrent system to be analyzed is formalized; and (2) a property specification level, in which the properties to be model checked—for example, temporal logic formulae—are specified. The Maude LTL model checker has been designed with the goal of combining a very expressive and general system specification language (Maude [1]) with an advanced on-the-fly explicit-state LTL model checking engine.}}
@ARTICLE{Alves-Foss_2006,title={The MILS architecture for high-assurance embedded systems},year={2006},author={Jim Alves-Foss and Jim Alves-Foss and Paul Oman and Paul W. Oman and Carol A. Taylor and Carol Taylor and W. Scott Harrison and W. Scott Harrison},doi={10.1504/ijes.2006.014859},pmid={null},pmcid={null},mag_id={2098592421},journal={International Journal of Embedded Systems},abstract={High-assurance systems require a level of rigor, in both design and analysis, not typical of conventional systems. This paper provides an overview of the Multiple Independent Levels of Security and Safety (MILS) approach to high-assurance system design for security and safety critical embedded systems. MILS enables the development of a system using manageable units, each of which can be analysed separately, avoiding costly analysis required of more conventional designs. MILS is particularly well suited to embedded systems that must provide guaranteed safety or security properties.}}
@ARTICLE{Herber_2011,title={Transforming SystemC Transaction Level Models into UPPAAL timed automata},year={2011},author={Paula Herber and Paula Herber and Marcel Pockrandt and Marcel Pockrandt and Sabine Glesner and Sabine Glesner},doi={10.1109/memcod.2011.5970523},pmid={null},pmcid={null},mag_id={2099186927},journal={null},abstract={The SystemC Transaction Level Modeling (TLM) standard is widely used for modeling and simulation in hardware/software co-design. However, the semantics of the TLM core interfaces is only informally defined. This makes it impossible to apply formal verification techniques to transaction level models that conform to the TLM standard. To solve this problem, we propose a formal semantics of the TLM transport mechanisms using timed automata. We achieve this by providing a set of timed automata templates that precisely capture the semantics of the TLM core interfaces. Then, we use this set to transform a given SystemC-TLM model into a semantically equivalent timed automata model. The transformation is an extension of our previously proposed transformation from SystemC into Uppaal timed automata and can be used to verify safety, liveness, and timing properties of TLM models using the Uppaal model checker. We demonstrate the applicability and performance of our approach with two case studies, namely a loosely-timed model that uses a blocking transport and an approximately-timed model that uses a 4-phase non-blocking transport.}}
@ARTICLE{Grossi_2007,title={A formal road from institutional norms to organizational structures},year={2007},author={Davide Grossi and Davide Grossi and Frank Dignum and Frank Dignum and John‐Jules Ch. Meyer and John-Jules Ch. Meyer},doi={10.1145/1329125.1329234},pmid={null},pmcid={null},mag_id={2099251444},journal={null},abstract={Up to now, the way institutions and organizations have been used in the development of open systems has not often gone further than a useful heuristics. In order to develop systems actually implementing institutions and organizations, formal methods should take the place of heuristic ones. The paper presents a formal semantics for the notion of institution and its components (abstract and concrete norms, empowerment of agents, roles) and defines a formal relation between institutions and organizational structures. As a result, it is shown how institutional norms can be refined to constructs---organizational structures---which are closer to an implemented system. It is also shown how such a refinement process can be fully formalized and it is therefore amenable to rigorous verification.}}
@ARTICLE{Bouajjani_2007,title={Context-bounded analysis of multithreaded programs with dynamic linked structures},year={2007},author={Ahmed Bouajjani and Ahmed Bouajjani and Séverine Fratani and Séverine Fratani and Shaz Qadeer and Shaz Qadeer},doi={10.1007/978-3-540-73368-3_24},pmid={null},pmcid={null},mag_id={2099608328},journal={null},abstract={Bounded context switch reachability analysis is a useful and efficient approach for detecting bugs in multithreaded programs. In this paper, we address the application of this approach to the analysis of multithreaded programs with procedure calls and dynamic linked structures. We define a program semantics based on concurrent pushdown systems with visible heaps as stack symbols. A visible heap is the part of the heap reachable from global and local variables. We use pushdown analysis techniques to define an algorithm that explores the entire configuration space reachable under given bounds on the number of context switches and the size of visible heaps.}}
@ARTICLE{Sarikaya_1984,title={Synchronization and Specification Issues in Protocol Testing},year={1984},author={Behçet Sarikaya and Behçet Sarikaya and Behçet Sarikaya and Gregor von Bochmann and Gregor von Bochmann},doi={10.1109/tcom.1984.1096074},pmid={null},pmcid={null},mag_id={2099708429},journal={IEEE Transactions on Communications},abstract={Protocol testing for the purpose of certifying the implementation's adherence to the protocol specification can be done with a test architecture consisting of remote tester and local responder processes generating specific input stimuli, called test sequences, and observing the output produced by the implementation under test. It is possible to adapt test sequence generation techniques for finite state machines, such as transition tour, characterization, and checking sequence methods, to generate test sequences for protocols specified as incomplete finite state machines. For certain test sequences, the tester or responder processes are forced to consider the timing of an interaction in which they have not taken part; these test sequences are called nonsynchronizable. The three test sequence generation algorithms are modified to obtain synchronizable test sequences. The checking of a given protocol for intrinsic synchronization problems is also discussed. Complexities of synchronizable test sequence generation algorithms are given and complete testing of a protocol is shown to be infeasible. To extend the applicability of the characterization and checking sequences, different methods are proposed to enhance the protocol specifications: special test input interactions are defined and a methodology is developed to complete the protocol specifications.}}
@ARTICLE{Bérard_2005,title={Comparison of the expressiveness of timed automata and time petri nets},year={2005},author={Béatrice Bérard and Béatrice Bérard and Franck Cassez and Franck Cassez and Serge Haddad and Serge Haddad and Didier Lime and Didier Lime and Olivier H. Roux and Olivier Roux},doi={10.1007/11603009_17},pmid={null},pmcid={null},mag_id={2099924351},journal={null},abstract={In this paper we consider the model of Time Petri Nets (TPN) where time is associated with transitions. We also consider Timed Automata (TA) as defined by Alur & Dill, and compare the expressiveness of the two models w.r.t. timed language acceptance and (weak) timed bisimilarity. We first prove that there exists a TA $\mathcal{A}$s.t. there is no TPN (even unbounded) that is (weakly) timed bisimilar to $\mathcal{A}$. We then propose a structural translation from TA to (1-safe) TPNs preserving timed language acceptance. Further on, we prove that the previous (slightly extended) translation also preserves weak timed bisimilarity for a syntactical subclass $\mathcal{T}_{syn}(\leq,\geq)$of TA. For the theory of TPNs, the consequences are: 1) TA, bounded TPNs and 1-safe TPNs are equally expressive w.r.t. timed language acceptance; 2) TA are strictly more expressive than bounded TPNs w.r.t. timed bisimilarity; 3) The subclass $\mathcal{T}_{syn}(\leq,\geq)$, bounded and 1-safe TPNs “a la Merlin” are equally expressive w.r.t. timed bisimilarity.}}
@ARTICLE{Timo_2011,title={Test Selection for Data-Flow Reactive Systems Based on Observations},year={2011},author={Omer Nguena Timo and Omer Nguena-Timo and Antoine Rollet and Antoine Rollet},doi={10.1109/icstw.2011.71},pmid={null},pmcid={null},mag_id={2099940665},journal={null},abstract={Conformance testing amounts to verifying adequacy between the behaviors and the specified behaviors of an implementation. In this paper, we handle model-based conformance testing for data-flow critical systems with time constraints. Specifications are described with a formal model adapted for such systems and called Variable Driven Timed Automata (VDTA). VDTA are inspired by timed automata but they use input/output communication variables, allowing clear and short specifications. We present a conformance relation for this model and we propose a symbolic test selection algorithm based on a test purpose. The selection algorithm computes the variations on inputs allowing to reach an expected state of the implementation. Then we propose an on-line testing algorithm.}}
@ARTICLE{Dalal_1999,title={Model-based testing in practice},year={1999},author={Siddhartha R. Dalal and Siddhartha R. Dalal and Ashish Jain and Ashish Jain and N. Karunanithi and Nachimuthu Karunanithi and N. Karunanithi and J. M. Leaton and J. M. Leaton and Christopher M. Lott and C. M. Lott and C. M. Lott and G.C. Patton and Gardner Conde Patton and B. M. Horowitz and B. M. Horowitz},doi={10.1145/302405.302640},pmid={null},pmcid={null},mag_id={2100162598},journal={null},abstract={Model-based testing is a new and evolving technique for generating a suite of test cases from requirements. Testers using this approach concentrate on a data model and generation infrastructure instead of hand-crafting individual tests. Several relatively small studies have demonstrated how combinatorial test generation techniques allow testers to achieve broad coverage of the input domain with a small number of tests. We have conducted several relatively large projects in which we applied these techniques to systems with millions of lines of code. Given the complexity of testing, the model-based testing approach was used in conjunction with test automation harnesses. Since no large empirical study has been conducted to measure efficacy of this new approach, we report on our experience with developing tools and methods in support of model-based testing. The four case studies presented here offer details and results of applying combinatorial test-generation techniques on a large scale to diverse applications. Based on the four projects, we offer our insights into what works in practice and our thoughts about obstacles to transferring this technology into testing organizations.}}
@ARTICLE{Rowson_1997,title={Interface-based design},year={1997},author={Jim Rowson and James A. Rowson and Alberto Sangiovanni‐Vincentelli and Alberto Sangiovanni-Vincentelli},doi={10.1145/266021.266060},pmid={null},pmcid={null},mag_id={2100519472},journal={null},abstract={A new system design methodology is proposed that separates communicationfrom behavior. To demonstrate the methodology weapplied it to a simple ATM design. Since verification is clearly amajor stumbling block for large system design, we focussed on theverification aspects of our methodology.In particular, a simulator was developed that is based on the communicationparadigm typical of our methodology. The simulatorgives substantial performance improvements without sacrificinguser access to detail.Finally, the potential for this methodology to improve verification,modeling and synthesis is explored.}}
@ARTICLE{Böhme_2010,title={HOL-Boogie--An Interactive Prover-Backend for the Verifying C Compiler},year={2010},author={Sascha Böhme and Sascha Böhme and Michał Moskal and Michał Moskal and Wolfram Schulte and Wolfram Schulte and Burkhart Wolff and Burkhart Wolff},doi={10.1007/s10817-009-9142-9},pmid={null},pmcid={null},mag_id={2100855196},journal={Journal of Automated Reasoning},abstract={Boogie is a verification condition generator for an imperative core language. It has front-ends for the programming languages C# and C enriched by annotations in first-order logic, i.e. pre- and postconditions, assertions, and loop invariants. Moreover, concepts like ghost fields, ghost variables, ghost code and specification functions have been introduced to support a specific modeling methodology. Boogie's verification conditions--constructed via a wp calculus from annotated programs--are usually transferred to automated theorem provers such as Simplify or Z3. This also comprises the expansion of language-specific modeling constructs in terms of a theory describing memory and elementary operations on it; this theory is called a machine/memory model. In this paper, we present a proof environment, HOL-Boogie, that combines Boogie with the interactive theorem prover Isabelle/HOL, for a specific C front-end and a machine/memory model. In particular, we present specific techniques combining automated and interactive proof methods for code verification. The main goal of our environment is to help program verification engineers in their task to "debug" annotations and to find combined proofs where purely automatic proof attempts fail.}}
@ARTICLE{King_1976,title={Symbolic execution and program testing},year={1976},author={J.F. King and James C. King},doi={10.1145/360248.360252},pmid={null},pmcid={null},mag_id={2101512909},journal={Communications of The ACM},abstract={This paper describes the symbolic execution of programs. Instead of supplying the normal inputs to a program (e.g. numbers) one supplies symbols representing arbitrary values. The execution proceeds as in a normal execution except that values may be symbolic formulas over the input symbols. The difficult, yet interesting issues arise during the symbolic execution of conditional branch type statements. A particular system called EFFIGY which provides symbolic execution for program testing and debugging is also described. It interpretively executes programs written in a simple PL/I style programming language. It includes many standard debugging features, the ability to manage and to prove things about symbolic expressions, a simple program testing manager, and a program verifier. A brief discussion of the relationship between symbolic execution and program proving is also included.}}
@ARTICLE{Leucker_2009,title={A Brief Account of Runtime Verification},year={2009},author={Martin Leucker and Martin Leucker and Christian Schallhart and Christian Schallhart},doi={10.1016/j.jlap.2008.08.004},pmid={null},pmcid={null},mag_id={2101623441},journal={The Journal of Logic and Algebraic Programming},abstract={In this paper, a brief account of the field of runtime verification is given. Starting with a definition of runtime verification, a comparison to well-known verification techniques like model checking and testing is provided, and applications in which runtime verification brings out its distinguishing features are pointed out. Moreover, extensions of runtime verification such as monitor-oriented programming, and monitor-based runtime reflection are sketched and their similarities and differences are discussed. Finally, the use of runtime verification for contract enforcement is briefly pointed out.}}
@ARTICLE{Schwoon_2002,title={Model-Checking Pushdown Systems},year={2002},author={Stefan Schwoon and Stefan Schwoon},doi={null},pmid={null},pmcid={null},mag_id={2101936540},journal={null},abstract={The thesis investigates an approach to automated software verification based on pushdown systems. Pushdown systems are, roughly speaking, transition systems whose states include a stack of unbounded length; there is a natural correspondence between them and the execution sequences of programs with (possibly recursive) subroutines. The thesis examines model-checking problems for pushdown systems, improving previously known algorithms in terms of both asymptotic complexity and practical usability. The improved algorithms are used in a tool called Moped. The tool acts as a model-checker for linear-time logic (LTL) on pushdown systems. Two different symbolic techniques are combined to make the model-checking feasible: automata-based techniques are used to handle infinities raised by a program's control, and Binary Decision Diagrams (BDDs) to combat the state explosion raised by its data. It is shown how the resulting system can be used to verify properties of algorithms with recursive procedures by means of several examples. The checker has also been used on automatically derived abstractions of some large C programs. Moreover, the thesis investigates several optimizations which served to improve the efficiency of the checker.}}
@ARTICLE{Daws_1996,title={Reducing the number of clock variables of timed automata},year={1996},author={Conrado Daws and Conrado Daws and Sergio Yovine and Sergio Yovine},doi={10.1109/real.1996.563702},pmid={null},pmcid={null},mag_id={2102380292},journal={null},abstract={We propose a method for reducing the number of clocks of a timed automaton by combining two algorithms. The first one consists in detecting active clocks, that is, those clocks whose values are relevant for the evolution of the system. The second one detects sets of clocks that are always equal. We implemented the algorithms and applied them to several case studies. These experimental results show that an appropriate encoding of the state space, based on the output of the algorithms, leads to a considerable reduction of the memory space allowing a more eficient Verification.}}
@ARTICLE{Jung_2008,title={Priority-based scheduling of dynamic segment in FlexRay network},year={2008},author={Kwangho Jung and Kwang-Ho Jung and Moogeun Song and Moo-Geun Song and Dongik Lee and Dongik Lee and Sung‐Ho Jin and Sung Ho Jin},doi={10.1109/iccas.2008.4694307},pmid={null},pmcid={null},mag_id={2102510848},journal={null},abstract={FlexRay is an in-vehicle communication network for safety-critical automotive applications such as network-based braking or steering systems. The FlexRay protocol offers various benefits over the existing in-vehicle networks by using a hybrid approach integrating the static segment and the dynamic segment. While the static segment is based on time division multiple access (TDMA) to achieve deterministic behavior, the dynamic segment adopts so-called flexible TDMA (FTDMA) to achieve flexible and efficient use of the network. Therefore, scheduling of the dynamic segment in an optimal manner is critical for designing an efficient FlexRay-based system. This paper presents a scheduling method, named recursive qualification (RQ), for the dynamic segment in FlexRay. The proposed method is based on the multiple-slot assignment so that the dynamic segment can be used to efficiently transmit the messages with bounded delays. An index of bus accessibility (BA) is also adopted to determine the priority of each node. The effectiveness of the proposed method is verified through a set of experiments.}}
@ARTICLE{Alpern_2000,title={The Jalapeño virtual machine},year={2000},author={Bowen Alpern and Bowen Alpern and C. R. Attanasio and Clement Richard Attanasio and John Barton and John Barton and Michael G. Burke and Michael G. Burke and Perry Cheng and Perry Cheng and Jong-Deok Choi and Jong-Deok Choi and Anthony Cocchi and Anthony Cocchi and Stephen J. Fink and Stephen J. Fink and David Grove and David Grove and Michael Hind and Michael Hind and Susan Flynn Hummel and Susan Flynn Hummel and Derek Lieber and Derek Lieber and Vassily Litvinov and Vassily Litvinov and Mark Mergen and Mark F. Mergen and Tram A. Ngo and Ton Ngo and James R. Russell and J. R. Russell and Vivek Sarkar and Vivek Sarkar and Mauricio J. Serrano and Mauricio J. Serrano and Janice Shepherd and J. C. Shepherd and Steve Smith and Stephen Edwin Smith and Stephen Edwin Smith and Stephen Edwin Smith and Vugranam C. Sreedhar and Vugranam C. Sreedhar and Harini Srinivasan and Harini Srinivasan and John Whaley and John Whaley},doi={10.1147/sj.391.0211},pmid={null},pmcid={null},mag_id={2102839400},journal={Ibm Systems Journal},abstract={Jalapeno is a virtual machine for JavaTM servers written in the Java language. To be able to address the requirements of servers (performance and scalability in particular), Jalapeno was designed "from scratch" to be as self-sufficient as possible. Jalapeno's unique object model and memory layout allows a hardware null-pointer check as well as fast access to array elements, fields, and methods. Run-time services conventionally provided in native code are implemented primarily in Java. Java threads are multiplexed by virtual processors (implemented as operating system threads). A family of concurrent object allocators and parallel type-accurate garbage collectors is supported. Jalapeno's interoperable compilers enable quasi-preemptive thread switching and precise location of object references. Jalapeno's dynamic optimizing compiler is designed to obtain high quality code for methods that are observed to be frequently executed or computationally intensive.}}
@ARTICLE{Ural_1991,title={A test sequence selection method for protocol testing},year={1991},author={Hasan Ural and H. Ural and Bo Yang and Bo Yang and Bo Yang and B. Yang},doi={10.1109/26.81739},pmid={null},pmcid={null},mag_id={2103095043},journal={IEEE Transactions on Communications},abstract={A method for automated selection of test sequences from a protocol specification given in Estelle for the purpose of testing both control and data flow aspects of a protocol implementation is discussed. First, a flowgraph modeling the flow of both control and data expressed in the given specification is constructed. In the flowgraph, definitions and uses of each context variable, as well as each input and output interaction parameter employed in the specification, are identified. Based on this information, associations between each output and those inputs that influence the output are established. Test sequences are selected to cover each such association at least once. The resulting test sequences are shown to provide the capability of checking whether a protocol implementation under test establishes the desired flow of both control and data expressed in the protocol specification. The proposed method is illustrated by using the class 0 transport protocol as an example. >}}
@ARTICLE{Müller_2006,title={Modular invariants for layered object structures},year={2006},author={Péter Müller and Peter Müller and Arnd Poetzsch-Heffter and Arnd Poetzsch-Heffter and Gary T. Leavens and Gary T. Leavens},doi={10.1016/j.scico.2006.03.001},pmid={null},pmcid={null},mag_id={2103591546},journal={Science of Computer Programming},abstract={Classical specification and verification techniques support invariants for individual objects whose fields are primitive values, but do not allow sound modular reasoning about invariants involving more complex object structures. Such non-trivial object structures are common, and occur in lists, hash tables, and whenever systems are built in layers. A sound and modular verification technique for layered object structures has to deal with the well-known problem of representation exposure and the problem that invariants of higher layers are potentially violated by methods in lower layers; such methods cannot be modularly shown to preserve these invariants.We generalize classical techniques to cover layered object structures using a refined semantics for invariants based on an ownership model for alias control. This semantics enables sound and modular reasoning. We further extend this ownership technique to even more expressive invariants that gain their modularity by imposing certain visibility requirements.}}
@ARTICLE{Das_2002,title={ESP: path-sensitive program verification in polynomial time},year={2002},author={Manuvir Das and Manuvir Das and Sorin Lerner and Sorin Lerner and Mark Seigle and Mark C. Seigle and Mark Seigle},doi={10.1145/512529.512538},pmid={null},pmcid={null},mag_id={2103714221},journal={null},abstract={In this paper, we present a new algorithm for partial program verification that runs in polynomial time and space. We are interested in checking that a program satisfies a given temporal safety property. Our insight is that by accurately modeling only those branches in a program for which the property-related behavior differs along the arms of the branch, we can design an algorithm that is accurate enough to verify the program with respect to the given property, without paying the potentially exponential cost of full path-sensitive analysis.We have implemented this "property simulation" algorithm as part of a partial verification tool called ESP. We present the results of applying ESP to the problem of verifying the file I/O behavior of a version of the GNU C compiler (gcc, 140,000 LOC). We are able to prove that all of the 646 calls to .fprintf in the source code of gcc are guaranteed to print to valid, open files. Our results show that property simulation scales to large programs and is accurate enough to verify meaningful properties.}}
@ARTICLE{Dijkstra_1976,title={A Discipline of Programming},year={1976},author={Edsger W. Dijkstra and Edsger W. Dijkstra},doi={null},pmid={null},pmcid={null},mag_id={2103953153},journal={null},abstract={null}}
@ARTICLE{Navet_2005,title={Trends in Automotive Communication Systems},year={2005},author={Nicolas Navet and Nicolas Navet and Nicolas Navet and Nicolas Navet and Ye‐Qiong Song and Ye-Qiong Song and Françoise Simonot‐Lion and Françoise Simonot-Lion and Françoise Simonot-Lion and Cédric Wilwert and Cédric Wilwert},doi={10.1109/jproc.2005.849725},pmid={null},pmcid={null},mag_id={2104807148},journal={null},abstract={The use of networks for communications between the electronic control units (ECU) of a vehicle in production cars dates from the beginning of the 1990s. The specific requirements of the different car domains have led to the development of a large number of automotive networks such as Local Interconnect Network, J1850, CAN, TTP/C, FlexRay, media-oriented system transport, IDB1394, etc. This paper first introduces the context of in-vehicle embedded systems and, in particular, the requirements imposed on the communication systems. Then, a comprehensive review of the most widely used automotive networks, as well as the emerging ones, is given. Next, the current efforts of the automotive industry on middleware technologies, which may be of great help in mastering the heterogeneity, are reviewed. Finally, we highlight future trends in the development of automotive communication systems.}}
@ARTICLE{Bracciali_2005,title={A formal approach to component adaptation},year={2005},author={Andrea Bracciali and Andrea Bracciali and Antonio Brogi and Antonio Brogi and Carlos Canal and Carlos Canal},doi={10.1016/j.jss.2003.05.007},pmid={null},pmcid={null},mag_id={2104807732},journal={null},abstract={Component adaptation is widelyrecognised to be one of the crucial problems in Component-Based Software Engineering (CBSE). We present a formal methodologyfor adapting components with mismatching interaction behaviour. The three main ingredients of the methodologyare: (1) the inclusion of behaviour specifications in component interfaces, (2) a simple, high-level notation for expressing adaptor specifications, and (3) a fullyautomated procedure to derive concrete adaptors from given high-level specifications.}}
@ARTICLE{Okasaki_1998,title={Purely functional data structures},year={1998},author={Chris Okasaki and Chris Okasaki},doi={null},pmid={null},pmcid={null},mag_id={2105045857},journal={null},abstract={When a C programmer needs an efficient data structure for a particular problem, he or she can often simply look one up in any of a number of good textbooks or handbooks. Unfortunately, programmers in functional languages such as Standard ML or Haskell do not have this luxury. Although some data structures designed for imperative languages such as C can be quite easily adapted to a functional setting, most cannot, usually because they depend in crucial ways on assignments, which are disallowed, or at least discouraged, in functional languages. To address this imbalance, we describe several techniques for designing functional data structures, and numerous original data structures based on these techniques, including multiple variations of lists, queues, double-ended queues, and heaps, many supporting more exotic features such as random access or efficient catenation.
In addition, we expose the fundamental role of lazy evaluation in amortized functional data structures. Traditional methods of amortization break down when old versions of a data structure, not just the most recent, are available for further processing. This property is known as persistence, and is taken for granted in functional languages. On the surface, persistence and amortization appear to be incompatible, but we show how lazy evaluation can be used to resolve this conflict, yielding amortized data structures that are efficient even when used persistently. Turning this relationship between lazy evaluation and amortization around, the notion of amortization also provides the first practical techniques for analyzing the time requirements of non-trivial lazy programs.
Finally, our data structures offer numerous hints to programming language designers, illustrating the utility of combining strict and lazy evaluation in a single language, and providing non-trivial examples using polymorphic recursion and higher-order, recursive modules.}}
@ARTICLE{Fujiwara_1991,title={Test selection based on finite state models},year={1991},author={Susumu Fujiwara and Susumu Fujiwara and Gregor von Bochmann and Gregor von Bochmann and Ferhat Khendek and Ferhat Khendek and Mokhtar Amalou and Mokhtar Amalou and A. Ghedamsi and A. Ghedamsi},doi={10.1109/32.87284},pmid={null},pmcid={null},mag_id={2105394284},journal={IEEE Transactions on Software Engineering},abstract={A method for the selection of appropriate test case, an important issue for conformance testing of protocol implementations as well as software engineering, is presented. Called the partial W-method, it is shown to have general applicability, full fault-detection power, and yields shorter test suites than the W-method. Various other issues that have an impact on the selection of a suitable test suite including the consideration of interaction parameters, various test architectures for protocol testing and the fact that many specifications do not satisfy the assumptions made by most test selection methods (such as complete definition, a correctly implemented reset function, a limited number of states in the implementation, and determinism), are discussed. >}}
@ARTICLE{Sun_2007,title={Web services choreography and orchestration in Reo and constraint automata},year={2007},author={Min Sun and Sun Meng and Farhad Arbab and Farhad Arbab},doi={10.1145/1244002.1244085},pmid={null},pmcid={null},mag_id={2105658676},journal={null},abstract={Currently web services constitute one of the most important topics in the realm of the World Wide Web. Composition of web services lets developers create applications on top of service-oriented computing platforms. Current web services choreography and orchestration proposals, such as BPELAWS, WSCDL, and WSCI, provide notations for describing the message flows in web service interactions. However, such proposals remain at the description level, without providing any kind of formal reasoning mechanisms or tool support for checking the compatibility of web services based on the proposed notations. In this paper, we present our work on compositional construction of web services using the Reo coordination language and constraint automata. Reo is an exogenous coordinational language based on channels. We investigate the possibility of representing the behaviour of web services using constraint automata as black-box components within Reo circuits. We describe the orchestration of web services by the product of corresponding constraint automata, and use Reo circuits for choreography of web services. We investigate the issues of description, orchestration, and choreography of web services at a unifying abstract level, based on constraint automata, which have been used as the semantics of the coordination language Reo, allowing us to derive a natural correspondence relationship between orchestration and choreography.}}
@ARTICLE{Kundu_2009,title={A Novel Approach to Generate Test Cases from UML Activity Diagrams},year={2009},author={Debasish Kundu and Debasish Kundu and Debasis Samanta and Debasis Samanta},doi={10.5381/jot.2009.8.3.a1},pmid={null},pmcid={null},mag_id={2106696288},journal={The Journal of Object Technology},abstract={Model-based test case generation is gaining acceptance to the software practitioners. Advantages of this are the early detection of faults, reducing software development time etc. In recent times, researchers have considered difierent UML diagrams for generating test cases. Few work on the test case generation using activity diagrams is reported in literatures. However, the existing work consider activity diagrams in method scope and mainly follow UML 1:x for modeling. In this paper, we present an approach of generating test cases from activity diagrams using UML 2:0 syntax and with use case scope. We consider a test coverage criterion, called activity path coverage criterion. The test cases generated using our approach are capable of detecting more faults like synchronization faults, loop faults unlike the existing approaches.}}
@ARTICLE{Leino_1995,title={Towards Reliable Modular Programs},year={1995},author={K. Rustan M. Leino and K. R Leino},doi={null},pmid={null},pmcid={null},mag_id={2107283383},journal={null},abstract={Software is being applied in an ever-increasing number of areas. Computer programs and systems are becoming more complex and consisting of more delicately interconnected components. Errors surfacing in programs are still a conspicuous and costly problem. It's about time we employ some techniques that guide us toward higher reliability of practical programs. The goal of this thesis is just that.
This thesis presents a theory for verifying programs based on Dijkstra's weakest-precondition calculus. A variety of program paradigms used in practice, such as exceptions, procedures, object orientation, and modularity, are dealt with.
The thesis sheds new light on the theory behind programs with exceptions. It develops an elegant algebra, and shows it to be the foundation on which the semantics of exceptions rests. It develops a trace semantics for programs with exceptions, from which the weakest-precondition semantics is derived. It also proves a theorem on programming methodology relating to exceptions, and applies this theorem in the novel derivation of a simple program.
The thesis presents a simple model for object-oriented data types, in which concerns have been separated, resulting in the simplicity of the model.
To deal with large programs, this thesis takes a practical look at modularity and abstraction. It reveals a problem that arises in writing specifications for modular programs where previous techniques fail. The thesis introduces a new specification construct that solves that problem, and gives a formal proof of soundness for modular verification using that construct. The model is a generalization of Hoare's classical data refinement. However, there are more problems to be solved. The thesis reports on some of these problems and suggests some future directions toward more reliable modular programs.}}
@ARTICLE{Miller_1998,title={Passive testing of networks using a CFSM specification},year={1998},author={Raymond E. Miller and R.E. Miller},doi={10.1109/pccc.1998.659924},pmid={null},pmcid={null},mag_id={2107341872},journal={null},abstract={We introduce a variant of the communicating finite state machine model for the specification of networks in order to determine correct or faulty behavior using passive testing. The appropriateness of the model is first argued, followed by an initial study of how the passive testing procedures developed for finite state machines could be applied. Approaches for, and limitations of, fault detection and fault location using this approach are discussed.}}
@ARTICLE{Ma_2002,title={Inter-class mutation operators for Java},year={2002},author={Yu-Seung Ma and Yu-Seung Ma and Yu-Seung Ma and Yonghwi Kwon and Yong-Rae Kwon and Jeff Offutt and Jeff Offutt},doi={10.1109/issre.2002.1173287},pmid={null},pmcid={null},mag_id={2107388311},journal={null},abstract={The effectiveness of mutation testing depends heavily on the types of faults that the mutation operators are designed to represent. Therefore, the quality of the mutation operators is key to mutation testing. Mutation testing has traditionally been applied to procedural-based languages, and mutation operators have been developed to support most of their language features. Object-oriented programming languages contain new language features, most notably inheritance, polymorphism, and dynamic binding. Not surprisingly; these language features allow new kinds of faults, some of which are not modeled by traditional mutation operators. Although mutation operators for OO languages have previously been suggested, our work in OO faults indicate that the previous operators are insufficient to test these OO language features, particularly at the class testing level. This paper introduces a new set of class mutation operators for the OO language Java. These operators are based on specific OO faults and can be used to detect faults involving inheritance, polymorphism, and dynamic binding, thus are useful for inter-class testing. An initial Java mutation tool has recently been completed, and a more powerful version is currently under construction.}}
@ARTICLE{Lee_2006,title={Network protocol system monitoring: a formal approach with passive testing},year={2006},author={David Lee and David Lee and Dongluo Chen and Dongluo Chen and Rujiang Hao and Ruibing Hao and Raymond E. Miller and Raymond E. Miller and Jianping Wu and Jianping Wu and Jianping Wu and Jianping Wu and Xia Yin and Xia Yin},doi={10.1109/tnet.2006.872572},pmid={null},pmcid={null},mag_id={2107424259},journal={IEEE ACM Transactions on Networking},abstract={We study network protocol system monitoring for fault detection using a formal technique of passive testing that is a process of detecting system faults by passively observing its input/output behaviors without interrupting its normal operations. After describing a formal model of event-driven extended finite state machines, we present two algorithms for passive testing of protocol system control and data portions. Experimental results on OSPF and TCP are reported.}}
@ARTICLE{Aho_1991,title={An optimization technique for protocol conformance test generation based on UIO sequences and rural Chinese postman tours},year={1991},author={Alfred V. Aho and Alfred V. Aho and Anton Dahbura and A.T. Dahbura and D. Lee and D. Lee and David Lee and M. Ümit Uyar and M.U. Uyar},doi={10.1109/26.111442},pmid={null},pmcid={null},mag_id={2107580999},journal={IEEE Transactions on Communications},abstract={A method for generating test sequences for checking the conformance of a protocol implementation to its specification is described. A rural Chinese postman tour problem algorithm is used to determine a minimum-cost tour of the transition graph of a finite-state machine. It is shown that, when the unique input/output sequence (UIO) is used in place of the more cumbersome distinguishing sequence, both the controllability and observability problems of the protocol testing problem are addressed, providing an efficient method for computing a test sequence for protocol conformance testing. >}}
@ARTICLE{Minamide_2005,title={Static approximation of dynamically generated Web pages},year={2005},author={Yasuhiko Minamide and Yasuhiko Minamide},doi={10.1145/1060745.1060809},pmid={null},pmcid={null},mag_id={2107604680},journal={null},abstract={Server-side programming is one of the key technologies that support today's WWW environment. It makes it possible to generate Web pages dynamically according to a user's request and to customize pages for each user. However, the flexibility obtained by server-side programming makes it much harder to guarantee validity and security of dynamically generated pages.To check statically the properties of Web pages generated dynamically by a server-side program, we develop a static program analysis that approximates the string output of a program with a context-free grammar. The approximation obtained by the analyzer can be used to check various properties of a server-side program and the pages it generates.To demonstrate the effectiveness of the analysis, we have implemented a string analyzer for the server-side scripting language PHP. The analyzer is successfully applied to publicly available PHP programs to detect cross-site scripting vulnerabilities and to validate pages they generate dynamically.}}
@ARTICLE{Cardell-Oliver_2002,title={Conformance test experiments for distributed real-time systems},year={2002},author={Rachel Cardell-Oliver and Rachel Cardell-Oliver},doi={10.1145/566172.566196},pmid={null},pmcid={null},mag_id={2107649010},journal={null},abstract={This paper introduces a new technique for testing that a distributed real-time system satisfies a formal timed automata specification. It outlines how to write test specifications in the language of Uppaal timed automata, how to translate those specifications into program code for executing the tests, and describes the results of test experiments on a distributed real-time system with limited hardware and software resources.}}
@ARTICLE{Gulavani_2006,title={SYNERGY: a new algorithm for property checking},year={2006},author={Bhargav S. Gulavani and Bhargav S. Gulavani and Thomas A. Henzinger and Thomas A. Henzinger and Yamini Kannan and Yamini Kannan and Aditya V. Nori and Aditya V. Nori and Sriram K. Rajamani and Sriram K. Rajamani},doi={10.1145/1181775.1181790},pmid={null},pmcid={null},mag_id={2107794009},journal={null},abstract={We consider the problem if a given program satisfies a specified safety property. Interesting programs have infinite state spaces, with inputs ranging over infinite domains, and for these programs the property checking problem is undecidable. Two broad approaches to property checking are testing and verification. Testing tries to find inputs and executions which demonstrate violations of the property. Verification tries to construct a formal proof which shows that all executions of the program satisfy the property. Testing works best when errors are easy to find, but it is often difficult to achieve sufficient coverage for correct programs. On the other hand, verification methods are most successful when proofs are easy to find, but they are often inefficient at discovering errors. We propose a new algorithm, S ynergy , which combines testing and verification. S ynergy  unifies several ideas from the literature, including counterexample-guided model checking, directed testing, and partition refinement.This paper presents a description of the S ynergy  algorithm, its theoretical properties, a comparison with related algorithms, and a prototype implementation called Y ogi .}}
@ARTICLE{Tretmans_1993,title={A Formal Approach to Conformance Testing},year={1993},author={Jan Tretmans and Jan Tretmans},doi={null},pmid={null},pmcid={null},mag_id={2108394166},journal={null},abstract={In order to assure successful communication between computer systems from different manufacturers, standardized communication protocols are being developed and specified. As a next step implementations of these protocols are needed that conform to these specifications. Testing is a way to check correctness of protocol implementations with respect to their specifications. This activity is known as protocol conformance testing. This thesis deals with a formal approach to protocol conformance testing. Testing is performed based on a formal specification of the protocol. The final aim is to obtain methods for the (automatic) derivation of useful sets of tests from formal specifications. The derived tests should be provably correct, which means that they should not detect errors in correct implementations. Moreover, the derived tests should be meaningful: erroneous implementations should be detected with a high probability. An important aspect is a formal definition of what constitutes correctness, i.e. when does a protocol implementation conform to a protocol specification.}}
@ARTICLE{Clarke_1997,title={Automatic test generation for the analysis of a real-time system: Case study},year={1997},author={David M. Clarke and D. Clarke and I. Lee and Insup Lee},doi={10.1109/rttas.1997.601349},pmid={null},pmcid={null},mag_id={2108664553},journal={null},abstract={We present a framework for testing timing constraints of real-time systems. Our tests are automatically derived from specifications of minimum and maximum allowable delays between input/output events in the execution of a system. Our test derivation scheme uses a graphical specification formalism for timing constraints, and the real-time process algebra Algebra of Communicating Shared Resources (ACSR) for representing tests and process models. The use of ACSR to describe test sequences has two main advantages. First, tests can be applied to an ACSR model of the software system within the ACSR semantic framework for model validation purposes. Second, ACSR has concise notation and a precise semantics that will facilitate the translation of real-time tests into a software test language for software validation purposes. The major benefit of our approach is that it can be used to validate a design specification which has too many states for exhaustive state space exploration based analysis. As an illustration of this benefit, we describe the case study of using the automatic derivation of tests from timing specifications for the analysis of the Philips Audio Control Protocol.}}
@ARTICLE{Gabbay_1999,title={A new approach to abstract syntax involving binders},year={1999},author={Murdoch J. Gabbay and Murdoch J. Gabbay and Andrew M. Pitts and Andrew M. Pitts},doi={10.1109/lics.1999.782617},pmid={null},pmcid={null},mag_id={2108857165},journal={null},abstract={The Fraenkel-Mostowski permutation model of set theory with atoms (FM-sets) can serve as the semantic basis of meta-logics for specifying and reasoning about formal systems involving name binding, /spl alpha/-conversion, capture avoiding substitution, and so on. We show that in FM-set theory one can express statements quantifying over 'fresh' names and we use this to give a novel set-theoretic interpretation of name abstraction. Inductively defined FM-sets involving this name abstraction set former (together with cartesian product and disjoint union) can correctly encode object-level syntax module e-conversion. In this way, the standard theory of algebraic data types can be extended to encompass signatures involving binding operators. In particular, there is an associated notion of structural recursion for defining syntax-manipulating functions (such as capture avoiding substitution, set of free variables, etc.) and a notion of proof by structural induction, both of which remain pleasingly close to informal practice.}}
@ARTICLE{Heerink_1998,title={Ins and Outs in Refusal Testing},year={1998},author={A.W. Heerink and Alexander Wilhelmus Heerink},doi={null},pmid={null},pmcid={null},mag_id={2108980444},journal={null},abstract={This thesis is about black-box formal conformance testing for reactive systems. A reactive system is a system that exchanges information with, or interacts with, its environment. Such system does not continuously operate in an autonomic manner, but its behaviour depends on the interactions that can be performed between the system and its environment. Many realistic systems behave as reactive systems, e.g., cash dispensers, television sets, coee machines, communication protocols, etc. Although the topics in this thesis are not necessarily restricted to the domain of communication protocols, most of the concepts will be explained with such applications in mind. Formal conformance testing for reactive systems assumes the presence of a specication in a formal language, and aims at checking, by means of testing, whether implementations comply with the specied behaviour. In black-box formal conformance testing no internal details of the implementations are known: only the interactions between the system and its environment (i.e., the observer) are visible. The objectives of this thesis are (i) to develop a formal conformance testing theory that is applicable to a large class of existing reactive systems and that is of signicant practical interest, and (ii) to apply the standard `Formal Methods in Conformance Testing' (FMCT) [ISO96]. This standard defines a framework for the use of formal methods in conformance testing and is complementary to the international standard IS-9646 `OSI Conformance 
Testing Methodology and Framework' that is mainly intended for specications written in a natural language. By instantiating the abstract testing concepts described in FMCT with the concrete ones based on the testing theory described in this thesis and by applying these instantiated concepts to a concrete example, we hope to show viability of the testing theory in practical situations.}}
@ARTICLE{Hierons_2004,title={Testing from a nondeterministic finite state machine using adaptive state counting},year={2004},author={Robert M. Hierons and Robert M. Hierons},doi={10.1109/tc.2004.85},pmid={null},pmcid={null},mag_id={2109354640},journal={IEEE Transactions on Computers},abstract={The problem of generating a checking experiment from a nondeterministic finite state machine has been represented in terms of state counting. However, test techniques that use state counting traditionally produce preset test suites. We extend the notion of state counting in order to allow the input/output sequences observed in testing to be utilized: Adaptive state counting is introduced. The main benefit of the proposed approach is that it may result in a reduction in the size of the test suite used. An additional benefit is that, where a failure is observed, it is possible to terminate test generation at this point.}}
@ARTICLE{Kästner_2007,title={A Case Study Implementing Features Using AspectJ},year={2007},author={Christian Kästner and Christian Kästner and Sven Apel and Sven Apel and Don Batory and Don Batory},doi={10.1109/splc.2007.5},pmid={null},pmcid={null},mag_id={2109692881},journal={null},abstract={Software product lines aim to create highly configurable programs from a set of features. Common belief and recent studies suggest that aspects are well-suited for implementing features. We evaluate the suitability of AspectJ with respect to this task by a case study that refactors the embedded database system Berkeley DB into 38 features. Contrary to our initial expectations, the results were not encouraging. As the number of aspects in a feature grows, there is a noticeable decrease in code readability and maintainability. Most of the unique and powerful features of AspectJ were not needed. We document where AspectJ is unsuitable for implementing features of refactored legacy applications and explain why.}}
@ARTICLE{Lee_1993,title={Conformance testing of protocols specified as communicating FSMs},year={1993},author={D. Lee and David Lee and Krishan K. Sabnani and Krishan Kumar Sabnani and David M. Kristol and David M. Kristol and Sanjoy Kumar Paul and Sanjoy Paul and Sanjoy Paul and Meltem Uyar and M.U. Uyar},doi={10.1109/infcom.1993.253242},pmid={null},pmcid={null},mag_id={2109801341},journal={null},abstract={An approach for conformance testing of protocols specified as a collection of communicating finite state machines (FSMs) with two parts, pruning and a guided random walk procedure, is presented. First the protocol is pruned to various sets of machines; each set provides only one service. This significantly reduces the test sequence length. Then a guided random walk procedure that attempts to cover all transitions in the component FSMs is used. The results of applying the procedure to the full-duplex alternating bit protocol and the asynchronous transfer mode (ATM) adaptation layer convergence protocol are presented. For the ATM adaptation layer, 99% of component FSMs' edges can be covered in a test with 11692 input steps. Previous approaches cannot generate conformance tests for standard protocols (such as ATM adaptation layer) specified as a collection of communicating FSMs. >}}
@ARTICLE{Leavens_2006,title={Design by Contract with JML},year={2006},author={Gary T. Leavens and Gary T. Leavens and Yoonsik Cheon and Yoonsik Cheon},doi={null},pmid={null},pmcid={null},mag_id={2109879514},journal={null},abstract={This document gives a tutorial introduction to the Java Modeling Language (JML), and explains how JML can be used as a powerful design by contract (DBC) tool for Java. JML is a formal behavioral interface specification language for Java that contains the essential notations used in DBC as a subset. The basic concepts of DBC are explained with a particular emphasis on how to use JML notations to specify Java classes and interfaces. JML tools such as JML compiler (jmlc) are also introduced, with examples of their use.}}
@ARTICLE{Wallace_2001,title={FAILURE MODES IN MEDICAL DEVICE SOFTWARE: AN ANALYSIS OF 15 YEARS OF RECALL DATA},year={2001},author={Dolores R Wallace and Dolores R. Wallace and D. Richard Kuhn and D. Richard Kuhn},doi={10.1142/s021853930100058x},pmid={null},pmcid={null},mag_id={2109993252},journal={International Journal of Reliability, Quality and Safety Engineering},abstract={Most complex systems today contain software, and systems failures activated by software faults can provide lessons for software development practices and software quality assurance. This paper presents an analysis of software-related failures of medical devices that caused no death or injury but led to recalls by the manufacturers. The analysis categorizes the failures by their symptoms and faults, and discusses methods of preventing and detecting faults in each category. The nature of the faults provides lessons about the value of generally accepted quality practices for prevention and detection methods applied prior to system release. It also provides some insight into the need for formal requirements specification and for improved testing of complex hardware-software systems.}}
@ARTICLE{Hoare_1972,title={Proof of correctness of data representations},year={1972},author={C. A. R. Hoare and C. A. R. Hoare},doi={10.1007/bf00289507},pmid={null},pmcid={null},mag_id={2110050316},journal={Acta Informatica},abstract={A powerful method of simplifying the proofs of program correctness is suggested; and some new light is shed on the problem of functions with side-effects.}}
@ARTICLE{Albert_2011,title={Closed-Form Upper Bounds in Static Cost Analysis},year={2011},author={Elvira Albert and Elvira Albert and Puri Arenas and Puri Arenas and Samir Genaim and Samir Genaim and Germán Puebla and Germán Puebla},doi={10.1007/s10817-010-9174-1},pmid={null},pmcid={null},mag_id={2111366004},journal={Journal of Automated Reasoning},abstract={The classical approach to automatic cost analysis consists of two phases. Given a program and some measure of cost, the analysis first produces cost relations (CRs), i.e., recursive equations which capture the cost of the program in terms of the size of its input data. Second, CRs are converted into closed-form, i.e., without recurrences. Whereas the first phase has received considerable attention, with a number of cost analyses available for a variety of programming languages, the second phase has been comparatively less studied. This article presents, to our knowledge, the first practical framework for the generation of closed-form upper bounds for CRs which (1) is fully automatic, (2) can handle the distinctive features of CRs, originating from cost analysis of realistic programming languages, (3) is not restricted to simple complexity classes, and (4) produces reasonably accurate solutions. A key idea in our approach is to view CRs as programs, which allows applying semantic-based static analyses and transformations to bound them, namely our method is based on the inference of ranking functions and loop invariants and on the use of partial evaluation.}}
@ARTICLE{Miller_2001,title={Fault identification in networks by passive testing},year={2001},author={Raymond E. Miller and R.E. Miller and K.A. Arisha and K.A. Arisha},doi={10.1109/simsym.2001.922142},pmid={null},pmcid={null},mag_id={2111428774},journal={null},abstract={We employ the finite state machine (FSM) model for networks to investigate fault identification using passive testing. First we introduce the concept of passive testing. Then, we introduce the FSM model with necessary assumptions and justification. We introduce the fault model and the fault detection algorithm using passive testing. Extending this result, we develop the theorems and algorithms for fault identification. An example is given illustrating our approach. Then, extensions to our approach are introduced to achieve better fault identification. We then illustrate our technique through a simulation of a practical X.25 example. Finally future extensions and potential trends are discussed.}}
@ARTICLE{Balzarotti_2008,title={Saner: Composing Static and Dynamic Analysis to Validate Sanitization in Web Applications},year={2008},author={Davide Balzarotti and Davide Balzarotti and Marco Cova and Marco Cova and Viktoria Felmetsger and Viktoria Felmetsger and Nenad Jovanović and N. Jovanovic and Nenad Jovanovic and Engin Kirda and Engin Kirda and Christopher Kruegel and Christopher Kruegel and Giovanni Vigna and Giovanni Vigna},doi={10.1109/sp.2008.22},pmid={null},pmcid={null},mag_id={2111487235},journal={null},abstract={Web applications are ubiquitous, perform mission- critical tasks, and handle sensitive user data. Unfortunately, web applications are often implemented by developers with limited security skills, and, as a result, they contain vulnerabilities. Most of these vulnerabilities stem from the lack of input validation. That is, web applications use malicious input as part of a sensitive operation, without having properly checked or sanitized the input values prior to their use. Past research on vulnerability analysis has mostly focused on identifying cases in which a web application directly uses external input in critical operations. However, little research has been performed to analyze the correctness of the sanitization process. Thus, whenever a web application applies some sanitization routine to potentially malicious input, the vulnerability analysis assumes that the result is innocuous. Unfortunately, this might not be the case, as the sanitization process itself could be incorrect or incomplete. In this paper, we present a novel approach to the analysis of the sanitization process. More precisely, we combine static and dynamic analysis techniques to identify faulty sanitization procedures that can be bypassed by an attacker. We implemented our approach in a tool, called Saner, and we applied it to a number of real-world applications. Our results demonstrate that we were able to identify several novel vulnerabilities that stem from erroneous sanitization procedures.}}
@ARTICLE{Ducasse_2006,title={Traits: A mechanism for fine-grained reuse},year={2006},author={Sté́phane Ducasse and Stéphane Ducasse and Oscar Nierstrasz and Oscar Nierstrasz and Nathanael Schärli and Nathanael Schärli and Roel Wuyts and Roel Wuyts and Andrew P. Black and Andrew P. Black},doi={10.1145/1119479.1119483},pmid={null},pmcid={null},mag_id={2111898165},journal={ACM Transactions on Programming Languages and Systems},abstract={Inheritance is well-known and accepted as a mechanism for reuse in object-oriented languages. Unfortunately, due to the coarse granularity of inheritance, it may be difficult to decompose an application into an optimal class hierarchy that maximizes software reuse. Existing schemes based on single inheritance, multiple inheritance, or mixins, all pose numerous problems for reuse. To overcome these problems we propose traits, pure units of reuse consisting only of methods. We develop a formal model of traits that establishes how traits can be composed, either to form other traits, or to form classes. We also outline an experimental validation in which we apply traits to refactor a nontrivial application into composable units.}}
@ARTICLE{Untch_1993,title={Mutation analysis using mutant schemata},year={1993},author={Roland H. Untch and Roland H. Untch and A. Jefferson Offutt and A. Jefferson Offutt and Mary Jean Harrold and Mary Jean Harrold},doi={10.1145/154183.154265},pmid={null},pmcid={null},mag_id={2112025128},journal={null},abstract={Mutation analysis is a powerful technique for assessing and improving the quality of test data used to unit test software. Unfortunately, current automated mutation analysis systems suffer from severe performance problems. This paper presents a new method for performing mutation analysis that uses  program schemata  to encode all mutants for a program into one  metaprogram , which is subsequently compiled and run at speeds substantially higher than achieved by previous interpretive systems. Preliminary performance improvements of over 300% are reported. This method has the additional advantages of being easier to implement than interpretive systems, being simpler to port across a wide range of hardware and software platforms, and using the same compiler and run-time support system that is used during development and/or deployment.}}
@ARTICLE{Bonifácio_2013,title={A new method for testing timed systems},year={2013},author={Adilson Luiz Bonifácio and Adilson Luiz Bonifácio and Adilson Luiz Bonifácio and Arnaldo Vieira Moura and Arnaldo Vieira Moura},doi={10.1002/stvr.454},pmid={null},pmcid={null},mag_id={2112617346},journal={Software Testing, Verification & Reliability},abstract={SUMMARY

Devising formal techniques and methods that can automatically generate test suites for timed systems has remained a challenge. In this paper Timed Input/Output Automata (TIOA) are used as a formal specification model for timed systems. This work proposes and proves the correctness of a new and more general discretization method that can be used to obtain grid automata corresponding to specification TIOA, using almost any granularity of interest. Such flexibility to find a suitable granularity opens the possibility for a more compact construction of grid automata. It is also shown how test purposes can be used together with the specification TIOA in order to generate grid automata that capture the behavior of both the specification and the test purpose. From such grid automata one can algorithmically extract test suites that can be used to verify whether given implementations adhere to the specification and reflect the properties modeled using the test purposes. Copyright © 2011 John Wiley & Sons, Ltd.}}
@ARTICLE{Aştefǎnoaei_2009,title={On the Semantics and Verification of Normative Multi-Agent Systems},year={2009},author={Lǎcrǎmioara Aştefǎnoaei and Lacramioara Astefanoaei and Mehdi Dastani and Mehdi Dastani and John‐Jules Ch. Meyer and John-Jules Ch. Meyer and Frank S. de Boer and Frank S. de Boer},doi={null},pmid={null},pmcid={null},mag_id={2113049446},journal={Journal of Universal Computer Science},abstract={This paper presents a programming language that facilitates the implementation of coordination artifacts which in turn can be used to regulate the behaviour of individual agents. The programming language provides constructs inspired by social and organisational concepts. Depending on the scheduling mechanism of such constructs, different operational semantics can be defined. We show how one such possible operational semantics can be prototyped in Maude, which is a rewriting logic software. Prototyping by means of rewriting is important since it allows us both to design and to experiment with the language definitions. To illustrate this, we define particular properties (like enforcement and regimentation) of the coordination artifacts which we then verify with the Maude LTL model-checker.}}
@ARTICLE{Batory_2005,title={Feature models, grammars, and propositional formulas},year={2005},author={Don Batory and Don Batory},doi={10.1007/11554844_3},pmid={null},pmcid={null},mag_id={2113201637},journal={null},abstract={Feature models are used to specify members of a product-line. Despite years of progress, contemporary tools often provide limited support for feature constraints and offer little or no support for debugging feature models. We integrate prior results to connect feature models, grammars, and propositional formulas. This connection allows arbitrary propositional constraints to be defined among features and enables off-the-shelf satisfiability solvers to debug feature models. We also show how our ideas can generalize recent results on the staged configuration of feature models.}}
@ARTICLE{Rushby_2002,title={An Overview of Formal Verification for the Time-Triggered Architecture},year={2002},author={John Rushby and John Rushby},doi={10.1007/3-540-45739-9_7},pmid={null},pmcid={null},mag_id={2113624447},journal={Lecture Notes in Computer Science},abstract={We describe formal verification of some of the key algorithms in the Time-Triggered Architecture (TTA) for real-time safety-critical control applications. Some of these algorithms pose formidable challenges to current techniques and have been formally verified only in simplified form or under restricted fault assumptions. We describe what has been done and what remains to be done and indicate some directions that seem promising for the remaining cases and for increasing the automation that can be applied. We also describe the larger challenges posed by formal verification of the interaction of the constituent algorithms and of their emergent properties.}}
@ARTICLE{Hierons_2007,title={Mutation Testing from Probabilistic Finite State Machines},year={2007},author={Robert M. Hierons and Robert M. Hierons and Mercedes G. Merayo and Mercedes G. Merayo},doi={10.1109/taic.part.2007.20},pmid={null},pmcid={null},mag_id={2113908384},journal={null},abstract={Mutation testing traditionally involves mutating a program in order to produce a set of mutants and using these mutants in order to either estimate the effectiveness of a test suite or to drive test generation. Recently, however, this approach has been applied to specifications such as those written as finite state machines. This paper extends mutation testing to finite state machine models in which transitions have associated probabilities. The paper describes several ways of mutating a probabilistic finite state machine (PFSM) and shows how test sequences that distinguish between a PFSM and its mutants can be generated. Testing then involves applying each test sequence multiple times, observing the resultant output sequences and using results from statistical sampling theory in order to compare the observed frequency of each output sequence with that expected.}}
@ARTICLE{Sen_2005,title={VESTA: A statistical model-checker and analyzer for probabilistic systems},year={2005},author={Koushik Sen and Koushik Sen and Mahesh Viswanathan and Mahesh Viswanathan and Gul Agha and Gul Agha},doi={10.1109/qest.2005.42},pmid={null},pmcid={null},mag_id={2114158268},journal={null},abstract={We give a brief overview of a statistical model-checking and analysis tool VESTA. VESTA is a tool for statistical analysis of probabilistic systems. It supports statistical model-checking and statistical evaluation of expected values of temporal expressions.}}
@ARTICLE{Moskal_2008,title={E-matching for Fun and Profit},year={2008},author={Michał Moskal and Michał Moskal and Jakub Łopuszański and Jakub Łopuszański and Joseph R. Kiniry and Joseph R. Kiniry},doi={10.1016/j.entcs.2008.04.078},pmid={null},pmcid={null},mag_id={2114917372},journal={Electronic Notes in Theoretical Computer Science},abstract={Efficient handling of quantifiers is crucial for solving software verification problems. E-matching algorithms are used in satisfiability modulo theories solvers that handle quantified formulas through instantiation. Two novel, efficient algorithms for solving the E-matching problem are presented and compared to a well-known algorithm described in the literature.}}
@ARTICLE{Barnett_2003,title={Towards a Tool Environment for Model-Based Testing with AsmL},year={2003},author={Mike Barnett and Mike Barnett and Wolfgang Grieskamp and Wolfgang Grieskamp and Lev Nachmanson and Lev Nachmanson and Wolfram Schulte and Wolfram Schulte and Nikolai Tillmann and Nikolai Tillmann and Margus Veanes and Margus Veanes},doi={10.1007/978-3-540-24617-6_18},pmid={null},pmcid={null},mag_id={2114920227},journal={null},abstract={We present work on a tool environment for model-based testing with the Abstract State Machine Language (AsmL). Our environment supports semi-automatic parameter generation, call sequence generation and conformance testing. We outline the usage of the environment by an example, discuss its underlying technologies, and report on some applications conducted in the Microsoft environment.}}
@ARTICLE{Holzmann_1997,title={The model checker SPIN},year={1997},author={Gerard J. Holzmann and Gerard J. Holzmann},doi={10.1109/32.588521},pmid={null},pmcid={null},mag_id={2115309705},journal={null},abstract={SPIN is an efficient verification system for models of distributed software systems. It has been used to detect design errors in applications ranging from high-level descriptions of distributed algorithms to detailed code for controlling telephone exchanges. The paper gives an overview of the design and structure of the verifier, reviews its theoretical foundation, and gives an overview of significant practical applications.}}
@ARTICLE{Lee_1997,title={Passive testing and applications to network management},year={1997},author={D. Lee and David Lee and Arun N. Netravali and A.N. Netravali and Krishan K. Sabnani and K.K. Sabnani and B. Sugla and B. Sugla and B. Sugla and Binay Sugla and A. John and Ajita John},doi={10.1109/icnp.1997.643699},pmid={null},pmcid={null},mag_id={2115327632},journal={null},abstract={An important aspect of network management is fault management-determining, locating, isolating and correcting faults in the network. The paper deals with the algorithms for detecting faults, i.e., behavior of the network different from specifications. It is important for communication networks to detect faults "in-process" i.e., while the network is in its normal operation. Thus, we detect faults by examining the input-output behavior without forcing the system to specialized inputs explicitly for testing. Such testing is commonly called passive testing. We model the network as a finite state machine and develop procedures for passive testing including the required data structure, efficient implementations and the complexity of our procedures. We start with fully observable and deterministic machines and then study more realistic models: partially observable and nondeterministic machines. We also discuss extensions to communicating finite state machines and machines extended with parameters and variables. We apply our techniques to management of a signaling network operating under the Signaling System 7 (SS7) and report experimental results, which show the feasibility of applying passive testing to practical systems.}}
@ARTICLE{Bayer_2000,title={PuLSE-I: Deriving instances from a product line infrastructure},year={2000},author={Joachim Bayer and Joachim Bayer and Cristina Gacek and Cristina Gacek and Dirk Muthig and Dirk Muthig and D. Muthig and Tanya Widen and Tanya Widen and T. Widen},doi={10.1109/ecbs.2000.839882},pmid={null},pmcid={null},mag_id={2115458362},journal={null},abstract={Reusing assets during application engineering promises to improve the efficiency of systems development. However in order to benefit from reusable assets, application engineering processes must incorporate when and how to use the reusable assets during single system development. However, when and how to use a reusable asset depends on what types of reusable assets have been created. Product line engineering approaches produce a reusable infrastructure for a set of products. In this paper, we present the application engineering process associated with the PuLSE product line software engineering method-PuLSE-I. PuLSE-I details how single systems can be built efficiently from the reusable product line infrastructure built during the other PuLSE activities.}}
@ARTICLE{Bijl_2007,title={Atomic Action Refinement in Model Based Testing},year={2007},author={Machiel van der Bijl and Machiel van der Bijl and Arend Rensink and Arend Rensink and Jan Tretmans and Jan Tretmans},doi={null},pmid={null},pmcid={null},mag_id={2115658586},journal={CTIT technical report series},abstract={In model based testing (MBT) test cases are derived from a specification of the system that we want to test. In general the specification is more abstract than the implementation. This may result in 1) test cases that are not executable, because their actions are too abstract (the implementation does not understand them); or 2) test cases that are incorrect, because the specification abstracts from relevant behavior. The standard approach to remedy this problem is to rewrite the specification by hand to the required level of detail and regenerate the test cases. This is error-prone and time consuming. Another approach is to do some translation during test execution. This solution has no basis in the theory of MBT. We propose a framework to add the required level of detail automatically to the abstract specification and/or abstract test cases.

This paper focuses on general atomic action refinement. This means that an abstract action is replaced by more complex behavior (expressed as a labeled transition system). With general we mean that we impose as few restrictions as possible. Atomic means that the actions that are being refined behave as if they were atomic, i.e., no other actions are allowed to interfere.}}
@ARTICLE{Batory_1997,title={Composition validation and subjectivity in GenVoca generators},year={1997},author={Don Batory and Don Batory and Bart J. Geraci and Bart J. Geraci},doi={10.1109/32.585497},pmid={null},pmcid={null},mag_id={2115703981},journal={IEEE Transactions on Software Engineering},abstract={GenVoca generators synthesize software systems by composing components from reuse libraries. GenVoca components are designed to export and import standardized interfaces, and thus be plug-compatible, interchangeable, and interoperable with other components. We examine two different but important issues in software system synthesis. First, not all syntactically correct compositions of components are semantically correct. We present simple, efficient, and domain-independent algorithms for validating compositions of GenVoca components. Second, components that export and import immutable interfaces are too restrictive for software system synthesis. We show that the interfaces and bodies of GenVoca components are subjective, i.e., they mutate and enlarge upon instantiation. This mutability enables software systems with customized interfaces to be composed from components with "standardized" interfaces.}}
@ARTICLE{Abran_2003,title={Metrology, measurement and metrics in software engineering},year={2003},author={Alain Abran and Alain Abran and Asma Sellami and Asma Sellami and Witold Suryn and Witold Suryn},doi={10.1109/metric.2003.1232451},pmid={null},pmcid={null},mag_id={2116295136},journal={null},abstract={Up until recently software 'metrics' have been most often proposed as the quantitative tools of choice in software engineering, and the analysis of these had been most often discussed from the perspective referred to as 'measurement theory'. However, in other disciplines, it is the domain of knowledge referred to as 'metrology' that is the foundation for the development and use of measurement instruments and measurement processes. The IEEE-computer society, with the support of a consortium of industrial sponsors, has recently published a guide to the Software Engineering Body of Knowledge (SWEBOK) and, throughout this guide, measurement is pervasive as a fundamental engineering tool. We use our initial modelling of the sets of measurement concepts documented in the ISO international vocabulary of basic and general terms in metrology to investigate and position the measurement concepts within this body of knowledge, and to identify gaps where further research on software measurement is required.}}
@ARTICLE{Silva_2004,title={An automatic testbench generation tool for a SystemC functional verification methodology},year={2004},author={Karina R. G. da Silva and Karina R. G. da Silva and Elmar U. K. Melcher and Elmar U. K. Melcher and Guido Araújo and Guido Araujo and Valdiney Alves Pimenta and Valdiney Alves Pimenta},doi={10.1145/1016568.1016592},pmid={null},pmcid={null},mag_id={2116517177},journal={null},abstract={The advent of new 90 nm/130 nm VLSI technology and SoC design methodologies, has brought an explosive growth in the complexity of modern electronic circuits. As a result, functional verification has become the major bottleneck in any design flow. New methods are required that allow for easier, quicker and more reusable verification. In this paper we propose an automatic verification methodology approach that enables fast, transaction-level, coverage-driven, self-checking and random-constraint functional verification. Our approach uses the systemC verification library (SCV), to synthesize a tool capable of automatically generating testbench templates. A case study from a real MP3 design is used to show the effectiveness of our approach.}}
@ARTICLE{Huisman_2010,title={CVPP: a tool set for compositional verification of control-flow safety properties},year={2010},author={Marieke Huisman and Marieke Huisman and Dilian Gurov and Dilian Gurov},doi={10.1007/978-3-642-18070-5_8},pmid={null},pmcid={null},mag_id={2116612980},journal={null},abstract={This paper describes CVPP, a tool set for compositional verification of control-flow safety properties for programs with procedures. The compositional verification principle that underlies CVPP is based on maximal models constructed from component specifications. Maximal models replace the actual components when verifying the whole program, either for the purposes of modularity of verification or due to unavailability of the component implementations at verification time. A characteristic feature of the principle and the tool set is the distinction between program structure and behaviour. While behavioural properties are more abstract and convenient for specification purposes, structural ones are easier to manipulate, in particular when it comes to verification or the construction of maximal models. Therefore, CVPP also contains the means to characterise a given behavioural formula by a set of structural formulae. The paper presents the underlying framework for compositional verification and the components of the tool set. Several verification scenarios are described, as well as wrapper tools that support the automatic execution of such scenarios, providing appropriate pre- and post-processing to interface smoothly with the user and to encapsulate the inner workings of the tool set.}}
@ARTICLE{Clarke_1986,title={Automatic verification of finite-state concurrent systems using temporal logic specifications},year={1986},author={Edmund M. Clarke and Edmund M. Clarke and E. Allen Emerson and E. A. Emerson and A. Prasad Sistla and A. P. Sistla},doi={10.1145/5397.5399},pmid={null},pmcid={null},mag_id={2117189826},journal={ACM Transactions on Programming Languages and Systems},abstract={We give an efficient procedure for verifying that a finite-state concurrent system meets a specification expressed in a (propositional, branching-time) temporal logic. Our algorithm has complexity linear in both the size of the specification and the size of the global state graph for the concurrent system. We also show how this approach can be adapted to handle fairness. We argue that our technique can provide a practical alternative to manual proof construction or use of a mechanical theorem prover for verifying many finite-state concurrent systems. Experimental results show that state machines with several hundred states can be checked in a matter of seconds.}}
@ARTICLE{Brinksma_2000,title={Testing Transition Systems: An Annotated Bibliography},year={2000},author={Ed Brinksma and Ed Brinksma and Jan Tretmans and Jan Tretmans},doi={10.1007/3-540-45510-8_9},pmid={null},pmcid={null},mag_id={2117268802},journal={null},abstract={Labelled transition system based test theory has made remarkable progress over the past 15 years. From a theoretically interesting approach to the semantics of reactive systems it has developed into a field where testing theory is (slowly) narrowing the gap with testing practice. In particular, new test generation algorithms are being designed that can be used in realistic situations whilst maintaining a sound theoretical basis. In this paper we present an annotated bibliography of labelled transition system based test theory and its applications covering the main developments.}}
@ARTICLE{Kent_2002,title={Negotiable Interfaces for Components},year={2002},author={Simon Kent and Simon D. Kent and Chris Ho-Stuart and Chris Ho-Stuart and Paul Roe and Paul Roe},doi={10.5381/jot.2002.1.3.a14},pmid={null},pmcid={null},mag_id={2117821728},journal={The Journal of Object Technology},abstract={Component speci cations are vital for communicating a component’s requirements,as components are subject to third-party composition. Most modern programminglanguages lack sucient features to express the full requirements of a component,however, much less enforce them. Pre- and post-conditions can capture functionalaspects of a component’s requirements, but are unable to express many temporalconstraints such as re-entrance restrictions or changing availability of services overthe lifetime of a component instance (object).The approach described within forms a basis for extending the speci cation of compo-nents at the programming language level, thus making such speci cations enforceable.Interfaces are extended with a factorable, abstract state, and methods of interfacesare extended with state transformation behaviours. A new programming languagecommand, the USEstatement, allows clients to negotiate for those services providedby an object. A mixture of static and dynamic checking ensures the consistency ofan object’s state according to the speci cation of the object’s interfaces.The mechanism proposed allows a clearer expression of re-entrance conditions anddynamic service availability, and a greater level of checking that allows erroneouscases to be prevented or detected during development time. The mechanism alsoacts as a self-documenting feature for interfaces.}}
@ARTICLE{Andrés_2009,title={Formal Correctness of a Passive Testing Approach for Timed Systems},year={2009},author={César Andrés and César Andrés and Mercedes G. Merayo and Mercedes G. Merayo and Manuel Núñez and Manuel Núñez},doi={10.1109/icstw.2009.34},pmid={null},pmcid={null},mag_id={2117884306},journal={null},abstract={In this paper we extend our previous work on passive testing of timed systems to establish a formal criterion to determine correctness of an implementation under test. In our framework, an invariant expresses the fact that if theimplementation under test performs a given sequence of actions, then it must exhibit a behavior in a lapse of time reflected in the invariant. In a previous paper we gave an algorithm to establish the correctness of an invariant with respect to a specification. In this paper we continue the work by providing an algorithm to check the correctness of a log, recorded form the implementation under test, with respect to an invariant. We show the soundness of our method by relating it to an implementation relation. In addition to the theoretical framework we have developed a tool, called PASTE, that facilitates the automation of our passive testing approach.}}
@ARTICLE{Koopman_2003,title={Testing reactive systems with GAST},year={2003},author={Pieter Koopman and Pieter Koopman and M.J. Plasmeijer and Marinus J. Plasmeijer},doi={null},pmid={null},pmcid={null},mag_id={2117893191},journal={null},abstract={TFP 2003: Fourth Symposium on Trends in Functional Programming, TFP 2003, Edinburgh, United Kingdom, 11-12 September 2003}}
@ARTICLE{Rütz_2011,title={An Experience Report on an Industrial Case-Study about Timed Model-Based Testing with UPPAAL-TRON},year={2011},author={Carsten Rütz and Carsten Rütz and Julien Schmaltz and Julien Schmaltz},doi={10.1109/icstw.2011.92},pmid={null},pmcid={null},mag_id={2118136937},journal={null},abstract={Several theories have been proposed for timed model-based testing, but only few case-studies have been reported. In this paper, we report about our experience in using UPPAAL-TRON to test the conformance of an industrial implementation Auto trust of Automatic Trust Anchor Updating, a protocol to help securing DNS. We created models for specific parts of the protocol focussing on security key states and critical timing behaviours. We developed testing environments to test one or multiple keys and to check a security-relevant feature. This case-study also illustrates several challenges when testing timed systems, namely, quiescence, latency, and coverage.}}
@ARTICLE{Viganò_2005,title={An event driven approach to norms in artificial institutions},year={2005},author={Francesco Viganò and Francesco Viganò and Nicoletta Fornara and Nicoletta Fornara and Marco Colombetti and Marco Colombetti},doi={10.1007/11775331_10},pmid={null},pmcid={null},mag_id={2118356672},journal={null},abstract={The notion of artificial institution is crucial for the specification of open and dynamic interaction frameworks where heterogeneous and autonomous agents can interact to face problems in various fields. In our view the specification of artificial institutions requires a clear standard definition of some basic concepts: the notion of ontology, authorizations, conventions, and the normative component. In this paper we propose an event driven approach to the definition of norms that is mainly based on the manipulation of commitments. We will discuss the crucial differences between the notion of authorization and permission and how the notion of permissions, obligations, and prohibitions can be expressed in our model. We will investigate the connections among the specification of different artificial institutions, in particular how an institution can enrich or further regulate the entities defined in another one. Finally we will briefly present the specification of the Dutch Auction Institution and of the Auction House Institution in order to exemplify the model presented in this paper.}}
@ARTICLE{Alur_2008,title={First-Order and Temporal Logics for Nested Words},year={2008},author={Rajeev Alur and Rajeev Alur and Marcelo Arenas and Marcelo Arenas and Pablo Barceló and Pablo Barceló and Kousha Etessami and Kousha Etessami and Neil Immerman and Neil Immerman and Leonid Libkin and Leonid Libkin},doi={10.2168/lmcs-4(4:11)2008},pmid={null},pmcid={null},mag_id={2118416409},journal={Logical Methods in Computer Science},abstract={Nested words are a structured model of execution paths in procedural
programs, reflecting their call and return nesting structure. Finite nested
words also capture the structure of parse trees and other tree-structured data,
such as XML. We provide new temporal logics for finite and infinite nested
words, which are natural extensions of LTL, and prove that these logics are
first-order expressively-complete. One of them is based on adding a "within"
modality, evaluating a formula on a subword, to a logic CaRet previously
studied in the context of verifying properties of recursive state machines
(RSMs). The other logic, NWTL, is based on the notion of a summary path that
uses both the linear and nesting structures. For NWTL we show that
satisfiability is EXPTIME-complete, and that model-checking can be done in time
polynomial in the size of the RSM model and exponential in the size of the NWTL
formula (and is also EXPTIME-complete). Finally, we prove that first-order
logic over nested words has the three-variable property, and we present a
temporal logic for nested words which is complete for the two-variable fragment
of first-order.}}
@ARTICLE{Mathur_1991,title={Performance, effectiveness, and reliability issues in software testing},year={1991},author={Aditya P. Mathur and Aditya P. Mathur},doi={10.1109/cmpsac.1991.170248},pmid={null},pmcid={null},mag_id={2119283374},journal={null},abstract={The author has identified two problems that need to be overcome in order that some of the powerful testing techniques be used in practice: performance and effectiveness. The testing methods referred to are dataflow and mutation testing. >}}
@ARTICLE{Ancona_2011,title={Idealized coinductive type systems for imperative object-oriented programs},year={2011},author={Davide Ancona and Davide Ancona and Giovanni Lagorio and Giovanni Lagorio},doi={10.1051/ita/2011009},pmid={null},pmcid={null},mag_id={2119286564},journal={Theoretical Informatics and Applications},abstract={In recent work we have proposed a novel approach to define idealized type systems for object-oriented languages, based on abstract compilation  of programs into Horn formulas which are interpreted w.r.t. the coinductive (that is, the greatest) Herbrand model. In this paper we investigate how this approach can be applied also in the presence of imperative features. This is made possible by considering a natural translation of Static Single Assignment  intermediate form programs into Horn formulas, where φ  functions correspond to union types.}}
@ARTICLE{Kang_1997,title={Interoperability Test Suite Derivation for Symmetric Communication Protocols},year={1997},author={Sungwan Kang and Sungwan Kang and Myungchul Kim and Myungchul Kim},doi={10.1007/978-0-387-35271-8_4},pmid={null},pmcid={null},mag_id={2119449499},journal={null},abstract={Communication protocols are commonly designed in such a way that implementations of the same protocol can be used as peers for communication. Such a protocol is said to be symmetric. When two or more entities are employed to perform a certain task as in the case of communication protocols, the capability to do so is called interoperability and considered as the essential aspect of correctness of communicating systems. This paper deals with the problem of deriving interoperability test suite for control part of symmetric protocols. A new approach to efficient interoperability testing is described with justifications and the method of interoperability test suite derivation is shown with the example of the ATM Signaling protocol.}}
@ARTICLE{Briones_2004,title={A test generation framework for quiescent real-time systems},year={2004},author={Laura Brandán Briones and Laura Brandan Briones and Ed Brinksma and Ed Brinksma},doi={10.1007/978-3-540-31848-4_5},pmid={null},pmcid={null},mag_id={2119488343},journal={null},abstract={We present an extension of Tretmans' theory and algorithm for test generation for input-output transition systems to real-time systems. Our treatment is based on an operational interpretation of the notion of quiescence in the context of real-time behaviour. This gives rise to a family of implementation relations parameterized by observation durations for quiescence. We define a nondeterministic (parameterized) test generation algorithm that generates test cases that are sound with respect to the corresponding implementation relation. Also, the test generation is exhaustive in the sense that for each non-conforming implementation a test case can be generated that detects the non-conformance.}}
@ARTICLE{Brucker_2008,title={Extensible Universes for Object-Oriented Data Models},year={2008},author={Achim D. Brucker and Achim D. Brucker and Burkhart Wolff and Burkhart Wolff},doi={10.1007/978-3-540-70592-5_19},pmid={null},pmcid={null},mag_id={2119533840},journal={null},abstract={We present a datatype package that enables the shallow embedding technique to object-oriented specification and programming languages. This datatype package incrementally compiles an object-oriented data model to a theory containing object-universes, constructors, accessors functions, coercions between dynamic and static types, characteristic sets, their relations reflecting inheritance, and the necessary class invariants. The package is conservative, i.e., all properties are derived entirely from axiomatic definitions. As an application, we use the package for an object-oriented core-language called  imp++ , for which correctness of a Hoare-Logic with respect to an operational semantics is proven.}}
@ARTICLE{Drusinsky_2000,title={The Temporal Rover and the ATG Rover},year={2000},author={Doron Drusinsky and Doron Drusinsky},doi={10.1007/10722468_19},pmid={null},pmcid={null},mag_id={2119965246},journal={null},abstract={The Temporal Rover is a specification based verification tool for applications written in C, C++, Java, Verilog and VHDL. The tool combines formal specification, using Linear-Time Temporal Logic (LTL) and Metric Temporal Logic (MTL), with conventional simulation/execution based testing. The Temporal Rover is tailored for the verification of complex protocols and reactive systems where behavior is time dependent. The Temporal Rover generates executable code from LTL and MTL assertions written as comments in the source code. This executable source code is compiled and linked as part of the application under test. During application execution the generated code validates the executing program against the formal temporal specification requirements. Using MTL, real time and relative time constraints can be validated. A special code generator supports validation of such constraints in the field, on an embedded target.}}
@ARTICLE{Ball_2004,title={A theory of predicate-complete test coverage and generation},year={2004},author={Thomas Ball and Thomas Ball},doi={10.1007/11561163_1},pmid={null},pmcid={null},mag_id={2120713211},journal={null},abstract={Consider a program with m statements and n predicates, where the predicates are derived from the conditional statements and assertions in a program. An observable state is an evaluation of the n predicates under some state at a program statement. The goal of predicate-complete testing (PCT) is to evaluate all the predicates at every program state. That is, we wish to cover every reachable observable state (at most m × 2n of them) in a program. PCT coverage subsumes many existing control-flow coverage criteria and is incomparable to path coverage. To support the generation of tests to achieve high PCT coverage, we show how to define an upper bound U and lower bound L to the (unknown) set of reachable observable states R. These bounds are constructed automatically using Boolean (predicate) abstraction over modal transition systems and can be used to guide test generation via symbolic execution. We define a static coverage metric as |L|/|U|, which measures the ability of the Boolean abstraction to achieve high PCT coverage.}}
@ARTICLE{Razavi_2010,title={Sysfier: Actor-based formal verification of SystemC},year={2010},author={Niloofar Razavi and Niloofar Razavi and Razieh Behjati and Razieh Behjati and Razieh Behjati and Hamideh Sabouri and Hamideh Sabouri and Ehsan Khamespanah and Ehsan Khamespanah and Amin Shali and Amin Shali and Marjan Sirjani and Marjan Sirjani},doi={10.1145/1880050.1880055},pmid={null},pmcid={null},mag_id={2120872535},journal={ACM Transactions in Embedded Computing Systems},abstract={SystemC is a system-level modeling language that can be used effectively for hardware/software co-design. Since a major goal of SystemC is to enable verification at higher levels of abstraction, the tendency is now directing to introducing formal verification approaches for SystemC. In this article, we propose an approach for formal verification of SystemC designs, and provide the semantics of SystemC using Labeled Transition Systems (LTS) for this purpose. An actor-based language, Rebeca, is used as an intermediate language. SystemC designs are mapped to Rebeca models and then Rebeca verification toolset is used to verify LTL and CTL properties. To tackle the state-space explosion, Rebeca model checkers offer some reduction policies that make them appropriate for SystemC verification. The approach also benefits from the modular verification and program slicing techniques applied on Rebeca models. To show the applicability of our approach, we verified a single-cycle MIPS design and two hardware/software co-designs. The results show that our approach can effectively be used both in hardware and hardware/software co-verification.}}
@ARTICLE{Bundy_2006,title={Constructing Induction Rules for Deductive Synthesis Proofs},year={2006},author={Alan Bundy and Alan Bundy and Lucas Dixon and Lucas Dixon and Jeremy Gow and Jeremy Gow and Jacques Fleuriot and Jacques Fleuriot},doi={10.1016/j.entcs.2005.08.003},pmid={null},pmcid={null},mag_id={2120926795},journal={Electronic Notes in Theoretical Computer Science},abstract={We describe novel computational techniques for constructing induction rules for deductive synthesis proofs. Deductive synthesis holds out the promise of automated construction of correct computer programs from specifications of their desired behaviour. Synthesis of programs with iteration or recursion requires inductive proof, but standard techniques for the construction of appropriate induction rules are restricted to recycling the recursive structure of the specifications. What is needed is induction rule construction techniques that can introduce novel recursive structures. We show that a combination of rippling and the use of meta-variables as a least-commitment device can provide such novelty.}}
@ARTICLE{Harel_1997,title={Executable object modeling with statecharts},year={1997},author={David Harel and David Harel and Eran Gery and Eran Gery and Eran Gery and Eran Gery},doi={10.1109/2.596624},pmid={null},pmcid={null},mag_id={2121163444},journal={IEEE Computer},abstract={Statecharts, popular for modeling system behavior in the structural analysis paradigm, are part of a fully executable language set for modeling object-oriented systems. The languages form the core of the emerging Unified Modeling Language. The authors embarked on an effort to develop an integrated set of diagrammatic languages for object modeling, built around statecharts, and to construct a supporting tool that produces a fully executable model and allows automatic code synthesis. The language set includes two constructive modeling languages (languages containing the information needed to execute the model or translate it into executable code).}}
@ARTICLE{Andrés_2008,title={Passive Testing of Timed Systems},year={2008},author={César Andrés and César Andrés and Mercedes G. Merayo and Mercedes G. Merayo and Manuel Núñez and Manuel Núñez},doi={10.1007/978-3-540-88387-6_36},pmid={null},pmcid={null},mag_id={2121649425},journal={null},abstract={This paper presents a methodology to perform passive testing based on invariants for systems that present temporal restrictions. Invariants represent the most relevant expected properties of the implementation under test. Intuitively, an invariant expresses the fact that each time the implementation under test performs a given sequence of actions, then it must exhibit a behavior in a lapse of time reflected in the invariant. In particular, the algorithm presented in this paper are fully implemented.}}
@ARTICLE{Gaston_2005,title={11 Evaluating Coverage Based Testing},year={2005},author={Christophe Gaston and Christophe Gaston and Christophe Gaston and Dirk Seifert and Dirk Seifert},doi={10.1007/11498490_14},pmid={null},pmcid={null},mag_id={2122246338},journal={Lecture Notes in Computer Science},abstract={In the previous chapters, various formal testing theories have been discussed. The correctness of an implementation with respect to a model is denoted by a so-called conformance relation. Conformance relations are relations between mathematical abstractions of implementations and models. Based on these conformance relations, different testing strategies have been defined. In this chapter, we concentrate on formal objects used to select test suites. These formal objects are so-called coverage criteria. A coverage criterion is a property that a selected test suite has to satisfy. We explore to which purposes these coverage criteria can be used for. Then we concentrate on the fault detection ability of a test suite satisfying a given coverage criterion.}}
@ARTICLE{Ural_2007,title={An Improved Approach to Passive Testing of FSM-based Systems},year={2007},author={Hasan Ural and Hasan Ural and Zhi Xu and Zhi Xu and Fan Zhang and Fan Zhang},doi={10.1109/ast.2007.1},pmid={null},pmcid={null},mag_id={2122336022},journal={null},abstract={Fault detection is a fundamental part of passive testing which determines whether a system under test (SUT) is faulty by observing the input/output behavior of the SUT without interfering its normal operations. In this paper, we propose a new approach to finite state machine (FSM)-based passive fault detection which improves the performance of the approach in [4] and gathers more information during testing compared with the approach in [4]. The results of theoretical and experimental evaluations are reported.}}
@ARTICLE{Ábrahám_2009,title={Behavioral interface description of an object-oriented language with futures and promises ⋆},year={2009},author={Erika Ábrahám and Erika Ábrahám and Immo Grabe and Immo Grabe and Andreas Grüner and Andreas Grüner and Martín Steffen and Martin Steffen},doi={10.1016/j.jlap.2009.01.001},pmid={null},pmcid={null},mag_id={2122576055},journal={The Journal of Logic and Algebraic Programming},abstract={This paper formalizes the observable interface behavior of a concurrent, object-oriented language with futures and promises. The calculus captures the core of Creol, a language, featuring in particular asynchronous method calls and, since recently, first-class futures. The focus of the paper are open systems and we formally characterize their behavior in terms of interactions at the interface between the program and its environment. The behavior is given by transitions between typing judgments, where the absent environment is represented abstractly by an assumption context. A particular challenge is the safe treatment of promises: The erroneous situation that a promise is fulfilled twice, i.e., bound to code twice, is prevented by a resource aware type system, enforcing linear use of the writepermission to a promise. We show subject reduction and the soundness of the abstract interface description.}}
@ARTICLE{Godefroid_2010,title={Proving memory safety of floating-point computations by combining static and dynamic program analysis},year={2010},author={Patrice Godefroid and Patrice Godefroid and Johannes Kinder and Johannes Kinder},doi={10.1145/1831708.1831710},pmid={null},pmcid={null},mag_id={2123267559},journal={null},abstract={Whitebox fuzzing is a novel form of security testing based on dynamic symbolic execution and constraint solving. Over the last couple of years, whitebox fuzzers have found many new security vulnerabilities (buffer overflows) in Windows and Linux applications, including codecs, image viewers and media players. Those types of applications tend to use floating-point instructions available on modern processors, yet existing whitebox fuzzers and SMT constraint solvers do not handle floating-point arithmetic. Are there new security vulnerabilities lurking in floating-point code?   A naive solution would be to extend symbolic execution to floating-point (FP) instructions (months of work), extend SMT solvers to reason about FP constraints (months of work or more), and then face more complex constraints and an even worse path explosion problem. Instead, we propose an alternative approach, based on the rough intuition that FP code should only perform memory safe data-processing of the "payload" of an image or video file, while the non-FP part of the application should deal with buffer allocations and memory address computations, with only the latter being prone to buffer overflows and other security critical bugs. Our approach combines (1) a lightweight local path-insensitive "may" static analysis of FP instructions with (2) a high-precision whole-program path-sensitive "must" dynamic analysis of non-FP instructions. The aim of this combination is to prove memory safety of the FP part of each execution and a form of non-interference between the FP part and the non-FP part with respect to memory address computations.   We have implemented our approach using two existing tools for, respectively, static and dynamic x86 binary analysis. We present preliminary results of experiments with standard JPEG, GIF and ANI Windows parsers. For a given test suite of diverse input files, our mixed static/dynamic analysis is able to prove memory safety of FP code in those parsers for a small upfront static analysis cost and a marginal runtime expense compared to regular dynamic symbolic execution.}}
@ARTICLE{Dias_2007,title={Code-coverage Based Test Vector Generation for SystemC Designs},year={2007},author={Alair Dias and A. Dias Junior and D. Cecilio da Silva and D. Cecilio da Silva Junior},doi={10.1109/isvlsi.2007.31},pmid={null},pmcid={null},mag_id={2123459237},journal={null},abstract={This work presents a methodology for the automatic test vector generation for SystemC combinational designs based on code coverage analysis which is complementary to the functional testing. The method uses coverage information to generate test vectors capable of covering the portions of code not exercised by the black-box testing. Vectors are generated using an instrumented code followed by a numerical optimization method. This approach does not suffer from restrictions related to symbolic execution such as defining array reference values and loop boundaries, as the code is really executed together with the optimization. We expect this combined methodology to achieve total code coverage of the design and reduce the fault of omission problem, undetectable by structural testing alone.}}
@ARTICLE{Große_2005,title={HW/SW Co-Verification of a RISC CPU using Bounded Model Checking},year={2005},author={Daniel Große and Daniel Grosse and Ulrich Kühne and Ulrich Kühne and Rolf Drechsler and Rolf Drechsler},doi={10.1109/mtv.2005.12},pmid={null},pmcid={null},mag_id={2124012377},journal={null},abstract={Today, the underlying hardware of embedded systems is often verified successfully. In this context formal verification techniques allow to prove the correctness. But in embedded system design the integration of software components becomes more and more important. In this paper the authors present an integrated approach for formal verification of hardware and software. The approach is demonstrated on a RISC CPU. The verification is based on bounded model checking. Besides correctness proofs of the underlying hardware the hardware/software interface and programs using this interface can be formally verified}}
@ARTICLE{McCarthy_1993,title={Towards a Mathematical Science of Computation},year={1993},author={John McCarthy and John McCarthy and John J. McCarthy},doi={10.1007/978-94-011-1793-7_2},pmid={null},pmcid={null},mag_id={2124141583},journal={null},abstract={In this paper I shall discuss the prospects for a mathematical science of computation. In a mathematical science, it is possible to deduce from the basic assumptions, the important properties of the entities treated by the science. Thus, from Newton’s law of gravitation and his laws of motion, one can deduce that the planetary orbits obey Kepler’s laws.}}
@ARTICLE{Janota_2007,title={Reachability analysis for annotated code},year={2007},author={Mikoláš Janota and Mikoláš Janota and Radu Grigore and Radu Grigore and Michał Moskal and Michał Moskal},doi={10.1145/1292316.1292319},pmid={null},pmcid={null},mag_id={2124382086},journal={null},abstract={Well-specified programs enable code reuse and therefore techniques that help programmers to annotate code correctly are valuable. We devised an automated analysis that detects unreachable code in the presence of code annotations. We implemented it as an enhancement of the extended static checker ESC/Java2 where it serves as a check of coherency of specifications and code. In this article we define the notion of semantic unreachability, describe an algorithm for checking it and demonstrate on a case study that it detects a class of errors previously undetected, as well as describe different scenarios of these errors.}}
@ARTICLE{Otto_2010,title={Automated Termination Analysis of Java Bytecode by Term Rewriting},year={2010},author={Carsten Otto and Carsten Otto and Marc Brockschmidt and Marc Brockschmidt and Christian von Essen and Christian von Essen and Jürgen Giesl and Jürgen Giesl},doi={10.4230/lipics.rta.2010.259},pmid={null},pmcid={null},mag_id={2124765047},journal={null},abstract={We present an automated approach to prove termination of Java Bytecode (JBC) programs by automatically transforming them to term rewrite systems (TRSs). In this way, the numerous techniques and tools developed for TRS termination can now be used for imperative object-oriented languages like Java, which can be compiled into JBC.}}
@ARTICLE{Baldoni_2009,title={Choice, interoperability, and conformance in interaction protocols and service choreographies},year={2009},author={Matteo Baldoni and Matteo Baldoni and Cristina Baroglio and Cristina Baroglio and Amit K. Chopra and Amit K. Chopra and Narayan Desai and Nirmit Desai and Viviana Patti and Viviana Patti and Munindar P. Singh and Munindar P. Singh},doi={10.1145/1558109.1558129},pmid={null},pmcid={null},mag_id={2125336031},journal={null},abstract={Many real-world applications of multiagent systems require independently designed (heterogeneous) and operated (autonomous) agents to interoperate. We consider agents who offer business services and collaborate in interesting business service engagements. We formalize notions of interoperability and conformance, which appropriately support agent heterogeneity and autonomy. With respect to autonomy, our approach considers the choices that each agent has, and how their choices are coordinated so that at any time one agent leads and its counterpart follows, but with initiative fluidly shifting among the participants. With respect to heterogeneity, we characterize the variations in the agents' designs, and show how an agent may conform to a specification or substitute for another agent. Our approach addresses a challenging problem with multi-party interactions that existing approaches cannot solve. Further, we introduce a set of edit operations by which to modify an agent design so as to ensure its conformance with others.}}
@ARTICLE{Clarke_2010,title={Abstract delta modeling},year={2010},author={Dave Clarke and Dave Clarke and Dave Clarke and Dave Clarke and Michiel Helvensteijn and Michiel Helvensteijn and Ina Schaefer and Ina Schaefer},doi={10.1145/1868294.1868298},pmid={null},pmcid={null},mag_id={2125674137},journal={null},abstract={Delta modeling is an approach to facilitate automated product derivation for software product lines. It is based on a set of deltas specifying modifications that are incrementally applied to a core product. The applicability of deltas depends on feature-dependent conditions. This paper presents abstract delta modeling, which explores delta modeling from an abstract, algebraic perspective.   Compared to previous work, we take a more flexible approach with respect to conflicts between modifications and introduce the notion of conflict-resolving deltas. We present conditions on the structure of deltas to ensure unambiguous product generation.}}
@ARTICLE{Dutertre_2006,title={A fast linear-arithmetic solver for DPLL(T)},year={2006},author={Bruno Dutertre and Bruno Dutertre and Leonardo de Moura and Leonardo de Moura},doi={10.1007/11817963_11},pmid={null},pmcid={null},mag_id={2125967324},journal={null},abstract={We present a new Simplex-based linear arithmetic solver that can be integrated efficiently in the DPLL(T) framework. The new solver improves over existing approaches by enabling fast backtracking, supporting a priori simplification to reduce the problem size, and providing an efficient form of theory propagation. We also present a new and simple approach for solving strict inequalities. Experimental results show substantial performance improvements over existing tools that use other Simplex-based solvers in DPLL(T) decision procedures. The new solver is even competitive with state-of-the-art tools specialized for the difference logic fragment.}}
@ARTICLE{Kalaji_2009,title={Generating Feasible Transition Paths for Testing from an Extended Finite State Machine (EFSM)},year={2009},author={Abdul Salam Kalaji and Abdul Salam Kalaji and Robert M. Hierons and Robert M. Hierons and Stephen Swift and Stephen Swift},doi={10.1109/icst.2009.29},pmid={null},pmcid={null},mag_id={2126130789},journal={null},abstract={The problem of testing from an extended finite state machine (EFSM) can be expressed in terms of finding suitable paths through the EFSM and then deriving test data to follow the paths. A chosen path may be infeasible and so it is desirable to have methods that can direct the search for appropriate paths through the EFSM towards those that are likely to be feasible. However, generating feasible transition paths (FTPs) for model based testing is a challenging task and is an open research problem. This paper introduces a novel fitness metric that analyzes data flow dependence among the actions and conditions of the transitions of a path in order to estimate its feasibility. The proposed fitness metric is evaluated by being used in a genetic algorithm to guide the search for FTPs.}}
@ARTICLE{Hierons_2004,title={Testing conformance of a deterministic implementation against a non-deterministic stream X-machine},year={2004},author={Robert M. Hierons and Robert M. Hierons and Mark Harman and Mark Harman},doi={10.1016/j.tcs.2004.04.002},pmid={null},pmcid={null},mag_id={2126308222},journal={Theoretical Computer Science},abstract={Stream X-machines are a formalisation of extended finite state machines that have been used to specify systems. One of the great benefits of using stream X-machines, for the purpose of specification, is the associated test generation technique which produces a test that is guaranteed to determine correctness under certain design for test conditions. This test generation algorithm has recently been extended to the case where the specification is non-deterministic. However, the algorithms for testing from a non-deterministic stream X-machine currently have limitations: either they test for equivalence, rather than conformance or they restrict the source of nondeterminism allowed in the specification. This paper introduces a new test generation algorithm that overcomes both of these limitations, for situations where the implementation is known to be deterministic.}}
@ARTICLE{Artho_2005,title={Combining test case generation and runtime verification},year={2005},author={Cyrille Artho and Cyrille Artho and Howard Barringer and Howard Barringer and Allen Goldberg and Allen Goldberg and Klaus Havelund and Klaus Havelund and Sarfraz Khurshid and Sarfraz Khurshid and Mike Lowry and Michael Lowry and Corina S. Pǎsǎreanu and Corina S. Pasareanu and Corina S. Pasareanu and Grigore Roşu and Grigore Rosu and Koushik Sen and Koushik Sen and Willem Visser and Willem Visser and Richard Washington and Rich Washington},doi={10.1016/j.tcs.2004.11.007},pmid={null},pmcid={null},mag_id={2126606154},journal={Theoretical Computer Science},abstract={null}}
@ARTICLE{Reed_1988,title={A timed model for communicating sequential processes},year={1988},author={G. M. Reed and G. M. Reed and A. W. Roscoe and A. W. Roscoe},doi={10.1016/0304-3975(88)90030-8},pmid={null},pmcid={null},mag_id={2126860147},journal={Theoretical Computer Science},abstract={The parallel language CSP [9], an earlier version of which was described in [7], has become a major tool for the analysis of structuring methods and proof systems involving parallelism. The significance of CSP is in the elegance by which a few simply stated constructs (e.g., sequential and parallel composition, nondeterministic choice, concealment, and recursion) lead to a language capable of expressing the full complexity of distributed computing. The difficulty in achieving satisfactory semantic models containing these constructs has been in providing an adequate treatment of nondeterminism, deadlock, and divergence. Fortunately, as a result of an evolutionay development in [S], [lo], [15], [l], [14], [2], and [4] we now have several such models. The purpose of this paper is to report the development of the first real-time models of CSP to be compatible with the properties and proof systems of the abovementioned untimed models. Our objective in this development is the construction of a timed CSP model which satisfies the following: (1) Continuous with respect to time. The time domain should consist of all nonnegative real numbers, and there should be no lower bound on the time difference between consecutive observable events from two processes operating asynchronously in parallel. (2) Realistic. A given process should engage in only finitely many events in a bounded period of time. (3) Continuous and distributive with respect to semantic operators. All semantic operators should be continuous, and all the basic operators as defined in [2], except recursion, should distribute over nondeterministic choice. (4) Verijiable design. The model should provide a basis for the definition, specification, and verification of time critical processes with an adequate treatment of nondeterminism, which assists in avoidance of deadlock and divergence.}}
@ARTICLE{Tahat_2001,title={Requirement-based automated black-box test generation},year={2001},author={Luay Tahat and Luay Tahat and Boris Vaysburg and B. Vaysburg and Bogdan Korel and Bogdan Korel and Atef Bader and A.J. Bader},doi={10.1109/cmpsac.2001.960658},pmid={null},pmcid={null},mag_id={2126860866},journal={null},abstract={Testing large software systems is very laborious and expensive. Model-based test generation techniques are used to automatically generate tests for large software systems. However, these techniques require manually created system models that are used for test generation. In addition, generated test cases are not associated with individual requirements. In this paper, we present a novel approach of requirement-based test generation. The approach accepts a software specification as a set of individual requirements expressed in textual and SDL formats (a common practice in the industry). From these requirements, system model is automatically created with requirement information mapped to the model. The system model is used to automatically generate test cases related to individual requirements. Several test generation strategies are presented. The approach is extended to requirement-based regression test generation related to changes on the requirement level. Our initial experience shows that this approach may provide significant benefits in terms of reduction in number of test cases and increase in quality of a test suite.}}
@ARTICLE{Dolan_1989,title={Medical decision making using the analytic hierarchy process: choice of initial antimicrobial therapy for acute pyelonephritis.},year={1989},author={James G. Dolan and James G. Dolan},doi={10.1177/0272989x8900900109},pmid={2643020},pmcid={null},mag_id={2127164016},journal={Medical Decision Making},abstract={The analytic hierarchy process (AHP) was used to determine which of seven recommended antibiotic regimens represented optimal initial therapy for a young woman hospitalized for treat ment of acute pyelonephritis. The model included the following criteria: maximize cure, minimize adverse effects (broken down into very serious, serious, and limited), minimize antibiotic resis tance, and minimize cost (divided into total cost and patient cost). The criteria were weighted according to judgments made by 61 practicing clinicians. Alternatives were compared relative to the criteria using published information on the expected frequencies of urinary pathogens and drug toxicity, local antibiotic sensitivities and antibiotic charges, and expert opinion regarding their propensities for inducing antimicrobial resistance. The analysis identified ampicillin com bined with gentamicin as the optimal regimen. This study illustrates several features of the AHP that make it promising for use in medical decision making: its a...}}
@ARTICLE{Salem_2003,title={Formal Semantics of Synchronous SystemC},year={2003},author={Ashraf Salem and Ashraf Salem},doi={10.1109/date.2003.1253637},pmid={null},pmcid={null},mag_id={2127322244},journal={null},abstract={In this article, a denotational definition of a synchronous subset of SystemC is proposed. The subset treated includes modules, processes, threads, wait statement, ports and signals. We propose a formal model for SystemC delta delay. Also, we give a complete semantic definition for the language's two-phase scheduler. The proposed semantic can constitute a base for validating the equivalence of synchronous HDL subsets.}}
@ARTICLE{Harel_1996,title={Executable object modeling with statecharts},year={1996},author={David Harel and David Harel and Eran Gery and Eran Gery and Eran Gery and Eran Gery},doi={10.1109/icse.1996.493420},pmid={null},pmcid={null},mag_id={2127964314},journal={null},abstract={This paper reports on an effort to develop an integrated set of diagrammatic languages for modeling object-oriented systems, and to construct a supporting tool. The goal is for models to be intuitive and well-structured, yet fully executable and analyzable, enabling automatic synthesis of usable and efficient code in object-oriented languages such as C++. At the heart of the modeling method is the language of statecharts for specifying object behavior, and a hierarchical OMT-like language for describing the structure of classes and their inter-relationships, that we call O-charts. Objects can interact by event generation, or by direct invocation of operations. In the interest of keeping the exposition manageable, we leave out some technically involved topics, such as multiple-thread concurrency and active objects.}}
@ARTICLE{Deng_2007,title={Kiasan/KUnit: Automatic Test Case Generation and Analysis Feedback for Open Object-oriented Systems},year={2007},author={Xianghua Deng and Xianghua Deng and . Robby and Robby and John Hatcliff and John Hatcliff},doi={10.1109/taic.part.2007.32},pmid={null},pmcid={null},mag_id={2127973326},journal={null},abstract={We demonstrate how a static analysis feedback and unit test case generation framework, KUnit, built on the Bogor/Kiasan symbolic execution engine provides an effective unit test case generation for sequential heap-intensive Java programs (whose computation structures are incomplete - open systems). KUnit leverages method contract information to better deal with open object-oriented systems and to support automatic mock object creation. To facilitate application to realistic software, KUnit allows the scope/cost of the analysis and test case generation to be controlled via notions of heap configuration coverage. In a broad experimental study on 23 Java data structure modules, we show that it is able to: (a) achieve 100% feasible branch coverage on almost all methods by using only small heap configurations; (b) improve on competing tools for coverage achieved; size of test suites; and time to generate test suites.}}
@ARTICLE{Kuhn_2004,title={Software fault interactions and implications for software testing},year={2004},author={D. Richard Kuhn and D.R. Kuhn and Dolores R Wallace and Dolores R. Wallace and Al Gallo and A M. Gallo},doi={10.1109/tse.2004.24},pmid={null},pmcid={null},mag_id={2128204165},journal={IEEE Transactions on Software Engineering},abstract={Exhaustive testing of computer software is intractable, but empirical studies of software failures suggest that testing can in some cases be effectively exhaustive. We show that software failures in a variety of domains were caused by combinations of relatively few conditions. These results have important implications for testing. If all faults in a system can be triggered by a combination of n or fewer parameters, then testing all n-tuples of parameters is effectively equivalent to exhaustive testing, if software behavior is not dependent on complex event sequences and variables have a small set of discrete values.}}
@ARTICLE{Noble_1999,title={Object ownership for dynamic alias protection},year={1999},author={James Noble and James Noble and David G. Clarke and Dave Clarke and Dave Clarke and Dave Clarke and John Potter and John Potter},doi={10.1109/tools.1999.809424},pmid={null},pmcid={null},mag_id={2128445548},journal={null},abstract={Interobject references in object-oriented programs allow arbitrary aliases between objects. By breaching objects' encapsulation boundaries, these aliases can make programs hard to understand and especially hard to debug. We propose using an explicit, run-time notion of object ownership to control aliases between objects in dynamically typed languages. Dynamically checking object ownership as a program runs ensures the program maintains the encapsulation topology intended by the programmer.}}
@ARTICLE{Summers_2011,title={Freedom before commitment: a lightweight type system for object initialisation},year={2011},author={Alexander J. Summers and Alexander J. Summers and Peter Mueller and Peter Mueller},doi={10.1145/2048066.2048142},pmid={null},pmcid={null},mag_id={2128769069},journal={null},abstract={One of the main purposes of object initialisation is to establish invariants such as a field being non-null or an immutable data structure containing specific values. These invariants are then implicitly assumed by the rest of the implementation, for instance, to ensure that a field may be safely dereferenced or that immutable data may be accessed concurrently. Consequently, letting an object escape from its constructor is dangerous; the escaping object might not yet satisfy its invariants, leading to errors in code that relies on them. Nevertheless, preventing objects entirely from escaping from their constructors is too restrictive; it is often useful to call auxiliary methods on the object under initialisation or to pass it to another constructor to set up mutually-recursive structures.   We present a type system that tracks which objects are fully initialised and which are still under initialisation. The system can be used to prevent objects from escaping, but also to allow safe escaping by making explicit which objects might not yet satisfy their invariants. We designed, formalised and implemented our system as an extension to a non-null type system, but it is not limited to this application. Our system is conceptually simple and requires little annotation overhead; it is sound and sufficiently expressive for many common programming idioms. Therefore, we believe it to be the first such system suitable for mainstream use.}}
@ARTICLE{Benavides_2010,title={Automated analysis of feature models 20 years later: A literature review},year={2010},author={David Benavides and David Benavides and Sergio Segura and Sergio Segura and Antonio Ruiz–Cortés and Antonio Ruiz-Cortés},doi={10.1016/j.is.2010.01.001},pmid={null},pmcid={null},mag_id={2128967738},journal={Information Systems},abstract={Software product line engineering is about producing a set of related products that share more commonalities than variabilities. Feature models are widely used for variability and commonality management in software product lines. Feature models are information models where a set of products are represented as a set of features in a single model. The automated analysis of feature models deals with the computer-aided extraction of information from feature models. The literature on this topic has contributed with a set of operations, techniques, tools and empirical results which have not been surveyed until now. This paper provides a comprehensive literature review on the automated analysis of feature models 20 years after of their invention. This paper contributes by bringing together previously disparate streams of work to help shed light on this thriving area. We also present a conceptual framework to understand the different proposals as well as categorise future contributions. We finally discuss the different studies and propose some challenges to be faced in the future.}}
@ARTICLE{Sangiorgi_1998,title={On the bisimulation proof method},year={1998},author={Davide Sangiorgi and Davide Sangiorgi},doi={10.1017/s0960129598002527},pmid={null},pmcid={null},mag_id={2128995870},journal={Mathematical Structures in Computer Science},abstract={The most popular method for establishing bisimilarities among processes is to exhibit bisimulation relations. By definition, R is a bisimulation relation if R progresses to R itself, i.e., pairs of processes in R can match each other's actions and their derivatives are again in R.We study generalisations of the method aimed at reducing the size of the relations to be exhibited and hence relieving the proof work needed to establish bisimilarity results. We allow a relation R to progress to a different relation F (R), where F is a function on relations. Functions that can be safely used in this way (i.e., such that if R progresses to F (R), then R only includes pairs of bisimilar processes) are sound. We give a simple condition that ensures soundness. We show that the class of sound functions contains non-trivial functions and we study the closure properties of the class with respect to various important function constructors, like composition, union and iteration. These properties allow us to construct sophisticated sound functions – and hence sophisticated proof techniques for bisimilarity – from simpler ones.The usefulness of our proof techniques is supported by various non-trivial examples drawn from the process algebras CCS and π-calculus. They include the proof of the unique solution of equations and the proof of a few properties of the replication operator. Among these, there is a novel result that justifies the adoption of a simple form of prefix-guarded replication as the only form of replication in the π-calculus.}}
@ARTICLE{Sassone_1996,title={Models for concurrency: towards a classification},year={1996},author={Vladimiro Sassone and Vladimiro Sassone and Mogens Nielsen and Mogens Nielsen and Glynn Winskel and Glynn Winskel},doi={10.1016/s0304-3975(96)00011-4},pmid={null},pmcid={null},mag_id={2129296169},journal={Theoretical Computer Science},abstract={Models for concurrency can be classified with respect to three relevant parameters: behaviour/ system, interleaving/noninterleaving, linear/branching time. When modelling a process, a choice concerning such parameters corresponds to choosing the level of abstraction of the resulting semantics. In this paper, we move a step towards a classification of models for concurrency based on the parameters above. Formally, we choose a representative of any of the eight classes of models obtained by varying the three parameters, and we study the formal relationships between them using the language of category theory.}}
@ARTICLE{Harel_1996,title={The STATEMATE semantics of statecharts},year={1996},author={David Harel and David Harel and A. Naamad and A. Naamad},doi={10.1145/235321.235322},pmid={null},pmcid={null},mag_id={2129395888},journal={ACM Transactions on Software Engineering and Methodology},abstract={We describe the semantics of statecharts as implemented in the STATEMATE system. This was the first executable semantics defined for the language and has been in use for almost a decade. In terms of the controversy around whether changes made in a given step should take effect in the current step or in the next one, this semantics adopts the latter approach.}}
@ARTICLE{McNaughton_1960,title={Regular Expressions and State Graphs for Automata},year={1960},author={Robert McNaughton and Robert McNaughton and Robert McNaughton and Hisao Yamada and Hisao Yamada and H. Yamada},doi={10.1109/tec.1960.5221603},pmid={null},pmcid={null},mag_id={2129414328},journal={Ire Transactions on Electronic Computers},abstract={Algorithms are presented for 1) converting a state graph describing the behavior of an automaton to a regular expression describing the behavior of the same automaton (section 2), and 2) for converting a regular expression into a state graph (sections 3 and 4). These algorithms are justified by theorems, and examples are given. The first section contains a brief introduction to state graphs and the regular-expression language.}}
@ARTICLE{Manna_1995,title={STeP: The Stanford Temporal Prover},year={1995},author={Zohar Manna and Zohar Manna and Nikolaj Bjørner and Nikolaj Bjørner and Nikolaj Bjørner and Anca Browne and Anca Browne and Edward Yi Chang and Edward Yi Chang and Edward Y. Chang and Michael A. Colón and Michael Colón and Luca de Alfaro and Luca de Alfaro and Harish Devarajan and Harish Devarajan and Arjun Kapur and Arjun Kapur and Jaejin Lee and Jaejin Lee and Henny B. Sipma and Henny B. Sipma and Tomás E. Uribe and Tomás E. Uribe},doi={10.1007/3-540-59293-8_237},pmid={null},pmcid={null},mag_id={2129664123},journal={null},abstract={We describe the Stanford Temporal Prover (STeP), a system being developed to support the computer-aided formal verification of concurrent and reactive systems based on temporal specifications. Unlike systems based on model-checking, STeP is not restricted to finite-state systems. It combines model checking and deductive methods to allow the verification of a broad class of systems, including programs with infinite data domains, N-process programs, and N-component circuit designs, for arbitrary N. In short, STeP has been designed with the objective of combining the expressiveness of deductive methods with the simplicity of model checking. The verification process is for the most part automatic. User interaction occurs mostly at the highest, most intuitive level, primarily through a graphical proof language of verification diagrams. Efficient simplification methods, decision procedures, and invariant generation techniques are then invoked automatically to prove resulting first-order verification conditions with minimal assistance. We describe the performance of the system when applied to several examples, including the N-process dining philosopher''s program, Szymanski''s N-process mutual exclusion algorithm, and a distributed N-way arbiter circuit.}}
@ARTICLE{Nielsen_2001,title={Test generation for time critical systems: Tool and case study},year={2001},author={Brian Nielsen and Brian Nielsen and Arne Skou and Arne Skou},doi={10.1109/emrts.2001.934021},pmid={null},pmcid={null},mag_id={2129931199},journal={null},abstract={Generating timed test sequences by hand is error-prone and time consuming, and it is easy to overlook important scenarios. The paper presents a tool based on formal methods that automatically computes a test suite for conformance testing of time critical systems. The generated tests are selected on the basis of a coverage criterion of the specification. The tool guarantees production of sound test cases only, and is able to produce a complete covering test suite. We demonstrate the tool by generating test cases for the Philips Audio Protocol.}}
@ARTICLE{Bouyer_2008,title={Optimal infinite scheduling for multi-priced timed automata},year={2008},author={Patricia Bouyer and Patricia Bouyer and Ed Brinksma and Ed Brinksma and Kim Guldstrand Larsen and Kim Guldstrand Larsen},doi={10.1007/s10703-007-0043-4},pmid={null},pmcid={null},mag_id={2130136014},journal={null},abstract={This paper is concerned with the derivation of infinite schedules for timed automata that are in some sense optimal. To cover a wide class of optimality criteria we start out by introducing an extension of the (priced) timed automata model that includes both costs and rewards as separate modelling features. A precise definition is then given of what constitutes optimal infinite behaviours for this class of models. We subsequently show that the derivation of optimal non-terminating schedules for such double-priced timed automata is computable. This is done by a reduction of the problem to the determination of optimal mean-cycles in finite graphs with weighted edges. This reduction is obtained by introducing the so-called corner-point abstraction, a powerful abstraction technique of which we show that it preserves optimal schedules.}}
@ARTICLE{Claßen_2010,title={Model checking lots of systems: efficient verification of temporal properties in software product lines},year={2010},author={Andreas Claßen and Andreas Classen and Patrick Heymans and Patrick Heymans and Pierre‐Yves Schobbens and Pierre-Yves Schobbens and Axel Legay and Axel Legay and Jean-François Raskin and Jean-François Raskin},doi={10.1145/1806799.1806850},pmid={null},pmcid={null},mag_id={2130195901},journal={null},abstract={In product line engineering, systems are developed in families and differences between family members are expressed in terms of features. Formal modelling and verification is an important issue in this context as more and more critical systems are developed this way. Since the number of systems in a family can be exponential in the number of features, two major challenges are the scalable modelling and the efficient verification of system behaviour. Currently, the few attempts to address them fail to recognise the importance of features as a unit of difference, or do not offer means for automated verification.   In this paper, we tackle those challenges at a fundamental level. We first extend transition systems with features in order to describe the combined behaviour of an entire system family. We then define and implement a model checking technique that allows to verify such transition systems against temporal properties. An empirical evaluation shows substantial gains over classical approaches.}}
@ARTICLE{Albert_2008,title={COSTA: Design and Implementation of a Cost and Termination Analyzer for Java Bytecode},year={2008},author={Elvira Albert and Elvira Albert and Puri Arenas and Puri Arenas and Samir Genaim and Samir Genaim and Germán Puebla and Germán Puebla and Damiano Zanardini and Damiano Zanardini},doi={10.1007/978-3-540-92188-2_5},pmid={null},pmcid={null},mag_id={2130413583},journal={null},abstract={This paper describes the architecture of  costa  , an abstract interpretation based  cos  t and  t  ermination  a  nalyzer for Java bytecode. The system receives as input a bytecode program, (a choice of) a resource of interest and tries to obtain an upper bound of the resource consumption of the program.  costa  provides several non-trivial notions of cost, as the consumption of the heap, the number of bytecode instructions executed and the number of calls to a specific method. Additionally,  costa  tries to prove termination of the bytecode program which implies the boundedness of any resource consumption. Having cost and termination together is interesting, as both analyses share most of the machinery to, respectively, infer cost upper bounds and to prove that the execution length is always finite (i.e., the program terminates). We report on experimental results which show that  costa  can deal with programs of realistic size and complexity, including programs which use Java libraries. To the best of our knowledge, this system provides for the first time evidence that resource usage analysis can be applied to a realistic object-oriented, bytecode programming language.}}
@ARTICLE{Neto_2007,title={A survey on model-based testing approaches: a systematic review},year={2007},author={Arilo Cláudio Dias Neto and Arilo Claudio Dias Neto and Rajesh Subramanyan and Rajesh Subramanyan and Marlon Vieira and Marlon Vieira and Guilherme Horta Travassos and Guilherme Horta Travassos},doi={10.1145/1353673.1353681},pmid={null},pmcid={null},mag_id={2130422196},journal={null},abstract={This paper describes a systematic review performed on model-based testing (MBT) approaches. A selection criterion was used to narrow the initially identified four hundred and six papers to focus on seventy-eight papers. Detailed analysis of these papers shows where MBT approaches have been applied, the characteristics, and the limitations. The comparison criteria includes representation models, support tools, test coverage criteria, the level of automation, intermediate models, and the complexity. This paper defines and explains the review methodology and presents some results.}}
@ARTICLE{Henzinger_1994,title={Symbolic model checking for real-time systems},year={1994},author={Thomas A. Henzinger and Thomas A. Henzinger and Xavier Nicollin and Xavier Nicollin and Joseph Sifakis and Joseph Sifakis and Sergio Yovine and Sergio Yovine},doi={10.1006/inco.1994.1045},pmid={null},pmcid={null},mag_id={2130773092},journal={Information & Computation},abstract={Abstract   We describe finite-state programs over real-numbered time in a guarded-command language with real-valued clocks or, equivalently, as finite automata with real-valued clocks. Model checking answers the question which states of a real-time program satisfy a branching-time specification (given in an extension of CTL with clock variables). We develop an algorithm that computes this set of states symbolically as a fixpoint of a functional on state predicates, without constructing the state space. For this purpose, we introduce a μ-calculus on computation trees over real-numbered time. Unfortunately, many standard program properties, such as response for all nonzeno execution sequences (during which time diverges), cannot be characterized by fixpoints: we show that the expressiveness of the timed μ-calculus is incomparable to the expressiveness of timed CTL. Fortunately, this result does not impair the symbolic verification of "implementable" real-time programs-those whose safety constraints are machine-closed with respect to diverging time and whose fairness constraints are restricted to finite upper bounds on clock values. All timed CTL properties of such programs are shown to be computable as finitely approximable fixpoints in a simple decidable theory.}}
@ARTICLE{Bringmann_2008,title={Model-Based Testing of Automotive Systems},year={2008},author={Eckard Bringmann and E. Bringmann and Andreas Krämer and A. Kramer},doi={10.1109/icst.2008.45},pmid={null},pmcid={null},mag_id={2130966309},journal={null},abstract={In recent years the development of automotive embedded devices has changed from an electrical and mechanical engineering discipline to a combination of software and electrical/mechanical engineering. The effects of this change on development processes, methods, and tools as well as on required engineering skills were very significant and are still ongoing today. At present there is a new trend in the automotive industry towards model-based development. Software components are no longer handwritten in C or Assembler code but modeled with MATLAB/Simulinktrade, Statemate, or similar tools. However, quality assurance of model-based developments, especially testing, is still poorly supported. Many development projects require creation of expensive proprietary testing solutions. In this paper we discuss the characteristics of automotive model-based development processes, the consequences for test development and the need to reconsider testing procedures in practice. Furthermore, we introduce the test tool "TPT" which masters the complexity of model-based testing in the automotive domain. To illustrate this statement we present a small automotive case study. TPT is based on graphical test models that are not only easy to understand but also powerful enough to express very complex, fully automated closed loop tests in real-time. TPT has been initially developed by Daimler Software Technology Research. It is already in use in many production-vehicle development projects at car manufacturers and suppliers.}}
@ARTICLE{Malinský_2010,title={VERIFICATION OF FLEXRAY START-UP MECHANISM BY TIMED AUTOMATA},year={2010},author={Jan Malinský and Jan Malinský and Jiří Novák and Jiří Novák and Jiří Novák},doi={10.2478/v10178-010-0039-z},pmid={null},pmcid={null},mag_id={2131323984},journal={Metrology and Measurement Systems},abstract={This contribution deals with the modelling of a select ed part of a new automotive communication standard called FlexRay. In particular, it focuses on the mechanism en suring the start-up of a FlexRay network. The model has been created with the use of timed automata and verifi ed. For this purpose the UPPAAL software tool has been used that allows the modelling of discrete event syste ms with the use of timed automata, and subsequently the verification of the model with the use of suitable que ries compiled in the so called computation tree logic. This model can be used to look for incorrect settings of ti me parameters of communication nodes in the network that prevent network start-up and subsequently the start of the car. The existenc e of this model also opens the way for finding possible errors in the standard. On the basis of the model, the work gives a case study of the start-up mechanism behaviour verification in a FlexRay network consisting of three communication nodes.}}
@ARTICLE{Hartog_2002,title={Probabilistic Extensions of Semantical Models},year={2002},author={den Jeremy Ian Hartog and den Jeremy Ian Hartog},doi={null},pmid={null},pmcid={null},mag_id={2132500409},journal={null},abstract={null}}
@ARTICLE{Stark_2005,title={Free-algebra models for the π-calculus},year={2005},author={Ian Stark and Ian Stark},doi={10.1007/978-3-540-31982-5_10},pmid={null},pmcid={null},mag_id={2132524091},journal={null},abstract={The finite @p-calculus has an explicit set-theoretic functor-category model that is known to be fully abstract for strong late bisimulation congruence. We characterize this as the initial free algebra for an appropriate set of operations and equations in the enriched Lawvere theories of Plotkin and Power. Thus we obtain a novel algebraic description for models of the @p-calculus, and validate an existing construction as the universal such model. The algebraic operations are intuitive, covering name creation, communication of names over channels, and nondeterministic choice; the equations then combine these features in a modular fashion. We work in an enriched setting, over a ''possible worlds'' category of sets indexed by available names. This expands significantly on the classical notion of algebraic theories: we can specify operations that act only on fresh names, or have arities that vary as processes evolve. Based on our algebraic theory of @p we describe a category of models for the @p-calculus, and show that they all preserve bisimulation congruence. We develop a direct construction of free models in this category; and generalise previous results to prove that all free-algebra models are fully abstract. We show how local modifications to the theory can give alternative models for @pI and the early @p-calculus. From the theory of @p we also obtain a Moggi-style computational monad, suitable for a programming language semantics of mobile communicating systems. This addresses the challenging area of correctly combining computational monads: in this case those for concurrency, name generation, and communication.}}
@ARTICLE{Andrés_2009,title={Passive Testing of Stochastic Timed Systems},year={2009},author={César Andrés and César Andrés and Mercedes G. Merayo and Mercedes G. Merayo and Manuel Núñez and Manuel Núñez},doi={10.1109/icst.2009.35},pmid={null},pmcid={null},mag_id={2132616926},journal={null},abstract={In this paper we introduce a formal methodology to perform passive testing, based on invariants, for systems where the passing of time is represented in probabilistic terms by means of probability distributions functions. In our approach, invariants express the fact that each time the implementation under test performs a given sequence of actions, then it must exhibit a behavior according to the probability distribution functions reflected in the invariant. We present algorithms to decide the correctness of the proposed invariants with respect to a given specification. Once we know that an invariant is correct, we check whether the execution traces observed from the implementation respect the invariant. In addition to the theoretical framework we have developed a tool, called PASTE, that helps in the automation of our passive testing approach. We have used the tool to obtain experimental results from the application of our methodology.}}
@ARTICLE{Tretmans_1996,title={Test Generation with Inputs, Outputs, and Quiescence},year={1996},author={Jan Tretmans and Jan Tretmans},doi={10.1007/3-540-61042-1_42},pmid={null},pmcid={null},mag_id={2132622501},journal={null},abstract={This paper studies testing based on labelled transition systems, using the assumption that implementations communicate with their environment via inputs and outputs. Such implementations are formalized by restricting the class of transition systems to those systems that can always accept input actions, as in input/output automata. Implementation relations, formalizing the notion of conformance of these implementations with respect to labelled transition system specifications, are defined analogous to the theory of testing equivalence and preorder. A test generation algorithm is given, which is proved to produce a sound and exhaustive test suite from a specification, i.e., a test suite that fully characterizes the set of correct implementations.}}
@ARTICLE{Brillout_2009,title={Mutation-based test case generation for simulink models},year={2009},author={Angelo Brillout and Angelo Brillout and Nannan He and Nannan He and Michele Mazzucchi and Michele Mazzucchi and Daniel Kroening and Daniel Kroening and Mitra Purandare and Mitra Purandare and Mitra Purandare and Philipp Rümmer and Philipp Rümmer and Georg Weißenbacher and Georg Weissenbacher},doi={10.1007/978-3-642-17071-3_11},pmid={null},pmcid={null},mag_id={2133350402},journal={null},abstract={The Matlab/Simulink language has become the standard formalism for modeling and implementing control software in areas like avionics, automotive, railway, and process automation. Such software is often safety critical, and bugs have potentially disastrous consequences for people and material involved. We define a verification methodology to assess the correctness of Simulink programs by means of automated test-case generation. In the style of fault- and mutation-based testing, the coverage of a Simulink program by a test suite is defined in terms of the detection of injected faults. Using bounded model checking techniques, we are able to effectively and automatically compute test suites for given fault models. Several optimisations are discussed to make the approach practical for realistic Simulink programs and fault models, and to obtain accurate coverage measures.}}
@ARTICLE{Godefroid_2007,title={Compositional dynamic test generation},year={2007},author={Patrice Godefroid and Patrice Godefroid},doi={10.1145/1190216.1190226},pmid={null},pmcid={null},mag_id={2133612077},journal={null},abstract={Dynamic test generation is a form of dynamic program analysis that attempts to compute test inputs to drive a program along a specific program path. Directed Automated Random Testing, or DART for short, blends dynamic test generation with model checking techniques with the goal of systematically executing all feasible program paths of a program while detecting various types of errors using run-time checking tools (like Purify, for instance). Unfortunately, systematically executing all feasible program paths does not scale to large, realistic programs.This paper addresses this major limitation and proposes to perform dynamic test generation compositionally, by adapting known techniques for interprocedural static analysis. Specifically, we introduce a new algorithm, dubbed SMART for Systematic Modular Automated Random Testing, that extends DART by testing functions in isolation, encoding test results as function summaries expressed using input preconditions and output postconditions, and then re-using those summaries when testing higher-level functions. We show that, for a fixed reasoning capability, our compositional approach to dynamic test generation (SMART) is both sound and complete compared to monolithic dynamic test generation (DART). In other words, SMART can perform dynamic test generation compositionally without any reduction in program path coverage. We also show that, given a bound on the maximum number of feasible paths in individual program functions, the number of program executions explored by SMART is linear in that bound, while the number of program executions explored by DART can be exponential in that bound. We present examples of C programs and preliminary experimental results that illustrate and validate empirically these properties.}}
@ARTICLE{Chen_2008,title={Coverage-driven automatic test generation for uml activity diagrams},year={2008},author={Mingsong Chen and Mingsong Chen and Prabhat Mishra and Prabhat Mishra and D. Kalita and Dhrubajyoti Kalita},doi={10.1145/1366110.1366145},pmid={null},pmcid={null},mag_id={2133744424},journal={null},abstract={Due to the increasing complexity of today's embedded systems, the analysis and validation of such systems is becoming a major challenge. UML is gradually adopted in the embedded system design as a system level specification. One of the major bottlenecks in the validation of UML activity diagrams is the lack of automated techniques for directed test generation. This paper proposes an automated test generation approach for the UML activity diagrams. The contribution of this paper is the use of specification coverage to generate properties as well as design models to enable directed test generation using model checking. Our experimental results demonstrate that our approach can drastically reduce the validation effort in both specification and implementation levels.}}
@ARTICLE{Hong_2003,title={Data flow testing as model checking},year={2003},author={Henry Hong and Hyoung Seok Hong and Sung Deok and Sungdeok Cha and Insup Lee and Insup Lee and Oleg Sokolsky and Oleg Sokolsky and Hasan Ural and Hasan Ural},doi={10.1109/icse.2003.1201203},pmid={null},pmcid={null},mag_id={2133853708},journal={null},abstract={This paper presents a model checking-based approach to data flow testing. We characterize data flow oriented coverage criteria in temporal logic such that the problem of test generation is reduced to the problem of finding witnesses for a set of temporal logic formulas. The capability of model checkers to construct witnesses and counterexamples allows test generation to be fully automatic. We discuss complexity issues in minimal cost test generation and describe heurstic test generation algorithms. We illustrate our approach using CTL as temporal logic and SMV as model checker.}}
@ARTICLE{Wassermann_2008,title={Static detection of cross-site scripting vulnerabilities},year={2008},author={Gary Wassermann and Gary Wassermann and Zhendong Su and Zhendong Su},doi={10.1145/1368088.1368112},pmid={null},pmcid={null},mag_id={2134646643},journal={null},abstract={Web applications support many of our daily activities, but they often have security problems, and their accessibility makes them easy to exploit. In cross-site scripting (XSS), an attacker exploits the trust a web client (browser) has for a trusted server and executes injected script on the browser with the server's privileges. In 2006, XSS constituted the largest class of newly reported vulnerabilities making it the most prevalent class of attacks today. Web applications have XSS vulnerabilities because the validation they perform on untrusted input does not suffice to prevent that input from invoking a browser's JavaScript interpreter, and this validation is particularly difficult to get right if it must admit some HTML mark-up. Most existing approaches to finding XSS vulnerabilities are taint-based and assume input validation functions to be adequate, so they either miss real vulnerabilities or report many false positives.   This paper presents a static analysis for finding XSS vulnerabilities that directly addresses weak or absent input validation. Our approach combines work on tainted information flow with string analysis. Proper input validation is difficult largely because of the many ways to invoke the JavaScript interpreter; we face the same obstacle checking for vulnerabilities statically, and we address it by formalizing a policy based on the W3C recommendation, the Firefox source code, and online tutorials about closed-source browsers. We provide effective checking algorithms based on our policy. We implement our approach and provide an extensive evaluation that finds both known and unknown vulnerabilities in real-world web applications.}}
@ARTICLE{Boyer_1977,title={A fast string searching algorithm},year={1977},author={Robert Boyer and Robert S. Boyer and Robert S. Boyer and J. Strother Moore and J. Strother Moore},doi={10.1145/359842.359859},pmid={null},pmcid={null},mag_id={2134826720},journal={Communications of The ACM},abstract={An algorithm is presented that searches for the location, “ i l” of the first occurrence of a character string, “ pat ,” in another string, “ string .” During the search operation, the characters of  pat  are matched starting with the last character of  pat . The information gained by starting the match at the end of the pattern often allows the algorithm to proceed in large jumps through the text being searched. Thus the algorithm has the unusual property that, in most cases, not all of the first  i  characters of  string  are inspected. The number of characters actually inspected (on the average) decreases as a function of the length of  pat . For a random English pattern of length 5, the algorithm will typically inspect  i /4 characters of  string  before finding a match at  i . Furthermore, the algorithm has been implemented so that (on the average) fewer than  i  +  patlen  machine instructions are executed. These conclusions are supported with empirical evidence and a theoretical analysis of the average behavior of the algorithm. The worst case behavior of the algorithm is linear in  i  +  patlen , assuming the availability of array space for tables linear in  patlen  plus the size of the alphabet. 3~}}
@ARTICLE{Jia_2011,title={An Analysis and Survey of the Development of Mutation Testing},year={2011},author={Yunyi Jia and Yue Jia and Mark Harman and Mark Harman},doi={10.1109/tse.2010.62},pmid={null},pmcid={null},mag_id={2135841285},journal={IEEE Transactions on Software Engineering},abstract={Mutation Testing is a fault-based software testing technique that has been widely studied for over three decades. The literature on Mutation Testing has contributed a set of approaches, tools, developments, and empirical results. This paper provides a comprehensive analysis and survey of Mutation Testing. The paper also presents the results of several development trend analyses. These analyses provide evidence that Mutation Testing techniques and tools are reaching a state of maturity and applicability, while the topic of Mutation Testing itself is the subject of increasing interest.}}
@ARTICLE{Clarke_2003,title={External Uniqueness Is Unique Enough},year={2003},author={Dave Clarke and Dave Clarke and Dave Clarke and Dave Clarke and Tobias Wrigstad and Tobias Wrigstad},doi={10.1007/978-3-540-45070-2_9},pmid={null},pmcid={null},mag_id={2135868037},journal={null},abstract={External uniqueness is a surprising new way to add unique references to an OOPL. The idea is that an externally unique reference is the only reference into an aggregate from outside the aggregate. Internal references which do not escape the boundary of the aggregate are innocuous and therefore permitted. Based on ownership types, our proposal not only overcomes an abstraction problem from which existing uniqueness proposals suffer, it also enables many examples which are inherently not unique, such as a unique reference to a set of links in a doubly-linked list, without losing the benefits of uniqueness.}}
@ARTICLE{Schneider_2003,title={Verification of Reactive Systems: Formal Methods and Algorithms},year={2003},author={Klaus Schneider and Klaus Schneider},doi={null},pmid={null},pmcid={null},mag_id={2136099086},journal={null},abstract={This book is a solid foundation of the most important formalisms used for specification and verification of reactive systems. In particular,the textpresents all important results on m-calculus, w-automata, and temporal logics, shows the relationships between these formalisms and describes state-of-the-art verification procedures for them. It also discusses advantages and disadvantages of these formalisms, and shows up their strengths and weaknesses. Most results are given with detailed proofs, so that the presentation is almost self-contained. Includes all definitionswithout relying on other material Proves all theoremsin detail Presents detailed algorithms in pseudo-code for verification as well as translations to other formalisms}}
@ARTICLE{Leino_2002,title={Data abstraction and information hiding},year={2002},author={K. Rustan M. Leino and K. Rustan M. Leino and Greg Nelson and Greg Nelson},doi={10.1145/570886.570888},pmid={null},pmcid={null},mag_id={2136371406},journal={ACM Transactions on Programming Languages and Systems},abstract={This article describes an approach for verifying programs in the presence of data abstraction and information hiding, which are key features of modern programming languages with objects and modules. This article draws on our experience building and using an automatic program checker, and focuses on the property of modular soundness: that is, the property that the separate verifications of the individual modules of a program suffice to ensure the correctness of the composite program. We found this desirable property surprisingly difficult to achieve. A key feature of our methodology for modular soundness is a new specification construct: the abstraction dependency, which reveals which concrete variables appear in the representation of a given abstract variable, without revealing the abstraction function itself. This article discusses in detail two varieties of abstraction dependencies: static and dynamic. The article also presents a new technical definition of modular soundness as a monotonicity property of verifiability with respect to scope and uses this technical definition to formally prove the modular soundness of a programming discipline for static dependencies.}}
@ARTICLE{Artzi_2010,title={Finding Bugs in Web Applications Using Dynamic Test Generation and Explicit-State Model Checking},year={2010},author={Shay Artzi and Shay Artzi and Adam Kieżun and Adam Kiezun and Julian Dolby and Julian Dolby and Frank Tip and Frank Tip and Danny Dig and Danny Dig and Amit Paradkar and Amit Paradkar and M. Ernst and Michael D. Ernst},doi={10.1109/tse.2010.31},pmid={null},pmcid={null},mag_id={2136579066},journal={IEEE Transactions on Software Engineering},abstract={Web script crashes and malformed dynamically generated webpages are common errors, and they seriously impact the usability of Web applications. Current tools for webpage validation cannot handle the dynamically generated pages that are ubiquitous on today's Internet. We present a dynamic test generation technique for the domain of dynamic Web applications. The technique utilizes both combined concrete and symbolic execution and explicit-state model checking. The technique generates tests automatically, runs the tests capturing logical constraints on inputs, and minimizes the conditions on the inputs to failing tests so that the resulting bug reports are small and useful in finding and fixing the underlying faults. Our tool Apollo implements the technique for the PHP programming language. Apollo generates test inputs for a Web application, monitors the application for crashes, and validates that the output conforms to the HTML specification. This paper presents Apollo's algorithms and implementation, and an experimental evaluation that revealed 673 faults in six PHP Web applications.}}
@ARTICLE{Noble_1998,title={Flexible Alias Protection},year={1998},author={James Noble and James Noble and Jan Vítek and Jan Vitek and John Potter and John Potter},doi={10.1007/bfb0054091},pmid={null},pmcid={null},mag_id={2136839747},journal={null},abstract={Aliasing is endemic in object oriented programming. Because an object can be modified via any alias, object oriented programs are hard to understand, maintain, and analyse. Flexible alias protection is a conceptual model of inter-object relationships which limits the visibility of changes via aliases, allowing objects to be aliased but mitigating the undesirable effects of aliasing. Flexible alias protection can be checked statically using programmer supplied aliasing modes and imposes no runtime overhead. Using flexible alias protection, programs can incorporate mutable objects, immutable values, and updatable collections of shared objects, in a natural object oriented programming style, while avoiding the problems caused by aliasing.}}
@ARTICLE{Reynolds_2002,title={Separation logic: a logic for shared mutable data structures},year={2002},author={John Reynolds and John C. Reynolds},doi={10.1109/lics.2002.1029817},pmid={null},pmcid={null},mag_id={2137628566},journal={null},abstract={In joint work with Peter O'Hearn and others, based on early ideas of Burstall, we have developed an extension of Hoare logic that permits reasoning about low-level imperative programs that use shared mutable data structure. The simple imperative programming language is extended with commands (not expressions) for accessing and modifying shared structures, and for explicit allocation and deallocation of storage. Assertions are extended by introducing a "separating conjunction" that asserts that its subformulas hold for disjoint parts of the heap, and a closely related "separating implication". Coupled with the inductive definition of predicates on abstract data structures, this extension permits the concise and flexible description of structures with controlled sharing. In this paper, we survey the current development of this program logic, including extensions that permit unrestricted address arithmetic, dynamically allocated arrays, and recursive procedures. We also discuss promising future directions.}}
@ARTICLE{Batth_2006,title={Multiple Fault Models for Timed FSMs},year={2006},author={Samrat S. Batth and S.S. Batth and M. Ümit Uyar and M. Umit Uyar and M.U. Uyar and Yu Wang and Yu Wang and Mariusz A. Fecko and Mariusz A. Fecko},doi={10.1109/imtc.2006.328260},pmid={null},pmcid={null},mag_id={2138117051},journal={null},abstract={An implementation under test (IUT) can be formally described using finite-state machines (FSMs). Due to the presence of inherent timing constraints and variables in a communication protocol, an IUT is modeled more accurately by using extended finite-state machines (EFSMs). However, infeasible paths due to the conflicts among timing condition and action variables of EFSMs can complicate the test generation process. The fault detection capability of the graph augmentation method given in M. U. Uyar et al. (2005) and M. A. Fecko et al. (2000) are analyzed in the presence of multiple timing faults. The complexity increases with the consideration of the concurrent running and expiring of timers in a protocol. It is proven that, by using our graph augmentation models, a faulty IUT will be detected for the multiple occurrences of pairwise combinations of a class of timing faults}}
@ARTICLE{Voelter_2007,title={Product Line Implementation using Aspect-Oriented and Model-Driven Software Development},year={2007},author={Markus Voelter and Markus Voelter and Iris Groher and Iris Groher},doi={10.1109/splc.2007.28},pmid={null},pmcid={null},mag_id={2138295157},journal={null},abstract={Software product line engineering aims to reduce development time, effort, cost, and complexity by taking advantage of the commonality within a portfolio of similar products. The effectiveness of a software product line approach directly depends on how well feature variability within the portfolio is implemented and managed throughout the development lifecycle, from early analysis through maintenance and evolution. This paper presents an approach that facilitates variability implementation, management and tracing by integrating model-driven and aspect-oriented software development. Features are separated in models and composed by aspect-oriented composition techniques on model level. Model transformations support the transition from problem to solution domain. Aspect-oriented techniques enable the explicit expression and modularization of variability on model, code, and template level The presented concepts are illustrated with a case study of a home automation system.}}
@ARTICLE{Garavel_2009,title={Verification of an industrial systemC/TLM model using LOTOS and CADP},year={2009},author={Hubert Garavel and Hubert Garavel and Claude Helmstetter and Claude Helmstetter and Olivier Ponsini and Olivier Ponsini and Wendelin Serwe and Wendelin Serwe and Wendelin Serwe and Wendelin Serwe},doi={10.1109/memcod.2009.5185377},pmid={null},pmcid={null},mag_id={2138446696},journal={null},abstract={SystemC/TLM is a widely used standard for system level descriptions of complex architectures. It is particularly useful for fast simulation, thus allowing early development and testing of the targeted software. In general, formal verification of SystemC/TLM relies on the translation of the complete model into a language accepted by a verification tool. In this paper, we present an approach to the validation of a SystemC/TLM description by translation into LOTOS, reusing as much as possible of the original SystemC/TLM C++ code. To this end, we exploit a feature offered by the formal verification toolbox CADP, namely the import of external C code in a LOTOS model. We report on experiments of our approach on the BDisp, a complex graphical processing unit designed by STMicroelectronics.}}
@ARTICLE{Baeten_1991,title={Real time process algebra},year={1991},author={J.C.M. Baeten and Jos C. M. Baeten and JA Jan Bergstra and Jan A. Bergstra},doi={10.1007/bf01898401},pmid={null},pmcid={null},mag_id={2138549619},journal={null},abstract={We describe an axiom system ACPp that incorporates real timed actions. Many examples are provided in order to explain the intuitive contents of the notation. ACPp is a generalisation of ACP. This implies that some of the axioms have to be relaxed and that ACP can be recovered as a special case from it. The purpose of ACPp is to serve as a specification language for real time systems. The axioms of ACPp explain its operational meaning in an algebraic form.}}
@ARTICLE{Clarke_2003,title={Object ownership and containment},year={2003},author={David G. Clarke and David Gerard Clarke},doi={null},pmid={null},pmcid={null},mag_id={2138855072},journal={null},abstract={Object-oriented programming relies on inter-object aliases to implement but it is when mutable state interacts with aliasing that problems arise. Through aliasing an object's state can be changed without the object being aware of the changes, potentially violating the object's invariants. This problem is fundamentally unresolvable. Many idioms such as the Observer design pattern rely on it. Hence aliasing cannot be eliminated from object-oriented programming, it can only be managed. 
Various proposals have appeared in the literature addressing the issue of alias management. The most promising are based on alias encapsulation, which limits access to objects to within certain well-defined boundaries. Our approach called ownership types falls into this category. An object can specify the objects it owns, called its representation, and which objects can access its representation. A type system protects the representation by enforcing a well-defined containment invariant. 
Our approach is a formal one. Ownership types are cast as a type system using an minor extension to Abadi and Cardelli's object calculus with subtyping. With this formalisation we prove the soundness of our ownership types system and demonstrate that well-typed programs satisfy the containment invariant. In addition, we also provide a firm grounding to enable ownership types to be safely added to an object-oriented programming language with inheritance, subtyping, and nested classes, as well as offering a sound basis for future work. Our type system can model aggregate objects with multiple interface objects sharing representation and friendly functions which access multiple objects' private representations, among other examples, thus overcoming weaknesses in existing alias management schemes.}}
@ARTICLE{López_2001,title={A Testing Theory for Generally Distributed Stochastic Processes},year={2001},author={Natalia López and Natalia López and Manuel Núñez and Manuel Núñez},doi={10.1007/3-540-44685-0_22},pmid={null},pmcid={null},mag_id={2138879983},journal={null},abstract={In this paper we present a testing theory for stochastic processes. This theory is developed to deal with processes which probability distributions are not restricted to be exponential. In order to define this testing semantics, we compute the probability with which a process passes a test before an amount of time has elapsed. Two processes will be equivalent if they return the same probabilities for any test T and any time t. The key idea consists in joining all the random variables associated with the computations that the composition of process and test may perform. The combination of the values that this random variable takes and the probabilities of executing the actions belonging to the computation will give us the desired probabilities. Finally, we relate our stochastic testing semantics with other notions of testing.}}
@ARTICLE{Sinha_1999,title={Criteria for testing exception-handling constructs in Java programs},year={1999},author={Saurabh Sinha and Saurabh Sinha and Mary Jean Harrold and Mary Jean Harrold and Mary Jean Harrold},doi={10.1109/icsm.1999.792624},pmid={null},pmcid={null},mag_id={2139250089},journal={null},abstract={Exception-handling constructs provide a mechanism for mixing exceptions and a facility for designating protected code by attaching exception handlers to blocks of code. Despite the frequency of their occurrences, the behavior of exception-handling constructs is often the least understood and poorly tested part of a program. The presence of such constructs introduces new structural elements, such as control-flow paths, in a program. To adequately test such programs, these new structural elements must be considered for coverage during structural testing. In this paper, we describe a class of adequacy criteria that can be used to test the behavior of exception-handling constructs. We present a subsumption hierarchy of the criteria, and illustrate the relationship of the criteria to those found in traditional subsumption hierarchies. We describe techniques for generating the testing requirements for the criteria using our control-flow representations. We also describe a methodology for applying the criteria to unit and integration testing of programs that contain exception-handling constructs.}}
@ARTICLE{Sammapun_2003,title={Formalizing Java-MaC},year={2003},author={Usa Sammapun and Usa Sammapun and Raman Sharykin and Raman Sharykin and Margaret DeLap and Margaret DeLap and Myong Kim and Myong Kim and Steve Zdancewic and Steve Zdancewic},doi={10.1016/s1571-0661(04)81048-x},pmid={null},pmcid={null},mag_id={2139744838},journal={Electronic Notes in Theoretical Computer Science},abstract={Abstract   The Java-MaC framework is a run-time verification system for Java programs that can be used to dynamically test and enforce safety policies. This paper presents a formal model of the Java-MaC safety properties in terms of an operational semantics for Middleweight Java, a realistic subset of full Java. This model is intended to be used as a framework for studying the correctness of Java-MaC program instrumentation, optimizations, and future experimentation with run-time monitor expressiveness. As a preliminary demonstration of this model's applicability for these tasks, the paper sketches a correctness result for a simple program instrumentation scheme.}}
@ARTICLE{Cheon_2005,title={Model Variables: Cleanly Supporting Abstraction in Design By Contract},year={2005},author={Yoonsik Cheon and Yoonsik Cheon and Gary T. Leavens and Gary T. Leavens and Murali Sitaraman and Murali Sitaraman and Stephen H. Edwards and Stephen H. Edwards},doi={10.1002/spe.649},pmid={null},pmcid={null},mag_id={2140001025},journal={Software - Practice and Experience},abstract={In design by contract (DBC), assertions are typically written using program variables and query methods. The lack of separation between program code and assertions is confusing, because readers do not know what code is intended for use in the program and what code is only intended for specification purposes. This lack of separation also creates a potential runtime performance penalty, even when runtime assertion checks are disabled, due to both the increased memory footprint of the program and the execution of code maintaining that part of the program's state intended for use in specifications. To solve these problems, we present a new way of writing and checking DBC assertions without directly referring to concrete program states, using ‘model’, i.e. specification-only, variables and methods. The use of model variables and methods does not incur the problems mentioned above, but it also allow one to write more easily assertions that are abstract, concise, and independent of representation details, and hence more readable and maintainable. We implemented these features in the runtime assertion checker for the Java Modeling Language (JML), but the approach could also be implemented in other DBC tools. Copyright © 2005 John Wiley & Sons, Ltd.}}
@ARTICLE{Gönenç_1970,title={A Method for the Design of Fault Detection Experiments},year={1970},author={Güney Gönenç and Güney Gönenç},doi={10.1109/t-c.1970.222975},pmid={null},pmcid={null},mag_id={2140047593},journal={IEEE Transactions on Computers},abstract={A methodical procedure for organization of fault detection experiments for synchronous sequential machines possessing distinguishing sequences (DS) is given. The organization is based on the transition checking approach. The checking experiment is considered in three concatenative parts: 1) the initial sequence which brings the machine under test into a specific state, 2) the α-sequence to recognize all the states and to establish the information about the next states under the input DS, and 3) the β-sequence to check all the individual transitions in the state table.}}
@ARTICLE{Clarke_1979,title={Programming Language Constructs for Which It Is Impossible To Obtain Good Hoare Axiom Systems},year={1979},author={Edmund M. Clarke and Edmund M. Clarke},doi={10.1145/322108.322121},pmid={null},pmcid={null},mag_id={2140295918},journal={Journal of the ACM},abstract={Hoare axiom systems for establishing partial correctness of programs may fail to be complete because of (a) incompleteness of the assertion language relative to the underlying interpretation or (b) inability of the assertion language to express the mvanants of loops Cook has shown that if there IS a complete proof system for the assertion language (l e all true formulas of the assertion language) and if the assertion language satisfies a natural expresstbthty condition then a sound and complete axiom system for a large subset of Algol may be devised We exhibit programming language constructs for which it ms impossible to obtain sound and complete sets of Hoare axioms even in this special sense of Cook's These constructs include (0 recursive procedures with procedure parameters in a programming language which uses static scope of ldenufiers and (u) coroutmes in a language which allows parameterless recurslve procedures Modifications of these constructs for which sound and complete systems of axioms may be obtained are also discussed}}
@ARTICLE{Hong_2001,title={Automatic Test Generation From Statecharts Using Model Checking},year={2001},author={Henry Hong and Hyoung Seok Hong and Insup Lee and Insup Lee and Oleg Sokolsky and Oleg Sokolsky},doi={null},pmid={null},pmcid={null},mag_id={2140320019},journal={null},abstract={This paper describes a method for automatic generation of tests from specifications written in Statecharts. These tests are to be applied to an implementation to validate the consistency of the implementation with respect to the specification. For test coverage, we adapt the notions of control-flow coverage and data-flow coverage used traditionally in software testing to Statecharts. In particular, we redefine these notions for Statecharts and formulate test generation problem as finding a counterexample during the model checking of a Statecharts specification. The ability to generate a counterexample allows test generation to be automatic. To illustrate our approach, we show how to translate Statecharts to SMV, after defining the semantics of Statecharts using Kripke structures. We, then, describe how to formulate various test coverage criteria in CTL, and show how the SMV model checker can be used to generate only executable tests.}}
@ARTICLE{Rivest_1993,title={Inference of Finite Automata Using Homing Sequences},year={1993},author={Ronald L. Rivest and Ronald L. Rivest and Robert E. Schapire and Robert E. Schapire},doi={10.1006/inco.1993.1021},pmid={null},pmcid={null},mag_id={2140606869},journal={Information & Computation},abstract={Abstract   We present new algorithms for inferring an unknown finite-state automaton from its input/output behavior, even in the absence of a means of resetting the machine to a start state. A key technique used is inference of a homing sequence for the unknown automaton. Our inference procedures experiment with the unknown machine, and from time to time require a teacher to supply counterexamples to incorrect conjectures about the structure of the unknown automaton. In this setting, we describe a learning algorithm that, with probability 1 − δ, outputs a correct description of the unknown machine in time polynomial in the automaton′s size, the length of the longest counterexample, and log(1/δ). We present an analogous algorithm that makes use of a diversity-based representation of the finite-state system. Our algorithms are the first which are provably effective for these problems, in the absence of a "reset." We also present probabilistic algorithms for permutation automata which do not require a teacher to supply counterexamples. For inferring a permutation automaton of diversity D, we improve the best previous time bound by roughly a factor of D3/log D.}}
@ARTICLE{Khoumsi_2000,title={A new method for testing real time systems},year={2000},author={Ahmed Khoumsi and Ahmed Khoumsi},doi={10.1109/rtcsa.2000.896424},pmid={null},pmcid={null},mag_id={2140932781},journal={null},abstract={In this article, we present a framework for testing real-time systems. More precisely, (1) we propose a method for generating test sequences, (2) we determine constraints which guarantee the executability of test sequences, and (3) we propose a procedure and a test architecture for executing test sequences. Contrary to other test methods for real-time systems, our method avoids state explosion.}}
@ARTICLE{Ludewig_2003,title={Models in software engineering – an introduction},year={2003},author={Jochen Ludewig and Jochen Ludewig},doi={10.1007/s10270-003-0020-3},pmid={null},pmcid={null},mag_id={2141871659},journal={Software and Systems Modeling},abstract={Modelling is a concept fundamental for software engineering. In this paper, the word is defined and discussed from various perspectives. The most important types of models are presented, and examples are given.}}
@ARTICLE{Cavalli_2003,title={New approaches for passive testing using an Extended Finite State Machine specification},year={2003},author={Ana Cavalli and Ana Cavalli and Christine Gervy and Caroline Gervy and Svetlana Prokopenko and Svetlana A. Prokopenko},doi={10.1016/s0950-5849(03)00063-6},pmid={null},pmcid={null},mag_id={2142840138},journal={Information & Software Technology},abstract={null}}
@ARTICLE{Nilsson_2006,title={Test Case Generation for Mutation-based Testing of Timeliness},year={2006},author={Robert Nilsson and Robert Nilsson and Robert Nilsson and Jeff Offutt and Jeff Offutt and Jonas Mellin and Jonas Mellin},doi={10.1016/j.entcs.2006.10.010},pmid={null},pmcid={null},mag_id={2142909298},journal={Electronic Notes in Theoretical Computer Science},abstract={Temporal correctness is crucial for real-time systems. Few methods exist to test temporal correctness and most methods used in practice are ad-hoc. A problem with testing real-time applications is the response-time dependency on the execution order of concurrent tasks. Execution order in turn depends on execution environment properties such as scheduling protocols, use of mutual exclusive resources as well as the point in time when stimuli is injected. Model based mutation testing has previously been proposed to determine the execution orders that need to be verified to increase confidence in timeliness. An effective way to automatically generate such test cases for dynamic real-time systems is still needed. This paper presents a method using heuristic-driven simulation to generate test cases.}}
@ARTICLE{Lauenroth_2009,title={Model Checking of Domain Artifacts in Product Line Engineering},year={2009},author={Kim Lauenroth and Kim Lauenroth and Klaus Pohl and Klaus Pohl and Simon Toehning and Simon Toehning},doi={10.1109/ase.2009.16},pmid={null},pmcid={null},mag_id={2142960988},journal={null},abstract={In product line engineering individual products are derived from the domain artifacts of the product line. The reuse of the domain artifacts is constraint by the product line variability. Since domain artifacts are reused in several products, product line engineering benefits from the verification of domain artifacts. For verifying development artifacts, model checking is a well-established technique in single system development. However, existing model checking approaches do not incorporate the product line variability and are hence of limited use for verifying domain artifacts. In this paper we present an extended model checking approach which takes the product line variability into account when verifying domain artifacts. Our approach is thus able to verify that every permissible product (specified with I/O-automata) which can be derived from the product line fulfills the specified properties (specified with CTL). Moreover, we use two examples to validate the applicability of our approach and report on the preliminary validation results.}}
@ARTICLE{Tretmans_1996,title={Conformance testing with labelled transition systems: implementation relations and test generation},year={1996},author={Jan Tretmans and Jan Tretmans},doi={10.1016/s0169-7552(96)00017-7},pmid={null},pmcid={null},mag_id={2143236904},journal={Computer Networks and Isdn Systems},abstract={This paper studies testing based on labelled transition systems, presenting two test generation algorithms with their corresponding implementation relations. The first algorithm assumes that implementations communicate with their environment via symmetric, synchronous interactions. It is based on the theory of testing equivalence and preorder, as is most of the testing theory for labelled transition systems, and it is found in the literature in some slightly different variations. The second algorithm is based on the assumption that implementations communicate with their environment via inputs and outputs. Such implementations are formalized by restricting the class of labelled transition systems to those systems that can always accept input actions. For these implementations a testing theory is developed, analogous to the theory of testing equivalence and preorder. It consists of implementation relations formalizing the notion of conformance of these implementations with respect to labelled transition system specifications, test cases and test suites, test execution, the notion of passing a test suite, and the test generation algorithm, which is proved to produce sound test suites for one of the implementation relations.}}
@ARTICLE{Sebesta_1989,title={Concepts of programming languages},year={1989},author={Robert W. Sebesta},doi={null},pmid={null},pmcid={null},mag_id={2143436119},journal={null},abstract={From the Publisher:
This best-selling book, now in its fourth edition, provides a wide-ranging and in-depth discussion of programming language concepts. As in previous editions, the author describes fundamental concepts of programming languages by presenting design issues of the various language constructs, examining the design choices for these constructs in a few common languages, and critically comparing the design alternatives. The book covers the most widely used methods of syntax description and introduces the most common approaches to describing the semantics of programming languages. Discussions of implementation methods and issues are integrated throughout the book.}}
@ARTICLE{Grieskamp_2011,title={Model-based quality assurance of protocol documentation: tools and methodology},year={2011},author={Wolfgang Grieskamp and Wolfgang Grieskamp and Nicolás Kicillof and Nicolas Kicillof and Keith Stobie and Keith Stobie and Vı́ctor Braberman and Victor Braberman},doi={10.1002/stvr.427},pmid={null},pmcid={null},mag_id={2144062101},journal={Software Testing, Verification & Reliability},abstract={Microsoft is producing interoperability documentation for Windows client–server and server–server protocols. The Protocol Engineering Team in the Windows organization is responsible for verifying the documentation to ensure that it is of the highest quality. Various test-driven methods are being applied including, when appropriate, a model-based approach. This paper describes core aspects of the quality assurance process and tools that were put in place, and specifically focuses on model-based testing (MBT). Experience so far confirms that MBT works and that it scales, provided it is accompanied by sound tool support and clear methodological guidance. Copyright © 2010 John Wiley & Sons, Ltd.}}
@ARTICLE{Ferrante_1987,title={The program dependence graph and its use in optimization},year={1987},author={Jeanne Ferrante and Jeanne Ferrante and Karl J. Ottenstein and Karl J. Ottenstein and Joe Warren and Joe D. Warren},doi={10.1145/24039.24041},pmid={null},pmcid={null},mag_id={2144344516},journal={ACM Transactions on Programming Languages and Systems},abstract={In this paper we present an intermediate program representation, called the  program dependence graph  ( PDG ), that makes explicit both the data and control dependences for each operation in a program. Data dependences have been used to represent only the relevant data flow relationships of a program. Control dependences are introduced to analogously represent only the essential control flow relationships of a program. Control dependences are derived from the usual control flow graph. Many traditional optimizations operate more efficiently on the PDG. Since dependences in the PDG connect computationally related parts of the program, a single walk of these dependences is sufficient to perform many optimizations. The PDG allows transformations such as vectorization, that previously required special treatment of control dependence, to be performed in a manner that is uniform for both control and data dependences. Program transformations that require interaction of the two dependence types can also be easily handled with our representation. As an example, an incremental approach to modifying data dependences resulting from branch deletion or loop unrolling is introduced. The PDG supports incremental optimization, permitting transformations to be triggered by one another and applied only to affected dependences.}}
@ARTICLE{Dssouli_1999,title={Test development for communication protocols: towards automation},year={1999},author={Rachida Dssouli and Rachida Dssouli and Kassem Saleh and Kassem Saleh and E.M. Aboulhamid and El Mostapha Aboulhamid and Abdeslam En‐Nouaary and Abdeslam En-Nouaary and C. Bourhfir and C. Bourhfir},doi={10.1016/s1389-1286(99)00063-8},pmid={null},pmcid={null},mag_id={2146112670},journal={Computer Networks},abstract={null}}
@ARTICLE{Vanoverberghe_2008,title={Using Dynamic Symbolic Execution to Improve Deductive Verification},year={2008},author={Dries Vanoverberghe and Dries Vanoverberghe and Dries Vanoverberghe and Nikolaj Bjørner and Nikolaj Bjørner and Jonathan de Halleux and Jonathan de Halleux and Jonathan de Halleux and Wolfram Schulte and Wolfram Schulte and Nikolai Tillmann and Nikolai Tillmann},doi={10.1007/978-3-540-85114-1_4},pmid={null},pmcid={null},mag_id={2146544336},journal={null},abstract={One of the most challenging problems in deductive program verification is to find inductive program invariants typically expressed using quantifiers. With strong-enough invariants, existing provers can often prove that a program satisfies its specification. However, provers by themselves do not find such invariants. We propose to automatically generate executable test cases from failed proof attempts using dynamic symbolic execution by exploring program code as well as contracts with quantifiers. A developer can analyze the test cases with a traditional debugger to determine the cause of the error; the developer may then correct the program or the contracts and repeat the process.}}
@ARTICLE{Raz_2002,title={Semantic anomaly detection in online data sources},year={2002},author={Orna Raz and Orna Raz and Philip Koopman and Philip Koopman and Mary Shaw and Mary Shaw},doi={10.1145/581339.581378},pmid={null},pmcid={null},mag_id={2146908974},journal={null},abstract={Much of the software we use for everyday purposes incorporates elements developed and maintained by someone other than the developer. These elements include not only code and databases but also dynamic data feeds from online data sources. Although everyday software is not mission critical, it must be dependable enough for practical use. This is limited by the dependability of the incorporated elements. It is particularly difficult to evaluate the dependability of dynamic data feeds, because they may be changed by their proprietors as they are used. Further, the specifications of these data feeds are often even sketchier than the specifications of software components. We demonstrate a method of inferring invariants about the normal behavior of dynamic data feeds. We use these invariants as proxies for specifications to perform on-going detection of anomalies in the data feed. We show the feasibility of our approach and demonstrate its usefulness for semantic anomaly detection: identifying occasions when a dynamic data feed is delivering unreasonable values, even though its behavior may be superficially acceptable (i.e., it is delivering parsable results in a timely fashion).}}
@ARTICLE{Bouyer_2006,title={Extended Timed Automata and Time Petri Nets},year={2006},author={Patricia Bouyer and Patricia Bouyer and Serge Haddad and Serge Haddad and Pierre-Alain Reynier and Pierre-Alain Reynier},doi={10.1109/acsd.2006.6},pmid={null},pmcid={null},mag_id={2147041049},journal={null},abstract={Timed Automata (TA) and Time Petri Nets (TPN) are two well-established formal models for real-time systems. Recently, a linear transformation of TA to TPNs preserving reachability properties and timed languages has been proposed, which does however not extend to larger classes of TA which would allow diagonal constraints or more general resets of clocks. Though these features do not add expressiveness, they yield exponentially more concise models. In this work, we propose two translations: one from extended TA to TPNs whose size is either linear or quadratic in the size of the original TA, depending on the features which are allowed; another one from a parallel composition of TA to TPNs, which is also linear. As a consequence, we get that TPNs are exponentially more concise than TA. Keywords: Time Petri Nets, Timed Automata, Conciseness, Reachability Analysis.}}
@ARTICLE{Brockschmidt_2011,title={Automated detection of non-termination and nullpointerexceptions for Java Bytecode},year={2011},author={Marc Brockschmidt and Marc Brockschmidt and Thomas Ströder and Thomas Ströder and Carsten Otto and Carsten Otto and Jürgen Giesl and Jürgen Giesl},doi={10.1007/978-3-642-31762-0_9},pmid={null},pmcid={null},mag_id={2147263168},journal={null},abstract={Recently, we developed an approach for automated termination proofs of Java Bytecode (JBC), which is based on constructing and analyzing termination graphs. These graphs represent all possible program executions in a finite way. In this paper, we show that this approach can also be used to detect non-termination or NullPointerExceptions. Our approach automatically generates witnesses, i.e., calling the program with these witness arguments indeed leads to non-termination resp. to a NullPointerException. Thus, we never obtain "false positives". We implemented our results in the termination prover AProVE and provide experimental evidence for the power of our approach.}}
@ARTICLE{Hänsel_2011,title={An Evolutionary Algorithm for the Generation of Timed Test Traces for Embedded Real-Time Systems},year={2011},author={Joachim Hänsel and Joachim Hänsel and Daniela De Rose and Daniela Rose and Paula Herber and Paula Herber and Sabine Glesner and Sabine Glesner},doi={10.1109/icst.2011.37},pmid={null},pmcid={null},mag_id={2147338786},journal={null},abstract={In safety-critical applications, the real-time behavior is crucial for the correctness of the overall system and must be tested thoroughly. However, the generation of test traces that cover most or all of the desired behavior of a real-time system is a difficult challenge. In this paper, we present an evolutionary algorithm that generates timed test traces, which achieve a given transition coverage. We generate these traces from a timed automata model. Our main contribution is a novel approach to encode timed test traces as individuals of an evolutionary algorithm. The major difficulty in doing so is that test traces for embedded real-time systems have to be very long. To solve this problem, we introduce the notion of blocks, which simplify long traces by cutting them into pieces. With that, we reduce the search space significantly. Furthermore, we have implemented crossover and mutation operators and a fitness function that takes time-dependent behavior implicitly into account. We show the success of our approach by experimental results from an anti-lock braking system.}}
@ARTICLE{Zee_2008,title={Full functional verification of linked data structures},year={2008},author={Karen Zee and Karen Zee and Viktor Kunčak and Viktor Kuncak and Martin Rinard and Martin Rinard},doi={10.1145/1375581.1375624},pmid={null},pmcid={null},mag_id={2147650421},journal={null},abstract={We present the first verification of full functional correctness for a range of linked data structure implementations, including mutable lists, trees, graphs, and hash tables. Specifically, we present the use of the Jahob verification system to verify formal specifications, written in classical higher-order logic, that completely capture the desired behavior of the Java data structure implementations (with the exception of properties involving execution time and/or memory consumption). Given that the desired correctness properties include intractable constructs such as quantifiers, transitive closure, and lambda abstraction, it is a challenge to successfully prove the generated verification conditions.   Our Jahob verification system uses integrated reasoning to split each verification condition into a conjunction of simpler subformulas, then apply a diverse collection of specialized decision procedures, first-order theorem provers, and, in the worst case, interactive theorem provers to prove each subformula. Techniques such as replacing complex subformulas with stronger but simpler alternatives, exploiting structure inherently present in the verification conditions, and, when necessary, inserting verified lemmas and proof hints into the imperative source code make it possible to seamlessly integrate all of the specialized decision procedures and theorem provers into a single powerful integrated reasoning system. By appropriately applying multiple proof techniques to discharge different subformulas, this reasoning system can effectively prove the complex and challenging verification conditions that arise in this context.}}
@ARTICLE{Uyar_2008,title={Algorithms for Modeling a Class of Single Timing Faults in Communication Protocols},year={2008},author={M. Ümit Uyar and M.U. Uyar and Samrat S. Batth and S.S. Batth and Yu Wang and Yu Wang and Mariusz A. Fecko and Mariusz A. Fecko},doi={10.1109/tc.2007.70772},pmid={null},pmcid={null},mag_id={2148000467},journal={IEEE Transactions on Computers},abstract={A set of graph augmentation algorithms is introduced to model a class of timing faults in timed-EFSM models. It is shown that the test sequences generated based on our models can detect 1 -clock and n-clock timing faults and incorrect timer setting faults in an implementation under test (IUT). It is proven that the size of the augmented graph resulting from our augmentation algorithms is on the same order of magnitude as that of the original specification.}}
@ARTICLE{Ancona_2007,title={RPython: a step towards reconciling dynamically and statically typed OO languages},year={2007},author={Davide Ancona and Davide Ancona and Massimo Ancona and Massimo Ancona and Massimo Ancona and Massimo Ancona and Antonio Cuni and Antonio Cuni and Nicholas D. Matsakis and Nicholas D. Matsakis},doi={10.1145/1297081.1297091},pmid={null},pmcid={null},mag_id={2148535470},journal={null},abstract={Although the C-based interpreter of Python is reasonably fast, implementations on the CLI or the JVM platforms offers some advantages in terms of robustness and interoperability. Unfortunately, because the CLI and JVM are primarily designed to execute statically typed, object-oriented languages, most dynamic language implementations cannot use the native bytecodes for common operations like method calls and exception handling; as a result, they are not able to take full advantage of the power offered by the CLI and JVM.   We describe a different approach that attempts to preserve the flexibility of Python, while still allowing for efficient execution. This is achieved by limiting the use of the more dynamic features of Python to an initial, bootstrapping phase. This phase is used to construct a final RPython (Restricted Python) program that is actually executed. RPython is a proper subset of Python, is statically typed, and does not allow dynamic modification of class or method definitions; however, it can still take advantage of Python features such as mixins and first-class methods and classes.   This paper presents an overview of RPython, including its design and its translation to both CLI and JVM bytecode. We show how the bootstrapping phase can be used to implement advanced features, like extensible classes and generative programming. We also discuss what work remains before RPython is truly ready for general use, and compare the performance of RPython with that of other approaches.}}
@ARTICLE{Grotker_2002,title={System Design with SystemC},year={2002},author={T. Grotker and Thorsten Grotker},doi={null},pmid={null},pmcid={null},mag_id={2148740615},journal={null},abstract={The emergence of the system-on-chip (SoC) era is creating many new challenges at all stages of the design process. Engineers are reconsidering how designs are specified, partitioned and verified. With systems and software engineers programming in C/C++ and their hardware counterparts working in hardware description languages such as VHDL and Verilog, problems arise from the use of different design languages, incompatible tools and fragmented tool flows. Momentum is building behind the SystemC language and modeling platform as the best solution for representing functionality, communication, and software and hardware implementations at various levels of abstraction. The reason is clear: increasing design complexity demands very fast executable specifications to validate system concepts, and only C/C++ delivers adequate levels of abstraction, hardware-software integration, and performance. System design today also demands a single common language and modeling foundation in order to make interoperable system--level design tools, services and intellectual property a reality. SystemC is entirely based on C/C++ and the complete source code for the SystemC reference simulator can be freely downloaded from www.systemc.org and executed on both PCs and workstations. System Design and SystemC provides a comprehensive introduction to the powerful modeling capabilities of the SystemC language, and also provides a large and valuable set of system level modeling examples and techniques. Written by experts from Cadence Design Systems, Inc. and Synopsys, Inc. who were deeply involved in the definition and implementation of the SystemC language and reference simulator, this book will provide you with the key concepts you need to be successful with SystemC. System Design with SystemC thoroughly covers the new system level modeling capabilities available in SystemC 2.0 as well as the hardware modeling capabilities available in earlier versions of SystemC. designed and implemented the SystemC language and reference simulator, this book will provide you with the key concepts you need to be successful with SystemC. System Design with SystemC will be of interest to designers in industry working on complex system designs, as well as students and researchers within academia. All of the examples and techniques described within this book can be used with freely available compilers and debuggers e no commercial software is needed. Instructions for obtaining the free source code for the examples obtained within this book are included in the first chapter.}}
@ARTICLE{Noroozi_2011,title={Synchronizing asynchronous conformance testing},year={2011},author={Neda Noroozi and N Neda Noroozi and Ramtin Khosravi and Ramtin Khosravi and Mohammad Reza Mousavi and Mohammad Reza Mousavi and Tim A. C. Willemse and Tim A. C. Willemse},doi={10.1007/978-3-642-24690-6_23},pmid={null},pmcid={null},mag_id={2149542769},journal={null},abstract={We present several theorems and their proofs which enable using synchronous testing techniques such as input output conformance testing (ioco) in order to test implementations only accessible through asynchronous communication channels. These theorems define when the synchronous test-cases are sufficient for checking all aspects of conformance that are observable by asynchronous interaction with the implementation under test.}}
@ARTICLE{Beyer_2008,title={Program Analysis with Dynamic Precision Adjustment},year={2008},author={Dirk Beyer and Dirk Beyer and Thomas A. Henzinger and Thomas A. Henzinger and Grégory Théoduloz and Grégory Théoduloz},doi={10.1109/ase.2008.13},pmid={null},pmcid={null},mag_id={2149647957},journal={null},abstract={We present and evaluate a framework and tool for combining multiple program analyses which allows the dynamic (on-line) adjustment of the precision of each analysis depending on the accumulated results. For example, the explicit tracking of the values of a variable may be switched off in favor of a predicate abstraction when and where the number of different variable values that have been encountered has exceeded a specified threshold. The method is evaluated on verifying the SSH client/server software and shows significant gains compared with predicate abstraction-based model checking.}}
@ARTICLE{Zeng_2010,title={Timing Analysis and Optimization of FlexRay Dynamic Segment},year={2010},author={Haibo Zeng and Haibo Zeng and Arkadeb Ghosal and Arkadeb Ghosal and Arkadeb Ghosal and Marco Di Natale and Marco Di Natale},doi={10.1109/cit.2010.329},pmid={null},pmcid={null},mag_id={2149791408},journal={null},abstract={FlexRay is a new high bandwidth communication protocol for automotive systems, providing time-triggered transmission of periodic frames in a static segment and priority-based scheduling in a dynamic segment. Analysis techniques are required to compute bounds for the FlexRay frame response times, before the standard is used for safety- and time-critical applications. Moreover, the design of a FlexRay schedule is not an easy task because of protocol constraints and multiple design objectives. In this paper, we first study the problem of timing analysis of frames transmitted in the FlexRay dynamic segment, providing a tight upper bound to the worst case response times. Then, we propose a novel algorithm to assign identifiers (priorities) to frames, to optimize a design objective. We show the results of the application of the method to a vehicle communication system.}}
@ARTICLE{Woodcock_2009,title={Formal methods: Practice and experience},year={2009},author={Jim Woodcock and Jim Woodcock and Peter Gorm Larsen and Peter Gorm Larsen and Juan Bicarregui and Juan Bicarregui and JS Fitzgerald and John Fitzgerald},doi={10.1145/1592434.1592436},pmid={null},pmcid={null},mag_id={2150189917},journal={ACM Computing Surveys},abstract={Formal methods use mathematical models for analysis and verification at any part of the program life-cycle. We describe the state of the art in the industrial use of formal methods, concentrating on their increasing use at the earlier stages of specification and design. We do this by reporting on a new survey of industrial use, comparing the situation in 2009 with the most significant surveys carried out over the last 20 years. We describe some of the highlights of our survey by presenting a series of industrial projects, and we draw some observations from these surveys and records of experience. Based on this, we discuss the issues surrounding the industrial adoption of formal methods. Finally, we look to the future and describe the development of a Verified Software Repository, part of the worldwide Verified Software Initiative. We introduce the initial projects being used to populate the repository, and describe the challenges they address.}}
@ARTICLE{Alur_2004,title={Visibly pushdown languages},year={2004},author={Rajeev Alur and Rajeev Alur and P. Madhusudan and P. Madhusudan},doi={10.1145/1007352.1007390},pmid={null},pmcid={null},mag_id={2151033407},journal={null},abstract={We propose the class of visibly pushdown languages as embeddings of context-free languages that is rich enough to model program analysis questions and yet is tractable and robust like the class of regular languages. In our definition, the input symbol determines when the pushdown automaton can push or pop, and thus the stack depth at every position. We show that the resulting class V pl  of languages is closed under union, intersection, complementation, renaming, concatenation, and Kleene-*, and problems such as inclusion that are undecidable for context-free languages are E xptime -complete for visibly pushdown automata. Our framework explains, unifies, and generalizes many of the decision procedures in the program analysis literature, and allows algorithmic verification of recursive programs with respect to many context-free properties including access control properties via stack inspection and correctness of procedures with respect to pre and post conditions. We demonstrate that the class V pl  is robust by giving two alternative characterizations: a logical characterization using the monadic second order (MSO) theory over words augmented with a binary matching predicate, and a correspondence to regular tree languages. We also consider visibly pushdown languages of infinite words and show that the closure properties, MSO-characterization and the characterization in terms of regular trees carry over. The main difference with respect to the case of finite words turns out to be determinizability: nondeterministic Buchi visibly pushdown automata are strictly more expressive than deterministic Muller visibly pushdown automata.}}
@ARTICLE{Barnett_2004,title={Verification of object-oriented programs with invariants},year={2004},author={Mike Barnett and Mike Barnett and Robert DeLine and Robert DeLine and Manuel Fähndrich and Manuel Fähndrich and K. Rustan M. Leino and K. Rustan M. Leino and Wolfram Schulte and Wolfram Schulte},doi={10.5381/jot.2004.3.6.a2},pmid={null},pmcid={null},mag_id={2151131364},journal={The Journal of Object Technology},abstract={An object invariant defines what it means for an object’s data to be in a consistent state. Object invariants are central to the design and correctness of objectoriented programs. This paper defines a programming methodology for using object invariants. The methodology, which enriches a program’s state space to express when each object invariant holds, deals with owned object components, ownership transfer, and subclassing, and is expressive enough to allow many interesting object-oriented programs to be specified and verified. Lending itself to sound modular verification, the methodology also provides a solution to the problem of determining what state a method is allowed to modify.}}
@ARTICLE{Boer_2010,title={Dating concurrent objects: real-time modeling and schedulability analysis},year={2010},author={Frank S. de Boer and Frank S. de Boer and Mohammad Mahdi Jaghoori and Mohammad Mahdi Jaghoori and Einar Broch Johnsen and Einar Broch Johnsen},doi={10.1007/978-3-642-15375-4_1},pmid={null},pmcid={null},mag_id={2151493262},journal={null},abstract={In this paper we introduce a real-time extension of the concurrent object modeling language Creol which is based on duration statements indicating best and worst case execution times and deadlines. We show how to analyze schedulability of an abstraction of real-time concurrent objects in terms of timed automata. Further, we introduce techniques for testing the conformance between these behavioral abstractions and the executable semantics of Real-Time Creol in Real-Time Maude.

As a case study we model and analyze the schedulability of thread pools in an industrial communication platform.}}
@ARTICLE{Johnsen_2008,title={An Object-Oriented Component Model for Heterogeneous Nets},year={2008},author={Einar Broch Johnsen and Einar Broch Johnsen and Olaf Owe and Olaf Owe and Joakim Bjørk and Joakim Bjørk and Marcel Kyas and Marcel Kyas},doi={10.1007/978-3-540-92188-2_11},pmid={null},pmcid={null},mag_id={2151987729},journal={null},abstract={Many distributed applications can be understood in terms of components interacting in an open environment. This interaction is not always uniform as the network may consist of subnets with different quality: Some components are tightly connected with order preservation of communicated messages, whereas others are more loosely connected such that overtaking of messages and even message loss may occur. Furthermore, certain components may communicate over wireless networks, where sending and receiving must be synchronized, since the wireless medium cannot buffer messages. This paper proposes a formal framework for such systems, which allows high-level modeling and formal analysis of distributed systems where interaction is managed by a variety of nets, including wireless ones. We introduce a simple modeling language for object-oriented components, extending the Creol language. An operational semantics for the language is defined in rewriting logic, which directly provides an executable implementation in Maude.}}
@ARTICLE{Mathaikutty_2007,title={Model-driven test generation for system level validation},year={2007},author={Deepak Mathaikutty and Deepak Mathaikutty and Sumit Ahuja and Sumit Ahuja and Ajit Dingankar and Ajit Dingankar and Sandeep K. Shukla and Sandeep K. Shukla},doi={10.1109/hldvt.2007.4392792},pmid={null},pmcid={null},mag_id={2152111130},journal={null},abstract={Functional validation of System Level Models, such as those modeled with SystemC, is an important and complex problem. One of the problems in their functional validation is the test case generation with good coverage and higher potential to find faults in the design. We propose a coverage-directed test generation framework for system level design validation by combining the synchronous language ESTEREL, and its advanced verification capability, with C++ based system level language SystemC. The main contributions of this paper are (i) the integrated framework for model-driven development and validation of system-level designs with a combination of ESTEREL, and SystemC; and (ii) the test generation framework for generating test suites to satisfy traditional coverage metrics such as the statement and branch as well as a complex metric such as modified condition/decision coverage (MCDC) employed in the validation of safety-critical software systems. The framework also generates tests that attain functional coverage using properties specified in a temporal language and assertion-based verification (namely PSL). We demonstrate the methodology with a case study by developing and validating a critical power state machine component that is used for power management in embedded systems.}}
@ARTICLE{Cassez_2007,title={Timed control with observation based and stuttering invariant strategies},year={2007},author={Franck Cassez and Franck Cassez and Albert David and Alexandre David and Kim G. Larsen and Kim Guldstrand Larsen and Didier Lime and Didier Lime and Jean-François Raskin and Jean-François Raskin},doi={10.1007/978-3-540-75596-8_15},pmid={null},pmcid={null},mag_id={2152295777},journal={null},abstract={In this paper we consider the problem of controller synthesis for timed games under imperfect information. Novel to our approach is the requirements to strategies: they should be based on a finite collection of observations and must be stuttering invariant in the sense that repeated identical observations will not change the strategy. We provide a constructive transformation to equivalent finite games with perfect information, giving decidability as well as allowing for an efficient on-the-fly forward algorithm. We report on application of an initial experimental implementation.}}
@ARTICLE{Nicollin_1992,title={Compiling real-time specifications into extended automata},year={1992},author={Xavier Nicollin and Xavier Nicollin and Joseph Sifakis and Joseph Sifakis and Sergio Yovine and Sergio Yovine},doi={10.1109/32.159837},pmid={null},pmcid={null},mag_id={2152815305},journal={IEEE Transactions on Software Engineering},abstract={A method for the implementation and analysis of real-time systems, based on the compilation of specification extended automata is proposed. The method is illustrated for a simple specification language that can be viewed as the extension of a language for the description of systems of communicating processes, by adding timeout and watchdog constructs. The main result is that such a language can be compiled into timed automata, which are extended automata with timers. Timers are special state variables that can be set to zero by transitions, and whose values measure the time elapsed since their last reset. Timed automata do not make any assumption about the nature of time and adopt an event-driven execution mode. Their complexity does not depend on the values of the parameters of timeouts and watchdogs used in specifications. These features allow the application on timed automata of efficient code generation and analysis techniques. In particular, it is shown how symbolic model-checking of real-time properties can be directly applied to this model. >}}
@ARTICLE{Giusto_2002,title={Automotive virtual integration platforms: why's, what's, and how's},year={2002},author={Paolo Giusto and Paolo Giusto and Alberto Ferrari and Alberto Ferrari and Luciano Lavagno and Luciano Lavagno and Julien Brunel and Jean-Yves Brunel and Eliane Fourgeau and E. Fourgeau and Alberto Sangiovanni‐Vincentelli and Alberto Sangiovanni-Vincentelli},doi={10.1109/iccd.2002.1106796},pmid={null},pmcid={null},mag_id={2152931522},journal={null},abstract={In this paper, we present the new concept of virtual integration platform for automotive electronics. The platform provides the basis for a novel methodology in which the integration of sub-systems is performed much earlier in the design cycle. As a result, cost reduction in the final implementation and in the design process can be achieved. In addition, early and repeatable fault analysis can be performed therefore easing the task of system safety proving.}}
@ARTICLE{Nayak_2009,title={Model-based test cases synthesis using UML interaction diagrams},year={2009},author={Ashalatha Nayak and Ashalatha Nayak and Debasis Samanta and Debasis Samanta},doi={10.1145/1507195.1507209},pmid={null},pmcid={null},mag_id={2153128367},journal={ACM Sigsoft Software Engineering Notes},abstract={UML 2.0 interaction diagrams model interactions in complex systems by means of operation fragments and a systematic testing approach is required for the identification and selection of test cases. The major problem for test cases synthesis from such an interaction diagram is to arrive at a comprehensive system behavior in the presence of multiple, nested fragments. In this regard, our approach is towards systematic interpretation of flow of controls as well as their subsequent usage in the test case synthesis. We also simplify the proposed flow of controls on the basis of control primitives resulting from UML 2.0 fragments and bring it to a testable form known as intermediate testable model (ITM), which is suitable for deriving system level test cases.}}
@ARTICLE{Wirsing_2008,title={Sensoria Patterns: Augmenting Service Engineering with Formal Analysis, Transformation and Dynamicity},year={2008},author={Martin Wirsing and Martin Wirsing and M. Hölzl and Matthias Hölzl and Lucia Acciai and Lucia Acciai and Federico Banti and Federico Banti and Allan Clark and Allan Clark and Alessandro Fantechi and Alessandro Fantechi and Stephen Gilmore and Stephen Gilmore and Stefania Gnesi and Stefania Gnesi and László Gönczy and László Gönczy and Nora Koch and Nora Koch and Alessandro Lapadula and Alessandro Lapadula and Philip Mayer and Philip Mayer and Franco Mazzanti and Franco Mazzanti and Rosario Pugliese and Rosario Pugliese and Andreas Schröeder and Andreas Schroeder and Andreas Schroeder and Francesco Tiezzi and Francesco Tiezzi and Mirco Tribastone and Mirco Tribastone and Dániel Varró and Dániel Varró},doi={10.1007/978-3-540-88479-8_13},pmid={null},pmcid={null},mag_id={2153204881},journal={null},abstract={The IST-FET Integrated Project Sensoria is developing a novel comprehensive approach to the engineering of service-oriented software systems where foundational theories, techniques and methods are fully integrated into pragmatic software engineering processes. The techniques and tools of Sensoria encompass the whole software development cycle, from business and architectural design, to quantitative and qualitative analysis of system properties, and to transformation and code generation. The Sensoria approach takes also into account reconfiguration of service-oriented architectures (SOAs) and re-engineering of legacy systems.}}
@ARTICLE{Čerāns_1992,title={Decidability of Bisimulation Equivalences for Parallel Timer Processes},year={1992},author={Kārlis Čerāns and Karlis Cerans},doi={10.1007/3-540-56496-9_24},pmid={null},pmcid={null},mag_id={2153273838},journal={null},abstract={In this paper an abstract model of parallel timer processes (PTPs), allowing specification of temporal quantitative constraints on the behaviour of real time systems, is introduced. The parallel timer processes are defined in a dense time domain and are able to model both concurrent (with delay intervals overlapping on the time axis) and infinite behaviour. Both the strong and weak (abstracted from internal actions) bisimulation equivalence problems for PTPs are proved decidable. It is proved also that, if one provides the PTP model additionally with memory cells for moving timer value information along the time axis, the bisimulation equivalence (and even the vertex reachability) problems become undecidable.}}
@ARTICLE{Hiraoka_2004,title={Fault tolerant design for X-by-wire vehicle},year={2004},author={Toshihiro Hiraoka and Toshihiro Hiraoka and Shinji Eto and S. Eto and Osamu Nishihara and Osamu Nishihara and Hiromitsu Kumamoto and Hiromitsu Kumamoto},doi={10.11499/sicep.2004.0_124_5},pmid={null},pmcid={null},mag_id={2153469981},journal={null},abstract={This paper proposes a hardware/software design for a fault tolerant X-by-wire system that consists of two front steering motors and four drive motors. The vehicle lateral motion is controlled by active front steering system in the normal state, and by direct yaw-moment control system when the steering systems fail. Numerical simulations demonstrate the fault tolerance ability of the proposed system.}}
@ARTICLE{Kicillof_2007,title={Achieving both model and code coverage with automated gray-box testing},year={2007},author={Nicolás Kicillof and Nicolas Kicillof and Wolfgang Grieskamp and Wolfgang Grieskamp and Nikolai Tillmann and Nikolai Tillmann and Vı́ctor Braberman and Victor Braberman},doi={10.1145/1291535.1291536},pmid={null},pmcid={null},mag_id={2153601374},journal={null},abstract={We have devised a novel technique to automatically generate test cases for a software system, combining black-box model-based testing with white-box parameterized unit testing. The former provides general guidance for the structure of the tests in the form of test sequences, as well as the oracle to check for conformance of an application under test with respect to a behavioral model. The latter finds a set of concrete parameter values that maximize code coverage using symbolic analysis. By applying these techniques together, we can produce test definitions (expressed as code to be run in a test management framework) that exercise all selected paths in the model, while also covering code branches specific to the implementation. These results cannot be obtained from any of the individual approaches alone, as the model cannot predict what values are significant to a particular implementation, while parameterized unit testing requires manually written test sequences and correctness validations. We provide tool support, integrated into our model-based testing tool.}}
@ARTICLE{Kroening_2005,title={Formal verification of SystemC by automatic hardware/software partitioning},year={2005},author={Daniel Kroening and Daniel Kroening and Natasha Sharygina and Natasha Sharygina},doi={10.1109/memcod.2005.1487900},pmid={null},pmcid={null},mag_id={2154206750},journal={null},abstract={Variants of general-purpose programming languages, like SystemC, are increasingly used to specify system designs that have both hardware and software parts. The system-level languages allow a flexible partitioning in the design of the hardware and software. Moreover, many properties depend on the combination of hardware and software and cannot be verified on either part alone. Existing tools either apply non-formal approaches or handle only the low-level parts of the language. This papers presents a new technique that handles both hardware and software parts of a system description. This is done by automatically partitioning the uniform system description into synchronous (hardware) and asynchronous (software) parts. This technique has been implemented and applied to system level descriptions of several industrial examples. The hardware/software partitioning improves the performance of the verification compared to the monolithic approach.}}
@ARTICLE{Núñez_2005,title={Conformance testing relations for timed systems},year={2005},author={Manuel Núñez and Manuel Núñez and Ismael Rodrı́guez and Ismael Rodríguez},doi={10.1007/11759744_8},pmid={null},pmcid={null},mag_id={2154373595},journal={null},abstract={This paper presents a formal framework to test both the functional and temporal behaviors in systems where temporal aspects are critical. Different implementation relations, depending on both the interpretation of time and on the (non-)determinism of specifications and/or implementations, are presented and related. We also study how tests cases are defined and applied to implementations. A test derivation algorithm, producing sound and complete test suites, is presented.}}
@ARTICLE{Trakhtenbrot_2007,title={New Mutations for Evaluation of Specification and Implementation Levels of Adequacy in Testing of Statecharts Models},year={2007},author={Mark Trakhtenbrot and M. Trakhtenbrot},doi={10.1109/taic.part.2007.23},pmid={null},pmcid={null},mag_id={2154796853},journal={null},abstract={In model-based development of embedded real-time systems, statecharts are widely used for formal specification of their behavior, providing a sound basis for test generation and for detection of faults early in the development process. The paper presents a variety of new mutations for adequacy evaluation of tests used in validation of statecharts-based models. These mutations focus on key features of statecharts used in modeling of embedded systems: hierarchy, orthogonality and time expressions. We distinguish between two levels of tests adequacy. In the first, test results are expected to strictly follow the statecharts semantics. The second one takes into account possible deviations from this semantics based on typical implementation oriented decisions, e.g. mapping of orthogonal components into separate tasks executed concurrently. The considered mutations address both types of adequacy. In particular, we consider interleaving enforcing mutations, for testing of alternatives to the canonical "maximum parallelism " execution of statecharts.}}
@ARTICLE{Zhu_1997,title={Software unit test coverage and adequacy},year={1997},author={Hong Zhu and Hong Zhu and Patrick A. V. Hall and Patrick A. V. Hall and John H R May and John H R May},doi={10.1145/267580.267590},pmid={null},pmcid={null},mag_id={2154897437},journal={ACM Computing Surveys},abstract={Objective measurement of test quality is one of the key issues in software testing. It has been a major research focus for the last two decades. Many test criteria have been proposed and studied for this purpose. Various kinds of rationales have been presented in support of one criterion or another. We survey the research work in this area. The notion of adequacy criteria is examined together with its role in software dynamic testing. A review of criteria classification is followed by a summary of the methods for comparison and assessment of criteria.}}
@ARTICLE{Lowe_1996,title={Breaking and Fixing the Needham-Schroeder Public-Key Protocol Using FDR},year={1996},author={Gavin Lowe and Gavin Lowe},doi={10.1007/3-540-61042-1_43},pmid={null},pmcid={null},mag_id={2155032609},journal={null},abstract={In this paper we analyse the well known Needham-Schroeder Public-Key Protocol using FDR, a refinement checker for CSP. We use FDR to discover an attack upon the protocol, which allows an intruder to impersonate another agent. We adapt the protocol, and then use FDR to show that the new protocol is secure, at least for a small system. Finally we prove a result which tells us that if this small system is secure, then so is a system of arbitrary size.}}
@ARTICLE{Bouyer_2009,title={From Qualitative to Quantitative Analysis of Timed Systems},year={2009},author={Patricia Bouyer and Patricia Bouyer},doi={null},pmid={null},pmcid={null},mag_id={2155492335},journal={null},abstract={null}}
@ARTICLE{Zhao_2004,title={Scoped types for real-time Java},year={2004},author={Tian Zhao and Tian Zhao and James Noble and James Noble and Jan Vítek and Jan Vitek},doi={10.1109/real.2004.51},pmid={null},pmcid={null},mag_id={2155544344},journal={null},abstract={A memory model based on scoped areas is one of the distinctive features of the Real-Time Specification for Java (RTSJ). Scoped types ensure timely reclamation of memory and predictable performance. The price to pay for these benefits is an unfamiliar programming model that, at the same time, is complex, requires checking all memory accesses, and rewards design-time errors with run-time crashes. We investigate an alternative approach, referred to as scoped types, that simplifies the task of managing memory in real-time codes. The key feature of our proposal is that the run-time partition of memory imposed by scoped areas is straightforwardly mirrored in the program text. Thus cursory inspection of a program reveals which objects inhabit the different scopes, significantly simplifying the task of understanding real-time Java programs. Moreover, we introduce a type system which ensures that no run-time errors due to memory access checks occur. Thus a RTSJ-compliant virtual machine does not require memory access checks. The contributions of this paper are the concept of scoped types, and a proof soundness of the type system. Experimental results are described in future work.}}
@ARTICLE{Nicolescu_2009,title={Model-Based Design for Embedded Systems},year={2009},author={Gabriela Nicolescu and Gabriela Nicolescu and Pieter J. Mosterman and Pieter J. Mosterman},doi={null},pmid={null},pmcid={null},mag_id={2155817791},journal={null},abstract={The demands of increasingly complex embedded systems and associated performance computations have resulted in the development of heterogeneous computing architectures that often integrate several types of processors, analog and digital electronic components, and mechanical and optical componentsall on a single chip. As a result, now the most prominent challenge for the design automation community is to efficiently plan for such heterogeneity and to fully exploit its capabilities. A compilation of work from internationally renowned authors, Model-Based Design for Embedded Systems elaborates on related practices and addresses the main facets of heterogeneous Model-Based Design for embedded systems, including the current state of the art, important challenges, and the latest trends. Focusing on computational models as the core design artifact, this book presents the cutting-edge results that have helped establish Model-Based Design and continue to expand its parameters.The book is organized into three sections: Real-Time and Performance Analysis in Heterogeneous Embedded Systems, Design Tools and Methodology for Multiprocessor System-on-Chip, and Design Tools and Methodology for Multidomain Embedded Systems. The respective contributors share their considerable expertise on the automation of design refinement and how to relate properties throughout this refinement while enabling analytic and synthetic qualities. They focus on multi-core methodological issues, real-time analysis, and modeling and validation, taking into account how optical, electronic, and mechanical components often interface. Model-Based Design is emerging as a solution to bridge the gap between the availability of computational capabilities and our inability to make full use of them yet. This approach enables teams to start the design process using a high-level model that is gradually refined through abstraction levels to ultimately yield a prototype. When executed well, Model-Based Design encourages enhanced performance and quicker time to market for a product. Illustrating a broad and diverse spectrum of applications such as in the automotive aerospace, health care, consumer electronics, this volume provides designers with practical, readily adaptable modeling solutions for their own practice.}}
@ARTICLE{Chevalley_2001,title={Automated generation of statistical test cases from UML state diagrams},year={2001},author={Philippe Chevalley and P. Chevalley and Pascale Thévenod-Fosse and P. Thevenod-Fosse},doi={10.1109/cmpsac.2001.960618},pmid={null},pmcid={null},mag_id={2155988378},journal={null},abstract={The adoption of the object-oriented (OO) technology for the development of critical software raises important testing issues. This paper addresses one of these issues: how to create effective tests from OO specification documents? More precisely, the paper describes a technique that adapts a probabilistic method, called statistical functional testing, to the generation of test cases from UML state diagrams, using transition coverage as the testing criterion. Emphasis is put on defining an automatic way to produce both the input values and the expected outputs. The technique is automated with the aid of the Rational Software Corporation's Rose RealTime tool. An industrial case study from the avionics domain, formally specified and implemented in Java, is used to illustrate the feasibility of the technique at the subsystem level. Results of first test experiments are presented to exemplify the fault revealing power of the created statistical test cases.}}
@ARTICLE{Löding_2010,title={Timed Moore Automata: Test Data Generation and Model Checking},year={2010},author={Helge Löding and Helge Löding and Jan Peleška and Jan Peleska},doi={10.1109/icst.2010.60},pmid={null},pmcid={null},mag_id={2156077234},journal={null},abstract={In this paper we introduce Timed Moore Automata, a specification formalism which is used in industrial train control applications for specifying the real-time behavior of cooperating reactive software components. We define an operational semantics for the sequential components (units) with an abstraction of time that is suitable for checking timeout behavior of these units. A model checking algorithm for live lock detection is presented, and two alternative methods of test case/test data generation techniques are introduced. The first one is based on Kripke structures as used in explicit model checking, while the second method does not require an explicit representation but relies on SAT solving techniques.}}
@ARTICLE{Hamon_2004,title={Generating efficient test sets with a model checker},year={2004},author={Grégoire Hamon and G. Hamon and Leonardo de Moura and L. de Moura and John Rushby and John Rushby},doi={10.1109/sefm.2004.21},pmid={null},pmcid={null},mag_id={2156176134},journal={null},abstract={It is well-known that counterexamples produced by model checkers can provide a basis for automated generation of test cases. However when this approach is used to meet a coverage criterion, it generally results in very inefficient test sets having many tests and much redundancy. We describe an improved approach that uses model checkers to generate efficient test sets. Furthermore, the generation is itself efficient, and is able to reach deep regions of the statespace. We have prototyped the approach using the model checkers of our SAL system and have applied it to model-based designs developed in Stateflow. In one example, our method achieves complete state and transition coverage in a Stateflow model for the shift scheduler of a 4-speed automatic transmission with a single test case.}}
@ARTICLE{Zimmerman_2009,title={A Verification-Centric Software Development Process for Java},year={2009},author={Daniel M. Zimmerman and Daniel M. Zimmerman and Joseph R. Kiniry and Joseph R. Kiniry},doi={10.1109/qsic.2009.18},pmid={null},pmcid={null},mag_id={2157219772},journal={null},abstract={Design by Contract (DBC) is an oft-cited, but rarely followed, programming practice that focuses on writing formal specifications first, and writing code that fulfills those specifications second. The development of static analysis tools over the past several years has made it possible to fully embrace DBC in Java systems by writing, type checking, and consistency checking rich behavioral specifications for Java before writing any code. This paper discusses a DBC-based, verification-centric software development process for Java that integrates the Business Object Notation (BON), the Java Modeling Language, and several associated tools including the BON compiler BONc, the ESC/Java2 static checker, a runtime assertion checker, and a specification-based unit test generator. This verification-centric process, reinforced by its rich open source tool support, is one of the most advanced, concrete, open, practical, and usable processes available today for rigorously designing and developing software systems.}}
@ARTICLE{Merayo_2008,title={Extending EFSMs to Specify and Test Timed Systems with Action Durations and Time-Outs},year={2008},author={Mercedes G. Merayo and Mercedes G. Merayo and Manuel Núñez and Manuel Núñez and Ismael Rodrı́guez and Ismael Rodríguez},doi={10.1109/tc.2008.15},pmid={null},pmcid={null},mag_id={2157820788},journal={IEEE Transactions on Computers},abstract={In this paper, we introduce a timed extension of the extended finite state machines model. On one hand, we consider that (output) actions take time to be performed. This time may depend on several factors, such as the value of variables. On the other hand, our formalism allows us to specify time-outs. In addition to presenting our language, we develop a testing theory. First, we define 10 timed conformance relations and relate them. Second, we introduce a notion of timed test and define how to apply tests to implementations. Finally, we give an algorithm to derive sound and complete test suites with respect to the implementation relations presented in the paper. This paper represents an extended and improved version of [1].}}
@ARTICLE{Weyuker_2010,title={Evaluating Software Complexity Measures},year={2010},author={Elaine J. Weyuker and Elaine J. Weyuker},doi={null},pmid={null},pmcid={null},mag_id={2158335989},journal={null},abstract={A set of properties of syntactic software complexity measures is proposed to serve as a basis for the evaluation of such measures. Four known complexity measures are evaluated and compared using these criteria. This formalized evaluation clarifies the strengths and weaknesses of the examined complexity measures, which include the statement count, cyclomatic number, effort measure, and data flow complexity measures. None of these measures possesses all nine properties, and several are found to fail to possess particularly fundamental properties; this failure calls into question their usefulness in measuring synthetic complexity. >}}
@ARTICLE{Adjir_2008,title={Test of preemptive real-time systems},year={2008},author={Noureddine Adjir and Noureddine Adjir and Pierre de Saqui-Sannes and P. de Saqui-Sannes and Kamel Mustapha Rahmouni and Kamel Mustapha Rahmouni},doi={10.1109/aiccsa.2008.4493609},pmid={null},pmcid={null},mag_id={2158608106},journal={null},abstract={Time Petri nets with stopwatches not only model system/environment interactions and time constraints. They further enable modeling of suspend/resume operations in real-time systems. Assuming the modelled systems are non deterministic and partially observable, the paper proposes a test generation approach which implements an online testing policy and outputs test results that are valid for the (part of the) selected environment. A relativized conformance relation named rswtioco is defined and a test generation algorithm is presented. The proposed approach is illustrated on an example.}}
@ARTICLE{Bergstra_1985,title={Algebra of communicating processes with abstraction},year={1985},author={Jan A. Bergstra and Jan A. Bergstra and Jan Willem Klop and Jan Willem Klop},doi={10.1016/0304-3975(85)90088-x},pmid={null},pmcid={null},mag_id={2158942489},journal={null},abstract={We present an axiom system ACP, for communicating processes with silent actions (‘τ-steps’). The system is an extension of ACP, Algebra of Communicating Processes, with Milner's τ-laws and an explicit abstraction operator. By means of a model of finite acyclic process graphs for ACPτ, syntactic properties such as consistency and conservativity over ACP are proved. Furthermore, the Expansion Theorem for ACP is shown to carry over to ACPτ. Finally, termination of rewriting terms according to the ACPτ, axioms is probed using the method of recursive path orderings.}}
@ARTICLE{Styp_2010,title={A conformance testing relation for symbolic timed automata},year={2010},author={Sabrina von Styp and Sabrina von Styp and Henrik Bohnenkamp and Henrik Bohnenkamp and Henrik Bohnenkamp and Julien Schmaltz and Julien Schmaltz},doi={10.1007/978-3-642-15297-9_19},pmid={null},pmcid={null},mag_id={2159011965},journal={null},abstract={We introduce Symbolic Timed Automata, an amalgamation of symbolic transition systems and timed automata, which allows to express nondeterministic data-dependent control flow with inputs and outputs and real-time behaviour. In particular, input data can influence the timing behaviour. We define two semantics for STA, a concrete one as timed labelled transition systems and another one on a symbolic level. We show that the symbolic semantics is complete and correct w.r.t. the concrete one. Finally, we introduce symbolic conformance relation stioco, which is an extension of the well-known ioco conformance relation. Relation stioco is defined using FO-logic on a purely symbolic level. We show that stioco corresponds on the concrete semantic level to Krichen and Tripakis' implementation relation tioco for timed labelled transition systems.}}
@ARTICLE{Hagiescu_2007,title={Performance analysis of FlexRay-based ECU networks},year={2007},author={Andrei Hagiescu and Andrei Hagiescu and Unmesh D. Bordoloi and Unmesh D. Bordoloi and Samarjit Chakraborty and Samarjit Chakraborty and Prahladavaradan Sampath and Prahladavaradan Sampath and Prasanth Ganesan and Prasanna Ganesan and P. Vignesh V. Ganesan and S. Ramesh and S. Ramesh and S. Ramesh},doi={10.1145/1278480.1278554},pmid={null},pmcid={null},mag_id={2159675308},journal={null},abstract={It is now widely believed that FlexRay will emerge as the predominant protocol for in-vehicle automotive communication systems. As a result, there has been a lot of recent interest in timing and predictability analysis techniques that are specifically targeted towards FlexRay. In this paper we propose a compositional performance analysis framework for a network of electronic control units (ECUs) that communicate via a FlexRay bus. Given a specification of the tasks running on the different ECUs, the scheduling policy used at each ECU, and a specification of the FlexRay bus (e.g. slot sizes and message priorities), our framework can answer questions related to the maximum end-to-end delay experienced by any message, the amount of buffer required at each communication controller and the utilization of the different ECUs and the bus. In contrast to previous timing analysis techniques which analyze the FlexRay bus in isolation, our framework is fully compositional and allows the modeling of the schedulers at the ECUs and the FlexRay protocol in a seamless manner. As a result, it can be used to analyze large systems and does not involve any computationally expensive step like solving an ILP (which previous approaches require). We illustrate our framework using detailed examples and also present results from a Matlab-based implementation.}}
@ARTICLE{Langerak_1999,title={A Complete Finite Prefix for Process Algebra},year={1999},author={Rom Langerak and Rom Langerak and Ed Brinksma and Ed Brinksma},doi={10.1007/3-540-48683-6_18},pmid={null},pmcid={null},mag_id={2159758671},journal={null},abstract={In this paper we show how to use McMillan's complete finite prefix approach for process algebra. We present the model of component event structures as a semantics for process algebra, and show how to construct a complete finite prefix for this model. We present a simple adequate order (using an order on process algebra expressions) as an optimization to McMillan's original algorithm.}}
@ARTICLE{Farchi_2003,title={Concurrent bug patterns and how to test them},year={2003},author={Eitan Farchi and Eitan Farchi and Yarden Nir and Yarden Nir and Shmuel Ur and Shmuel Ur},doi={10.1109/ipdps.2003.1213511},pmid={null},pmcid={null},mag_id={2159765571},journal={null},abstract={We present and categorize a taxonomy of concurrent bug patterns. We then use the taxonomy to create new timing heuristics for ConTest. Initial industrial experience indicates that these heuristics improve the bug finding ability of ConTest. We also show how concurrent bug patterns can be derived from concurrent design patterns. Further research is required to complete the concurrent bug taxonomy and formal experiments are needed to show that heuristics derived from the taxonomy improve the bug finding ability of ConTest.}}
@ARTICLE{Hubert_2010,title={Sawja: static analysis workshop for java},year={2010},author={Laurent Hubert and Nicolas Barré and Frédéric Besson and Delphine Demange and Thomas Jensen and Vincent Monfort and David Pichardie and Tiphaine Turpin},doi={10.1007/978-3-642-18070-5_7},pmid={null},pmcid={null},mag_id={2159835737},journal={null},abstract={Static analysis is a powerful technique for automatic verification of programs but raises major engineering challenges when developing a full-fledged analyzer for a realistic language such as Java. Efficiency and precision of such a tool rely partly on low level components which only depend on the syntactic structure of the language and therefore should not be redesigned for each implementation of a new static analysis. This paper describes the Sawja library: a static analysis workshop fully compliant with Java 6 which provides OCaml modules for efficiently manipulating Java bytecode programs. We present the main features of the library, including i) efficient functional data-structures for representing a program with implicit sharing and lazy parsing, ii) an intermediate stack-less representation, and iii) fast computation and manipulation of complete programs. We provide experimental evaluations of the different features with respect to time, memory and precision.}}
@ARTICLE{Misra_2004,title={A programming model for the orchestration of Web services},year={2004},author={Jayadev Misra and Jayadev Misra},doi={10.1109/sefm.2004.2},pmid={null},pmcid={null},mag_id={2160257474},journal={null},abstract={We explore the following quintessential problem: given a set of basic computing elements how do we compose them to yield interesting computation patterns. Our goal is to study composition operators which apply across a broad spectrum of computing elements, from sequential programs to distributed transactions over computer networks; so, our theory makes very few assumptions about the nature of the basic elements. In particular, we do not assume that an element's computation always terminates, or that it is deterministic. We develop a theory which can provide useful guidance for application designs, from integration of sequential programs to coordination of distributed tasks. The primary application of interest for us is the orchestration of Web services over the Internet.}}
@ARTICLE{Czarnecki_2007,title={Feature Diagrams and Logics: There and Back Again},year={2007},author={Krzysztof Czarnecki and Krzysztof Czarnecki and Andrzej Wąsowski and Andrzej Wasowski},doi={10.1109/splc.2007.19},pmid={null},pmcid={null},mag_id={2160325552},journal={null},abstract={Feature modeling is a notation and an approach for modeling commonality and variability in product families. In their basic form, feature models contain mandatory/optional features, feature groups, and implies and excludes relationships. It is known that such feature models can be translated into propositional formulas, which enables the analysis and configuration using existing logic- based tools. In this paper, we consider the opposite translation problem, that is, the extraction of feature models from propositional formulas. We give an automatic and efficient procedure for computing a feature model from a formula. As a side effect we characterize a class of logical formulas equivalent to feature models and identify logical structures corresponding to their syntactic elements. While many different feature models can be extracted from a single formula, the computed model strives to expose graphically the maximum of the original logical structure while minimizing redundancies in the representation. The presented work furthers our understanding of the semantics of feature modeling and its relation to logics, opening avenues for new applications in reverse engineering and refactoring of feature models.}}
@ARTICLE{Hoare_1971,title={Proof of a program: FIND},year={1971},author={C. A. R. Hoare and C. A. R. Hoare},doi={10.1145/362452.362489},pmid={null},pmcid={null},mag_id={2160363852},journal={Communications of The ACM},abstract={A proof is given of the correctness of the algorithm “Find.” First, an informal description is given of the purpose of the program and the method used. A systematic technique is described for constructing the program proof during the process of coding it, in such a way as to prevent the intrusion of logical errors. The proof of termination is treated as a separate exercise. Finally, some conclusions relating to general programming methodology are drawn.}}
@ARTICLE{Brockschmidt_2011,title={Modular Termination Proofs of Recursive Java Bytecode Programs by Term Rewriting},year={2011},author={Marc Brockschmidt and Marc Brockschmidt and Carsten Otto and Carsten Otto and Jürgen Giesl and Jürgen Giesl},doi={10.4230/lipics.rta.2011.155},pmid={null},pmcid={null},mag_id={2160568515},journal={null},abstract={In [5, 15] we presented an approach to prove termination of non-recursive Java Bytecode (JBC) programs automatically. Here, JBC programs are first transformed to finite termination graphs which represent all possible runs of the program. Afterwards, the termination graphs are translated to term rewrite systems (TRSs) such that termination of the resulting TRSs implies termination of the original JBC programs. So in this way, existing techniques and tools from term rewriting can be used to prove termination of JBC automatically. In this paper, we improve this approach substantially in two ways: (1) We extend it in order to also analyze recursive JBC programs. To this end, one has to represent call stacks of arbitrary size. (2) To handle JBC programs with several methods, we modularize our approach in order to reuse termination graphs and TRSs for the separate methods and to prove termination of the resulting TRS in a modular way. We implemented our approach in the tool AProVE. Our experiments show that the new contributions increase the power of termination analysis for JBC significantly. 1998 ACM Subject Classification D.1.5 - Object-oriented Programming, D.2.4 - Software/Program Verification, D.3.3 - Language Constructs and Features, F.3 - Logics and Meanings of Programs, F.4.2 - Grammars and Other Rewriting Systems, I.2.2 - Automatic Programming}}
@ARTICLE{Min_2004,title={Using Smart Connectors to Resolve Partial Matching Problems in COTS Component Acquisition},year={2004},author={Hyun Gi Min and Hyun Gi Min and Si Won Choi and Si Won Choi and Soo Dong Kim and Soo Dong Kim},doi={10.1007/978-3-540-24774-6_5},pmid={null},pmcid={null},mag_id={2160730807},journal={null},abstract={Components, especially commercial-off-the-shelf (COTS) components, are mainly for inter-organizational reuse. One of the essential tasks in component-based development (CBD) is to locate and reuse the right components that provide the functionality and interface required by component consumers. However, if a candidate component provides a limited applicability and customizability so that it does not completely satisfy the functionality and interface needed, then a component consumer cannot reuse the component in application development. We call it a partial matching problem in component acquisition. To resolve this problem, we propose smart connectors that fill the gap between candidate components and the specification of components required. By using connectors, partially matched components become reusable in application development without sacrificing the component consumer’s requirements. Consequently, the effort and cost to develop new components and applications can be greatly reduced. In this paper, we propose four types of connectors, and each connector type is specified with its applicable situation and instructions to design correct connectors.}}
@ARTICLE{Bengtsson_1996,title={UPPAAL—a tool suite for automatic verification of real-time systems},year={1996},author={Johan Bengtsson and Johan Bengtsson and Kim Guldstrand Larsen and Kim Guldstrand Larsen and Fredrik Larsson and Fredrik Larsson and Fredrik Larsson and Paul Pettersson and Paul Pettersson and Wang Yi and Wang Yi},doi={10.1007/bfb0020949},pmid={null},pmcid={null},mag_id={2160883697},journal={BRICS Report Series},abstract={Uppaal is a tool suite for automatic verification of safety and bounded liveness properties of real-time systems modeled as networks of timed automata. It includes: a graphical interface that supports graphical and textual representations of networks of timed automata, and automatic transformation from graphical representations to textual format, a compiler that transforms a certain class of linear hybrid systems to networks of timed automata, and a model-checker which is implemented based on constraint-solving techniques. Uppaal also supports diagnostic model-checking providing diagnostic information in case verification of a particular real-time systems fails.}}
@ARTICLE{Apel_2010,title={An algebraic foundation for automatic feature-based program synthesis},year={2010},author={Sven Apel and Sven Apel and Christian Lengauer and Christian Lengauer and Bernhard Möller and Bernhard Möller and Christian Kästner and Christian Kästner},doi={10.1016/j.scico.2010.02.001},pmid={null},pmcid={null},mag_id={2160885640},journal={Science of Computer Programming},abstract={Feature-Oriented Software Development provides a multitude of formalisms, methods, languages, and tools for building variable, customizable, and extensible software. Along different lines of research, different notions of a feature have been developed. Although these notions have similar goals, no common basis for evaluation, comparison, and integration exists. We present a feature algebra that captures the key ideas of feature orientation and that provides a common ground for current and future research in this field, on which also alternative options can be explored. Furthermore, our algebraic framework is meant to serve as a basis for the development of the technology of automatic feature-based program synthesis and architectural metaprogramming.}}
@ARTICLE{Lai_1999,title={Software selection: a case study of the application of the analytical hierarchical process to the selection of a multimedia authoring system},year={1999},author={Vincent S. Lai and Vincent S. Lai and Robert P. Trueblood and Robert P. Trueblood and Bo K. Wong and Bo K. Wong},doi={10.1016/s0378-7206(99)00021-x},pmid={null},pmcid={null},mag_id={2160950834},journal={Information & Management},abstract={null}}
@ARTICLE{Beyer_2004,title={Generating tests from counterexamples},year={2004},author={Dirk Beyer and Dirk Beyer and Adam Chlipala and Adam Chlipala and Thomas A. Henzinger and Thomas A. Henzinger and Ranjit Jhala and Ranjit Jhala and Rupak Majumdar and Rupak Majumdar},doi={10.1109/icse.2004.1317455},pmid={null},pmcid={null},mag_id={2161488870},journal={null},abstract={We have extended the software model checker BLAST to automatically generate test suites that guarantee full coverage with respect to a given predicate. More precisely, given a C program and a target predicate p, BLAST determines the set L of program locations which program execution can reach with p true, and automatically generates a set of test vectors that exhibit the truth of p at all locations in L. We have used BLAST to generate test suites and to detect dead code in C programs with up to 30 K lines of code. The analysis and test vector generation is fully automatic (no user intervention) and exact (no false positives).}}
@ARTICLE{Jeannet_2005,title={Symbolic test selection based on approximate analysis},year={2005},author={Bertrand Jeannet and Bertrand Jeannet and Thierry Jéron and Thierry Jéron and Vlad Rusu and Vlad Rusu and Elena Zinovieva and Elena Zinovieva},doi={10.1007/978-3-540-31980-1_23},pmid={null},pmcid={null},mag_id={2161786546},journal={null},abstract={This paper addresses the problem of generating symbolic test cases for testing the conformance of a black-box implementation with respect to a specification, in the context of reactive systems. The challenge we consider is the selection of test cases according to a test purpose, which is here a set of scenarios of interest that one wants to observe during test execution. Because of the interactions that occur between the test case and the implementation, test execution can be seen as a game involving two players, in which the test case attempts to satisfy the test purpose.

Efficient solutions to this problem have been proposed in the context of finite-state models, based on the use of fixpoint computations. We extend them in the context of infinite-state symbolic models, by showing how approximate fixpoint computations can be used in a conservative way. The second contribution we provide is the formalization of a quality criterium for test cases, and a result relating the quality of a generated test case to the approximations used in the selection algorithm.}}
@ARTICLE{Andrade_2013,title={Generating Test Cases for Real-Time Systems Based on Symbolic Models},year={2013},author={Wilkerson L. Andrade and Wilkerson L. Andrade and Patrícia D. L. Machado and Patrícia D. L. Machado},doi={10.1109/tse.2013.13},pmid={null},pmcid={null},mag_id={2162001502},journal={IEEE Transactions on Software Engineering},abstract={The state space explosion problem is one of the challenges to be faced by test case generation techniques, particularly when data values need to be enumerated. This problem gets even worse for real-time systems (RTS) that also have time constraints. The usual solution in this context, based on finite state machines or time automata, consists of enumerating data values (restricted to finite domains) while treating time symbolically. In this paper, a symbolic model for conformance testing of real-time systems software named TIOSTS that addresses both data and time symbolically is presented. Moreover, a test case generation process is defined to select more general test cases with variables and parameters that can be instantiated at testing execution time. Generation is based on a combination of symbolic execution and constraint solving for the data part and symbolic analysis for timed aspects. Furthermore, the practical application of the process is investigated through a case study.}}
@ARTICLE{Lee_1996,title={Conformance testing of protocols specified as communicating finite state machines-a guided random walk based approach},year={1996},author={D. Lee and David Lee and Krishan K. Sabnani and Krishan Kumar Sabnani and David M. Kristol and D.M. Kristol and David Morris Kristol and David M. Kristol and Sanjoy Kumar Paul and Sanjoy Paul and S. Paul},doi={10.1109/26.494307},pmid={null},pmcid={null},mag_id={2162578718},journal={IEEE Transactions on Communications},abstract={We present a new approach for conformance testing of protocols specified as a collection of communicating finite state machines (FSMs). Our approach uses a guided random walk procedure. This procedure attempts to cover all transitions in the component FSMs. We also introduce the concept of observers that check some aspect of protocol behavior. We present the result of applying our method to two example protocols: full-duplex alternating bit protocol and the ATM-adaptation-layer-convergence protocol. Applying our procedure to the ATM adaptation layer, 99% of component FSMs edges can be covered in a test with 11692 input steps. Previous approaches cannot do conformance test generation for standard protocols (such as asynchronous transfer mode (ATM) adaptation layer) specified as a collection of communicating FSMs.}}
@ARTICLE{Cimatti_2010,title={Verifying SystemC: a software model checking approach},year={2010},author={Alessandro Cimatti and Alessandro Cimatti and Andrea Micheli and Andrea Micheli and Iman Narasamdya and Iman Narasamdya and Marco Roveri and Marco Roveri},doi={null},pmid={null},pmcid={null},mag_id={2163477945},journal={null},abstract={SystemC is becoming a de-facto standard for the development of embedded systems. Verification of SystemC designs is critical since it can prevent error propagation down to the hardware. SystemC allows for very efficient simulations before synthesizing the RTL description, but formal verification is still at a preliminary stage. Recent works translate SystemC into the input language of finite-state model checkers, but they abstract away relevant semantic aspects, and show limited scalability. In this paper, we approach formal verification of SystemC by reduction to software model checking. We explore two directions. First, we rely on a translation from SystemC to a sequential C program, that contains both the mapping of the SystemC threads in form of C functions, and the coding of relevant semantic aspects (e.g. of the SystemC kernel). In terms of verification, this enables the “off-the-shelf” use of model checking techniques for sequential software, such as lazy abstraction. Second, we propose an approach that exploits the intrinsic structure of SystemC. In particular, each SystemC thread is translated into a separate sequential program and explored with lazy abstraction, while the overall verification is orchestrated by the direct execution of the SystemC scheduler. The technique can be seen as generalizing lazy abstraction to the case of multi-threaded software with exclusive threads and cooperative scheduling. The above approaches have been implemented in a new software model checker. An experimental evaluation carried out on several case studies taken from the SystemC distribution and from the literature demonstrate the potential of the approach.}}
@ARTICLE{Dasarathy_1985,title={Timing Constraints of Real-Time Systems: Constructs for Expressing Them, Methods of Validating Them},year={1985},author={B. Dasarathy and B. Dasarathy},doi={10.1109/tse.1985.231845},pmid={null},pmcid={null},mag_id={2163835031},journal={IEEE Transactions on Software Engineering},abstract={This paper examines timing constraints as features of realtime systems. It investigates the various constructs required in requirements languages to express timing constraints and considers how automatic test systems can validate systems that include timing constraints. Specifically, features needed in test languages to validate timing constraints are discussed. One of the distinguishing aspects of three tools developed at GTE Laboratories for real-time systems specification and testing is in their extensive ability to handle timing constraints. Thus, the paper highlights the timing constraint features of these tools.}}
@ARTICLE{Kiczales_2005,title={Aspect-oriented programming and modular reasoning},year={2005},author={Gregor Kiczales and Gregor Kiczales and Mira Mezini and Mira Mezini},doi={10.1145/1062455.1062482},pmid={null},pmcid={null},mag_id={2164067955},journal={null},abstract={Aspects cut new interfaces through the primary decomposition of a system. This implies that in the presence of aspects, the complete interface of a module can only be determined once the complete configuration of modules in the system is known. While this may seem anti-modular, it is an inherent property of crosscutting concerns, and using aspect-oriented programming enables modular reasoning in the presence of such concerns.}}
@ARTICLE{Demange_2010,title={A provably correct stackless intermediate representation for Java bytecode},year={2010},author={Delphine Demange and Delphine Demange and Delphine Demange and Thomas Jensen and Thomas Jensen and David Pichardie and David Pichardie},doi={10.1007/978-3-642-17164-2_8},pmid={null},pmcid={null},mag_id={2164186668},journal={null},abstract={The Java virtual machine executes stack-based bytecode. The intensive use of an operand stack has been identified as a major obstacle for static analysis and it is now common for static analysis tools to manipulate a stackless intermediate representation (IR) of bytecode programs. This paper provides such a bytecode transformation, describes its semantic correctness and evaluates its performance. We provide the semantic foundations for proving that an initial program and its IR behave similarly, in particular with respect to object creation and throwing of exceptions. The correctness of this transformation is proved with respect to a relation on execution traces taking into account that the object allocation order is not preserved by the transformation.}}
@ARTICLE{Abdulla_2008,title={Stochastic games with lossy channels},year={2008},author={Parosh Aziz Abdulla and Parosh Aziz Abdulla and Noomene Ben Henda and Noomene Ben Henda and Luca de Alfaro and Luca de Alfaro and Richard Mayr and Richard Mayr and Sven Sandberg and Sven Sandberg},doi={10.1007/978-3-540-78499-9_4},pmid={null},pmcid={null},mag_id={2164822567},journal={null},abstract={We consider turn-based stochastic games on infinite graphs induced by game probabilistic lossy channel systems (GPLCS), the game version of probabilistic lossy channel systems (PLCS). We study games with Buchi (repeated reachability) objectives and almost-sure winning conditions. These games are pure memoryless determined and, under the assumption that the target set is regular, a symbolic representation of the set of winning states for each player can be effectively constructed. Thus, turn-based stochastic games on GPLCS are decidable. This generalizes the decidability result for PLCS-induced Markov decision processes in [10].}}
@ARTICLE{Noda_2008,title={Aspect-Oriented Modeling for Variability Management},year={2008},author={Nao–Aki Noda and Natsuko Noda and Tomoji Kishi and Tomoji Kishi},doi={10.1109/splc.2008.44},pmid={null},pmcid={null},mag_id={2165298415},journal={null},abstract={In product line development (PLD), reusability is the key factor, and it is important to make architecture and components flexibly configurable. Thus far, several techniques for utilizing aspect-oriented technologies (AOTs) for PLD have been proposed. However, the application of AOTs to PLD is not simple and various issues related to the application, such as an invasive change problem that prevents reusability, have been reported. We have proposed an aspect oriented modeling (AOM) mechanism that separates concerns from their relationships with other concerns. In this study, we propose the application of our AOM to variability management and demonstrate the effectiveness of our approach with the help of a case study of embedded software.}}
@ARTICLE{Kim_2002,title={Computational Analysis of Run-time Monitoring: Fundamentals of Java-MaC},year={2002},author={Moonjoo Kim and Moonjoo Kim and Sampath Kannan and Sampath Kannan and Insup Lee and Insup Lee and Oleg Sokolsky and Oleg Sokolsky and Mahesh Viswanathan and Mahesh Viswanathan},doi={10.1016/s1571-0661(04)80578-4},pmid={null},pmcid={null},mag_id={2165414132},journal={null},abstract={Abstract   A run-time monitor shares computational resources, such as memory and CPU time, with the target program. Furthermore, heavy computation performed by a monitor for checking target program's execution with respect to requirement properties can be a bottleneck to the target program's execution. Therefore, computational characteristics of run-time monitoring cause a significant impact on the target program's execution.  We investigate computational issues on run-time monitoring. The first issue is the power of run-time monitoring. In other words, we study the class of properties run-time monitoring can evaluate. The second issue is computational complexity of evaluating properties written in process algebraic language. Third, we discuss sound abstraction of the target program's execution, which does not change the result of property evaluation. This abstraction can be used as a technique to reduce monitoring overhead. Theoretical understanding obtained from these issues affects the implementation of Java-MaC, a toolset for the run-time monitoring and checking of Java programs. Finally, we demonstrate the abstraction-based overhead reduction technique implemented in Java-MaC through a case study.}}
@ARTICLE{Nunes_2009,title={Bridging the Gap between Algebraic Specification and Object-Oriented Generic Programming},year={2009},author={Isabel Nunes and Isabel Nunes and Antónia Lopes and Antónia Lopes and Vasco T. Vasconcelos and Vasco T. Vasconcelos},doi={10.1007/978-3-642-04694-0_9},pmid={null},pmcid={null},mag_id={2166172769},journal={null},abstract={Although generics became quite popular in mainstream object- oriented languages and several specification languages exist that support the description of generic components, conformance relations between object-oriented programs and formal specifications that have been established so far do not address genericity. In this paper we propose a notion of refinement mapping that allows to define correspondences between parameterized specifications and generic Java classes. Based on such mappings, we put forward a conformance notion useful for the extension of ConGu, a tool-based approach we have been developing to support runtime conformance checking of Java programs against algebraic specifications, so that it becomes applicable to a more comprehensive range of situations, namely those that appear in the context of a typical Algorithms and Data Structures course.}}
@ARTICLE{Bowen_2002,title={FORTEST: formal methods and testing},year={2002},author={Jonathan P. Bowen and Jonathan P. Bowen and Kirill Bogdanov and Kirill Bogdanov and John A. Clark and John A. Clark and Mark Harman and Mark Harman and Robert M. Hierons and Robert M. Hierons and Paul Krause and Paul Krause},doi={10.1109/cmpsac.2002.1044538},pmid={null},pmcid={null},mag_id={2166304427},journal={null},abstract={Formal methods have traditionally been used for specification and development of software. However there are potential benefits for the testing stage as well. The panel session associated with this paper explores the usefulness or otherwise of formal methods in various contexts for improving software testing. A number of different possibilities for the use of formal methods are explored and questions raised. The contributors are all members of the UK FORTEST Network on formal methods and testing. Although the authors generally believe that formal methods are useful in aiding the testing process, this paper is intended to provoke discussion. Dissenters are encouraged to put their views to the panel or individually to the authors.}}
@ARTICLE{Hierons_2009,title={Mutation testing from probabilistic and stochastic finite state machines},year={2009},author={Robert M. Hierons and Robert M. Hierons and Mercedes G. Merayo and Mercedes G. Merayo},doi={10.1016/j.jss.2009.06.030},pmid={null},pmcid={null},mag_id={2166352318},journal={Journal of Systems and Software},abstract={Specification mutation involves mutating a specification, and for each mutation a test is derived that distinguishes the behaviours of the mutated and original specifications. This approach has been applied with finite state machine based models. This paper extends mutation testing to finite state machine models that contain non-functional properties. The paper describes several ways of mutating a finite state machine with probabilities (PFSM) or stochastic time (PSFSM) attached to its transitions and shows how we can generate test sequences that distinguish between such a model and its mutants. Testing then involves applying each test sequence multiple times, observing the resultant behaviours and using results from statistical sampling theory in order to compare the observed frequency and execution time of each output sequence with that expected.}}
@ARTICLE{Miller_2000,title={On fault location in networks by passive testing},year={2000},author={Raymond E. Miller and R.E. Miller and K.A. Arisha and K.A. Arisha},doi={10.1109/pccc.2000.830329},pmid={null},pmcid={null},mag_id={2167065106},journal={null},abstract={In this paper, we employ a variant of the communicating finite state machine (CFSM) model for networks to investigate fault detection and location using passive testing. First, we introduce the concept of passive testing, then we introduce the model with necessary assumptions and justification. Then, the model for the observer process is described and a 3-node case is studied to show how fault location information can be deduced. Extending this result, we propose a multiple node-cut approach for a general network, applying our technique for fault detection and location. An abstraction of a node-cut shows how the 3-node case can be used in the general case. We then illustrate our technique through a simulation of a practical X.25 example. Finally future extensions and potential trends are-discussed.}}
@ARTICLE{Yeh_1989,title={Expert system based automatic network fault management system},year={1989},author={Show-Way Yeh and S.-W. Yeh and Changshan Wu and C. Wu and Hong-Dah Sheng and H.-D. Sheng and Chi-Hsiang Hung and C.-K. Hung and R.-C. Lee and R.-C. Lee},doi={10.1109/cmpsac.1989.65178},pmid={null},pmcid={null},mag_id={2167173290},journal={null},abstract={An expert system for network management is designed and prototyped to do network troubleshooting automatically. The expert system employs management information provided by a monitoring mechanism of the network. The whole spectrum of the fault management information is analyzed. The management knowledge derived is categorized into five types: the physical property, the experience in the past, the heuristic rule of thumb, the predictable problems, and the deep knowledge. These knowledge types and related rules are divided into groups to improve reasoning speed. The expert system is composed of a problem manager, a problem analyzer, and many problem solvers. A prototyped expert system, using simulated faults, shows that the expert-system-based fault management system can automatically diagnose problems and take corrective actions. >}}
@ARTICLE{Siebert_2006,title={Proving the absence of RTSJ related runtime errors through data flow analysis},year={2006},author={Fridtjof Siebert and Fridtjof Siebert},doi={10.1145/1167999.1168025},pmid={null},pmcid={null},mag_id={2167300882},journal={null},abstract={The Real-Time Specification for Java (RTSJ) introduces region based memory management to avoid the need for garbage collection. This region based memory management, however, introduces new possible runtime errors. Ensuring that an RTSJ application executes correctly requires proving that no memory related runtime excpetions can occur.The use of program-wide pointer analysis for proving the absence of runtime error conditions such as null pointer uses or illegal casts is still not widespread. Current uses of program-wide pointer analysis focus on extracting information for optimisations in compilers. In this case, imprecise analysis results only in less agressive optimisation, which is often tolerable.This papers presents the application of a program-wide data flow analysis to prove the absence of memory related runtime errors such as those introduced by the RTSJ.}}
@ARTICLE{Ghinea_2005,title={Multicriteria decision making for enhanced perception-based multimedia communication},year={2005},author={Gheorghita Ghinea and Gheorghita Ghinea and George D. Magoulas and George D. Magoulas and C. Siamitros and C. Siamitros},doi={10.1109/tsmca.2005.851281},pmid={null},pmcid={null},mag_id={2167318586},journal={null},abstract={This paper proposes an approach that integrates technical concerns with user perceptual considerations for intelligent decision making in the construction of tailor-made multimedia communication protocols. Thus, the proposed approach, based on multicriteria decision making (MDM), incorporates not only classical networking considerations, but, indeed, user preferences as well. Furthermore, in keeping with the task-dependent nature consistently identified in multimedia scenarios, the suggested communication protocols also take into account the type of multimedia application that they are transporting. Lastly, this approach also opens the possibility for such protocols to dynamically adapt based on a changing operating environment and user's preferences.}}
@ARTICLE{Szyperski_2002,title={Component Software: Beyond Object-Oriented Programming},year={2002},author={Clemens Szyperski and Clemens Szyperski},doi={null},pmid={null},pmcid={null},mag_id={2167500728},journal={null},abstract={From the Publisher:
Component Software: Beyond Object-Oriented Programming explains the technical foundations of this evolving technology and its importance in the software market place. It provides in-depth discussion of both the technical and the business issues to be considered, then moves on to suggest approaches for implementing component-oriented software production and the organizational requirements for success. The author draws on his own experience to offer tried-and-tested solutions to common problems and novel approaches to potential pitfalls. Anyone responsible for developing software strategy, evaluating new technologies, buying or building software will find Clemens Szyperski's objective and market-aware perspective of this new area invaluable.}}
@ARTICLE{Ahrendt_2009,title={A Verification System for Distributed Objects with Asynchronous Method Calls},year={2009},author={Wolfgang Ahrendt and Wolfgang Ahrendt and Maximilian Dylla and Maximilian Dylla},doi={10.1007/978-3-642-10373-5_20},pmid={null},pmcid={null},mag_id={2167940834},journal={null},abstract={We present a verification system for Creol, an object-oriented modeling language for concurrent distributed applications. The system is an instance of KeY, a framework for object-oriented software verification, which has so far been applied foremost to sequential Java. Building on KeY characteristic concepts, like dynamic logic, sequent calculus, explicit substitutions, and the taclet rule language, the system presented in this paper addresses functional correctness of Creol models featuring local cooperative thread parallelism and global communication via asynchronous method calls. The calculus heavily operates on communication histories which describe the interfaces of Creol units. Two example scenarios demonstrate the usage of the system.}}
@ARTICLE{En‐Nouaary_2008,title={A Boundary Checking Technique for Testing Real-Time Systems Modeled as Timed Input Output Automata},year={2008},author={Abdeslam En‐Nouaary and Abdeslam En-Nouaary and Abdelwahab Hamou-Lhadj and Abdelwahab Hamou-Lhadj},doi={null},pmid={null},pmcid={null},mag_id={2168004790},journal={null},abstract={The behavior of real-time systems depends not only on their interaction with the environment but also on very rigid time constraints that puts restrictions on when these interactions take place. The timing aspect of such systems renders the testing process difficult without defining adequate test selection criteria that ensure good coverage of the system while keeping the number of needed test cases considerably low. In this paper, we propose a method for testing real-time systems, formally modeled as Timed Input Output Automata (TIOA), which aims at generating a set of test cases that would allow us to check every transition of the TIOA as soon as possible, as late as possible, and at the middle between these two executions. The execution times of every transition are determined based on the minimum and maximum delays between the source state of the transition and its clock guards.}}
@ARTICLE{David_2009,title={Timed Testing under Partial Observability},year={2009},author={Albert David and Alexandre David and Kim Guldstrand Larsen and Kim Guldstrand Larsen and Shuhao Li and Shuhao Li and Brian Nielsen and Brian Nielsen},doi={10.1109/icst.2009.38},pmid={null},pmcid={null},mag_id={2168033597},journal={null},abstract={This paper studies the problem of model-based testing of real-time systems that are only partially observable. We model the System Under Test (SUT) using Timed Game Automata (TGA) which has internal actions, uncontrollable outputs and timing uncertainty of outputs. We define the partial observability of SUT using a set of predicates over the TGA state space, and specify the test purposes in Computation Tree Logic (CTL) formulas. A recently developed partially observable timed game solver is used to generate winning strategies, which are used as test cases. We propose a conformance testing framework, define a partial observation-based conformance relation, present the test execution algorithms, and prove the soundness and completeness of this test method (i.e., a detected error really violates the conformance relation; and if the SUT violates the test purpose, then a test case can be generated to detect this violation). Experiments on some non-trivial examples show that this method yields encouraging results.}}
@ARTICLE{Taira_2004,title={Proof of Theorem 1.1},year={2004},author={Kazuaki Taira and Kazuaki Taira},doi={10.1007/978-3-662-43696-7_11},pmid={null},pmcid={null},mag_id={2168110243},journal={null},abstract={This chapter is devoted to the proof of Theorem 1.1. The idea of our proof is stated as follows. First, we reduce the study of the boundary value problem 

$$ \left\{ \begin{array}{l} ({\rm A - }\lambda {\rm )u = f }\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,{\rm in D, } \\ {\rm Lu = }\mu {\rm (x')}\frac{{\partial {\rm u}}}{{\partial {\rm n}}} + \Upsilon (x')u = \varphi \,on\,\partial D \\ \end{array} \right. $$

(1.1)

to that of a first-order pseudo-differential operator T(λ) = LP(λ) on the boundary ∂D, just as in Section 4.3. Then we prove that conditions (A) and (B) are sufficient for the validity of the a priori estimate 

$$\parallel u\parallel _{2,p} \le C(\lambda )(\parallel f\parallel _p + |\varphi| _{2 - 1/p,p} + \parallel u\parallel _p ).$$

(1.2)}}
@ARTICLE{Freund_2003,title={A Type System for the Java Bytecode Language and Verifier},year={2003},author={Stephen N. Freund and Stephen N. Freund and John C. Mitchell and John C. Mitchell},doi={10.1023/a:1025011624925},pmid={null},pmcid={null},mag_id={2168448595},journal={Journal of Automated Reasoning},abstract={The Java Virtual Machine executes bytecode programs that may have been sent from other, possibly untrusted, locations on the network. Since the transmitted code may be written by a malicious party or corrupted during network transmission, the Java Virtual Machine contains a bytecode verifier to check the code for type errors before it is run. As illustrated by reported attacks on Java run-time systems, the verifier is essential for system security. However, no formal specification of the bytecode verifier exists in the Java Virtual Machine Specification published by Sun. In this paper, we develop such a specification in the form of a type system for a subset of the bytecode language. The subset includes classes, interfaces, constructors, methods, exceptions, and bytecode subroutines. We also present a type checking algorithm and prototype bytecode verifier implementation, and we conclude by discussing other applications of this work. For example, we show how to extend our formal system to check other program properties, such as the correct use of object locks.}}
@ARTICLE{Walukiewicz_1995,title={Completeness of Kozen's axiomatisation of the propositional /spl mu/-calculus},year={1995},author={Igor Walukiewicz and Igor Walukiewicz and Igor Walukiewicz},doi={10.1109/lics.1995.523240},pmid={null},pmcid={null},mag_id={2169083900},journal={null},abstract={We consider the propositional /spl mu/-calculus as introduced by D. Kozen (1983). In that paper a natural proof system was proposed and its completeness stated as an open problem. We show that the system is complete.}}
@ARTICLE{Woodward_1988,title={From weak to strong, dead or alive? an analysis of some mutation testing issues},year={1988},author={Martin R. Woodward and M.R. Woodward and K. Halewood and K. Halewood},doi={10.1109/wst.1988.5370},pmid={null},pmcid={null},mag_id={2169249706},journal={null},abstract={The authors argue that strong mutation testing and weak mutation testing are in fact extreme ends of a spectrum of mutation approaches. The term firm mutation is introduced to represent the middle ground in this spectrum. The authors also argue, by means of a number of small examples, that there is a potential problem concerning the criterion for deciding whether a mutant is dead or live. A variety of solutions are suggested. Practical considerations for a firm-mutation testing system, with greater user control over the nature of result comparison, are discussed. Such a system is currently under development as part of an interpretive development environment. >}}
@ARTICLE{Fähndrich_2010,title={Embedded contract languages},year={2010},author={Manuel Fähndrich and Manuel Fähndrich and Mike Barnett and Mike Barnett and Francesco Logozzo and Francesco Logozzo},doi={10.1145/1774088.1774531},pmid={null},pmcid={null},mag_id={2169309022},journal={null},abstract={Specifying application interfaces (APIs) with information that goes beyond method argument and return types is a long-standing quest of programming language researchers and practitioners. The number of type system extensions or specification languages is a testament to that. Unfortunately, the number of such systems is also roughly equal to the number of tools that consume them. In other words, every tool comes with its own specification language.   In this paper we argue that for modern object-oriented languages, using an embedding of contracts as code is a better approach. We exemplify our embedding of Code Contracts on the Microsoft managed execution platform (.NET) using the C# programming language. The embedding works as well in Visual Basic. We discuss the numerous advantages of our approach and the technical challenges, as well as the status of tools that consume the embedded contracts.}}
@ARTICLE{Offutt_1994,title={A practical system for mutation testing: help for the common programmer},year={1994},author={A. Jefferson Offutt and A.J. Offutt},doi={10.1109/test.1994.528535},pmid={null},pmcid={null},mag_id={2169384550},journal={null},abstract={Mutation testing is a technique for unit testing software that, although powerful, is computationally expensive. Recent engineering advances have given us techniques and algorithms for significantly reducing the cost of mutation testing. These techniques include a new algorithmic execution technique called schema-based mutation, an approximation technique called weak mutation, a reduction technique called selective mutation, and algorithms for automatic test data generation. This paper outlines a design for a system that will approximate mutation, but in a way that will be accessible to everyday programmers. We envisage a system to which a programmer can submit a program unit, and get back a set of input/output pairs that are guaranteed to form an effective test of the unit by being close to mutation adequate.}}
@ARTICLE{Lefticaru_2007,title={Automatic State-Based Test Generation Using Genetic Algorithms},year={2007},author={Raluca Lefticaru and Raluca Lefticaru and Florentin Ipate and Florentin Ipate},doi={10.1109/synasc.2007.47},pmid={null},pmcid={null},mag_id={2169417364},journal={null},abstract={Although a lot of research has been done in the field of state-based testing, the automatic generation of test cases from a functional specification in the form of a state machine is not straightforward. This paper investigates the use of genetic algorithms in test data generation for the chosen paths in the state machine, so that the input parameters provided to the methods trigger the specified transitions.}}
@ARTICLE{Jansen_2002,title={A probabilistic extension of UML statecharts: Specification and Verification.},year={2002},author={David N. Jansen and Holger Hermanns and Holger Hermanns and Holger Hermanns and Joost-Pieter Katoen and Joost P. Katoen},doi={10.1007/3-540-45739-9},pmid={null},pmcid={null},mag_id={2170136168},journal={Lecture Notes in Computer Science},abstract={This paper is the extended technical report that corresponds to a published paper [14]. This paper introduces means to specify system randomness within UML statecharts, and to verify probabilistic temporal properties over such enhanced statecharts which we call probabilistic UML statecharts. To achieve this, we develop a general recipe to extend a statechart semantics with discrete probability distributions, resulting in Markov decision processes as semantic models. We apply this recipe to the requirements-level UML semantics of [8]. Properties of interest for probabilistic statecharts are expressed in PCTL, a probabilistic variant of CTL for processes that exhibit both non-determinism and probabilities. Verification is performed using the model checker Prism. A model checking example shows the feasibility of the suggested approach.}}
@ARTICLE{Kästner_2008,title={Granularity in software product lines},year={2008},author={Christian Kästner and Christian Kästner and Sven Apel and Sven Apel and Martin Kuhlemann and Martin Kuhlemann},doi={10.1145/1368088.1368131},pmid={null},pmcid={null},mag_id={2171002355},journal={null},abstract={Building software product lines (SPLs) with features is a challenging task. Many SPL implementations support features with coarse granularity - e.g., the ability to add and wrap entire methods. However, fine-grained extensions, like adding a statement in the middle of a method, either require intricate workarounds or obfuscate the base code with annotations. Though many SPLs can and have been implemented with the coarse granularity of existing approaches, fine-grained extensions are essential when extracting features from legacy applications. Furthermore, also some existing SPLs could benefit from fine-grained extensions to reduce code replication or improve readability. In this paper, we analyze the effects of feature granularity in SPLs and present a tool, called Colored IDE (CIDE), that allows features to implement coarse-grained and fine-grained extensions in a concise way. In two case studies, we show how CIDE simplifies SPL development compared to traditional approaches.}}
@ARTICLE{Belinfante_2010,title={JTorX: a tool for on-line model-driven test derivation and execution},year={2010},author={Axel Belinfante and Axel Belinfante},doi={10.1007/978-3-642-12002-2_21},pmid={null},pmcid={null},mag_id={2171109896},journal={null},abstract={We introduce JTorX, a tool for model-driven test derivation and execution, based on the ioco theory. This theory, originally presented in [12], has been refined in [13] with test-cases that are input-enabled. For models with underspecified traces [3] introduced uioco.

JTorX improves over its predecessor TorX [14] by using uioco and this newer ioco theory. By being much easier to deploy, due to improved installation, configuration and usage. And by integrating additional functionality, next to testing: checking for (u)ioco between models [6]; checking for underspecified traces in a model; interactive or guided simulation of a model. This makes JTorX an excellent vehicle for educational purposes in courses on model-based testing, as experience has shown – and its usefulness is not limited to education, as experience has shown too.}}
@ARTICLE{Richardson_1992,title={Specification-based test oracles for reactive systems},year={1992},author={Debra J. Richardson and Debra J. Richardson and Stephanie Leif Aha and Stephanie Leif Aha and T. Owen O'Malley and T. Owen O'Malley},doi={10.1145/143062.143100},pmid={null},pmcid={null},mag_id={2171441042},journal={null},abstract={The testing process is typically systematic in test data se- lection and test execution. For the most part, however, the effective use of test oracles has been neglected, even though they are a critical component of the testing process. Test or- acles prescribe acceptable behavior for test execution. In the absence of judging test results with formal oracles, testing does not achieve its goal of revealing failures or assuring cor- rect behavior in a practical manner; manual result checking is neither reliable nor cost-effective. We argue that test oracles should be derived from specifications in conjunction with testing criteria, represented in a common form, and their use made integral to the test- ing process. For complex, reactive systems, oracles must reflect the multiparadigm nature of the required behavior. Such systems are often specified using multiple languages, each selected for its utility specifying in a particular computational paradigm. Thus, we are developing an approach for deriving and using oracles based on multiparadigm and multilingual specifications to enable the verification of test results for reactive systems as well as less complex systems.}}
@ARTICLE{Boella_2008,title={Substantive and procedural norms in normative multiagent systems},year={2008},author={Guido Boella and Guido Boella and Leendert van der Torre and Leendert van der Torre},doi={10.1016/j.jal.2007.06.006},pmid={null},pmcid={null},mag_id={2171483400},journal={Journal of Applied Logic},abstract={Abstract   Procedural norms are instrumental norms addressed to agents playing a role in the normative system, for example to motivate these role playing agents to recognize violations or to apply sanctions. Procedural norms have first been discussed in law, where they address legal practitioners such as legislators, lawyers and policemen, but they are discussed now too in normative multiagent systems to motivate software agents. Procedural norms aim to achieve the social order specified using regulative norms like obligations and permissions, and constitutive norms like counts-as obligations. In this paper we formalize procedural, regulative and constitutive norms using input/output logic enriched with an agent ontology and an abstraction hierarchy. We show how our formalization explains Castelfranchi's notion of mutual empowerment, stating that not only the agents playing a role in a normative system are empowered by the normative system, but the normative system itself is also empowered by the agents playing a role in it. In our terminology, the agents are not only institutionally empowered, but they are also delegated normative goals from the system. Together, institutional empowerment and normative goal delegation constitute a mechanism which we call delegation of power, where agents acting on behalf of the normative system become in charge of recognizing which institutional facts follow from brute facts.}}
@ARTICLE{Visser_2004,title={Test input generation with java PathFinder},year={2004},author={Willem Visser and Willem Visser and Corina S. Pǎsǎreanu and Corina S. Pǎsǎreanu and Sarfraz Khurshid and Sarfraz Khurshid},doi={10.1145/1007512.1007526},pmid={null},pmcid={null},mag_id={2171683519},journal={null},abstract={We show how model checking and symbolic execution can be used to generate test inputs to achieve structural coverage of code that manipulates complex data structures. We focus on obtaining branch-coverage during unit testing of some of the core methods of the red-black tree implementation in the Java TreeMap library, using the Java PathFinder model checker. Three different test generation techniques will be introduced and compared, namely, straight model checking of the code, model checking used in a black-box fashion to generate all inputs up to a fixed size, and lastly, model checking used during white-box test input generation. The main contribution of this work is to show how efficient white-box test input generation can be done for code manipulating complex data, taking into account complex method preconditions.}}
@ARTICLE{Parkinson_2005,title={Separation logic and abstraction},year={2005},author={Matthew Parkinson and Matthew Parkinson and Gavin Bierman and Gavin Bierman},doi={10.1145/1040305.1040326},pmid={null},pmcid={null},mag_id={2171685273},journal={null},abstract={In this paper we address the problem of writing specifications for programs that use various forms of modularity, including procedures and Java-like classes. We build on the formalism of separation logic and introduce the new notion of an abstract predicate and, more generally, abstract predicate families. This provides a flexible mechanism for reasoning about the different forms of abstraction found in modern programming languages, such as abstract datatypes and objects. As well as demonstrating the soundness of our proof system, we illustrate its utility with a series of examples.}}
@ARTICLE{Artikis_2009,title={Specifying norm-governed computational societies},year={2009},author={Alexander Artikis and Alexander Artikis and Marek Sergot and Marek Sergot and Jeremy Pitt and Jeremy Pitt},doi={10.1145/1459010.1459011},pmid={null},pmcid={null},mag_id={2171687149},journal={ACM Transactions on Computational Logic},abstract={Electronic markets, dispute resolution and negotiation protocols are three types of application domains that can be viewed as open agent societies. Key characteristics of such societies are agent heterogeneity, conflicting individual goals and unpredictable behavior. Members of such societies may fail to, or even choose not to, conform to the norms governing their interactions. It has been argued that systems of this type should have a formal, declarative, verifiable, and meaningful semantics. We present a theoretical and computational framework being developed for the executable specification of open agent societies. We adopt an external perspective and view societies as instances of normative systems. In this article, we demonstrate how the framework can be applied to specifying and executing a contract-net protocol. The specification is formalized in two action languages, the C+ language and the Event Calculus, and executed using respective software implementations, the Causal Calculator and the Society Visualizer. We evaluate our executable specification in the light of the presented case study, discussing the strengths and weaknesses of the employed action languages for the specification of open agent societies.}}
@ARTICLE{Boehm_1981,title={Software Engineering Economics},year={1981},author={Barry Boehm and Barry Boehm},doi={null},pmid={null},pmcid={null},mag_id={2171816001},journal={null},abstract={This paper summarizes the current state of the art and recent trends in software engineering economics. It provides an overview of economic analysis techniques and their applicability to software engineering and management. It surveys the field of software cost estimation, including the major estimation techniques available, the state of the art in algorithmic cost models, and the outstanding research issues in software cost estimation.}}
@ARTICLE{Wang_1993,title={Fault detection with multiple observers},year={1993},author={Clark Wang and Clark Wang and M. Schwartz and Mischa Schwartz},doi={10.1109/90.222906},pmid={null},pmcid={null},mag_id={2173315326},journal={IEEE ACM Transactions on Networking},abstract={There is a pressing need for network management systems capable of handling faults. The use of a set of independent observers to detect faults in communication systems that are modeled by finite-state machines is proposed. An algorithm for constructing these observers and a fast real-time fault detection mechanism used by each observer are given. Since these observers run in parallel and independently, one immediate benefit is that of graceful degradation-one failed observer will not cause collapse of the fault management system. In addition, each observer has a simpler structure than the original system and can be operated at higher speed. This approach has the potential to be incorporated into the fault management system for a high-speed communication system. >}}
@ARTICLE{Sarikaya_1995,title={Synchronization and specification issues in protocol testing},year={1995},author={Behçet Sarikaya and Behçet Sarikaya and Behçet Sarikaya and Gregor von Bochmann and Gregor von Bochmann},doi={null},pmid={null},pmcid={null},mag_id={2173386945},journal={null},abstract={Protocol testing for the purpose of certifying the implementation's adherence to the protocol specification can be done with a test architecture consisting of remote tester and local responder processes generating specific input stimuli, called test sequences, and observing the output produced by the implementation under test. It is possible to adapt test sequence generation techniques for finite state machines, such as transition tour, characterization, and checking sequence methods, to generate test sequences for protocols specified as incomplete finite state machines. For certain test sequences, the tester or responder processes are forced to consider the timing of an interaction in which they have not taken part; these test sequences are called nonsynchronizable. The three test sequence generation algorithms are modified to obtain synchronizable test sequences. The checking of a given protocol for intrinsic synchronization problems is also discussed. Complexities of synchronizable test sequence generation algorithms are given and complete testing of a protocol is shown to be infeasible. To extend the applicability of the characterization and checking sequences, different methods are proposed to enhance the protocol specifications: special test input interactions are defined and a methodology is developed to complete the protocol specifications.}}
@ARTICLE{Dasarathy_1989,title={Timing constraints of real-time systems: constructs for expressing them, methods of validating them},year={1989},author={B. Dasarathy and B. Dasarathy},doi={null},pmid={null},pmcid={null},mag_id={2173645169},journal={null},abstract={This paper examines timing constraints as features of realtime systems. It investigates the various constructs required in requirements languages to express timing constraints and considers how automatic test systems can validate systems that include timing constraints. Specifically, features needed in test languages to validate timing constraints are discussed. One of the distinguishing aspects of three tools developed at GTE Laboratories for real-time systems specification and testing is in their extensive ability to handle timing constraints. Thus, the paper highlights the timing constraint features of these tools.}}
@ARTICLE{Bruns_2010,title={Verification of software product lines with delta-oriented slicing},year={2010},author={Daniel Bruns and Daniel Bruns and Vladimir Klebanov and Vladimir Klebanov and Ina Schaefer and Ina Schaefer},doi={10.1007/978-3-642-18070-5_5},pmid={null},pmcid={null},mag_id={2173900622},journal={Lecture Notes in Computer Science},abstract={Software product line (SPL) engineering is a well-known approach to develop industry-size adaptable software systems. SPL are often used in domains where high-quality software is desirable; the overwhelming product diversity, however, remains a challenge for assuring correctness. In this paper, we present delta-oriented slicing, an approach to reduce the deductive verification effort across an SPL where individual products are Java programs and their relations are described by deltas. On the specification side, we extend the delta language to deal with formal specifications. On the verification side, we combine proof slicing and similarity-guided proof reuse to ease the verification process.}}
@ARTICLE{Milner_1990,title={Operational and Algebraic Semantics of Concurrent Processes.},year={1990},author={Robin Milner and Robin Milner},doi={null},pmid={null},pmcid={null},mag_id={2174215409},journal={null},abstract={Publisher Summary   This chapter presents a particular class of mathematical models for concurrent communicating processes, namely, the class of algebraic calculi. The chapter describes a small language whose constructions reflect simple operational ideas; the meaning of these constructions is presented by means of structured operational semantics. Congruence relation is interpreted as equality of processes. The essence of the congruence is that it is built upon the idea of observing a process, which is taken to mean communicating with it. The idea is that the processes are equal if they are indistinguishable in any experiment based upon observation. Observation equivalence is characterized by a simple modal logic.}}
@ARTICLE{Fujiwara_1995,title={Test selection based on finite state models},year={1995},author={Susumu Fujiwara and Susumu Fujiwara and Gregor von Bochmann and Gregor von Bochmann and Ferhat Khendek and Ferhat Khendek and Mokhtar Amalou and Mokhtar Amalou and A. Ghedamsi and A. Ghedamsi},doi={null},pmid={null},pmcid={null},mag_id={2177967933},journal={null},abstract={A method for the selection of appropriate test case, an important issue for conformance testing of protocol implementations as well as software engineering, is presented. Called the partial W-method, it is shown to have general applicability, full fault-detection power, and yields shorter test suites than the W-method. Various other issues that have an impact on the selection of a suitable test suite including the consideration of interaction parameters, various test architectures for protocol testing and the fact that many specifications do not satisfy the assumptions made by most test selection methods (such as complete definition, a correctly implemented reset function, a limited number of states in the implementation, and determinism), are discussed. >}}
@ARTICLE{Aceto_1986,title={Testing Equivalences for Event Structures},year={1986},author={Luca Aceto and Luca Aceto and Rocco De Nicola and Rocco De Nicola and Alessandro Fantechi and Alessandro Fantechi},doi={10.1007/3-540-18419-8_9},pmid={null},pmcid={null},mag_id={2179445399},journal={null},abstract={A flexible abstraction mechanism for models of concurrency, which allows systems which “look the same” to be considered equivalent, is proposed. Using three classes of atomic observations (sequences of actions, sequences of multisets of actions and partial orderings of actions) different information on the causal and temporal structure of Event Structures, a basic model of parallelism, is captured. As a result, three different semantic models for concurrent systems are obtained. These models can be used as the basis for defining interleaving, multisets or partial ordering semantics of concurrent systems. The common framework used to define the models allows us to study the relationship between these three traditional approaches to the semantics of concurrent communicating systems.}}
@ARTICLE{Yi_1995,title={Automatic verification of real-time communicating systems by constraint-solving},year={1995},author={Wang Yi and Wang Yi and Paul Pettersson and Paul Pettersson and Mats Daniels and Mats Daniels},doi={10.1007/978-0-387-34878-0_18},pmid={null},pmcid={null},mag_id={2180475648},journal={null},abstract={In this paper, an algebra of timed processes with real-valued clocks is presented, which serves as a formal description language for real-time communicating systems. We show that requirements such as “a process will never reach an undesired state” can be verified by solving a simple class of constraint systems on the clock-variables. A complete method for reachability analysis associated with the language is developed, and implemented as an automatic verification tool based on constraint-solving techniques. Finally as examples, we study and verify the safety-properties of Fischer’s mutual exclusion protocol and a railway crossing controller.}}
@ARTICLE{Tripakis_2009,title={Modeling, Verification and Testing using Timed and Hybrid Automata},year={2009},author={Stavros Tripakis and Stavros Tripakis and Stavros Tripakis and Thao Dang and Thao Dang and Thao Dang},doi={null},pmid={null},pmcid={null},mag_id={2182272118},journal={null},abstract={ion. The main idea of this approach is to start with a rough (conservative and often discrete) approximation of a hybrid system and then iteratively refine it. This refinement is often local in the sense that it uses the previous analysis results to determine where the approximation error is too large to prove the property (see for example [96, 8, 28]). A popular abstraction approach is predicate abstraction where a conservative abstraction can be constructed by mapping the infinite set of states of the hybrid system to a finite set of abstract states using a set of predicates. The property is then verified in the abstract system. If it holds in the abstract system, it also holds in the concrete hybrid system. Oth1.3. EXHAUSTIVE VERIFICATION 25 erwise, a counter-example can be generated. If the abstract counter-example corresponds to a concrete trajectory, then the hybrid system does not satisfy the property; otherwise, the abstract counter-example is spurious because the abstraction is too conservative, and the abstraction can then be refined to achieve a better precision. In the following, we illustrate this approach by explaining the method using polynomials proposed in [96]. The continuous state space R is partitioned using the signs of a set of polynomials. As an example, an abstract state s defined by g1(x)   0 corresponds to a (possibly infinite) set c(s) of concrete states. Then, the abstract transition over-approximates the concrete one such that there is a transition from s to s′ if there exists a trajectory from a concrete state in c(s) to another concrete state in c(s′). More precisely, in this method, first the set of polynomials is saturated by adding all the high-order derivatives of the initial polynomials. Then, by looking at the sign of the polynomials, it is possible to decide whether a trajectory can go from one abstract state to another. For example, if there are only two polynomials g1 and g2 such that g2 = ġ1. Suppose that the abstract state s satisfies g1 = 0 and g2 > 0, then the new sign of g1 is positive and from s we add a transition to s′ satisfying gi > 0. The abstraction can be refined by adding more polynomials. Another abstraction method in [8] uses linear predicates to partition the continuous state space, and thus each abstract c(s) is a convex polyhedron. The abstract transition from s to s′ is determined by computing the reachable set from c(s) and check whether it reaches c(s′). This is less expensive than the reachability computation on the hybrid system which requires handling accumulated reachable sets with geometric complexity that grows after successive continuous and discrete evolutions. Box decompositions are also commonly used to define abstract systems, such as in [90, 59]. The abstract system can then be built by exploiting the properties of the system’s vector fields over such decompositions. The method proposed in [59] makes use of the following special property of multi-affine systems: the value of a multi-affine function f(x) with x inside some box can be expressed as a linear combination of the values of f at the vertices of the box. Using this, one can determine whether the derivative vector on the boundary 8Multi-affine systems are a particular class of polynomial systems such that if all the variables xi are constant, the derivatives are linear in xj with j not equal to i. 26CHAPTER 1. MODELING, VERIFICATION AND TESTING USING TIMED AND HYBRID AUTOMATA of a box points outwards or inwards, in order to over-approximate the reachability between adjacent boxes. While discrete abstractions allow benefiting from the well-developed verification algorithms for discrete systems, they might be too coarse to preserve interesting properties. Timed abstractions can be built by adding bounds on the time for the system to reach from one abstract state to another. A generalization of this idea is called hybridization [12] involving approximating a complex system with a simpler system, for which more efficient analysis tools are available. To this end, using a partition of the state space, one can approximate locally the system’s dynamics in each region by a simpler dynamics. Globally, the dynamics changes when moving from one region to another, and the resulting approximate system behaves like a hybrid system and this approximation process is therefore called hybridization. Then, the resulting system is used to yield approximate analysis results for the original system. The usefulness of this approach (in terms of accuracy and computational tractability) depends on the choice of the approximate system. For example, the hybridization methods using piecewise affine approximate systems, proposed in [12], allows approximating a nonlinear system with a good convergence rate and, additionally, preserving the attractors of the original system. In addition, the resulting approximate systems can be handled by the existing tools for piecewise affine systems (presented earlier in this section). 1.4 Partial verification Exhaustive verification is desirable since, if it succeeds, it guarantees that a model satisfies a property. But exhaustive verification has its limitations as we have seen: state-explosion or even undecidability. In fact, state-explosion is a phenomenon that is also prevalent in the exhaustive verification of much simpler, finite-state models. This phenomenon has so far hindered a wider adoption of exhaustive verification in industrial applications, because the size of the problems tackled there is far too big to treat exhaustively. Instead, practitioners use simulation as their main verification tool. Even though simulation cannot prove that a 9The term “verification” usually denotes simulation-based verification in industrial jargon, whereas “formal verification” is used to denote exhaustive verification. 1.4. PARTIAL VERIFICATION 27 property is satisfied, it can certainly reveal cases where it is not satisfied, that is, potential bugs of the real system, its model, or its specification. An advantage of simulation is that it has some time-scalability properties: running 200 simulations is better (i.e., likely to discover more bugs) than running 100 simulations, and running longer simulations is also better. Moreover, if 100 simulations can be run in one day, say, then in two days we can most likely run 200 simulations. In contrast, most exhaustive verification tools suffer from a “hitting the wall” type of problem. Once they exhaust the main memory of the computer that they run on, they start using disk space, which involves a lot of swapping on the OS side. Disk swapping virtually takes all processing time, leading verification to a halt. This means that the number of new states that are explored per unit of time radically decreases to practically zero, as illustrated in Figure 1.14. Usually this wall is hit after relatively little time, in the order of minutes. Then, running the tool for many hours will not improve the number of states that are explored compared to running it for ten minutes. This is not time-scalable.}}
@ARTICLE{Batory_2008,title={Modularizing Theorems for Software Product Lines: The Jbook Case Study},year={2008},author={Don Batory and Don Batory and Egon Börger and Egon Börger},doi={null},pmid={null},pmcid={null},mag_id={2183187959},journal={Journal of Universal Computer Science},abstract={A goal of software product lines is the economical assembly of programs in a family of programs. In this paper, we explore how theorems about program properties may be integrated into feature-based development of software product lines. As a case study, we analyze an existing Java/JVM compilation correctness proof for defining, interpreting, compiling, and executing bytecode for the Java language. We show how features modularize program source, theorem statements and their proofs. By composing features, the source code, theorem statements and proofs for a program are assembled. The investigation in this paper reveals a striking similarity of the refinement concepts used in Abstract State Machines (ASM) based system development and Feature-Oriented Programming (FOP) of software product lines. We suggest to exploit this observation for a fruitful interaction of researchers in the two communities.}}
@ARTICLE{Moskal_2009,title={Satisfiability Modulo Software},year={2009},author={Micha l Jan Moskal and Micha l Jan Moskal and Micha l Jan Moskal and Micha l Jan Moskal and Jan Moskal and Micha l Jan Moskal and Jan Moskal and Micha l Jan Moskal and Jan Moskal and Micha l Jan Moskal and Jan Moskal and Micha l Jan Moskal and Jan Moskal and Micha l Jan Moskal and Jan Moskal and Leszek Pacholski and Leszek Pacholski and Jan Moskal and Jan Moskal and Jan Moskal and Jan Moskal and Jan Moskal and Jan Moskal and Jan Moskal and Jan Moskal and Jan Moskal},doi={null},pmid={null},pmcid={null},mag_id={2185159868},journal={null},abstract={Formal verification is the act of proving correctness of a hardware or software system using formal methods of mathematics. In the last decade formal hardware verification has seen an increasing usage of Satisfiability Modulo Theories (SMT) solvers. SMT solvers check satisfiability of first-order formulas, where certain symbols are interpreted according to background theories like integer or bit-vector arithmetic. Since the formulas used to encode correctness of hardware design are mostly quantifier-free, SMT solvers are built as theory-aware extensions of propositional satisfiability solvers. As a consequence, SMT solvers do not “naturally” support quantified formulas, which are needed for verification of complex software systems. Thus, while SMT solvers are already an industrially viable tool for formal hardware verification, software applications are not as developed. This thesis focuses on both the software verification specific problems in the construction of SMT solvers, as well as SMT-specific parts of a software verification system. On the SMT side, we present algorithms for efficient non-ground reasoning through quantifier instantiation and techniques for proof generation and proof checking for quantifier-rich software verification problems. On the verification tool side, we present methods for transforming programs into formulas in a solver-friendly way, with particular emphasis on design of annotations guiding the SMT solver through the non-ground part of the problem. The theoretical developments presented here were experimentally validated in implementations of state-of-the-art tools: an SMT solver and a verifier for concurrent C programs. Systemy SMT w formalnej weryfikacji oprogramowania Micha l Jan Moskal Praca doktorska Promotor: Prof. Leszek Pacholski Instytut Informatyki Uniwersytet Wroc lawski ul. Joliot-Curie 15 50-383 Wroc law}}
@ARTICLE{Singer_2006,title={Static Program Analysis based on Virtual Register Renaming},year={2006},author={Jeremy Singer and Jeremy Singer},doi={null},pmid={null},pmcid={null},mag_id={2186102677},journal={null},abstract={Static single assignment form (SSA) is a popular program intermediate representation (IR) for static analysis. SSA programs differ from equivalent control flow graph (CFG) programs only in the names of virtual registers, which are systematically transformed to comply with the naming convention of SSA. Static single information form (SSI) is a recently proposed extension of SSA that enforces a greater degree of systematic virtual register renaming than SSA. This dissertation develops the principles, properties, and practice of SSI construction and data flow analysis. Further, it shows that SSA and SSI are two members of a larger family of related IRs, which are termed virtual register renaming schemes (VRRSs). SSA and SSI analyses can be generalized to operate on any VRRS family member. Analysis properties such as accuracy and efficiency depend on the underlying VRRS. This dissertation makes four significant contributions to the field of static analysis research. First, it develops the SSI representation. Although SSI was introduced five years ago, it has not yet received widespread recognition as an interesting IR in its own right. This dissertation presents a new SSI definition and an optimistic construction algorithm. It also sets SSI in context among the broad range of IRs for static analysis. Second, it demonstrates how to reformulate existing data flow analyses using new sparse SSI-based techniques. Examples include liveness analysis, sparse type inference and program slicing. It presents algorithms, together with empirical results of these algorithms when implemented within a research compiler framework. Third, it provides the only major comparative evaluation of the merits of SSI for data flow analysis. Several qualitative and quantitative studies in this dissertation compare SSI with other similar IRs. Last, it identifies the family of VRRSs, which are all CFGs with different virtual register naming conventions. Many extant IRs are classified as VRRSs. Several new IRs are presented, based on a consideration of previously unspecified members of the VRRS family. General analyses can operate on any family member. The required level of accuracy or efficiency can be selected by working in terms of the appropriate family member.}}
@ARTICLE{Campbell_2009,title={Amortised Memory Analysis Using the Depth of Data Structures},year={2009},author={B. K. Campbell and Brian Campbell},doi={10.1007/978-3-642-00590-9_14},pmid={null},pmcid={null},mag_id={2194223632},journal={null},abstract={Hofmann and Jost have presented a heap space analysis [1] that finds linear space bounds for many functional programs. It uses an amortised analysis: assigning hypothetical amounts of free space (called potential) to data structures in proportion to their sizes using type annotations. Constraints on these annotations in the type system ensure that the total potential assigned to the input is an upper bound on the total memory required to satisfy all allocations.

We describe a related system for bounding the stack space requirements which uses the depth of data structures, by expressing potential in terms of maxima as well as sums. This is achieved by adding extra structure to typing contexts (inspired by O'Hearn's bunched typing [2]) to describe the form of the bounds. We will also present the extra steps that must be taken to construct a typing during the analysis.}}
@ARTICLE{Saaty_1990,title={Multicriteria Decision Making: The Analytic Hierarchy Process: Planning, Priority Setting, Resource Allocation},year={1990},author={Thomas L. Saaty and Thomas L. Saaty},doi={null},pmid={null},pmcid={null},mag_id={2224485318},journal={null},abstract={null}}
@ARTICLE{Sinz_2010,title={A precise memory model for low-level bounded model checking},year={2010},author={Carsten Sinz and Carsten Sinz and Stephan Falke and Stephan Falke and F. Merz and Florian Merz},doi={null},pmid={null},pmcid={null},mag_id={2250988907},journal={null},abstract={Formalizing the semantics of programming languages like C or C++ for bounded model checking can be cumbersome if complete coverage of all language features is to be achieved. On the other hand, low-level languages that occur during translation (compilation) have a much simpler semantics since they are closer to the machine level. It thus makes sense to use these low-level languages for bounded model checking. In this paper we present a highly precise memory model suitable for bounded model checking of such low-level languages. Our method also takes memory protection (malloc/free) into account.}}
@ARTICLE{Hartmanns_2012,title={Modelling and decentralised runtime control of self-stabilising power micro grids},year={2012},author={Arnd Hartmanns and Arnd Hartmanns and Holger Hermanns and Holger Hermanns},doi={10.1007/978-3-642-34026-0_31},pmid={null},pmcid={null},mag_id={2266703950},journal={null},abstract={Electric power production infrastructures around the globe are shifting from centralised, controllable production to decentralised structures based on distributed microgeneration. As the share of renewable energy sources such as wind and solar power increases, electric power production becomes subject to unpredictable and significant fluctuations. This paper reports on formal behavioural models of future power grids with a substantial share of renewable, especially photovoltaic, microgeneration. We give a broad overview of the various system aspects of interest and the corresponding challenges in finding suitable abstractions and developing formal models. We focus on current developments within the German power grid, where enormous growth rates of microgeneration start to induce stability problems of a new kind. We build formal models to investigate runtime control algorithms for photovoltaic microgenerators in terms of grid stability, dependability and fairness. We compare the currently implemented and proposed runtime control strategies to a set of approaches that take up and combine ideas from randomised distributed algorithms widely used in communication protocols today. Our models are specified in Modest, an expressive modelling language for stochastic timed systems with a well-defined semantics. Current tool support for Modest allows the evaluation of the models using simulation as well as model-checking techniques.}}
@ARTICLE{Noubir_1998,title={Research: Signature-based method for run-time fault detection in communication protocols 1},year={1998},author={Guevara Noubir and G. Noubir and Guevara Noubir and K. Vijayananda and K. Vijayananda and Henri J. Nussbaumer and Henri J. Nussbaumer and H. J. Nussbaumer},doi={10.1016/s0140-3664(98)00121-2},pmid={null},pmcid={null},mag_id={2277160631},journal={Computer Communications},abstract={Run-time fault detection of communication protocols is essential because of faults that occur in the form of coding defects, memory problems and external disturbances. We propose a signature-based method to detect run-time faults. A polynomial using the state and event information as coefficients is used to transform a sequence of states and events into a number (signature). The static signature corresponding to the correct execution of the protocol is compared with the run-time signature. This technique is reliable, fast, and efficient compared to the existing techniques. The states and events are assigned values such that multiple paths leading to the same state result in a unique signature. This reduces the number of run-time comparisons required to verify the correct execution of the protocol. Fault-detection based on signatures is also much simpler than observer-based methods. We propose extensions to communication protocols that facilitate the application of signature-based techniques to detect run-time faults in communication protocols. In this paper, we present eXTP4, an extended transport layer protocol that facilitates run-time fault detection.}}
@ARTICLE{Lin_2000,title={Generating Real-Time Software Test Cases by Time Petri Nets},year={2000},author={Ja‐Chen Lin and Jin-Cherng Lin and Ian Ho and I. Ho},doi={10.1080/1206212x.2000.11441618},pmid={null},pmcid={null},mag_id={2278521946},journal={International Journal of Computers and Applications},abstract={Although real-time software must satisfy not only functional correctness requirements but also timeliness requirements, much recent real-time research has focused on analysis rather than testing. During the execution of real-time software, a sequence of events due to time take place between the concurrent processes. Because concurrent processes are unpredictable, multiple executions of a given real-time software with the same test cases may produce different results. This nondeterministic execution behaviour always creates problems in real-time software testing. This paper explores real-time software testing, combining decomposition of Petri nets model with expansion of its time criterion. It provides guidelines for handling the nondeterministic properties of real-time systems using time Petri nets model, rules for decomposing the Petri nets model into independent segment groups, and a description of the way test cases are generated using independent segment group and time variation method.}}
@ARTICLE{Ananian_2001,title={The static single information form},year={2001},author={C. Scott Ananian and C. Scott Ananian},doi={null},pmid={null},pmcid={null},mag_id={2284787681},journal={null},abstract={null}}
@ARTICLE{Alfaro_2001,title={Interface automata},year={2001},author={Luca de Alfaro and Luca de Alfaro and Thomas A. Henzinger and Thomas A. Henzinger},doi={10.1145/503209.503226},pmid={null},pmcid={null},mag_id={2293287097},journal={null},abstract={Conventional type systems specify interfaces in terms of values and domains. We present a light-weight formalism that captures the temporal aspects of software component interfaces. Specifically, we use an automata-based language to capture both input assumptions about the order in which the methods of a component are called, and output guarantees about the order in which the component calls external methods. The formalism supports automatic compatability checks between interface models, and thus constitutes a type system for component interaction. Unlike traditional uses of automata, our formalism is based on an optimistic approach to composition, and on an alternating approach to design refinement. According to the optimistic approach, two components are compatible if there is some environment that can make them work together. According to the alternating approach, one interface refines another if it has weaker input assumptions, and stronger output guarantees. We show that these notions have game-theoretic foundations that lead to efficient algorithms for checking compatibility and refinement.}}
@ARTICLE{Darvas_2009,title={Reasoning about data abstraction in contract languages},year={2009},author={ÃdÃ¡m PÃ©ter Darvas and ÃdÃ¡m PÃ©ter Darvas},doi={10.3929/ethz-a-005916525},pmid={null},pmcid={null},mag_id={2294943587},journal={null},abstract={null}}
@ARTICLE{Cameron_2007,title={Multiple ownership},year={2007},author={Nicholas Cameron and Nicholas Cameron and Sophia Drossopoulou and Sophia Drossopoulou and James Noble and James Noble and Matthew J. Smith and Matthew J. Smith},doi={10.1145/1297027.1297060},pmid={null},pmcid={null},mag_id={2295118110},journal={null},abstract={Existing ownership type systems require objects to have precisely one primary owner, organizing the heap into an ownership tree. Unfortunately, a tree structure is too restrictive for many programs, and prevents many common design patterns where multiple objects interact.   Multiple Ownership is an ownership type system where objects can have more than one owner, and the resulting ownership structure forms a DAG. We give a straightforward model for multiple ownership, focusing in particular on how multiple ownership can support a powerful effects system that determines when two computations interfere-in spite of the DAG structure.   We present a core programming language MOJO, Multiple ownership for Java-like Objects, including a type and effects system, and soundness proof. In comparison to other systems, MOJO imposes absolutely no restrictions on pointers, modifications or programs' structure, but in spite of this, MOJO's effects can be used to reason about or describe programs' behaviour.}}
@ARTICLE{Bleiholder_2009,title={Data fusion},year={2009},author={Jens Bleiholder and Jens Bleiholder and Felix Naumann and Felix Naumann},doi={10.1145/1456650.1456651},pmid={null},pmcid={null},mag_id={2295240344},journal={ACM Computing Surveys},abstract={The development of the Internet in recent years has made it possible and useful to access many different information systems anywhere in the world to obtain information. While there is much research on the integration of heterogeneous information systems, most commercial systems stop short of the actual integration of available data. Data fusion is the process of fusing multiple records representing the same real-world object into a single, consistent, and clean representation.   This article places data fusion into the greater context of data integration, precisely defines the goals of data fusion, namely, complete, concise, and consistent data, and highlights the challenges of data fusion, namely, uncertain and conflicting data values. We give an overview and classification of different ways of fusing data and present several techniques based on standard and advanced operators of the relational algebra and SQL. Finally, the article features a comprehensive survey of data integration systems from academia and industry, showing if and how data fusion is performed in each.}}
@ARTICLE{Cousot_1996,title={Abstract interpretation},year={1996},author={Patrick Cousot and Patrick Cousot},doi={10.1145/234528.234740},pmid={null},pmcid={null},mag_id={2295244175},journal={ACM Computing Surveys},abstract={article Free Access Share on Abstract interpretation Author: Patrick Cousot E´cole Normale Supe´rieure, Paris E´cole Normale Supe´rieure, ParisView Profile Authors Info & Claims ACM Computing SurveysVolume 28Issue 2June 1996 pp 324–328https://doi.org/10.1145/234528.234740Online:01 June 1996Publication History 113citation2,715DownloadsMetricsTotal Citations113Total Downloads2,715Last 12 Months407Last 6 weeks64 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF}}
@ARTICLE{Harel_2000,title={Dynamic Logic},year={2000},author={David Harel and David Harel and Dexter Kozen and Dexter Kozen and Jerzy Tiuryn and Jerzy Tiuryn},doi={null},pmid={null},pmcid={null},mag_id={2336123333},journal={null},abstract={From the Publisher:
Among the many approaches to formal reasoning about programs, Dynamic Logic enjoys the singular advantage of being strongly related to classical logic. Its variants constitute natural generalizations and extensions of classical formalisms. For example, Propositional Dynamic Logic (PDL) can be described as a blend of three complementary classical ingredients: propositional calculus, modal logic, and the algebra of regular events. In First-Order Dynamic Logic (DL), the propositional calculus is replaced by classical first-order predicate calculus. Dynamic Logic is a system of remarkable unity that is theoretically rich as well as of practical value. It can be used for formalizing correctness specifications and proving rigorously that those specifications are met by a particular program. Other uses include determining the equivalence of programs, comparing the expressive power of various programming constructs, and synthesizing programs from specifications. 
This book provides the first comprehensive introduction to Dynamic Logic. It is divided into three parts. The first part reviews the appropriate fundamental concepts of logic and computability theory and can stand alone as an introduction to these topics. The second part discusses PDL and its variants, and the third part discusses DL and its variants. Examples are provided throughout, and exercises and a short historical section are included at the end of each chapter.}}
@ARTICLE{Iec_2001,title={Enhancements to LOTOS (E-LOTOS)},year={2001},author={Iso Iec and Iso Iec},doi={null},pmid={null},pmcid={null},mag_id={2338767730},journal={null},abstract={null}}
@ARTICLE{Baeten_1990,title={Process algebra},year={1990},author={J.C.M. Baeten and J. C. M. Baeten and Jos C. M. Baeten and J. C. M. Baeten and W. P. Weijland and W. P. Weijland and W. P. Weijland and W. P. Weijland},doi={null},pmid={null},pmcid={null},mag_id={2340102009},journal={null},abstract={null}}
@ARTICLE{Clarke_1999,title={Model Checking},year={1999},author={Edmund M. Clarke and Edmund M. Clarke and Bernd-Holger Schlingloff and Bernd-Holger Schlingloff},doi={null},pmid={null},pmcid={null},mag_id={2340735175},journal={null},abstract={null}}
@ARTICLE{Emerson_1990,title={Temporal and Modal Logic.},year={1990},author={E. Allen Emerson and E. Allen Emerson},doi={null},pmid={null},pmcid={null},mag_id={2396555304},journal={null},abstract={Publisher Summary   This chapter discusses temporal and modal logic. The chapter describes a multiaxis classification of systems of temporal logic. The chapter describes the framework of linear temporal logic. In both its propositional and first-order forms, linear temporal logic has been widely employed in the specification and verification of programs. The chapter describes the competing framework of branching temporal logic, which has seen wide use. It also explains how temporal logic structures can be used to model concurrent programs using non-determinism and fairness. The chapter also discusses other modal and temporal logics in computer science. The chapter describes the formal syntax and semantics of Propositional Linear Temporal Logic (PLTL). The chapter also describes the formal syntax and semantics for two representative systems of propositional branching-time temporal logics.}}
@ARTICLE{Vernadat_2007,title={State Space Abstractions for Time Petri Nets.},year={2007},author={François Vernadat and François Vernadat and Bernard Berthomieu and Bernard Berthomieu and Bernard Berthomieu},doi={null},pmid={null},pmcid={null},mag_id={2405127419},journal={null},abstract={null}}
@ARTICLE{Popova-Zeugmann_2013,title={Time and Petri Nets},year={2013},author={Louchka Popova-Zeugmann and Louchka Popova-Zeugmann},doi={null},pmid={null},pmcid={null},mag_id={2405417647},journal={null},abstract={At first glance the concepts of time and of Petri nets are quite contrary: while time determines the occurrences of events in a system, classic Petri nets consider their causal relationships and they represent events as concurrent systems. But if we take a closer look at how time and causality are intertwined we realize that there are many possible ways in which time and Petri nets interact. This book takes a closer look at three time-dependent Petri nets: Time Petri nets, Timed Petri nets, and Petri nets with time windows. The author first explains classic Petri nets and their fundamental properties. Then the pivotal contribution of the book is the introduction of different algorithms that allow us to analyze time-dependent Petri nets. For Time Petri nets, the author presents an algorithm that proves the behavioral equivalence of a net where time is designed once with real and once with natural numbers, so we can reduce the state space and consider the integer states exclusively. For Timed Petri nets, the author introduces two time-dependent state equations, providing a sufficient condition for the non-reachability of states, and she also defines a local transformation for converting these nets into Time Petri nets. Finally, she shows that Petri nets with time-windows have the ability to realize every transition sequence fired in the net omitting time restrictions. These classes of time-dependent Petri nets show that time alone does not change the power of a Petri net, in fact time may or may not be used to force firing. For Time Petri nets and Timed Petri nets we can say that they are Turing-powerful, and thus more powerful than classic Petri nets, because there is a compulsion to fire at some point in time. By contrast, Petri nets with time-windows have no compulsion to fire, their expressiveness power is less than that of Turing-machines. This book derives from advanced lectures, and the text is supported throughout withexamples and exercises. It issuitable for graduate courses in computer science, mathematics, engineering, and related disciplines, and as a reference for researchers.}}
@ARTICLE{Chow_1995,title={Testing software design modeled by finite-state machines},year={1995},author={Tsun S. Chow and Tsun S. Chow},doi={null},pmid={null},pmcid={null},mag_id={2411746780},journal={null},abstract={We propose a method of testing the correctness of control structures that can be modeled by a finite-state machine. Test results derived from the design are evaluated against the specification. No "executable" prototype is required. The method is based on a result in automata theory and can be applied to software testing. Its error-detecting capability is compared with that of other approaches. Application experience is summarized.}}
@ARTICLE{iguez_2009,title={a general testability theory},year={2009},author={Ismael Rodr ´ iguez},doi={null},pmid={null},pmcid={null},mag_id={2414332752},journal={null},abstract={We present a general framework allowing to classify testing problems into five testability classes. Classes differ in the number of tests we must apply to precisely determine whether the system is correct or not. The conditions that enable/disable finite testability are analyzed. A general method to reduce a testing problem into another is presented. The complexity of finding complete test suites and measuring the suit- ability of incomplete suites is analyzed.}}
@ARTICLE{Andreae_2006,title={Scoped Types and Aspects for Real-Time Java},year={2006},author={Chris Andreae and Yvonne Coady and Celina Gibbs and James Noble and Jan Vitek and Tian Zhao},doi={null},pmid={null},pmcid={null},mag_id={2465844729},journal={Lecture Notes in Computer Science},abstract={Real-time systems are notoriously difficult to design and implement, and, as many real-time problems are safety-critical, their solutions must be reliable as well as efficient and correct. While higher-level programming models (such as the Real-Time Specification for Java) permit real-time programmers to use language features that most programmers take for granted (objects, type checking, dynamic dispatch, and memory safety) the compromises required for real-time execution, especially concerning memory allocation, can create as many problems as they solve. This paper presents Scoped Types and Aspects for Real-Time Systems (STARS) a novel programming model for real-time systems. Scoped Types give programmers a clear model of their programs' memory use, and, being statically checkable, prevent the run-time memory errors that bedevil models such as RTSJ. Our Aspects build on Scoped Types guarantees so that Real-Time concerns can be completely separated from applications' base code. Adopting the integrated Scoped Types and Aspects approach can significantly improve both the quality and performance of a real-time Java systems, resulting in simpler systems that are reliable, efficient, and correct.}}
@ARTICLE{Grogono_2000,title={Copying and comparing : Problems and solutions},year={2000},author={Peter Grogono and Markku Sakkinen},doi={null},pmid={null},pmcid={null},mag_id={2466884690},journal={Lecture Notes in Computer Science},abstract={In object oriented programming, it is sometimes necessary to copy objects and to compare them for equality or inequality. We discuss some of the issues involved in copying and comparing objects and we address the problem of generating appropriate copying and comparing operations automatically, a service that is not provided by most object oriented languages and environments. Automatic generation appears to be not only desirable, because hand-coding these methods is mechanical and yet error-prone, but also feasible, because the form of the code is simple and largely predictable. Some languages and some object models presented in the literature do support generic copying and comparing, typically defining separate shallow and deep versions of both operations. A close examination of these definitions reveals inadequacies. If the objects involved are simple, copying and comparing them is straightforward. However, there are at least three areas in which insufficient attention has been given to copying and comparing complex objects: (1) values are not distinguished from objects; (2) aggregation is not distinguished from association; and (3) the correct handling of linked structures other than trees is neglected. Solving the third problem requires a mechanism built into the language, such as exists in Eiffel. Building such a mechanism without modifying the language requires a language with sufficient reflexive facilities, such as Smalltalk. Even then, the task is difficult and the result is likely to be insecure. We show that fully automatic generation of copying and comparing operations is not feasible because compilers and other software tools have access only to the structure of the objects and not to their semantics. Nevertheless, it is possible to provide default methods that do most of the work correctly and can be fine-tuned with a small additional amount of hand-coding. We include an example that illustrates the application of our proposals to C++. It is based on additional declarations handled by a preprocessor.}}
@ARTICLE{Pohl_2005,title={Software Product Line Engineering},year={2005},author={Klaus Pohl and Klaus Pohl and Günter Böckle and Günter Böckle and Frank van der Linden and Frank van der Linden and Frank van der Linden},doi={10.1007/3-540-28901-1},pmid={null},pmcid={null},mag_id={2477378326},journal={null},abstract={I. Software Product Line Engineering Are you interested in producing software products or software-intensive systems at lower costs, in shorter time, and with higher quality? If so, you are holding th}}
@ARTICLE{Lopez-Herrejon_2005,title={Evaluating support for features in advanced modularization technologies},year={2005},author={Roberto E. Lopez-Herrejon and Don Batory and William R. Cook},doi={null},pmid={null},pmcid={null},mag_id={2479247030},journal={Lecture Notes in Computer Science},abstract={A software product-line is a family of related programs. Each program is defined by a unique combination of features, where a feature is an increment in program functionality. Modularizing features is difficult, as feature-specific code often cuts across class boundaries. New modularization technologies have been proposed in recent years, but their support for feature modules has not been thoroughly examined. In this paper, we propose a variant of the expression problem as a canonical problem in product-line design. The problem reveals a set of technology-independent properties that feature modules should exhibit. We use these properties to evaluate five technologies: AspectJ, Hyper/J, Jiazzi, Scala, and AHEAD. The results suggest an abstract model of feature composition that is technology-independent and that relates compositional reasoning with algebraic reasoning 1 .}}
@ARTICLE{Rau_2012,title={Strategic decision making},year={2012},author={Devaki Rau and Devaki Rau and Philip Bromiley and Philip Bromiley},doi={10.1036/1097-8542.yb120303},pmid={null},pmcid={null},mag_id={2480928464},journal={Access Science},abstract={Strategic decisions are decisions that critically influence the performance and survival of a firm. …}}
@ARTICLE{Cheon_2002,title={A simple and practical approach to unit testing: The JML and JUnit way},year={2002},author={Yoonsik Cheon and Gary T. Leavens},doi={null},pmid={null},pmcid={null},mag_id={2483314354},journal={Lecture Notes in Computer Science},abstract={Writing unit test code is labor-intensive, hence it is often not done as an integral part of programming. However, unit testing is a practical approach to increasing the correctness and quality of software; for example, the Extreme Programming approach relies on frequent unit testing. In this paper we present a new approach that makes writing unit tests easier. It uses a formal specification language's runtime assertion checker to decide whether methods are working correctly, thus automating the writing of unit test oracles. These oracles can be easily combined with hand-written test data. Instead of writing testing code, the programmer writes formal specifications (e.g., pre- and postconditions). This makes the programmer's task easier, because specifications are more concise and abstract than the equivalent test code, and hence more readable and maintainable. Furthermore, by using specifications in testing, specification errors are quickly discovered, so the specifications are more likely to provide useful documentation and inputs to other tools. We have implemented this idea using the Java Modeling Language (JML) and the JUnit testing framework, but the approach could be easily implemented with other combinations of formal specification languages and unit test tools.}}
@ARTICLE{Barkaoui_2006,title={Theoretical Aspects of Computing - ICTAC 2006},year={2006},author={Kamel Barkaoui and Kamel Barkaoui and Ana Cavalcanti and Ana Cavalcanti and António Cerone and Antonio Cerone and Antonio Cerone},doi={10.1007/11921240},pmid={null},pmcid={null},mag_id={2486211245},journal={null},abstract={null}}
@ARTICLE{Yovine_2004,title={Formal Techniques, Modelling and Analysis of Timed and Fault-Tolerant Systems},year={2004},author={Sergio Yovine and Yassine Lakhnech and Yassine Lakhnech and Sergio Yovine},doi={10.1007/b100824},pmid={null},pmcid={null},mag_id={2486801597},journal={null},abstract={null}}
@ARTICLE{Börger_2003,title={Abstract State Machines 2003},year={2003},author={Egon Börger and Egon Börger and Angelo Gargantini and Angelo Gargantini and Elvinia Riccobene and Elvinia Riccobene},doi={10.1007/3-540-36498-6},pmid={null},pmcid={null},mag_id={2488443646},journal={null},abstract={null}}
@ARTICLE{Alur_1996,title={Hybrid Systems III},year={1996},author={Rajeev Alur and Rajeev Alur and Thomas A. Henzinger and Thomas A. Henzinger and Thomas A. Henzinger and Eduardo D. Sontag and Eduardo D. Sontag},doi={10.1007/bfb0020931},pmid={null},pmcid={null},mag_id={2489001267},journal={null},abstract={This reference book documents the scientific outcome of the DIMACS/SYCON Workshop on Verification and Control of Hybrid Systems, held at Rutgers University in New Brunswick, NJ, in October 1995. A hyb}}
@ARTICLE{Vargas_1990,title={Prediction, Projection and Forecasting},year={1990},author={Luis G. Vargas and Luis G. Vargas and Thomas L. Saaty and Thomas L. Saaty},doi={null},pmid={null},pmcid={null},mag_id={2489602167},journal={null},abstract={null}}
@ARTICLE{Clark_2002,title={Object Modeling with the OCL},year={2002},author={Tony Clark and Tony Clark and Jos Warmer and Jos Warmer},doi={10.1007/3-540-45669-4},pmid={null},pmcid={null},mag_id={2489869394},journal={null},abstract={null}}
@ARTICLE{Vilela_2012,title={Testing for security vulnerabilities in software},year={2012},author={Plínio Vilela and Marco Machado and Eric Wong},doi={null},pmid={null},pmcid={null},mag_id={2498157584},journal={null},abstract={null}}
@ARTICLE{Fitzgerald_2005,title={FM 2005: Formal Methods},year={2005},author={John Fitzgerald and Ian J. Hayes and Andrzej Tarlecki},doi={10.1007/b27882},pmid={null},pmcid={null},mag_id={2501184301},journal={null},abstract={null}}
@ARTICLE{Gaudel_1996,title={FME'96: Industrial Benefit and Advances in Formal Methods},year={1996},author={Marie-Claude Gaudel and Jim Woodcock},doi={10.1007/3-540-60973-3},pmid={null},pmcid={null},mag_id={2501364339},journal={null},abstract={null}}
@ARTICLE{Barnett_2006,title={Boogie: a modular reusable verifier for object-oriented programs},year={2006},author={Mike Barnett and Mike Barnett and Bor-Yuh Evan Chang and Bor-Yuh Evan Chang and Robert DeLine and Robert DeLine and Bart Jacobs and Bart Jacobs and K. Rustan M. Leino and K. Rustan M. Leino},doi={null},pmid={null},pmcid={null},mag_id={2504100651},journal={Lecture Notes in Computer Science},abstract={A program verifier is a complex system that uses compiler technology, program semantics, property inference, verification-condition generation, automatic decision procedures, and a user interface. This paper describes the architecture of a state-of-the-art program verifier for object-oriented programs.}}
@ARTICLE{Butler_2011,title={FM 2011: Formal Methods},year={2011},author={Michael Butler and Michael Butler and Wolfram Schulte and Wolfram Schulte},doi={10.1007/978-3-642-21437-0},pmid={null},pmcid={null},mag_id={2504209588},journal={null},abstract={null}}
@ARTICLE{Misra_2006,title={FM 2006: Formal Methods},year={2006},author={Jayadev Misra and Jayadev Misra and Tim McComb and Tobias Nipkow and Tobias Nipkow and Graeme Smith and Emil Sekerinski and Emil Sekerinski and Emil Sekerinski},doi={10.1007/11813040},pmid={null},pmcid={null},mag_id={2504260463},journal={Lecture Notes in Computer Science},abstract={Object-Z allows coupling constraints between classes which, on the one hand, facilitate specification at a high level of abstraction, but, on the other hand, make class refinement non-compositional. The consequence of this is that refinement is not practical for large Systems. This paper overcomes this limitation by introducing a methodology for compositional class refinement in Object-Z. The key step is an equivalence transformation of an arbitrary Object-Z specification to one in which introduced constraints prohibit non-compositional refinements. The methodology also allows the constraints which couple classes to be refined yielding an unrestricted approach to compositional class refinement.}}
@ARTICLE{Dyer_2007,title={Distributed Embedded Systems - Validation Strategies},year={2007},author={Matthias Dyer and Matthias Dyer},doi={null},pmid={null},pmcid={null},mag_id={2514149753},journal={null},abstract={null}}
@ARTICLE{Lecomte_2007,title={Formal Methods in Safety-Critical Railway Systems},year={2007},author={Thierry Lecomte and thierry lecomte and Thierry Lecomte and thierry servat and thierry servat},doi={null},pmid={null},pmcid={null},mag_id={2542168174},journal={null},abstract={In this article we would like to present some recent applications of the B formal
method to the development of safety critical systems, namely platform screen door
controllers. These SIL3/SIL41 compliant systems have their functional specification based
on a formal model. This model has been proved, guaranteeing a correct by construction
behaviour of the system in absence of failure of its components. The constructive process
used during system specification and design leads to a high quality system which has been
qualified2 by French authorities.}}
@ARTICLE{Boehm_1993,title={Software engineering economics},year={1993},author={Barry Boehm and Barry Boehm},doi={null},pmid={null},pmcid={null},mag_id={2551072204},journal={null},abstract={This paper summarizes the current state of the art and recent trends in software engineering economics. It provides an overview of economic analysis techniques and their applicability to software engineering and management. It surveys the field of software cost estimation, including the major estimation techniques available, the state of the art in algorithmic cost models, and the outstanding research issues in software cost estimation.}}
@ARTICLE{Frühwirth_2009,title={Constraint Handling Rules},year={2009},author={Thom Frühwirth and Thom Frühwirth},doi={null},pmid={null},pmcid={null},mag_id={2570710533},journal={null},abstract={Constraint Handling Rules (CHR) is both a theoretical formalism based on logic and a practical programming language based on rules. This book, written by the creator of CHR, describes the theory of CHR and how to use it in practice. It is supported by a website containing teaching materials, online demos, and free downloads of the language. After a basic tutorial, the author describes in detail the CHR language and discusses guaranteed properties of CHR programs. The author then compares CHR with other formalisms and languages and illustrates how it can capture their essential features. Finally, larger programs are introduced and analyzed in detail. The book is ideal for graduate students and lecturers, and for more experienced programmers and researchers, who can use it for self-study. Exercises with selected solutions, and bibliographic remarks are included at the ends of chapters. The book is the definitive reference on the subject.}}
@ARTICLE{Meyers_1964,title={H = W},year={1964},author={Norman G. Meyers and Norman Meyers and James Serrin and James Serrin},doi={10.1073/pnas.51.6.1055},pmid={null},pmcid={null},mag_id={2575211436},journal={Proceedings of the National Academy of Sciences of the United States of America},abstract={null}}
@ARTICLE{Tretmans_2006,title={Model Based Testing with Labelled Transition Systems},year={2006},author={Jan Tretmans and Jan Tretmans},doi={null},pmid={null},pmcid={null},mag_id={2578987896},journal={CTIT technical report series},abstract={Model based testing is one of the promising technologies to meet the challenges imposed on software testing. In model based testing an implementation under test is tested for compliance with a model that describes the required behaviour of the implementation. This tutorial chapter describes a model based testing theory where models are expressed as labelled transition systems, and compliance is defined with the 'ioco' implementation relation. The ioco-testing theory, on the one hand, provides a sound and well-defined foundation for labelled transition system testing, having its roots in the theoretical area of testing equivalences and refusal testing. On the other hand, it has proved to be a practical basis for several model based test generation tools and applications. Definitions, underlying assumptions, an algorithm, properties, and several examples of the ioco-testing theory are discussed, involving specifications, implementations, tests, the ioco implementation relation and some of its variants, a test generation algorithm, and the soundness and exhaustiveness of this algorithm.}}
@ARTICLE{Jeannet_2005,title={Symbolic test selection based on approximate analysis},year={2005},author={Bertrand Jeannet and Thierry Jéron and Vlad Rusu and Elena Zinovieva},doi={null},pmid={null},pmcid={null},mag_id={2583392285},journal={Lecture Notes in Computer Science},abstract={This paper addresses the problem of generating symbolic test cases for testing the conformance of a black-box implementation with respect to a specification, in the context of reactive systems. The challenge we consider is the selection of test cases according to a test purpose, which is here a set of scenarios of interest that one wants to observe during test execution. Because of the interactions that occur between the test case and the implementation, test execution can be seen as a game involving two players, in which the test case attempts to satisfy the test purpose. Efficient solutions to this problem have been proposed in the context of finite-state models, based on the use of fixpoint computations. We extend them in the context of infinite-state symbolic models, by showing how approximate fixpoint computations can be used in a conservative way. The second contribution we provide is the formalization of a quality criterium for test cases, and a result relating the quality of a generated test case to the approximations used in the selection algorithm.}}
@ARTICLE{Barnett_2005,title={The Spec# Programming System: An overview},year={2005},author={Mike Barnett and K. Rustan M. Leino and Wolfram Schulte},doi={null},pmid={null},pmcid={null},mag_id={2583442040},journal={Lecture Notes in Computer Science},abstract={The Spec# programming system is a new attempt at a more cost effective way to develop and maintain high-quality software. This paper describes the goals and architecture of the Spec# programming system, consisting of the object-oriented Spec# programming language, the Spec# compiler, and the Boogie static program verifier. The language includes constructs for writing specifications that capture programmer intentions about how methods and data are to be used, the compiler emits run-time checks to enforce these specifications, and the verifier can check the consistency between a program and its specifications.}}
@ARTICLE{Núñez_2006,title={Conformance Testing Relations for Timed Systems},year={2006},author={Manuel Núñez and Ismael Rodríguez},doi={null},pmid={null},pmcid={null},mag_id={2583842542},journal={Lecture Notes in Computer Science},abstract={This paper presents a formal framework to test both the functional and temporal behaviors in systems where temporal aspects are critical. Different implementation relations, depending on both the interpretation of time and on the (non-)determinism of specifications and/or implementations, are presented and related. We also study how tests cases are defined and applied to implementations. A test derivation algorithm, producing sound and complete test suites, is presented.}}
@ARTICLE{Saaty_1990,title={How to make a decision: The analytic hierarchy process},year={1990},author={Thomas L. Saaty and Thomas L. Saaty},doi={10.1016/0377-2217(90)90057-i},pmid={null},pmcid={null},mag_id={2585206336},journal={European Journal of Operational Research},abstract={null}}
@ARTICLE{Barnett_2004,title={Towards a tool environment for model-based testing with AsmL},year={2004},author={Mike Barnett and Wolfgang Grieskamp and Lev Nachmanson and Wolfram Schulte and Nikolai Tillmann and Margus Veanes},doi={null},pmid={null},pmcid={null},mag_id={2585234173},journal={Lecture Notes in Computer Science},abstract={We present work on a tool environment for model-based testing with the Abstract State Machine Language (AsmL). Our environment supports semiautomatic parameter generation, call sequence generation and conformance testing. We outline the usage of the environment by an example, discuss its underlying technologies, and report on some applications conducted in the Microsoft environment.}}
@ARTICLE{Khoumsi_2004,title={Test cases generation for nondeterministic real-time systems},year={2004},author={Ahmed Khoumsi and Thierry Jéron and Hervé Marchand},doi={null},pmid={null},pmcid={null},mag_id={2585323532},journal={Lecture Notes in Computer Science},abstract={We study the generation of test cases for nondeterministic real-time systems. We define a class of Determinizable Timed Automata (DTA), in order to specify the system under test. The principle of our test method consists of two steps. In Step 1, we express the problem in a non-real-time form, by transforming a DTA into an equivalent finite state automaton. The latter uses two additional types of events, Set and Exp. In Step 2, we adapt a non-real-time test generation method.}}
@ARTICLE{Simon_2006,title={Coinductive logic programming},year={2006},author={Luke Simon and Ajay Mallya and Ajay Bansal and Gopal Gupta},doi={null},pmid={null},pmcid={null},mag_id={2585877797},journal={Lecture Notes in Computer Science},abstract={We extend logic programming's semantics with the semantic dual of traditional Herbrand semantics by using greatest fixed-points in place of least fixed-points. Executing a logic program then involves using coinduction to check inclusion in the greatest fixed-point. The resulting coinductive logic programming language is syntactically identical to, yet semantically subsumes logic programming with rational terms and lazy evaluation. We present a novel formal operational semantics that is based on synthesizing a coinductive hypothesis for this coinductive logic programming language. We prove that this new operational semantics is equivalent to the declarative semantics. Our operational semantics lends itself to an elegant and efficient goal directed proof search in the presence of rational terms and proofs. We describe a prototype implementation of this operational semantics along with applications of coinductive logic programming.}}
@ARTICLE{Barthe_2006,title={MOBIUS: mobility, ubiquity, security objectives and progress report},year={2006},author={Gilles Barthe and Gilles Barthe and Lennart Beringer and Lennart Beringer and Pierre Crégut and Pierre Crégut and Benjamin Grégoire and Benjamin Grégoire and Martin Hofmann and Martin Hofmann and Péter Müller and Peter Müller and Erik Poll and Erik Poll and Germán Puebla and Germán Puebla and Ian Stark and Ian Stark and Éric Vétillard and Eric Vétillard},doi={10.1007/978-3-540-75336-0_2},pmid={null},pmcid={null},mag_id={2598222867},journal={null},abstract={Through their global, uniform provision of services and their distributed nature, global computers have the potential to profoundly enhance our daily life. However, they will not realize their full potential, unless the necessary levels of trust and security can be guaranteed.

The goal of the MOBIUS project is to develop a Proof Carrying Code architecture to secure global computers that consist of Java-enabled mobile devices. In this progress report, we detail its objectives and provide a snapshot of the project results during its first year of activity.}}
@ARTICLE{Erdogan_2004,title={IEEE Computer Society 2004 Annual Symposium on VLSI (ISVLSI'04)},year={2004},author={A.T. Erdogan and Ahmet T. Erdogan and Tughrul Arslan and Tughrul Arslan},doi={null},pmid={null},pmcid={null},mag_id={2599079467},journal={null},abstract={null}}
@ARTICLE{Schmitt_2010,title={Dynamic Frames in Java Dynamic Logic. Formalisation and Proofs},year={2010},author={Peter H. Schmitt and Peter H. Schmitt and Mattias Ulbrich and Mattias Ulbrich and Benjamin Weiß and Benjamin Weiß},doi={10.5445/ir/1000018332},pmid={null},pmcid={null},mag_id={2612370131},journal={null},abstract={This report is a companion to the paper Dynamic Frames in Java Dynamic Logic [2]. It contains complementary formal definitions and proofs.}}
@ARTICLE{Petrenko_2008,title={Testing of Software and Communicating Systems},year={2008},author={Alexandre Petrenko and Alexandre Petrenko and Margus Veanes and Margus Veanes and Jan Tretmans and Jan Tretmans and Wolfgang Grieskamp and Wolfgang Grieskamp},doi={null},pmid={null},pmcid={null},mag_id={2623130623},journal={null},abstract={null}}
@ARTICLE{Tindell_1994,title={Calculating controller area network (can) message response times},year={1994},author={K. Tindell and K. Tindell and Alan Burns and Alan Burns and Andy Wellings and Andy Wellings},doi={10.1016/0967-0661(95)00112-8},pmid={null},pmcid={null},mag_id={2726481301},journal={IFAC Proceedings Volumes},abstract={null}}
@ARTICLE{Langerak_1992,title={Bundle event stuctures: a non-interleaving semantics for LOTOS},year={1992},author={Rom Langerak and Romanus Langerak},doi={null},pmid={null},pmcid={null},mag_id={2736386426},journal={null},abstract={null}}
@ARTICLE{Bijl_2004,title={Compositional based testing with ioco},year={2004},author={H.M. van der Bijl and H.M. van der Bijl and Arend Rensink and Arend Rensink and Arend Rensink and G.J. Tretmans and G.J. Tretmans},doi={null},pmid={null},pmcid={null},mag_id={2737364229},journal={null},abstract={Compositional testing concerns the testing of systems that consist of communicating components which can also be tested in isolation. Examples are component based testing and interoperability testing. We show that, with certain restrictions, the ioco-test theory for conformance testing is suitable for compositional testing, in the sense that the integration of fully conformant components is guaranteed to be correct. As a consequence, there is no need to re-test the integrated system for conformance.This result is also relevant for testing in context, since it implies that every failure of a system embedded in a test context can be reduced to a fault of the system itself.}}
@ARTICLE{Katoen_1999,title={Concepts, Algorithms, and Tools for Model Checking},year={1999},author={Joost P. Katoen and Joost P. Katoen and Joost P. Katoen},doi={null},pmid={null},pmcid={null},mag_id={2742644929},journal={null},abstract={null}}
@ARTICLE{Canal_2006,title={Synchronizing Behavioural Mismatch in Software Composition},year={2006},author={Carlos Canal and Carlos Canal and Pascal Poizat and Pascal Poizat and Gwen Salaün and Gwen Salaün},doi={null},pmid={null},pmcid={null},mag_id={2751348097},journal={Lecture Notes in Computer Science},abstract={Software Adaptation is a crucial issue for the development of a real market of components promoting software reuse. Recent work in this field has addressed several problems related to interface and behavioural mismatch. In this paper, we present our proposal for software adaptation, which builds on previous work overcoming some of its limitations, and makes a significant advance to solve pending issues. Our approach is based on the use of synchronous vectors and regular expressions for governing adaptation rules, and is supported by dedicated algorithms and tools.}}
@ARTICLE{Kerr_2008,title={Best Paper Prize},year={2008},author={Debra Kerr and Anne-Maree Kelly and Paul Dietze and Damien Jolley and Bill Barger and Kerry Leigh},doi={null},pmid={null},pmcid={null},mag_id={2756589053},journal={null},abstract={null}}
@ARTICLE{Bertrand_2012,title={Off-line test selection with test purposes for non-deterministic timed automata},year={2012},author={Nathalie Bertrand and Nathalie Bertrand and Thierry Jéron and Thierry Jéron and Amélie Staïner and Amélie Stainer and Moez Krichen and Moez Krichen and Moez Krichen},doi={10.2168/lmcs-8(4:8)2012},pmid={null},pmcid={null},mag_id={2762058236},journal={Logical Methods in Computer Science},abstract={This article proposes novel off-line test generation techniques from
non-deterministic timed automata with inputs and outputs (TAIOs) in the formal
framework of the tioco conformance theory. In this context, a fi?rst problem is
the determinization of TAIOs, which is necessary to foresee next enabled
actions after an observable trace, but is in general impossible because not all
timed automata are determinizable. This problem is solved thanks to an
approximate determinization using a game approach. The algorithm performs an
io-abstraction which preserves the tioco conformance relation and thus
guarantees the soundness of generated test cases. A second problem is the
selection of test cases from a TAIO speci?fication. The selection here relies
on a precise description of timed behaviors to be tested which is carried out
by expressive test purposes modeled by a generalization of TAIOs. Finally, an
algorithm is described which generates test cases in the form of TAIOs equipped
with verdicts, using a symbolic co-reachability analysis guided by the test
purpose. Properties of test cases are then analyzed with respect to the
precision of the approximate determinization: when determinization is exact,
which is the case on known determinizable classes, in addition to soundness,
properties characterizing the adequacy of test cases verdicts are also
guaranteed.}}
@ARTICLE{Ancona_2009,title={An idealized coinductive type system for imperative object-oriented programs.},year={2009},author={Davide Ancona and Davide Ancona and Giovanni Lagorio and Giovanni Lagorio},doi={null},pmid={null},pmcid={null},mag_id={2773178245},journal={null},abstract={In recent work we have proposed a novel approach to define idealized type systems for object-oriented languages, based on ab- stract compilation of programs into Horn formulas which are interpreted w.r.t. the coinductive (that is, the greatest) Herbrand model. In this paper we investigate how this approach can be applied also in the presence of imperative features. This is made possible by considering a natural translation of Static Single Assignment intermediate form programs into Horn formulas, where ϕ functions correspond to union types.}}
@ARTICLE{Sun_2014,title={Design Automation and Test in Europe (DATE)},year={2014},author={Luo Sun and Luo Sun and Jimson Mathew and Jimson Mathew and Rishad Shafik and Rishad A. Shafik and Dhiraj K. Pradhan and Dhiraj K. Pradhan},doi={null},pmid={null},pmcid={null},mag_id={2788481468},journal={null},abstract={null}}
@ARTICLE{Wadler_2009,title={Programming Languages and Systems},year={2009},author={Philip Wadler and Philip Wadler and Robert Bruce Findler and Robert Bruce Findler and Robert Bruce Findler},doi={10.1007/978-3-642-00590-9_1},pmid={null},pmcid={null},mag_id={2796564118},journal={null},abstract={On behalf of the organizing committee I would like to welcome you all to the second Asian Symposium on Programming Languages and Systems (APLAS 2004) held in Taipei on November 4–6, 2004. Since the ye}}
@ARTICLE{Humphrys_2006,title={Beyond Words},year={2006},author={John Humphrys and John Humphrys},doi={null},pmid={null},pmcid={null},mag_id={2798397604},journal={null},abstract={null}}
@ARTICLE{Lacomme_2001,title={Air and Spaceborne Radar Systems: An Introduction},year={2001},author={Philippe Lacomme and Philippe Lacomme and Philippe Lacomme and Eric Normant and Eric Normant and Jean Philippe Hardange and Jean-Philippe Hardange and Jean Philippe Hardange and Jean Claude MArcahis and Jean Claude MArcahis},doi={null},pmid={null},pmcid={null},mag_id={2798864447},journal={null},abstract={null}}
@ARTICLE{Garavel_2013,title={CADP 2011: a toolbox for the construction and analysis of distributed processes},year={2013},author={Hubert Garavel and Hubert Garavel and Frédéric Lang and Frédéric Lang and Radu Mateescu and Radu Mateescu and Wendelin Serwe and Wendelin Serwe and Wendelin Serwe and Wendelin Serwe},doi={10.1007/s10009-012-0244-z},pmid={null},pmcid={null},mag_id={2810491896},journal={International Journal on Software Tools for Technology Transfer},abstract={CADP (Construction and Analysis of Distributed Processes) is a comprehensive software toolbox that implements the results of concurrency theory. Started in the mid 80s, CADP has been continuously developed by adding new tools and enhancing existing ones. Today, CADP benefits from a worldwide user community, both in academia and industry. This paper presents the latest release, CADP 2011, which is the result of a considerable development effort spanning the last five years. The paper first describes the theoretical principles and the modular architecture of CADP, which has inspired several other recent model checkers. The paper then reviews the main features of CADP 2011, including compilers for various formal specification languages, equivalence checkers, model checkers, compositional verification tools, performance evaluation tools, and parallel verification tools running on clusters and grids. Finally, the paper surveys some significant case studies.}}
@ARTICLE{Brinksma_2001,title={testing transition systems an annotated bibliography},year={2001},author={Ed Brinksma and Jan Tretmans},doi={null},pmid={null},pmcid={null},mag_id={2895830905},journal={Lecture Notes in Computer Science},abstract={Labelled transition system based test theory has made remarkable progress over the past 15 years. From a theoretically interesting approach to the semantics of reactive systems it has developed into a field where testing theory is (slowly) narrowing the gap with testing practice. In particular, new test generation algorithms are being designed that can be used in realistic situations whilst maintaining a sound theoretical basis. In this paper we present an annotated bibliography of labelled transition system based test theory and its applications covering the main developments.}}
@ARTICLE{Alur_1999,title={Timed Automata},year={1999},author={Rajeev Alur and Rajeev Alur},doi={null},pmid={null},pmcid={null},mag_id={2911583164},journal={null},abstract={Model checking is emerging as a practical tool for automated debugging of complex reactive systems such as embedded controllers and network protocols (see [23] for a survey). Traditional techniques for model checking do not admit an explicit modeling of time, and are thus, unsuitable for analysis of real-time systems whose correctness depends on relative magnitudes of different delays. Consequently, timed automata [7] were introduced as a formal notation to model the behavior of real-time systems. Its definition provides a simple way to annotate state-transition graphs with timing constraints using finitely many real-valued clock variables. Automated analysis of timed automata relies on the construction of a finite quotient of the infinite space of clock valuations. Over the years, the formalism has been extensively studied leading to many results establishing connections to circuits and logic, and much progress has been made in developing verification algorithms, heuristics, and tools. This paper provides a survey of the theory of timed automata, and their role in specification and verification of real-time systems.}}
@ARTICLE{Mehlhorn_2011,title={Data structures},year={2011},author={Kurt Mehlhorn and K. Mehlhorn and Kurt Mehlhorn and K. Mehlhorn and A. Tsakalidis and Athanasios K. Tsakalidis},doi={null},pmid={null},pmcid={null},mag_id={2912714641},journal={null},abstract={null}}
@ARTICLE{Clarke_1996,title={Model checking},year={1996},author={Edmund M. Clarke and Edmund M. Clarke and Orna Grumberg and Orna Grumberg and David E. Long and D. Long},doi={null},pmid={null},pmcid={null},mag_id={2913459036},journal={null},abstract={null}}
@ARTICLE{Acree_1980,title={On mutation},year={1980},author={Jr. Allen Troy Acree and Jr. Allen Troy Acree},doi={null},pmid={null},pmcid={null},mag_id={2914201442},journal={null},abstract={Program mutation has been advanced by DeMillo et. al. as a powerful program testing technique. The present work is an empirical study of several factors important in the feasibility and reliability of mutation analysis. In the course of investigating the coupling effect, over a million complex mutants were subjected to test data sufficient for first order mutation. All but 45 of these were either eliminated by the test data or proved to be equivalent to the original programs. Most of the 45 could be addressed within the framework of mutation analysis by the addition of a new mutagenic operator. An experiment on the human evaluation of equivalence (of mutants to original programs) produced inconclusive results, but indicates the need for caution in the practice of mutation analysis in the step of judging mutants equivalent. A study of mutagenic operator efficiencies indicates that mutation analysis can be performed on compiled programs, rather than being limited to interpretive execution.}}
@ARTICLE{White_1997,title={Mobile agents},year={1997},author={J. E. White and James E. White},doi={null},pmid={null},pmcid={null},mag_id={2915003108},journal={null},abstract={null}}
@ARTICLE{Beyer_2009,title={CPAchecker: A Tool for Configurable Software Verification},year={2009},author={Dirk Beyer and Dirk Beyer and M. Erkan Keremoğlu and M. Erkan Keremoglu},doi={null},pmid={null},pmcid={null},mag_id={2949280820},journal={arXiv: Programming Languages},abstract={Configurable software verification is a recent concept for expressing different program analysis and model checking approaches in one single formalism. This paper presents CPAchecker, a tool and framework that aims at easy integration of new verification components. Every abstract domain, together with the corresponding operations, is required to implement the interface of configurable program analysis (CPA). The main algorithm is configurable to perform a reachability analysis on arbitrary combinations of existing CPAs. The major design goal during the development was to provide a framework for developers that is flexible and easy to extend. We hope that researchers find it convenient and productive to implement new verification ideas and algorithms using this platform and that it advances the field by making it easier to perform practical experiments. The tool is implemented in Java and runs as command-line tool or as Eclipse plug-in. We evaluate the efficiency of our tool on benchmarks from the software model checker BLAST. The first released version of CPAchecker implements CPAs for predicate abstraction, octagon, and explicit-value domains. Binaries and the source code of CPAchecker are publicly available as free software.}}
@ARTICLE{Budd_1978,title={the design of a prototype mutation system for program testing},year={1978},author={Timothy A. Budd and Richard J. Lipton and Richard A. DeMillo and Frederick G. Sayward},doi={null},pmid={null},pmcid={null},mag_id={2988947034},journal={null},abstract={null}}
@ARTICLE{undefined_2009,title={a verification system for distributed objects with asynchronous method calls},year={2009},author={},doi={10.1007/978-3-642-10373-5_20},pmid={null},pmcid={null},mag_id={3014153483},journal={null},abstract={We present a verification system for Creol, an object-oriented modeling language for concurrent distributed applications. The system is an instance of KeY, a framework for object oriented software verification, which has so far been applied foremost to sequential Java. Building on KeY characteristic concepts, like dynamic logic, sequent calculus, explicit substitutions, and the taclet rule language, the system presented in this paper addresses functional correctness of Creol models featuring local cooperative thread parallelism and global communication via asynchronous method calls. The calculus heavily operates on communication histories which describe the interfaces of Creol units. Two example scenarios demonstrate the usage of the system.}}
@ARTICLE{Wirsing_2008,title={Leveraging Applications of Formal Methods, Verification and Validation},year={2008},author={Martin Wirsing and Martin Wirsing and M. Hölzl and Matthias Hölzl and Lucia Acciai and Lucia Acciai and Federico Banti and Federico Banti and Allan Clark and Allan Clark and Alessandro Fantechi and Alessandro Fantechi and Stephen Gilmore and Stephen Gilmore and Stefania Gnesi and Stefania Gnesi and László Gönczy and László Gönczy and Nora Koch and Nora Koch and Alessandro Lapadula and Alessandro Lapadula and Philip Mayer and Philip Mayer and Franco Mazzanti and Franco Mazzanti and Rosario Pugliese and Rosario Pugliese and Andreas Schröeder and Andreas Schroeder and Andreas Schroeder and Francesco Tiezzi and Francesco Tiezzi and Mirco Tribastone and Mirco Tribastone and Dániel Varró and Dániel Varró},doi={10.1007/978-3-540-88479-8_13},pmid={null},pmcid={null},mag_id={3016163793},journal={null},abstract={null}}
@ARTICLE{Hoare_1972,title={Proof of correctness of data representations},year={1972},author={C. A. R. Hoare and C. A. R. Hoare},doi={10.1007/bf00289507},pmid={null},pmcid={null},mag_id={3023216518},journal={Acta Informatica},abstract={A powerful method of simplifying the proofs of program correctness is suggested; and some new light is shed on the problem of functions with side-effects.}}
@ARTICLE{Alur_2008,title={First-Order and Temporal Logics for Nested Words},year={2008},author={Rajeev Alur and Marcelo Arenas and Pablo Barceló and Kousha Etessami and Neil Immerman and Leonid Libkin},doi={10.2168/lmcs-4(4:11)2008},pmid={null},pmcid={null},mag_id={3100906898},journal={Logical Methods in Computer Science},abstract={Nested words are a structured model of execution paths in procedural
programs, reflecting their call and return nesting structure. Finite nested
words also capture the structure of parse trees and other tree-structured data,
such as XML. We provide new temporal logics for finite and infinite nested
words, which are natural extensions of LTL, and prove that these logics are
first-order expressively-complete. One of them is based on adding a "within"
modality, evaluating a formula on a subword, to a logic CaRet previously
studied in the context of verifying properties of recursive state machines
(RSMs). The other logic, NWTL, is based on the notion of a summary path that
uses both the linear and nesting structures. For NWTL we show that
satisfiability is EXPTIME-complete, and that model-checking can be done in time
polynomial in the size of the RSM model and exponential in the size of the NWTL
formula (and is also EXPTIME-complete). Finally, we prove that first-order
logic over nested words has the three-variable property, and we present a
temporal logic for nested words which is complete for the two-variable fragment
of first-order.}}
@ARTICLE{Bertrand_2012,title={Off-line test selection with test purposes for non-deterministic timed automata},year={2012},author={Nathalie Bertrand and Thierry Jéron and Amélie Stainer and Moez Krichen},doi={10.2168/lmcs-8(4:8)2012},pmid={null},pmcid={null},mag_id={3101366255},journal={Logical Methods in Computer Science},abstract={This article proposes novel off-line test generation techniques from
non-deterministic timed automata with inputs and outputs (TAIOs) in the formal
framework of the tioco conformance theory. In this context, a fi?rst problem is
the determinization of TAIOs, which is necessary to foresee next enabled
actions after an observable trace, but is in general impossible because not all
timed automata are determinizable. This problem is solved thanks to an
approximate determinization using a game approach. The algorithm performs an
io-abstraction which preserves the tioco conformance relation and thus
guarantees the soundness of generated test cases. A second problem is the
selection of test cases from a TAIO speci?fication. The selection here relies
on a precise description of timed behaviors to be tested which is carried out
by expressive test purposes modeled by a generalization of TAIOs. Finally, an
algorithm is described which generates test cases in the form of TAIOs equipped
with verdicts, using a symbolic co-reachability analysis guided by the test
purpose. Properties of test cases are then analyzed with respect to the
precision of the approximate determinization: when determinization is exact,
which is the case on known determinizable classes, in addition to soundness,
properties characterizing the adequacy of test cases verdicts are also
guaranteed.}}
@ARTICLE{Bernardo_2004,title={formal methods for the design of real time systems},year={2004},author={Marco Bernardo and Flavio Corradini},doi={10.1007/b110123},pmid={null},pmcid={null},mag_id={3108818942},journal={null},abstract={null}}
@ARTICLE{Budd_1978,title={the design of a prototype mutation system for program testing},year={1978},author={Timothy A. Budd and Richard J. Lipton and Richard A. DeMillo and Frederick G. Sayward},doi={null},pmid={null},pmcid={null},mag_id={3120364084},journal={null},abstract={When testing software the major question which must always be addressed is "If a program is correct for a finite number of test cases, can we assume it is correct in general. " Test data which possess this property is called Adequate test data, and, although adequate test data cannot in general be derived algorithmically, 1 several methods have recently emerged which allow one to gain confidence in one's test data's adequacy. Program mutation is a radically new approach to determining test data adequacy which holds promise of being a major breakthrough in the field of software testing. The concepts and philosophy of program mutation have been given elsewhere, 2 the following will merely present a brief introduction to the ideas underlying the system. Unlike previous work, program mutation assumes that competent programmers will produce programs which, if they are not correct, are "almost" correct. That is, if a program is not correct it is a "mutant"-it differs from a correct program by simple errors. Assuming this natural premise, a program P which is correct on test data T is subjected to a series of mutant operators to produce mutant programs which differ from P in very simple ways. The mutants are then executed on T. If all mutants give incorrect results then it is very likely that P is correct (i.e., T is adequate). On the other hand, if some mutants are correct on T then either: (1) the mutants are equivalent to P, or (2) the test data T is inadequate. In the latter case, T must be augmented by examining the non-equivalent mutants which are correct on T: a procedure which forces close examination of P with respect to the mutants. At first glance it would appear that if T is determined adequate by mutation analysis, then P might still contain some complex errors which are not explicitly mutants of P.}}
@ARTICLE{Hubert_2010,title={Sawja: Static Analysis Workshop for Java},year={2010},author={Laurent Hubert and Laurent Hubert and Nicolas Barré and Nicolas Barré and Frédéric Besson and Frédéric Besson and Delphine Demange and Delphine Demange and Thomas Jensen and Thomas Jensen and Vincent Monfort and Vincent Monfort and David Pichardie and David Pichardie and Tiphaine Turpin and Tiphaine Turpin},doi={10.1007/978-3-642-18070-5_7},pmid={null},pmcid={null},mag_id={3121650164},journal={arXiv: Programming Languages},abstract={Static analysis is a powerful technique for automatic verification of programs but raises major engineering challenges when developing a full-fledged analyzer for a realistic language such as Java. This paper describes the Sawja library: a static analysis framework fully compliant with Java 6 which provides OCaml modules for efficiently manipulating Java bytecode programs. We present the main features of the library, including (i) efficient functional data-structures for representing program with implicit sharing and lazy parsing, (ii) an intermediate stack-less representation, and (iii) fast computation and manipulation of complete programs.}}
@ARTICLE{Buss_1983,title={How to rank computer projects.},year={1983},author={M D Buss},doi={null},pmid={null},pmcid={null},mag_id={3140461596},journal={Harvard Business Review},abstract={When it comes to deciding which project proposals should get the nod, top executives, information systems managers, and users often have conflicting views. None of these should make the choice alone, says this author. With the IS manager as coordinator, users and top executives can contribute to an eight-step process that will reconcile differing perspectives and permit an orderly ranking of projects. Such a structured approach helps managers to make a more effective use of IS resources because it includes other elements relative to the priority-setting process, rather than just those that are purely financial.}}
@ARTICLE{Kastner_2007,title={A Case Study Implementing Features Using AspectJ},year={2007},author={Kastner and Kastner and Apel and Apel and Batory and Batory},doi={10.1109/spline.2007.12},pmid={null},pmcid={null},mag_id={3140738358},journal={null},abstract={Software product lines aim to create highly configurable programs from a set of features. Common belief and recent studies suggest that aspects are well-suited for implementing features. We evaluate the suitability of AspectJ with respect to this task by a case study that refactors the embedded database system Berkeley DB into 38 features. Contrary to our initial expectations, the results were not encouraging. As the number of aspects in a feature grows, there is a noticeable decrease in code readability and maintainability. Most of the unique and powerful features of AspectJ were not needed. We document where AspectJ is unsuitable for implementing features of refactored legacy applications and explain why.}}
@ARTICLE{Arapinis_2010,title={Computer Security Foundations Symposium (CSF), 2010 23rd IEEE},year={2010},author={Myrto Arapinis and Myrto Arapinis and T. Chothia and Tom Chothia and Eike Ritter and Eike Ritter and Mark Ryan and Mark Ryan},doi={10.1109/csf.2010.15},pmid={null},pmcid={null},mag_id={3142572285},journal={IEEE Computer Society},abstract={null}}
@ARTICLE{Bjørk_2010,title={Lightweight Time Modeling in Timed Creol},year={2010},author={Joakim Bjørk and Einar Broch Johnsen and Olaf Owe and Rudolf Schlatte},doi={10.4204/eptcs.36.4},pmid={null},pmcid={null},mag_id={3144525418},journal={null},abstract={Creol is an object-oriented modeling language in which inherently concurrent objects exchange asynchronous method calls. The operational semantics of Creol is written in an actor-based style, formulated in rewriting logic. The operational semantics yields a language interpreter in the Maude system, which can be used to analyze models. Recently, Creol has been applied to the modeling of systems with radio communication, such as sensor systems. With radio communication, messages expire and, if sent simultaneously, they may collide in the air. In order to capture these and other properties of distributed systems, we extended Creol’s operational semantics with a notion of time. We exploit the framework of a language interpreter to use a lightweight notion of time, in contrast to that needed for a general purpose specification language. This paper presents a timed extension of Creol, including the semantics and the implementation strategy, and discusses its properties using an extended example. The approach can be generalized to other concurrent object or actor-based systems.}}
@ARTICLE{Hoare_1971,title={Proof of a Program: FIND},year={1971},author={Tony Hoare and Tony Hoare},doi={null},pmid={null},pmcid={null},mag_id={3145693741},journal={Communications of The ACM},abstract={null}}
@ARTICLE{Shafik_2012,title={2012 IEEE COMPUTER SOCIETY ANNUAL SYMPOSIUM ON VLSI (ISVLSI)},year={2012},author={Rishad Shafik and Rishad A. Shafik and Bashir M. Al-Hashimi and Bashir M. Al-Hashimi and Jimson Mathew and Jimson Mathew and Dhiraj K. Pradhan and Dhiraj K. Pradhan and Saraju P. Mohanty and Saraju P. Mohanty and Saraju P. Mohanty and Saraju P. Mohanty and Saraju P. Mohanty},doi={10.1109/isvlsi.2012.42},pmid={null},pmcid={null},mag_id={3148267157},journal={null},abstract={null}}
@ARTICLE{Garavel_2009,title={Verification of an industrial SystemC/TLM model using LOTOS and CADP},year={2009},author={Garavel and Garavel and Claude Helmstetter and Helmstetter and Ponsini and Ponsini and Serwe and Serwe},doi={10.1109/memcod.2009.5185377},pmid={null},pmcid={null},mag_id={3149451953},journal={null},abstract={SystemC/TLM is a widely used standard for system level descriptions of complex architectures. It is particularly useful for fast simulation, thus allowing early development and testing of the targeted software. In general, formal verification of SystemC/TLM relies on the translation of the complete model into a language accepted by a verification tool. In this paper, we present an approach to the validation of a SystemC/TLM description by translation into LOTOS, reusing as much as possible of the original SystemC/TLM C++ code. To this end, we exploit a feature offered by the formal verification toolbox CADP, namely the import of external C code in a LOTOS model. We report on experiments of our approach on the BDisp, a complex graphical processing unit designed by STMicroelectronics.}}
@ARTICLE{Budd_1978,title={The design of a prototype mutation system for program testing.},year={1978},author={Timothy A. Budd and Timothy A. Budd and Richard J. Lipton and Richard J. Lipton and Richard A. DeMillo and Richard A. DeMillo and Frederick G. Sayward and Frederick G. Sayward},doi={null},pmid={null},pmcid={null},mag_id={3157610735},journal={null},abstract={When testing software the major question which must always be addressed is "If a program is correct for a finite number of test cases, can we assume it is correct in general. " Test data which possess this property is called Adequate test data, and, although adequate test data cannot in general be derived algorithmically, 1 several methods have recently emerged which allow one to gain confidence in one's test data's adequacy. Program mutation is a radically new approach to determining test data adequacy which holds promise of being a major breakthrough in the field of software testing. The concepts and philosophy of program mutation have been given elsewhere, 2 the following will merely present a brief introduction to the ideas underlying the system. Unlike previous work, program mutation assumes that competent programmers will produce programs which, if they are not correct, are "almost" correct. That is, if a program is not correct it is a "mutant"-it differs from a correct program by simple errors. Assuming this natural premise, a program P which is correct on test data T is subjected to a series of mutant operators to produce mutant programs which differ from P in very simple ways. The mutants are then executed on T. If all mutants give incorrect results then it is very likely that P is correct (i.e., T is adequate). On the other hand, if some mutants are correct on T then either: (1) the mutants are equivalent to P, or (2) the test data T is inadequate. In the latter case, T must be augmented by examining the non-equivalent mutants which are correct on T: a procedure which forces close examination of P with respect to the mutants. At first glance it would appear that if T is determined adequate by mutation analysis, then P might still contain some complex errors which are not explicitly mutants of P.}}
@ARTICLE{Ludewig_2003,title={Models in software engineering – an introduction},year={2003},author={Jochen Ludewig and Jochen Ludewig},doi={10.1007/s10270-003-0020-3},pmid={null},pmcid={null},mag_id={3163904326},journal={Software and Systems Modeling},abstract={Modelling is a concept fundamental for software engineering. In this paper, the word is defined and discussed from various perspectives. The most important types of models are presented, and examples are given. Models are very useful, but sometimes also dangerous, in particular to those who use them unconsciously. Such problems are shown. Finally, the role of models in software engineering research is discussed.}}
@ARTICLE{McCarthy_1962,title={Towards a Mathematical Science of Computation.},year={1962},author={John McCarthy and John J. McCarthy},doi={null},pmid={null},pmcid={null},mag_id={3181441684},journal={null},abstract={In this paper I shall discuss the prospects for a mathematical science of computation. In a mathematical science, it is possible to deduce from the basic assumptions, the important properties of the entities treated by the science. Thus, from Newton’s law of gravitation and his laws of motion, one can deduce that the planetary orbits obey Kepler’s laws.}}
@ARTICLE{Hennessy_2007,title={A Distributed Pi-Calculus},year={2007},author={Matthew Hennessy and Matthew Hennessy},doi={10.1017/cbo9780511611063},pmid={null},pmcid={null},mag_id={4212790255},journal={null},abstract={Distributed systems are fast becoming the norm in computer science. Formal mathematical models and theories of distributed behaviour are needed in order to understand them. This book proposes a distributed pi-calculus called Dpi, for describing the behaviour of mobile agents in a distributed world. It is based on an existing formal language, the pi-calculus, to which it adds a network layer and a primitive migration construct. A mathematical theory of the behaviour of these distributed systems is developed, in which the presence of types plays a major role. It is also shown how in principle this theory can be used to develop verification techniques for guaranteeing the behavior of distributed agents. The text is accessible to computer scientists with a minimal background in discrete mathematics. It contains an elementary account of the pi-calculus, and the associated theory of bisimulations. It also develops the type theory required by Dpi from first principles.}}
@ARTICLE{Baresi_null,title={Tutorial Introduction to Graph Transformation: A Software Engineering Perspective},year={null},author={Luciano Baresi and Luciano Baresi and Reiko Heckel and Reiko Heckel},doi={10.1007/978-3-540-30203-2_30},pmid={null},pmcid={null},mag_id={4231307995},journal={Lecture Notes in Computer Science},abstract={As a basic introduction to graph transformation, this tutorial is not only intended for software engineers. But applications typical to this domain, like the modeling of component-based, distributed, and mobile systems, model-based testing, and diagram languages provide well-known examples and are therefore used to give a survey of the motivations, concepts, applications, and tools of graph transformation.}}
@ARTICLE{Chakrabarti_null,title={Resource Interfaces},year={null},author={Arindam Chakrabarti and Arindam Chakrabarti and Luca de Alfaro and Luca de Alfaro and Thomas A. Henzinger and Thomas A. Henzinger and Mariëlle Stoelinga and Mariëlle Stoelinga},doi={10.1007/978-3-540-45212-6_9},pmid={null},pmcid={null},mag_id={4236251966},journal={Lecture Notes in Computer Science},abstract={AbstractWe present a formalism for specifying component interfaces that expose component requirements on limited resources. The formalism permits an algorithmic check if two or more components, when put together, exceed the available resources. Moreover, the formalism can be used to compute the quantity of resources necessary for satisfying the requirements of a collection of components. The formalism can be instantiated in several ways. For example, several components may draw power from the same source. Then, the formalism supports compatibility checks such as: can two components, when put together, achieve their tasks without ever exceeding the available amount of peak power? or, can they achieve their tasks by using no more than the initially available amount of energy (i.e., power accumulated over time)? The corresponding quantitative questions that our algorithms answer are the following: what is the amount of peak power needed for two components to be put together? what is the corresponding amount of initial energy? To solve these questions, we model interfaces with resource requirements as games with quantitative objectives. The games are played on state spaces where each state is labeled by a number (representing, e.g., power consumption), and a play produces an infinite path of labels. The objective may be, for example, to minimize the largest label that occurs during a play. We illustrate our approach by modeling compatibility questions for the components of robot control software, and of wireless sensor networks.KeywordsEnergy InterfaceWinning StrategyState LabelCompliant StateInput AssumptionThese keywords were added by machine and not by the authors. This process is experimental and the keywords may be updated as the learning algorithm improves.}}
@ARTICLE{Godefroid_null,title={DART},year={null},author={Patrice Godefroid and Patrice Godefroid and Nils Klarlund and Nils Klarlund and Koushik Sen and Koushik Sen},doi={10.1145/1065010.1065036},pmid={null},pmcid={null},mag_id={4237492309},journal={null},abstract={We present a new tool, named DART, for automatically testing software that combines three main techniques: (1) automated extraction of the interface of a program with its external environment using static source-code parsing; (2) automatic generation of a test driver for this interface that performs random testing to simulate the most general environment the program can operate in; and (3) dynamic analysis of how the program behaves under random testing and automatic generation of new test inputs to direct systematically the execution along alternative program paths. Together, these three techniques constitute Directed Automated Random Testing, or DART for short. The main strength of DART is thus that testing can be performed completely automatically on any program that compiles -- there is no need to write any test driver or harness code. During testing, DART detects standard errors such as program crashes, assertion violations, and non-termination. Preliminary experiments to unit test several examples of C programs are very encouraging.}}
@ARTICLE{Raz_null,title={Semantic anomaly detection in online data sources},year={null},author={Orna Raz and Orna Raz and Philip Koopman and Philip Koopman and Mary Shaw and Mary Shaw},doi={10.1145/581376.581378},pmid={null},pmcid={null},mag_id={4238123730},journal={null},abstract={null}}
@ARTICLE{Merayo_null,title={Formal Testing of Systems Presenting Soft and Hard Deadlines},year={null},author={Mercedes G. Merayo and Mercedes G. Merayo and Manuel Núñez and Manuel Núñez and Ismael Rodrı́guez and Ismael Rodriguez},doi={10.1007/978-3-540-75698-9_11},pmid={null},pmcid={null},mag_id={4244693874},journal={null},abstract={We present a formal framework to specify and test systems presenting both soft and hard deadlines. While hard deadlines must be always met on time, soft deadlines can be sometimes met in a different time, usually higher, from the specified one. It is this characteristic (to formally define sometimes) what produces several reasonable alternatives to define appropriate implementation relations, that is, relations to decide wether an implementation is correct with respect to a specification. In addition to introduce these relations, we define a testing framework to test implementations.KeywordsState MachineTest SuiteFinite State MachineLabel Transition SystemProcess AlgebraThese keywords were added by machine and not by the authors. This process is experimental and the keywords may be updated as the learning algorithm improves.}}
@ARTICLE{Schäfer_null,title={JCoBox: Generalizing Active Objects to Concurrent Components},year={null},author={Jan Schäfer and Jan Schäfer and Arnd Poetzsch-Heffter and Arnd Poetzsch-Heffter},doi={10.1007/978-3-642-14107-2_13},pmid={null},pmcid={null},mag_id={4251600813},journal={Lecture Notes in Computer Science},abstract={Concurrency in object-oriented languages is still waiting for a satisfactory solution. For many application areas, standard mechanisms like threads and locks are too low level and have shown to be error-prone and not modular enough. Lately the actor paradigm has regained attention as a possible solution to concurrency in OOLs.We propose JCoBox: a Java extension with an actor-like concurrency model based on the notion of concurrently running object groups, so-called coboxes. Communication is based on asynchronous method calls with standard objects as targets. Cooperative multi-tasking within coboxes allows for combining active and reactive behavior in a simple and safe way. Futures and promises lead to a data-driven synchronization of tasks.This paper describes the concurrency model, the formal semantics, and the implementation of JCoBox, and shows that the performance of the implementation is comparable to state-of-the-art actor-based language implementations for the JVM.KeywordsActive ObjectStandard ObjectDynamic SemanticConcurrency ModelThread PoolThese keywords were added by machine and not by the authors. This process is experimental and the keywords may be updated as the learning algorithm improves.}}
@ARTICLE{David_null,title={Model-Based Framework for Schedulability Analysis Using UPPAAL 4.1},year={null},author={Albert David and Alexandre David and Jacob Illum and Jacob Illum and Kim Guldstrand Larsen and Kim Larsen and Arne Skou and Arne Skou},doi={10.1201/9781420067859-c4},pmid={null},pmcid={null},mag_id={4252976664},journal={Computational analysis, synthesis, and design of dynamic models series},abstract={null}}
@ARTICLE{Agha_null,title={Actors},year={null},author={Gul Agha and Gul Agha},doi={10.7551/mitpress/1086.001.0001},pmid={null},pmcid={null},mag_id={4256363426},journal={null},abstract={The transition from sequential to parallel computation is an area of critical concern in today's computer technology, particularly in architecture, programming languages, systems, and artificial intelligence. This book addresses central issues in concurrency, and by producing both a syntactic definition and a denotational model of Hewitt's actor paradigm—a model of computation specifically aimed at constructing and analyzing distributed large-scale parallel systems—it substantially advances the understanding of parallel computation. Contents Introduction • General Design Decisions • Computation in ACTOR Systems • A More Expressive Language • A Model for ACTOR Systems • Concurrency Issues • Abstraction and Compositionality • Conclusions}}
